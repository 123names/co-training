{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One classifier each name: OCEN (Name group)\n",
    "This method throw away the record below threshold.\n",
    "\n",
    "Also this file evaluate different embedding method with OCEN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T02:46:41.152704Z",
     "start_time": "2019-01-20T02:46:41.101946Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import com_func\n",
    "\n",
    "# parameters\n",
    "#----- threshold for selecting set of name group -----------#\n",
    "threshold_select_name_group = 100\n",
    "#----- threshold for selecting min sample in name group ----#\n",
    "threshold_lower = 10\n",
    "threshold_upper = 110\n",
    "\n",
    "pp_textual = [\"tf\", \"tf_idf\", \"lsa\", \"pv_dm\", \"pv_dbow\"]\n",
    "\n",
    "Dataset = \"pubmed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T02:47:02.633362Z",
     "start_time": "2019-01-20T02:47:02.156876Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gensim\n",
    "# read trained rec to rec textual graph\n",
    "def read_textual_embedding(Dataset = \"pubmed\", emb_type = \"off\"):\n",
    "    textual_emb = []\n",
    "    emb_pid = []\n",
    "    while True:\n",
    "        if emb_type == \"tf\":\n",
    "            modelSaveDir = \"../../Data/\"+Dataset+\"/models/tf/textual_sample=140k/\"\n",
    "            with open(modelSaveDir+'tf_features.pickle', \"rb\") as input_file:\n",
    "                vec = pickle.load(input_file)\n",
    "            with open(modelSaveDir+'feature_pid.pickle', \"rb\") as input_file:\n",
    "                allPaperid = pickle.load(input_file)\n",
    "            textual_emb = vec.toarray()\n",
    "            emb_pid = allPaperid\n",
    "            break\n",
    "        elif emb_type == \"tf_idf\":\n",
    "            modelSaveDir = \"../../Data/\"+Dataset+\"/models/tf_idf/textual_sample=140k/\"\n",
    "            with open(modelSaveDir+'tf_idf_trained_features.pickle', \"rb\") as input_file:\n",
    "                vec = pickle.load(input_file)\n",
    "            with open(modelSaveDir+'feature_pid.pickle', \"rb\") as input_file:\n",
    "                allPaperid = pickle.load(input_file)\n",
    "            textual_emb = vec.toarray()\n",
    "            emb_pid = allPaperid\n",
    "            break\n",
    "        elif emb_type == \"lsa\":\n",
    "            modelSaveDir = \"../../Data/\"+Dataset+\"/models/lsa/textual_sample=140k/\"\n",
    "            with open(modelSaveDir+'lsa_Matrix.pickle', \"rb\") as input_file:\n",
    "                vec = pickle.load(input_file)\n",
    "            with open(modelSaveDir+'feature_pid.pickle', \"rb\") as input_file:\n",
    "                allPaperid = pickle.load(input_file)\n",
    "            textual_emb = vec\n",
    "            emb_pid = allPaperid\n",
    "            break\n",
    "        elif emb_type == \"pv_dm\":\n",
    "            modelSaveDir = \"../../Data/\"+Dataset+\"/models/doc2v/textual_sample=140k/\"\n",
    "            model = gensim.models.Doc2Vec.load(modelSaveDir+\"pv_dm/Doc2Vec(dmm,d100,n5,w5,mc2,s0.001,t24)\")\n",
    "            allPaperTags = model.docvecs.offset2doctag\n",
    "            for pid in allPaperTags:\n",
    "                vectorRepresentation = model.docvecs[pid].tolist()\n",
    "                vectorRepresentation = [float(i) for i in vectorRepresentation]\n",
    "                textual_emb.append(vectorRepresentation)\n",
    "            emb_pid = allPaperTags\n",
    "            break\n",
    "        elif emb_type == \"pv_dbow\":\n",
    "            modelSaveDir = \"../../Data/\"+Dataset+\"/models/doc2v/textual_sample=140k/\"\n",
    "            model = gensim.models.Doc2Vec.load(modelSaveDir+\"pv_dbow/Doc2Vec(dbow,d100,n5,mc2,s0.001,t24)\")\n",
    "\n",
    "            allPaperTags = model.docvecs.offset2doctag\n",
    "            for pid in allPaperTags:\n",
    "                vectorRepresentation = model.docvecs[pid].tolist()\n",
    "                vectorRepresentation = [float(i) for i in vectorRepresentation]\n",
    "                textual_emb.append(vectorRepresentation)\n",
    "            emb_pid = allPaperTags\n",
    "            break\n",
    "        elif emb_type == \"off\":\n",
    "            break\n",
    "        else:\n",
    "            print(\"Embedding type not available, selecting default setting\")\n",
    "            emb_type=\"off\"\n",
    "    print(\"Total textual vector records:\",len(textual_emb))\n",
    "    print(\"Vector dimension: \", len(textual_emb[0]))\n",
    "    return textual_emb, emb_pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T02:47:02.931348Z",
     "start_time": "2019-01-20T02:47:02.891011Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_labeled_file(infile):\n",
    "    LabeledRecords_original = []\n",
    "    with open(infile, 'r', encoding = 'utf8') as f:\n",
    "        for line in f:\n",
    "            read_data = line.split(\"\\t\")\n",
    "            # get ride of bad formated lines\n",
    "            if(len(read_data)==13 or len(read_data)==12):\n",
    "                paper_detail = {\"paperID\": read_data[0], \"authorID\":read_data[1], \n",
    "                                \"co-author\": read_data[5], \"venue_id\": read_data[7]}\n",
    "                LabeledRecords_original.append(paper_detail)\n",
    "            else:\n",
    "                print(len(read_data))\n",
    "        f.close()\n",
    "    return pd.DataFrame(LabeledRecords_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T02:47:03.550526Z",
     "start_time": "2019-01-20T02:47:03.499701Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_embedding(all_embedding, all_embedding_pid, wanted_pid_list):\n",
    "    extracted_emb = []\n",
    "    wanted_pid_list = wanted_pid_list.values.tolist()\n",
    "    # only if embedding exist\n",
    "    if len(all_embedding)>0:\n",
    "        # loop through wanted pid list to keep input order\n",
    "        for wanted_pid in wanted_pid_list:\n",
    "            # if wanted paper in all pretrained embeddings\n",
    "            if wanted_pid in all_embedding_pid:\n",
    "                emb_idx = all_embedding_pid.index(wanted_pid)\n",
    "                extracted_emb.append(all_embedding[emb_idx])\n",
    "            # if wanted paper not in all pretrained embeddings, fill missing sample with 0's\n",
    "            else:\n",
    "                print(\"Missing Sample: \", wanted_pid)\n",
    "                temp = [0] * len(all_embedding[0])\n",
    "                extracted_emb.append(temp)\n",
    "                \n",
    "    extracted_emb = pd.DataFrame(extracted_emb)\n",
    "    return extracted_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T02:47:04.237940Z",
     "start_time": "2019-01-20T02:47:04.060378Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_score, recall_score,f1_score,accuracy_score\n",
    "# cross validation\n",
    "def k_fold_cv(data, label, clf, k=10):\n",
    "    kf = KFold(n_splits=k, shuffle=False)\n",
    "    allTrueLabel = []\n",
    "    allPredLabel = []\n",
    "    for train_index, test_index in kf.split(data):\n",
    "        # print(\"TRAIN:\", train_index, \" \\n TEST:\", test_index)\n",
    "        # split train and test\n",
    "        data_train, data_test = data[train_index], data[test_index]\n",
    "        label_train, label_test = label[train_index], label[test_index]\n",
    "        # fit data to clf\n",
    "        clf.fit(data_train, label_train)\n",
    "        # get predicted label\n",
    "        label_pred = clf.predict(data_test)\n",
    "        allTrueLabel.extend(label_test)\n",
    "        allPredLabel.extend(label_pred)\n",
    "\n",
    "    accuracy = accuracy_score(allTrueLabel, allPredLabel)\n",
    "    f1 = f1_score(allTrueLabel, allPredLabel,average='macro')\n",
    "    \n",
    "    print(metrics.classification_report(allTrueLabel, allPredLabel))\n",
    "    print(metrics.confusion_matrix(allTrueLabel, allPredLabel).ravel())\n",
    "    \n",
    "    # accumulate statistic for entire model f1\n",
    "    cnf_matrix = confusion_matrix(allTrueLabel, allPredLabel)\n",
    "    TP = np.diag(cnf_matrix)\n",
    "    FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)  \n",
    "    FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "    TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "#     print(cnf_matrix)\n",
    "#     print(\"TP: \",TP, \"TN: \",TN, \"FP: \",FP,\"FN: \",FN)\n",
    "\n",
    "    return accuracy, f1, TP.sum(), TN.sum(), FP.sum(), FN.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-20T02:47:05.264Z"
    },
    "code_folding": [
     105,
     107,
     110,
     112,
     117
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load embedding:  tf\n",
      "Total textual vector records: 135796\n",
      "Vector dimension:  50000\n",
      "For name:  j_read\n",
      "j_read  pass\n",
      "For name:  f_esteves\n",
      "f_esteves  pass\n",
      "For name:  c_miller\n",
      "c_miller  pass\n",
      "For name:  r_jha\n",
      "r_jha  pass\n",
      "For name:  a_lowe\n",
      "a_lowe  pass\n",
      "For name:  a_vega\n",
      "a_vega  pass\n",
      "For name:  k_smith\n",
      "k_smith  pass\n",
      "For name:  j_gordon\n",
      "j_gordon  pass\n",
      "For name:  s_liao\n",
      "s_liao  pass\n",
      "For name:  j_qian\n",
      "j_qian  pass\n",
      "For name:  s_bernardi\n",
      "s_bernardi  pass\n",
      "For name:  t_hill\n",
      "t_hill  pass\n",
      "For name:  s_schindler\n",
      "s_schindler  pass\n",
      "For name:  j_williams\n",
      "j_williams  pass\n",
      "For name:  s_jacobson\n",
      "s_jacobson  pass\n",
      "For name:  e_andrade\n",
      "e_andrade  pass\n",
      "For name:  t_santos\n",
      "t_santos  pass\n",
      "For name:  k_kim\n",
      "Total sample size before apply threshold:  1111\n",
      "Counter({'0000-0002-6929-5359': 211, '0000-0001-9498-284X': 154, '0000-0002-5878-8895': 139, '0000-0002-1864-3392': 92, '0000-0002-7045-8004': 57, '0000-0001-7896-6751': 57, '0000-0002-7991-9428': 55, '0000-0002-4010-1063': 45, '0000-0002-2186-3484': 28, '0000-0002-4899-1929': 25, '0000-0003-0487-4242': 24, '0000-0002-3642-1486': 22, '0000-0001-9965-3535': 17, '0000-0002-4168-757X': 17, '0000-0001-6525-3744': 14, '0000-0002-3897-0278': 14, '0000-0002-1181-5112': 12, '0000-0003-1447-9385': 11, '0000-0002-7305-8786': 11, '0000-0002-2655-7806': 10, '0000-0003-3466-5353': 9, '0000-0002-7359-663X': 8, '0000-0003-4600-8668': 6, '0000-0002-1382-7088': 5, '0000-0002-9505-4882': 5, '0000-0003-3667-9900': 4, '0000-0001-9714-6038': 4, '0000-0002-4760-0228': 3, '0000-0003-4188-7915': 3, '0000-0001-9454-0427': 3, '0000-0002-0333-6808': 3, '0000-0003-2134-4964': 3, '0000-0002-6658-047X': 3, '0000-0003-1273-379X': 3, '0000-0002-7047-3183': 3, '0000-0002-1814-9546': 3, '0000-0003-4812-6297': 2, '0000-0001-6597-578X': 2, '0000-0002-5285-9138': 2, '0000-0002-6796-7844': 2, '0000-0002-1130-8698': 2, '0000-0001-8518-8150': 2, '0000-0002-7103-924X': 2, '0000-0002-5407-0202': 1, '0000-0001-6220-8411': 1, '0000-0002-7440-6703': 1, '0000-0002-1603-7559': 1, '0000-0003-0257-1707': 1, '0000-0001-8532-6517': 1, '0000-0001-6626-316X': 1, '0000-0002-3246-9861': 1, '0000-0002-7207-4389': 1, '0000-0001-9682-9654': 1, '0000-0002-0196-3832': 1, '0000-0001-8063-6081': 1, '0000-0003-2037-3333': 1, '0000-0001-8872-6751': 1})\n",
      "Total author before apply threshoid:  57\n",
      "['0000-0003-1447-9385', '0000-0001-6525-3744', '0000-0001-9498-284X', '0000-0002-3642-1486', '0000-0001-9965-3535', '0000-0002-7305-8786', '0000-0002-4899-1929', '0000-0002-6929-5359', '0000-0002-7045-8004', '0000-0002-2186-3484', '0000-0002-5878-8895', '0000-0002-4010-1063', '0000-0003-0487-4242', '0000-0001-7896-6751', '0000-0002-3897-0278', '0000-0002-7991-9428', '0000-0002-1864-3392', '0000-0002-4168-757X', '0000-0002-2655-7806', '0000-0002-1181-5112']\n",
      "Total author after apply threshoid:  20\n",
      "Total sample size after apply threshold:  1015\n",
      "(1015, 50000)\n",
      "(1015, 50000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gao137/intel/intelpython3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/gao137/intel/intelpython3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     precision    recall  f1-score   support\n",
      "\n",
      "0000-0001-6525-3744       0.00      0.00      0.00        14\n",
      "0000-0001-7896-6751       0.00      0.00      0.00        57\n",
      "0000-0001-9498-284X       0.37      0.97      0.54       154\n",
      "0000-0001-9965-3535       0.00      0.00      0.00        17\n",
      "0000-0002-1181-5112       0.00      0.00      0.00        12\n",
      "0000-0002-1864-3392       0.85      0.66      0.74        92\n",
      "0000-0002-2186-3484       0.00      0.00      0.00        28\n",
      "0000-0002-2655-7806       0.00      0.00      0.00        10\n",
      "0000-0002-3642-1486       0.00      0.00      0.00        22\n",
      "0000-0002-3897-0278       0.00      0.00      0.00        14\n",
      "0000-0002-4010-1063       0.00      0.00      0.00        45\n",
      "0000-0002-4168-757X       0.00      0.00      0.00        17\n",
      "0000-0002-4899-1929       0.00      0.00      0.00        25\n",
      "0000-0002-5878-8895       0.65      0.94      0.77       139\n",
      "0000-0002-6929-5359       0.62      1.00      0.77       211\n",
      "0000-0002-7045-8004       0.00      0.00      0.00        57\n",
      "0000-0002-7305-8786       0.00      0.00      0.00        11\n",
      "0000-0002-7991-9428       0.00      0.00      0.00        55\n",
      "0000-0003-0487-4242       0.00      0.00      0.00        24\n",
      "0000-0003-1447-9385       0.00      0.00      0.00        11\n",
      "\n",
      "          micro avg       0.54      0.54      0.54      1015\n",
      "          macro avg       0.12      0.18      0.14      1015\n",
      "       weighted avg       0.35      0.54      0.41      1015\n",
      "\n",
      "[  0   0   5   0   0   0   0   0   0   0   0   0   0   4   5   0   0   0\n",
      "   0   0   0   0  47   0   0   0   0   0   0   0   0   0   0   9   1   0\n",
      "   0   0   0   0   0   0 149   0   0   0   0   0   0   0   0   0   0   5\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0  17   0   0   0   0   0   0   0  10   0   0   0   0   0   0   0\n",
      "   0   0   0   2   0   0   0   0   0   0   0   0   0   0   0  61   0   0\n",
      "   0   0   0   0   0   1  30   0   0   0   0   0   0   0  18   0   0   0\n",
      "   0   0   0   0   0   0   0   8   2   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  10   0   0   0   0   0   0   0\n",
      "   1   0   0   9   0   0   0   0   0   0   0   0  12   0   0   0   0   0\n",
      "   0   0  10   0   0   0   0   0   0   0   0   0   0   4   0   0   0   0\n",
      "   0   0   0   0  40   0   0   0   0   0   0   0   0   0   0   5   0   0\n",
      "   0   0   0   0   0   0  12   0   0   0   0   0   0   0   0   0   0   4\n",
      "   1   0   0   0   0   0   0   0  17   0   0   0   0   0   0   0   0   0\n",
      "   0   5   3   0   0   0   0   0   0   0   8   0   0   0   0   0   0   0\n",
      "   0   0   0 131   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0 211   0   0   0   0   0   0   0  31   0   0   0\n",
      "   0   0   0   0   0   0   0  16  10   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  11   0   0   0   0   0   0   0\n",
      "  47   0   0   0   0   0   0   0   0   0   0   8   0   0   0   0   0   0\n",
      "   0   0   0   0   0   2   0   0   0   0   0   0   0   0  22   0   0   0\n",
      "   0   0   0   0   8   0   0   0   0   0   0   0   0   0   0   0   3   0\n",
      "   0   0   0   0]\n",
      "MNB Accuracy:  0.5438423645320197\n",
      "MNB F1:  0.14079550855433945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gao137/intel/intelpython3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/gao137/intel/intelpython3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     precision    recall  f1-score   support\n",
      "\n",
      "0000-0001-6525-3744       1.00      0.71      0.83        14\n",
      "0000-0001-7896-6751       0.92      0.77      0.84        57\n",
      "0000-0001-9498-284X       0.73      0.94      0.82       154\n",
      "0000-0001-9965-3535       0.00      0.00      0.00        17\n",
      "0000-0002-1181-5112       0.00      0.00      0.00        12\n",
      "0000-0002-1864-3392       0.77      0.88      0.82        92\n",
      "0000-0002-2186-3484       1.00      0.75      0.86        28\n",
      "0000-0002-2655-7806       0.00      0.00      0.00        10\n",
      "0000-0002-3642-1486       0.00      0.00      0.00        22\n",
      "0000-0002-3897-0278       0.00      0.00      0.00        14\n",
      "0000-0002-4010-1063       1.00      0.80      0.89        45\n",
      "0000-0002-4168-757X       0.00      0.00      0.00        17\n",
      "0000-0002-4899-1929       1.00      0.88      0.94        25\n",
      "0000-0002-5878-8895       0.80      0.96      0.87       139\n",
      "0000-0002-6929-5359       0.64      1.00      0.78       211\n",
      "0000-0002-7045-8004       0.83      0.44      0.57        57\n",
      "0000-0002-7305-8786       0.00      0.00      0.00        11\n",
      "0000-0002-7991-9428       0.72      0.71      0.72        55\n",
      "0000-0003-0487-4242       1.00      0.04      0.08        24\n",
      "0000-0003-1447-9385       0.00      0.00      0.00        11\n",
      "\n",
      "          micro avg       0.75      0.75      0.75      1015\n",
      "          macro avg       0.52      0.44      0.45      1015\n",
      "       weighted avg       0.70      0.75      0.70      1015\n",
      "\n",
      "[ 10   0   0   0   0   1   0   0   0   0   0   0   0   0   2   1   0   0\n",
      "   0   0   0  44   7   0   0   0   0   0   0   0   0   0   0   2   4   0\n",
      "   0   0   0   0   0   1 144   0   0   0   0   0   0   0   0   0   0   8\n",
      "   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0  17   0   0   0   0   0   0   0   6   0   0   0   0   0   0   0\n",
      "   0   0   0   4   2   0   0   0   0   0   0   0   0   0   0  81   0   0\n",
      "   0   0   0   0   0   0  11   0   0   0   0   0   0   1   3   0   0   0\n",
      "  21   0   0   0   0   0   0   0   3   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  10   0   0   0   0   0   0   0\n",
      "   0   0   0  18   0   0   0   0   0   0   0   0   4   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  14\n",
      "   0   0   0   0   2   0   0   0   0   0   0   0  36   0   0   4   2   1\n",
      "   0   0   0   0   0   1   7   0   0   0   0   0   0   0   0   0   0   4\n",
      "   4   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  22   0   1   2   0   0   0   0   0   0   6   0   0   0   0   0   0   0\n",
      "   0   0   0 133   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   1 210   0   0   0   0   0   0   0  11   0   0   0\n",
      "   0   0   0   0   0   0   0   6  15  25   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  11   0   0   0   0   0   0   0\n",
      "   7   0   0   0   0   0   0   0   0   0   0   4   4   1   0  39   0   0\n",
      "   0   0   0   0   0   5   0   0   0   0   0   0   0   0  18   0   0   0\n",
      "   1   0   0   1   3   0   0   0   0   0   0   0   0   0   0   0   7   0\n",
      "   0   0   0   0]\n",
      "LR Accuracy:  0.7546798029556651\n",
      "LR F1:  0.45116928624932173\n"
     ]
    }
   ],
   "source": [
    "# load the file\n",
    "import io\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from statistics import mean \n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "fileDir = \"../../Data/\"+Dataset+\"/canopies_labeled/\"\n",
    "listfiles = os.listdir(fileDir)\n",
    "\n",
    "lr_diff_embedding_result = []\n",
    "svm_diff_embedding_result = []\n",
    "\n",
    "# ----------------------- different textual embedding ----------------------#\n",
    "for embedding in pp_textual:\n",
    "    print(\"Load embedding: \", embedding)\n",
    "    # read pretrained embeddings\n",
    "    all_textual_embedding, all_textual_emb_pid = read_textual_embedding(emb_type = embedding)\n",
    "    \n",
    "    threshold_change_all_lr_f1s = []\n",
    "    threshold_change_all_svm_f1s = []\n",
    "    threshold_change = []\n",
    "    \n",
    "    # -------------- different threshold (step by 10) -----------------------#\n",
    "    for step_threshold in range(threshold_lower, threshold_upper, 10):\n",
    "        threshold_change.append(step_threshold)\n",
    "        # collect statistic to output\n",
    "        allname, num_class, per_class_count, all_sample_count = ([] for i in range(4))\n",
    "\n",
    "        all_mnb_accuracy, all_mnb_f1, all_LR_accuracy = ([] for i in range(3))\n",
    "        all_LR_f1, all_svcLinear_accuracy, all_svcLinear_f1 = ([] for i in range(3))\n",
    "        \n",
    "        total_selected_group = 0\n",
    "\n",
    "        # ------- different name group in all name group --------------------#\n",
    "        for file in listfiles:\n",
    "            # group name\n",
    "            temp = file.split(\"_\")\n",
    "            name = temp[1]+\"_\"+temp[-1]\n",
    "            print(\"For name: \",name)\n",
    "            # read needed content in labeled file\n",
    "            labeled_data = read_labeled_file(fileDir+file)\n",
    "            #----------- select name group contain productive author------------------------------------#\n",
    "            #----------- (contain pair of author write more than 100 papers) ---------------------------#\n",
    "            # count number of paper each author write based on author ID\n",
    "            authorCounter = collections.Counter(labeled_data[\"authorID\"])\n",
    "            # remove name group that do not contain pair of author write more than 100 papers\n",
    "            for k in list(authorCounter):\n",
    "                if authorCounter[k] < threshold_select_name_group:\n",
    "                    del authorCounter[k]\n",
    "            # if only have one class or no class pass the threshold, not applicable\n",
    "            if(len(authorCounter)==0) or (len(authorCounter)==1):\n",
    "                print(name, \" pass\")\n",
    "            else:\n",
    "                total_selected_group+= 1\n",
    "                #--------select authors in name group are very productive (more than threshold)---------#\n",
    "                print(\"Total sample size before apply threshold: \",len(labeled_data))\n",
    "                # count number of paper each author write based on author ID\n",
    "                paperCounter = collections.Counter(labeled_data[\"authorID\"])\n",
    "                print(paperCounter)\n",
    "                print(\"Total author before apply threshoid: \", len(paperCounter))\n",
    "                # collect per class statistic\n",
    "                for k in list(paperCounter):\n",
    "                    if paperCounter[k] < step_threshold:\n",
    "                        del paperCounter[k]\n",
    "                temp =list(paperCounter.keys())\n",
    "                print(temp)\n",
    "                print(\"Total author after apply threshoid: \", len(temp))\n",
    "                # remove samples that are smaller than threshold\n",
    "                labeled_data = labeled_data[labeled_data.authorID.isin(temp)]\n",
    "                print(\"Total sample size after apply threshold: \",len(labeled_data))\n",
    "                all_sample_count.append(len(labeled_data))\n",
    "                allname.append(name)\n",
    "                num_class.append(len(paperCounter))\n",
    "                per_class_count.append(paperCounter)\n",
    "                #------------ extract paper representation -------------------------------------------#\n",
    "                # shuffle the data\n",
    "                labeled_data = labeled_data.sample(frac=1).reset_index(drop=True)\n",
    "                # extract true label and pid\n",
    "                label = labeled_data[\"authorID\"]\n",
    "                pid = labeled_data[\"paperID\"]\n",
    "                # list of different data field\n",
    "                part_collection = []\n",
    "                # select feature wanted to fit to clustering/classification algorithm\n",
    "                # data part, textual information\n",
    "                data_part_textual = extract_embedding(all_textual_embedding, all_textual_emb_pid, pid)\n",
    "                print(data_part_textual.shape)\n",
    "                part_collection.append(data_part_textual)\n",
    "                # merge different part of data data together by concatenate it all together\n",
    "                # remove empty emb (when emb set off)\n",
    "                part_collection = [part for part in part_collection if len(part)!=0]\n",
    "                if len(part_collection)>1:\n",
    "                    combinedata = np.concatenate(part_collection,axis=1)\n",
    "                elif len(part_collection)==1:\n",
    "                    if isinstance(part_collection[0], pd.DataFrame):\n",
    "                        combinedata = part_collection[0].values\n",
    "                    else:\n",
    "                        combinedata = part_collection[0]\n",
    "                else:\n",
    "                    print(\"No data available\")\n",
    "                    break\n",
    "                print(combinedata.shape)\n",
    "                # -------------- using converted feature vector to train classifier-------------------#\n",
    "                if embedding == \"tf\":\n",
    "                    # using multinomial naive bayes\n",
    "                    clf = MultinomialNB()\n",
    "                    mnbaccuracy, mnbmarcof1, tp, tn, fp, fn = k_fold_cv(combinedata, label, clf, k=10)\n",
    "                    print(\"MNB Accuracy: \",mnbaccuracy)\n",
    "                    print(\"MNB F1: \", mnbmarcof1)\n",
    "                    all_mnb_accuracy.append(mnbaccuracy)\n",
    "                    all_mnb_f1.append(mnbmarcof1)\n",
    "                # using logistic regression\n",
    "                clf = LogisticRegression(multi_class='ovr')\n",
    "                LRaccuracy, LRmarcof1, tp, tn, fp, fn = k_fold_cv(combinedata, label, clf, k=10)\n",
    "                print(\"LR Accuracy: \",LRaccuracy)\n",
    "                print(\"LR F1: \", LRmarcof1)\n",
    "                all_LR_accuracy.append(LRaccuracy)\n",
    "                all_LR_f1.append(LRmarcof1)\n",
    "                # using SVM with linear kernal\n",
    "                clf = SVC(decision_function_shape='ovr', kernel='linear')\n",
    "                svcaccuracy, svcmarcof1, tp, tn, fp, fn = k_fold_cv(combinedata, label, clf, k=10)\n",
    "                print(\"svc Accuracy: \",svcaccuracy)\n",
    "                print(\"svc F1: \", svcmarcof1)\n",
    "                all_svcLinear_accuracy.append(svcaccuracy)\n",
    "                all_svcLinear_f1.append(svcmarcof1)\n",
    "            \n",
    "        if embedding == \"tf\":\n",
    "            # write evaluation result to excel\n",
    "            output = pd.DataFrame({'Name Group':allname,\"Class number\":num_class,\n",
    "                                   \"Per class size\":per_class_count, \"Total samples\":all_sample_count,\n",
    "                                   \"MNB Accuracy\":all_mnb_accuracy, \"MNB macro F1\": all_mnb_f1, \n",
    "                                   \"svc(linear) accuracy\":all_svcLinear_accuracy, \"svc(linear) macro f1\": all_svcLinear_f1, \n",
    "                                   \"logistic regression accuracy\":all_LR_accuracy, \"logistic regression macro f1\": all_LR_f1})\n",
    "        else:\n",
    "            # write evaluation result to excel\n",
    "            output = pd.DataFrame({'Name Group':allname, \"Class number\":num_class,\n",
    "                                   \"Per class size\":per_class_count, \"Total samples\":all_sample_count,\n",
    "                                   \"svc(linear) accuracy\":all_svcLinear_accuracy, \"svc(linear) macro f1\": all_svcLinear_f1, \n",
    "                                   \"logistic regression accuracy\":all_LR_accuracy, \"logistic regression macro f1\": all_LR_f1})\n",
    "\n",
    "        savePath = \"../../result/\"+Dataset+\"/OCEN_global_emb_sample=140k/\"\n",
    "        filename = \"(Global emb sample 140k) textual=\"+embedding+\"_threshold=\"+str(step_threshold)+\"_namegroupcount=\"+str(total_selected_group)+\".csv\"\n",
    "        com_func.write_csv_df(savePath, filename, output)\n",
    "        print(\"Done\")\n",
    "        \n",
    "        threshold_change_all_lr_f1s.append(all_LR_f1)\n",
    "        threshold_change_all_svm_f1s.append(all_svcLinear_f1)\n",
    "    \n",
    "    lr_diff_embedding_result.append(threshold_change_all_lr_f1s)\n",
    "    svm_diff_embedding_result.append(threshold_change_all_svm_f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T02:50:30.572953Z",
     "start_time": "2019-01-21T02:50:30.359531Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tf', 'tf_idf']\n",
      "[[[0.45116928624932173, 0.6087308555262901, 0.804042829946624, 0.5191865428561624, 0.5750274022647373, 0.47666400145135795, 0.8212260416370004, 0.4113122908760472, 0.3648547987315374, 0.9720807195811769, 0.6261647156850281, 0.4705157542686147, 0.4693023457875857, 0.4237545248804363, 0.6209358131784022, 0.5383465581683844, 0.6324486380282568, 0.3803108372589891], [0.707029802180778, 0.9601568429041234, 0.9277194149695388, 0.8591719820458303, 0.8237437787897838, 0.7386944393126801, 0.9379702260406314, 0.5316355257847761, 0.682923871575724, 0.9720807195811769, 0.7031639283499562, 0.9867177314211212, 0.5450257637192898, 0.6505844329515474, 0.9286003683241253, 0.6637916513452395, 0.6384428748301504, 0.6158179423628848], [0.8554935481317761, 0.9641587256435784, 0.92159576343686, 0.9844434021263291, 0.8582705444158516, 0.8498347620758043, 0.9422303675402176, 0.7169404570739202, 0.812434111163889, 0.9720807195811769, 0.7018175950853894, 0.990056428244624, 0.7572377139409037, 0.8724387959080465, 0.9423039027297104, 0.9588415971917508, 0.6524350902821282, 0.7195807050558825], [0.8478859033505536, 0.956149648026576, 0.9739609422320858, 0.9844434021263291, 0.8627722803163532, 0.8762066657801688, 0.9446365879830506, 0.8594567465330731, 0.8308902697108443, 0.9720807195811769, 0.8798494767601682, 0.9833664982907291, 0.8123796248796249, 0.9962270907672435, 0.9311744601094252, 0.9619166297747681, 0.8660057314924572, 0.8108620781805808], [0.8516938555594669, 0.9601568429041234, 0.9764172986801943, 0.9844434021263291, 0.9366930097100876, 0.9633067074806657, 0.9446365879830506, 0.9266841998986167, 0.9721122340924321, 0.9972196263486743, 0.8721801734219842, 0.990056428244624, 0.8319088319088319, 0.9924721853628662, 0.9763257575757576, 0.9557441749095893, 0.8508478234202981, 0.797091638223745], [0.9519295128122889, 0.9522335249889429, 0.978561808058064, 0.9818339495184627, 0.9583327898773529, 0.9542837391847293, 0.9479256451733514, 0.9266841998986167, 0.964502718639347, 0.9972196263486743, 0.9721410994130878, 0.9867177314211212, 0.9756117724867724, 0.9962270907672435, 0.9803146788071411, 0.9619166297747681, 0.8539721495324032, 0.85218441248946], [0.9464053826730146, 0.9561939980088181, 0.9836216951212031, 0.9844434021263291, 0.9747899159663866, 0.9792424242424241, 0.940363031654084, 0.9804713804713805, 0.9721217283872717, 0.9972196263486743, 0.9679771533523049, 0.9867177314211212, 0.9780595349825574, 0.9962270907672435, 0.9723184883737651, 0.9557441749095893, 0.8461073159696205, 0.8523855425101079], [0.9478330315129936, 0.9561939980088181, 0.9836216951212031, 0.9844434021263291, 0.9705877160461134, 0.9860737808477293, 0.9413183581046369, 0.9843936857211193, 0.9617446393762183, 0.9972196263486743, 0.9701180250376426, 0.9867177314211212, 0.9805050743450554, 0.9962270907672435, 0.9803146788071411, 0.9557441749095893, 0.9883281137707274, 0.9876051837875804], [0.9525519383735608, 0.9521882114849483, 0.9961596548004314, 0.9818339495184627, 0.9747899159663866, 0.9903623389861922, 0.9631220005051782, 0.9843936857211193, 0.9649251078954049, 0.9972196263486743, 0.9721410994130878, 0.9867177314211212, 0.9805050743450554, 0.9962270907672435, 0.9723184883737651, 0.9588415971917508, 0.9883281137707274, 0.9881065601335134], [0.9590293338680435, 0.9522335249889429, 0.9961596548004314, 0.979219423051821, 0.9747899159663866, 0.9951832897204919, 0.966874645490641, 0.9883073247218697, 0.9617446393762183, 0.9972196263486743, 0.9721410994130878, 0.9867177314211212, 0.9805050743450554, 0.9962270907672435, 0.9803146788071411, 0.9557441749095893, 0.986006289308176, 0.9881065601335134]], [[0.4534471932971035, 0.7714463803168062, 0.7947233938312406, 0.4392491208403772, 0.5864077422891936, 0.44883735578602385, 0.851026846716502, 0.4094630790529314, 0.37164428073379385, 0.9775064286239071, 0.5895776401393593, 0.47296469540037867, 0.444745841682488, 0.3782354399295422, 0.5786519058650766, 0.46801541831662313, 0.5912322236060069, 0.3804990074633166], [0.6909820722449415, 0.9720952305623665, 0.9329301214120441, 0.6704597984126597, 0.8341321429595894, 0.721418602926786, 0.9233935028975201, 0.4664657057219382, 0.7026840241728002, 0.9665398876029196, 0.6002138333310307, 0.9867177314211212, 0.5249101950788787, 0.6426349053583605, 0.8722799691229136, 0.6078414054109919, 0.6106781756710976, 0.6272485007963211], [0.8493452729485591, 0.9681556833259619, 0.9027068574779316, 0.9818339495184627, 0.8578920028963997, 0.839662651149781, 0.9200400556167917, 0.7055348413156352, 0.7889193299594799, 0.9775064286239071, 0.6548005344068785, 0.9867177314211212, 0.7254449131354388, 0.7965876138833096, 0.865273937532002, 0.9399089665687571, 0.6177978447620978, 0.7344063147303285], [0.8528544450743489, 0.9681556833259619, 0.9831174285694866, 0.9818339495184627, 0.8685510850509879, 0.8812149366774651, 0.9200400556167917, 0.8652294007183284, 0.8469718717073309, 0.9720807195811769, 0.8458968889794132, 0.9867177314211212, 0.7998711526807214, 0.9962270907672435, 0.8481999540184583, 0.9431236212336991, 0.834312783758767, 0.8197372136464062], [0.8499431192204406, 0.9681556833259619, 0.9831174285694866, 0.9844434021263291, 0.9317524187680843, 0.9722764164760213, 0.9200400556167917, 0.926943007602413, 0.97008079593934, 0.9972196263486743, 0.8570280522162724, 0.9867177314211212, 0.8114550700537961, 0.9962270907672435, 0.9803146788071411, 0.9431236212336991, 0.843745541190909, 0.801666357365604], [0.9583836734199007, 0.9681254743232988, 0.9836721657981176, 0.9818339495184627, 0.9566749804877251, 0.9773635229703449, 0.9200400556167917, 0.9243942722447605, 0.9754854627411019, 0.9972196263486743, 0.9470797865417154, 0.9867177314211212, 0.9829484797546373, 1.0, 0.9803146788071411, 0.9431236212336991, 0.8123461025442028, 0.8569006299703629], [0.9545644926355322, 0.9721234532783387, 0.9885377077823139, 0.9844434021263291, 0.9789912257472237, 0.9753247890050626, 0.9224927515133868, 0.9883073247218697, 0.9712356092972864, 0.9972196263486743, 0.9449598021026593, 0.9867177314211212, 0.9829484797546373, 0.9962270907672435, 0.9842857142857142, 0.9431236212336991, 0.8259053823971221, 0.8810894090582215], [0.9475992747892736, 0.9641587256435784, 0.9885377077823139, 0.9818339495184627, 0.9789912257472237, 0.9569699291012107, 0.9107813915813683, 0.9883073247218697, 0.9617275423556657, 0.9972196263486743, 0.9470797865417154, 0.9867177314211212, 0.9829609415980527, 0.9924358974358973, 0.9803146788071411, 0.9399089665687571, 0.988306767359852, 0.9925475096261334], [0.9467795553303475, 0.9641587256435784, 0.9961596548004314, 0.9844434021263291, 0.9789912257472237, 0.9951832897204919, 0.9325011558021266, 0.9883073247218697, 0.9617275423556657, 0.9972196263486743, 0.9513362850771335, 0.990056428244624, 0.9829484797546373, 0.9962270907672435, 0.9842857142857142, 0.9431236212336991, 0.988306767359852, 0.9900689655172413], [0.9600948244083805, 0.9681254743232988, 0.9961596548004314, 0.9870478967476553, 0.9789912257472237, 0.9951832897204919, 0.9325011558021266, 0.9883073247218697, 0.9649251078954049, 0.9972196263486743, 0.9492340825674158, 0.9867177314211212, 0.9829609415980527, 1.0, 0.9803146788071411, 0.9431236212336991, 0.9906540093249885, 0.9925475096261334]]]\n",
      "[[[0.688428938342307, 0.921730068818935, 0.9647452733214283, 0.833011441997134, 0.7994167234700494, 0.7312942717678023, 0.9691953640720243, 0.8224831036768697, 0.7491037644407385, 0.9880305916815822, 0.9271277763116292, 0.7354761281524282, 0.7596938428865323, 0.8392859442610662, 0.8839779718086398, 0.8556812340135458, 0.8701075343977461, 0.7106319370887823], [0.8688552675423091, 0.9721481013657536, 0.9871669815768984, 0.9898102794053075, 0.8958314076420172, 0.8956590888417562, 0.9656428461182281, 0.8435692047267145, 0.8683275603779461, 0.9880305916815822, 0.9274958128401938, 0.990056428244624, 0.7524461264277315, 0.9022145356330485, 0.9901713367910991, 0.9327114615256389, 0.867085011055427, 0.7867620148701007], [0.9313564858747119, 0.9761167624944714, 0.9827607870239422, 0.9974185848915871, 0.9040967440549703, 0.9452980510937768, 0.9688056507146406, 0.9069339491821192, 0.9177974934869874, 0.9880305916815822, 0.9219453017055449, 0.990056428244624, 0.9042898740022162, 0.9436303045796695, 0.9947839611547362, 0.9857500624764052, 0.8407625161125336, 0.8518012424287364], [0.9347164605769824, 0.980105786689824, 0.9839680009921841, 0.9948327793934357, 0.9094904656849994, 0.954745306095813, 0.9656428461182281, 0.937185938502607, 0.919904856184348, 0.9861858954658144, 0.9802106239932437, 0.990056428244624, 0.9188825529603242, 0.9962449561487252, 0.9953763815435286, 0.9915067581486, 0.9226613194092869, 0.9077972029712309], [0.937479349540405, 0.9761363636363636, 0.9839680009921841, 0.9974185848915871, 0.9558393953740271, 0.9858888148923446, 0.9688056507146406, 0.9575717668167592, 0.9881762481594544, 0.9972196263486743, 0.9761875789113479, 0.990056428244624, 0.9392356934808525, 0.9962449561487252, 0.9882393191634948, 0.9886380498145204, 0.927412776828724, 0.9005424307800758], [0.9775088220485995, 0.9761363636363636, 0.9835310144044721, 0.9974185848915871, 0.9663754116423923, 0.9839855187359776, 0.9688056507146406, 0.9534727034727034, 0.9881762481594544, 0.9972196263486743, 0.9902147846997283, 0.990056428244624, 0.9853898401018819, 0.9962449561487252, 0.9960960236226062, 0.9857500624764052, 0.928411422683547, 0.9274102478307341], [0.970424243687512, 0.9761167624944714, 0.9878753962133298, 1.0, 0.9747881355932203, 0.9864287865100874, 0.9656428461182281, 0.9883073247218697, 0.9915060526137823, 0.9972196263486743, 0.9961117376008399, 0.990056428244624, 0.9878292439986089, 0.9962449561487252, 0.9960960236226062, 0.9915067581486, 0.9323658273995298, 0.9300734632683658], [0.9738409278159376, 0.9681556833259619, 0.9902118530437114, 0.9974185848915871, 0.978988258144257, 0.9860915125571742, 0.9688056507146406, 0.9883073247218697, 0.9872590789206735, 0.9972196263486743, 0.9941610243833109, 0.990056428244624, 0.9853898401018819, 0.9962449561487252, 0.9960960236226062, 0.9915067581486, 0.9930214687847947, 0.9955427850034395], [0.9792267768581127, 0.9761167624944714, 0.9961596548004314, 0.9974185848915871, 0.9747899159663866, 0.9951832897204919, 0.9854145854145855, 0.9883073247218697, 0.9872590789206735, 0.9972196263486743, 0.9941610243833109, 0.990056428244624, 0.9878292439986089, 0.9962449561487252, 0.9882393191634948, 0.9915067581486, 0.9930214687847947, 0.9950344827586207], [0.9775090888246901, 0.9721481013657536, 0.9961596548004314, 0.9922424714940064, 0.9789912257472237, 0.9951832897204919, 0.9854145854145855, 0.9883073247218697, 0.9904449876758599, 0.9972196263486743, 0.9961118116855822, 0.990056428244624, 0.9878292439986089, 0.9962449561487252, 0.9960960236226062, 0.9915067581486, 0.9907032131366733, 0.9950344827586207]], [[0.7002505249710216, 0.9268869892651569, 0.9549775932438033, 0.8850237674289081, 0.8074292732161649, 0.7295282612095201, 0.9644388376760795, 0.8153467221516899, 0.7103532607504431, 0.9880305916815822, 0.9144276607920188, 0.6999715184984939, 0.7203931641857738, 0.8023840291004978, 0.9158116693183448, 0.8433683473389356, 0.8749640210203075, 0.6904689474933271], [0.8806435255225603, 0.9800881809130992, 0.9919069688264772, 0.9915281196108964, 0.9042298259537508, 0.8926132768825944, 0.9656428461182281, 0.8351374694734524, 0.8625896252453445, 0.9880305916815822, 0.908961672178987, 0.990056428244624, 0.7297767217482557, 0.8757934808412158, 0.9921595461089966, 0.9060707937689738, 0.8686731220269711, 0.8009216393402384], [0.9428701969257789, 0.9761167624944714, 0.987287911946335, 1.0, 0.9112991717860551, 0.9467433813984906, 0.9632282746358104, 0.9101329653386844, 0.9115719728611047, 0.9880305916815822, 0.8962991914800434, 0.990056428244624, 0.897190321372603, 0.946926890418569, 0.9947839611547362, 0.9857500624764052, 0.8473280876655951, 0.8592530998986998], [0.9450246487755646, 0.9800881809130992, 0.9933479450047225, 1.0, 0.9242749372459009, 0.9501409179016995, 0.9600443818949511, 0.9485268932536415, 0.9078713000202541, 0.9880305916815822, 0.9674979947835012, 0.990056428244624, 0.9191182507758595, 0.9962449561487252, 0.9895238095238095, 0.9857500624764052, 0.9505294420252856, 0.8982922364158209], [0.9472835492462905, 0.9800881809130992, 0.9960865245522795, 1.0, 0.9447462527555434, 0.9913843352601757, 0.9600443818949511, 0.9621517993477936, 0.9872564935064937, 0.9972196263486743, 0.9730703283376252, 0.990056428244624, 0.9479703180674054, 0.9962449561487252, 0.9882393191634948, 0.9857500624764052, 0.9518274081050683, 0.891062528972355], [0.9821105277574094, 0.9761167624944714, 0.989479021632526, 1.0, 0.9587496692844056, 0.9913843352601757, 0.9663864170895442, 0.9581006864183458, 0.9872564935064937, 0.9972196263486743, 0.9842760138744628, 0.990056428244624, 0.9902667797888385, 0.9962449561487252, 0.9882393191634948, 0.9799150995412378, 0.9502206264208306, 0.9370917463749147], [0.9821105277574094, 0.9761167624944714, 0.9951081556903494, 1.0, 0.9831920903954803, 0.9860915125571742, 0.9608041100642004, 0.9922126745435016, 0.9839124292241436, 0.9972196263486743, 0.9842760138744628, 0.990056428244624, 0.9878292439986089, 0.9962449561487252, 0.9921759417321042, 0.9886380498145204, 0.9500119425182091, 0.9373039355633905], [0.9786981413517619, 0.9761167624944714, 0.9902206496633741, 0.9974185848915871, 0.9831920903954803, 0.9932183363506647, 0.956834094368341, 0.9922126745435016, 0.9808847402597404, 0.9972196263486743, 0.9862709339958219, 0.990056428244624, 0.9878292439986089, 0.9962449561487252, 0.9921759417321042, 0.9857500624764052, 0.9953436296668341, 0.9925475096261334], [0.9769931416397131, 0.9761167624944714, 1.0, 1.0, 0.9831920903954803, 0.9951832897204919, 0.9817351598173516, 0.9922126745435016, 0.9808847402597404, 0.9972196263486743, 0.9882502170185855, 0.990056428244624, 0.9927025355596785, 0.9962449561487252, 0.9882393191634948, 0.9857500624764052, 0.9906872593336681, 0.9925475096261334], [0.9840251733981361, 0.9761363636363636, 1.0, 1.0, 0.9831920903954803, 0.9951832897204919, 0.9780418107986163, 0.9922126745435016, 0.9808847402597404, 0.9972196263486743, 0.9882502170185855, 0.990056428244624, 0.9902667797888385, 0.9962449561487252, 0.9882393191634948, 0.9886380498145204, 0.9953436296668341, 0.9925475096261334]]]\n",
      "[10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n"
     ]
    }
   ],
   "source": [
    "print(pp_textual)\n",
    "print(lr_diff_embedding_result)\n",
    "print(svm_diff_embedding_result)\n",
    "print(threshold_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-19T20:24:27.558Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combine n2v with textual embedding result together\n",
    "embedding_type = ['n2v']\n",
    "n2v_lr = [0.47620634494724273, 0.7498277720392034, 0.8281740036708996, 0.8647698255831765, 0.9028474372925239, 0.9201260416510354, 0.934862248014238, 0.9368194927355292, 0.9488163041946321, 0.9567160856788859]\n",
    "n2v_svm = [0.5345640404538299, 0.7711647264987531, 0.8389260785222713, 0.8659119023637266, 0.901553749220774, 0.9167757971008624, 0.9329983382418031, 0.9334481127313246, 0.9430090615079809, 0.9509157091525307]\n",
    "\n",
    "combined_emb = pp_textual+embedding_type\n",
    "\n",
    "combined_lr_result = lr_diff_embedding_result\n",
    "combined_lr_result.append(n2v_lr)\n",
    "\n",
    "combined_svm_result = svm_diff_embedding_result\n",
    "combined_svm_result.append(n2v_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-19T20:24:27.720Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "threshold_change = np.array(threshold_change)\n",
    "#--------------   logistic regression --------------------------#\n",
    "# process result into np array\n",
    "logistic_regression_result = np.array(combined_lr_result)\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "for emb_type, result in zip(combined_emb, logistic_regression_result):\n",
    "    plt.plot(threshold_change, result, label=emb_type)\n",
    "plt.legend()\n",
    "plt.title('Average f1 for different embedding in logistic regression')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('marco f1 score')\n",
    "\n",
    "# plt.savefig('diff_embedding_sample=140k_clf=logistic regression.eps', format='eps', dpi=300)\n",
    "\n",
    "\n",
    "# -------------------- svm -------------------------------------#\n",
    "svm_result = np.array(combined_svm_result)\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "for emb_type, result in zip(combined_emb, svm_result):\n",
    "    plt.plot(threshold_change, result, label=emb_type)\n",
    "plt.legend()\n",
    "plt.title('Average f1 for different embedding in SVM')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('marco f1 score')\n",
    "\n",
    "# plt.savefig('diff_embedding_sample=140k_clf=SVM.eps', format='eps', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-19T20:24:27.879Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(pp_textual)\n",
    "print(\"svc: \", modelSVCf1)\n",
    "print(\"lr: \", modelLRf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-19T20:24:28.287Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# accuracy\n",
    "from statistics import mean \n",
    "cleaned_svcLinear_accuracy = [x for x in all_svcLinear_accuracy if isinstance(x, float)]\n",
    "cleaned_lr_accuracy = [x for x in all_LR_accuracy if isinstance(x, float)]\n",
    "print(len(cleaned_svcLinear_accuracy))\n",
    "print(len(cleaned_lr_accuracy))\n",
    "print(mean(cleaned_svcLinear_accuracy))\n",
    "print(mean(cleaned_lr_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T01:26:32.881233Z",
     "start_time": "2018-12-14T01:26:32.856794Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# f1\n",
    "from statistics import mean \n",
    "# remove string from result\n",
    "cleaned_svcLinear_f1 = [x for x in all_svcLinear_f1 if isinstance(x, float)]\n",
    "cleaned_lr_f1 = [x for x in all_LR_f1 if isinstance(x, float)]\n",
    "print(len(cleaned_svcLinear_f1))\n",
    "print(len(cleaned_lr_f1))\n",
    "print(mean(cleaned_svcLinear_f1))\n",
    "print(mean(cleaned_lr_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T16:40:50.867327Z",
     "start_time": "2019-01-10T09:58:57.655751Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T16:40:50.910860Z",
     "start_time": "2019-01-10T16:40:50.871136Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
