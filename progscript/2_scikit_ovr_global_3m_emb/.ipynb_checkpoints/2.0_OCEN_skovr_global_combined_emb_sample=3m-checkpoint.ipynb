{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-16T01:46:13.748504Z",
     "start_time": "2019-02-16T01:46:13.715067Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import com_func\n",
    "# parameters\n",
    "#----- threshold for selecting set of name group -----------#\n",
    "threshold_select_name_group = 100\n",
    "#----- threshold for selecting min sample in name group ----#\n",
    "threshold_lower = 100\n",
    "threshold_upper = 110\n",
    "\n",
    "pp_textual = [\"lsa\", \"pv_dm\", \"pv_dbow\"]\n",
    "pp_citation = \"n2v\"\n",
    "\n",
    "Dataset = \"pubmed\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-16T01:46:41.764234Z",
     "start_time": "2019-02-16T01:46:17.804892Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load textual embedding:  pv_dbow\n",
      "Total textual vector records: 135796\n",
      "Vector dimension:  100\n",
      "Load citation embedding:  n2v\n",
      "Total citation vector records: 124922\n",
      "Vector dimension:  100\n",
      "For name:  j_read\n",
      "j_read  pass\n",
      "For name:  f_esteves\n",
      "f_esteves  pass\n",
      "For name:  c_miller\n",
      "c_miller  pass\n",
      "For name:  r_jha\n",
      "r_jha  pass\n",
      "For name:  a_lowe\n",
      "a_lowe  pass\n",
      "For name:  a_vega\n",
      "a_vega  pass\n",
      "For name:  k_smith\n",
      "k_smith  pass\n",
      "For name:  j_gordon\n",
      "j_gordon  pass\n",
      "For name:  s_liao\n",
      "s_liao  pass\n",
      "For name:  j_qian\n",
      "j_qian  pass\n",
      "For name:  s_bernardi\n",
      "s_bernardi  pass\n",
      "For name:  t_hill\n",
      "t_hill  pass\n",
      "For name:  s_schindler\n",
      "s_schindler  pass\n",
      "For name:  j_williams\n",
      "j_williams  pass\n",
      "For name:  s_jacobson\n",
      "s_jacobson  pass\n",
      "For name:  e_andrade\n",
      "e_andrade  pass\n",
      "For name:  t_santos\n",
      "t_santos  pass\n",
      "For name:  k_kim\n",
      "Total sample size before apply threshold:  1111\n",
      "Counter({'0000-0002-6929-5359': 211, '0000-0001-9498-284X': 154, '0000-0002-5878-8895': 139, '0000-0002-1864-3392': 92, '0000-0002-7045-8004': 57, '0000-0001-7896-6751': 57, '0000-0002-7991-9428': 55, '0000-0002-4010-1063': 45, '0000-0002-2186-3484': 28, '0000-0002-4899-1929': 25, '0000-0003-0487-4242': 24, '0000-0002-3642-1486': 22, '0000-0001-9965-3535': 17, '0000-0002-4168-757X': 17, '0000-0001-6525-3744': 14, '0000-0002-3897-0278': 14, '0000-0002-1181-5112': 12, '0000-0003-1447-9385': 11, '0000-0002-7305-8786': 11, '0000-0002-2655-7806': 10, '0000-0003-3466-5353': 9, '0000-0002-7359-663X': 8, '0000-0003-4600-8668': 6, '0000-0002-1382-7088': 5, '0000-0002-9505-4882': 5, '0000-0003-3667-9900': 4, '0000-0001-9714-6038': 4, '0000-0002-4760-0228': 3, '0000-0003-4188-7915': 3, '0000-0001-9454-0427': 3, '0000-0002-0333-6808': 3, '0000-0003-2134-4964': 3, '0000-0002-6658-047X': 3, '0000-0003-1273-379X': 3, '0000-0002-7047-3183': 3, '0000-0002-1814-9546': 3, '0000-0003-4812-6297': 2, '0000-0001-6597-578X': 2, '0000-0002-5285-9138': 2, '0000-0002-6796-7844': 2, '0000-0002-1130-8698': 2, '0000-0001-8518-8150': 2, '0000-0002-7103-924X': 2, '0000-0002-5407-0202': 1, '0000-0001-6220-8411': 1, '0000-0002-7440-6703': 1, '0000-0002-1603-7559': 1, '0000-0003-0257-1707': 1, '0000-0001-8532-6517': 1, '0000-0001-6626-316X': 1, '0000-0002-3246-9861': 1, '0000-0002-7207-4389': 1, '0000-0001-9682-9654': 1, '0000-0002-0196-3832': 1, '0000-0001-8063-6081': 1, '0000-0003-2037-3333': 1, '0000-0001-8872-6751': 1})\n",
      "Total author before apply threshoid:  57\n",
      "Total author after apply threshoid:  3\n",
      "Total sample size after apply threshold:  504\n",
      "Total missing sample:  0\n",
      "(504, 100)\n",
      "Missing Sample:  12919017\n",
      "Missing Sample:  17359037\n",
      "Missing Sample:  16999403\n",
      "Missing Sample:  12833599\n",
      "Missing Sample:  19358601\n",
      "Missing Sample:  11973332\n",
      "Missing Sample:  14769839\n",
      "Missing Sample:  15845571\n",
      "Missing Sample:  16606283\n",
      "Missing Sample:  19260493\n",
      "Missing Sample:  24511052\n",
      "Missing Sample:  23965436\n",
      "Missing Sample:  14623918\n",
      "Missing Sample:  20446751\n",
      "Missing Sample:  12495907\n",
      "Missing Sample:  24086948\n",
      "Missing Sample:  18578481\n",
      "Missing Sample:  20422867\n",
      "Missing Sample:  19618910\n",
      "Missing Sample:  22983732\n",
      "Missing Sample:  17571173\n",
      "Missing Sample:  17629037\n",
      "Missing Sample:  26614023\n",
      "Missing Sample:  16685393\n",
      "Missing Sample:  18052393\n",
      "Missing Sample:  18788720\n",
      "Missing Sample:  15787556\n",
      "Missing Sample:  18037618\n",
      "Missing Sample:  17564465\n",
      "Missing Sample:  17722888\n",
      "Missing Sample:  23214901\n",
      "Missing Sample:  23215215\n",
      "Missing Sample:  11559199\n",
      "Missing Sample:  12630031\n",
      "Missing Sample:  20218591\n",
      "Missing Sample:  15532789\n",
      "Missing Sample:  16204727\n",
      "Missing Sample:  16861879\n",
      "Missing Sample:  19471081\n",
      "Missing Sample:  12068151\n",
      "Missing Sample:  21661423\n",
      "Missing Sample:  17928753\n",
      "Missing Sample:  21828679\n",
      "Missing Sample:  16503553\n",
      "Missing Sample:  15227745\n",
      "Missing Sample:  11456562\n",
      "Missing Sample:  26329310\n",
      "Total missing sample:  47\n",
      "(504, 100)\n",
      "2\n",
      "(504, 200)\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "0000-0001-9498-284X       0.97      0.95      0.96       154\n",
      "0000-0002-5878-8895       0.96      0.96      0.96       139\n",
      "0000-0002-6929-5359       0.99      1.00      1.00       211\n",
      "\n",
      "          micro avg       0.97      0.97      0.97       504\n",
      "          macro avg       0.97      0.97      0.97       504\n",
      "       weighted avg       0.97      0.97      0.97       504\n",
      "\n",
      "[146   6   2   5 134   0   0   0 211]\n",
      "[[146   6   2]\n",
      " [  5 134   0]\n",
      " [  0   0 211]]\n",
      "TP:  [146 134 211] TN:  [345 359 291] FP:  [5 6 2] FN:  [8 5 0]\n",
      "LR Accuracy:  0.9742063492063492\n",
      "LR F1:  0.9710778482502539\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "0000-0001-9498-284X       0.97      0.95      0.96       154\n",
      "0000-0002-5878-8895       0.96      0.96      0.96       139\n",
      "0000-0002-6929-5359       0.99      1.00      0.99       211\n",
      "\n",
      "          micro avg       0.98      0.98      0.98       504\n",
      "          macro avg       0.97      0.97      0.97       504\n",
      "       weighted avg       0.98      0.98      0.98       504\n",
      "\n",
      "[147   5   2   4 134   1   0   0 211]\n",
      "[[147   5   2]\n",
      " [  4 134   1]\n",
      " [  0   0 211]]\n",
      "TP:  [147 134 211] TN:  [346 360 290] FP:  [4 5 3] FN:  [7 5 0]\n",
      "svc Accuracy:  0.9761904761904762\n",
      "svc F1:  0.9736347932261712\n"
     ]
    }
   ],
   "source": [
    "# load the file\n",
    "import io\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from statistics import mean \n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "fileDir = \"../../Data/\"+Dataset+\"/canopies_labeled/\"\n",
    "listfiles = os.listdir(fileDir)\n",
    "\n",
    "lr_diff_embedding_result = []\n",
    "svm_diff_embedding_result = []\n",
    "\n",
    "# ----------------------- different ciataion embedding ----------------------#\n",
    "for select_emb in pp_textual:\n",
    "    \n",
    "    # read pretrained embeddings\n",
    "    print(\"Load textual embedding: \", select_emb)\n",
    "    all_textual_embedding, all_textual_emb_pid = com_func.read_textual_embedding(emb_type=select_emb, training_size = \"3m\")\n",
    "    print(\"Load citation embedding: \", pp_citation)\n",
    "    all_citation_embedding, all_citation_emb_pid = com_func.read_citation_embedding(emb_type = pp_citation)\n",
    "    \n",
    "    threshold_change_all_lr_f1s = []\n",
    "    threshold_change_all_svm_f1s = []\n",
    "    threshold_change = []\n",
    "    \n",
    "    # -------------- different threshold (step by 10) -----------------------#\n",
    "    for step_threshold in range(threshold_lower, threshold_upper, 10):\n",
    "        threshold_change.append(step_threshold)\n",
    "        # collect statistic to output\n",
    "        allname, num_class, per_class_count, all_sample_count = ([] for i in range(4))\n",
    "        \n",
    "        all_mnb_accuracy, all_mnb_f1, all_LR_accuracy = ([] for i in range(3))\n",
    "        all_LR_f1, all_svcLinear_accuracy, all_svcLinear_f1 = ([] for i in range(3))\n",
    "    \n",
    "        total_selected_group = 0\n",
    "\n",
    "        # ------- different name group in all name group --------------------#\n",
    "        for file in listfiles:\n",
    "            # group name\n",
    "            temp = file.split(\"_\")\n",
    "            name = temp[1]+\"_\"+temp[-1]\n",
    "            print(\"For name: \",name)\n",
    "            # read needed content in labeled file\n",
    "            labeled_data = com_func.read_pid_aid(fileDir+file)\n",
    "            #----------- select name group contain productive author------------------------------------#\n",
    "            #----------- (contain pair of author write more than 100 papers) ---------------------------#\n",
    "            # count number of paper each author write based on author ID\n",
    "            authorCounter = com_func.select_productive_groups(labeled_data, threshold_select_name_group)\n",
    "            # if only have one class or no class pass the threshold, not applicable\n",
    "            if(len(authorCounter)==0) or (len(authorCounter)==1):\n",
    "                print(name, \" pass\")\n",
    "            else:\n",
    "                total_selected_group+= 1\n",
    "                #--------select authors in name group are very productive (more than threshold)---------#\n",
    "                labeled_data, author_list, paperCounter= com_func.only_select_productive_authors(labeled_data, step_threshold)\n",
    "                all_sample_count.append(len(labeled_data))\n",
    "                allname.append(name)\n",
    "                num_class.append(len(paperCounter))\n",
    "                per_class_count.append(paperCounter)\n",
    "                #------------ extract paper representation -------------------------------------------#\n",
    "                # shuffle the data\n",
    "                labeled_data = labeled_data.sample(frac=1).reset_index(drop=True)\n",
    "                # extract true label and pid\n",
    "                label = labeled_data[\"authorID\"]\n",
    "                pid = labeled_data[\"paperID\"]\n",
    "                # list of different data field\n",
    "                part_collection = []\n",
    "                # select feature wanted to fit to clustering/classification algorithm\n",
    "                # data part, textual information\n",
    "                data_part_textual = com_func.extract_embedding(all_textual_embedding, all_textual_emb_pid, pid)\n",
    "                print(data_part_textual.shape)\n",
    "                part_collection.append(data_part_textual)\n",
    "                # data part, citation information\n",
    "                data_part_citation = com_func.extract_embedding(all_citation_embedding, all_citation_emb_pid, pid)\n",
    "                print(data_part_citation.shape)\n",
    "                part_collection.append(data_part_citation)\n",
    "                # merge different part of data data together by concatenate it all together\n",
    "                # remove empty emb (when emb set off)\n",
    "                combinedata = com_func.merge_data_parts(part_collection)\n",
    "                print(combinedata.shape)\n",
    "                # -------------- using converted feature vector to train classifier-------------------#\n",
    "                # using logistic regression\n",
    "                clf = LogisticRegression(multi_class='ovr')\n",
    "                LRaccuracy, LRmarcof1, tp, tn, fp, fn = com_func.k_fold_cv_with_accumulate_statistic(combinedata, label, clf, k=10)\n",
    "                print(\"LR Accuracy: \",LRaccuracy)\n",
    "                print(\"LR F1: \", LRmarcof1)\n",
    "                all_LR_accuracy.append(LRaccuracy)\n",
    "                all_LR_f1.append(LRmarcof1)\n",
    "                # using SVM with linear kernal\n",
    "                clf = SVC(decision_function_shape='ovr', kernel='linear')\n",
    "                svcaccuracy, svcmarcof1, tp, tn, fp, fn = com_func.k_fold_cv_with_accumulate_statistic(combinedata, label, clf, k=10)\n",
    "                print(\"svc Accuracy: \",svcaccuracy)\n",
    "                print(\"svc F1: \", svcmarcof1)\n",
    "                all_svcLinear_accuracy.append(svcaccuracy)\n",
    "                all_svcLinear_f1.append(svcmarcof1)\n",
    "                break\n",
    "            \n",
    "#         # write evaluation result to excel\n",
    "#         output = pd.DataFrame({'Name Group':allname, \"Class number\":num_class,\n",
    "#                                \"Per class size\":per_class_count, \"Total samples\":all_sample_count,\n",
    "#                                \"logistic regression accuracy\":all_LR_accuracy, \"logistic regression macro f1\": all_LR_f1,\n",
    "#                                \"svc(linear) accuracy\":all_svcLinear_accuracy, \"svc(linear) macro f1\": all_svcLinear_f1})\n",
    "\n",
    "#         savePath = \"../../result/\"+Dataset+\"/OCEN_global_emb_sample=3m/\"\n",
    "#         filename = \"(Global emb sample 3m) citation = \"+pp_citation+\"_textual=\"+select_emb+\"_threshold=\"+str(step_threshold)+\"_namegroupcount=\"+str(total_selected_group)+\".csv\"\n",
    "#         com_func.write_csv_df(savePath, filename, output)\n",
    "#         print(\"Done\")\n",
    "        \n",
    "#         threshold_change_all_lr_f1s.append(all_LR_f1)\n",
    "#         threshold_change_all_svm_f1s.append(all_svcLinear_f1)\n",
    "    \n",
    "#     lr_diff_embedding_result.append(threshold_change_all_lr_f1s)\n",
    "#     svm_diff_embedding_result.append(threshold_change_all_svm_f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T16:17:13.269432Z",
     "start_time": "2019-01-15T16:16:24.891Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------- plot f1 score w.r.t each name group on different embedding -------------- #\n",
    "print(pp_textual)\n",
    "print(allname)\n",
    "# 3d, d1 diff emb, d2 diff threshold, d3 result for different author\n",
    "print(lr_diff_embedding_result)\n",
    "print(svm_diff_embedding_result)\n",
    "print(threshold_change)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T05:07:49.675275Z",
     "start_time": "2019-01-13T05:07:49.655569Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------- plot f1 score w.r.t each name group on different embedding -------------- #\n",
    "# -------------- extract result for plot --------------------- #\n",
    "lr_per_author = []\n",
    "lr_lsa_per_author_result = lr_diff_embedding_result[0][0]\n",
    "lr_pv_dm_per_author_result = lr_diff_embedding_result[1][0]\n",
    "lr_pv_dbow_per_author_result = lr_diff_embedding_result[2][0]\n",
    "lr_per_author.append(lr_lsa_per_author_result)\n",
    "lr_per_author.append(lr_pv_dm_per_author_result)\n",
    "lr_per_author.append(lr_pv_dbow_per_author_result)\n",
    "\n",
    "svm_per_author = []\n",
    "svm_lsa_per_author_result = lr_diff_embedding_result[0][0]\n",
    "svm_pv_dm_per_author_result = svm_diff_embedding_result[1][0]\n",
    "svm_pv_dbow_per_author_result = svm_diff_embedding_result[2][0]\n",
    "svm_per_author.append(svm_lsa_per_author_result)\n",
    "svm_per_author.append(svm_pv_dm_per_author_result)\n",
    "svm_per_author.append(svm_pv_dbow_per_author_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T05:09:37.213143Z",
     "start_time": "2019-01-13T05:09:36.165053Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------- plot f1 score w.r.t each name group on different embedding -------------- #\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "#--------------   logistic regression --------------------------#\n",
    "# process result into np array\n",
    "logistic_regression_result = np.array(lr_per_author)\n",
    "name_group = np.array(allname)\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "for emb_type, result in zip(pp_textual, logistic_regression_result):\n",
    "    emb_type = \"Concatenate (\"+emb_type+\", n2v)\"\n",
    "    plt.xticks(range(len(result)), name_group)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "    plt.plot(result, label=emb_type)\n",
    "ax.autoscale_view()\n",
    "plt.legend()\n",
    "plt.title('F1 for different embedding method in logistic regression')\n",
    "plt.xlabel('Name group')\n",
    "plt.ylabel('marco f1 score')\n",
    "# plt.savefig('diff_combined_embedding_sample=3m_clf=logistic regression_threshold=100.eps', format='eps', dpi=300)\n",
    "\n",
    "#--------------   svm  -------------- --------------------------#\n",
    "# process result into np array\n",
    "svm_result = np.array(svm_per_author)\n",
    "name_group = np.array(allname)\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "for emb_type, result in zip(pp_textual, svm_result):\n",
    "    emb_type = \"Concatenate (\"+emb_type+\", n2v)\"\n",
    "    plt.xticks(range(len(result)), name_group)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "    plt.plot(result, label=emb_type)\n",
    "ax.autoscale_view()\n",
    "plt.legend()\n",
    "plt.title('F1 for different embedding method in svm')\n",
    "plt.xlabel('Name group')\n",
    "plt.ylabel('marco f1 score')\n",
    "# plt.savefig('diff_combined_embedding_sample=3m_clf=svm_threshold=100.eps', format='eps', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T05:03:42.584723Z",
     "start_time": "2019-01-13T05:03:42.577526Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------- plot f1 score w.r.t threshold change on different embedding ------------- #\n",
    "print(pp_textual)\n",
    "print(lr_diff_embedding_result)\n",
    "print(svm_diff_embedding_result)\n",
    "print(threshold_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T10:51:34.139679Z",
     "start_time": "2019-01-11T10:51:32.918514Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------- plot f1 score w.r.t threshold change on different embedding ------------- #\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "threshold_change = np.array(threshold_change)\n",
    "#--------------   logistic regression --------------------------#\n",
    "# process result into np array\n",
    "logistic_regression_result = np.array(lr_diff_embedding_result)\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "for emb_type, result in zip(pp_textual, logistic_regression_result):\n",
    "    emb_type = \"Concatenate (\"+emb_type+\", n2v)\"\n",
    "    plt.plot(threshold_change, result, label=emb_type)\n",
    "plt.legend()\n",
    "plt.title('Average f1 for different embedding method in logistic regression')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('marco f1 score')\n",
    "# plt.savefig('diff_combined_embedding_sample=3m_clf=logistic regression.eps', format='eps', dpi=300)\n",
    "\n",
    "\n",
    "# -------------------- svm -------------------------------------#\n",
    "svm_result = np.array(svm_diff_embedding_result)\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "for emb_type, result in zip(pp_textual, svm_result):\n",
    "    emb_type = \"Concatenate (\"+emb_type+\", n2v)\"\n",
    "    plt.plot(threshold_change, result, label=emb_type)\n",
    "plt.legend()\n",
    "plt.title('Average f1 for different embedding in SVM')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('marco f1 score')\n",
    "# plt.savefig('diff_combined_embedding_sample=3m_clf=SVM.eps', format='eps', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
