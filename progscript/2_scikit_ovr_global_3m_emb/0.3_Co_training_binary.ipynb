{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-11T03:27:06.956790Z",
     "start_time": "2019-02-11T03:26:47.312843Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "\n",
    "# warnings.filterwarnings('error')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import com_func\n",
    "\n",
    "#----- threshold for selecting set of name group -----------#\n",
    "threshold_select_name_group = 100\n",
    "#----- threshold for selecting min sample in name group ----#\n",
    "threshold_lower = 100\n",
    "threshold_upper = 110\n",
    "\n",
    "apply_threshold_to_sample = True\n",
    "\n",
    "pp_textual = [\"lsa\", \"pv_dm\", \"pv_dbow\"]\n",
    "pp_citation = \"n2v\"\n",
    "\n",
    "Dataset = \"pubmed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-10T04:44:37.870549Z",
     "start_time": "2019-02-10T04:44:08.665424Z"
    },
    "code_folding": [
     9,
     26,
     40,
     73,
     140,
     222,
     244,
     251,
     280
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create co training classifier\n",
    "class Co_training_clf(object):\n",
    "    \n",
    "    import copy\n",
    "    \n",
    "    def __init__(self, clf1, clf2=None, p=1, n=1, k=30, u = 75):\n",
    "        \n",
    "        self.clf1 = clf1\n",
    "        # assume co_training on one classifier\n",
    "        if clf2 == None:\n",
    "            self.clf2 = self.copy.deepcopy(clf1)\n",
    "        else:\n",
    "            self.clf2 = clf2\n",
    "        # take p example from most confidently positive labels to example\n",
    "        self.p = p\n",
    "        # take n example from most confidently negative label to example\n",
    "        self.n = n\n",
    "        # number of iteration\n",
    "        self.k = k\n",
    "        # size of pool of unlabeled samples\n",
    "        self.u = u\n",
    "\n",
    "    def init_L_U_U_prime(self, labels):\n",
    "        # index of the samples that are initially labeled\n",
    "        L = [i for i, label_i in enumerate(labels) if label_i != -1]\n",
    "        # index of unlabeled samples\n",
    "        U = [i for i, label_i in enumerate(labels) if label_i == -1]\n",
    "        print(\"Initial L size: \", len(L))\n",
    "        print(\"Initial U size: \", len(U))\n",
    "        # random drawing sample from U\n",
    "        random.shuffle(U)\n",
    "        U_prime = U[-min(len(U), self.u):]\n",
    "        # remove the samples in U_prime from U\n",
    "        U = U[:-len(U_prime)]\n",
    "        return L, U, U_prime\n",
    "\n",
    "    def label_p_n_samples(self, proba, rank):\n",
    "        U_prime_size = len(proba)\n",
    "        print(U_prime_size)\n",
    "        p, n = [], []\n",
    "        for label, conf_measure in enumerate(rank):\n",
    "            # 0 positive sample\n",
    "            if label==0:\n",
    "                index = 0\n",
    "                while(len(p) < self.p):\n",
    "                    max_conf_sample_index = conf_measure[index]\n",
    "                    # ---- if positive predict proba is more than 50% ------- #\n",
    "                    if (proba[max_conf_sample_index][label] > 0.5):\n",
    "                        print('P: ', max_conf_sample_index, \" : \", proba[max_conf_sample_index])\n",
    "                        p.append(max_conf_sample_index)\n",
    "                    index +=1\n",
    "                    if (index>=U_prime_size):\n",
    "                        break\n",
    "            # 1 negative sample\n",
    "            elif label == 1:\n",
    "                index = 0\n",
    "                while(len(n) < self.n):\n",
    "                    max_conf_sample_index = conf_measure[index]\n",
    "                    # ---- if negative predict proba is more than 50% ------- #\n",
    "                    if (proba[max_conf_sample_index][label] > 0.5):\n",
    "                        print('N: ', max_conf_sample_index, \" : \", proba[max_conf_sample_index])\n",
    "                        n.append(max_conf_sample_index)\n",
    "                    index +=1\n",
    "                    if (index>=U_prime_size):\n",
    "                        break\n",
    "            else:\n",
    "                print(\"Class label error\")\n",
    "        return p, n\n",
    "\n",
    "    def get_self_labeled_sample(self):\n",
    "        '''\n",
    "        return:\n",
    "            self-labeled new positive, self-labeled new negative (Index)\n",
    "        '''\n",
    "        \n",
    "        return self.new_labeled_pos, self.new_labeled_neg\n",
    "\n",
    "    def plot_co_training_process(self, iterCount, data, label, new_pos_idx, new_neg_idx, plotSavingPath, name):\n",
    "        if not os.path.exists(plotSavingPath):\n",
    "            os.makedirs(plotSavingPath)\n",
    "        # split self_labeled samples from labeled samples\n",
    "        self_labeled_pos_idx, self_labeled_neg_idx = ([] for i in range(2))\n",
    "        if new_pos_idx:\n",
    "            self_labeled_pos_idx = [i for i, e in enumerate(list(data.index)) if e in new_pos_idx]\n",
    "        if new_neg_idx:\n",
    "            self_labeled_neg_idx = [i for i, e in enumerate(list(data.index)) if e in new_neg_idx]\n",
    "        self_labeled_sample_idx = self_labeled_pos_idx+self_labeled_neg_idx\n",
    "        # apply PCA on input data\n",
    "        pca = PCA(n_components=2)\n",
    "        pca_transformed = pca.fit_transform(X=data)\n",
    "        pca_one = pca_transformed[:,0]\n",
    "        pca_two = pca_transformed[:,1]\n",
    "        # plot the result\n",
    "        fig, ax = plt.subplots(figsize=(9,7))\n",
    "        for author in np.unique(label):\n",
    "            ix = np.where(label == author)\n",
    "            ax.scatter(pca_one[ix], pca_two[ix], cmap='viridis', label = author, s = 50, alpha = 0.5)\n",
    "        # mark self labeled result\n",
    "        temp = ax.scatter(pca_one[self_labeled_sample_idx], pca_two[self_labeled_sample_idx], edgecolor='black', linewidth='3', s=50)\n",
    "        temp.set_facecolor(\"none\")\n",
    "        temp.set_label(\"self-labeled\")\n",
    "        legend = ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        plt.title('Co-training iteration: '+ str(iterCount), fontsize=14)\n",
    "        plt.xlabel(\"PCA one\",fontsize=14)\n",
    "        plt.ylabel(\"PCA two\",fontsize=14)\n",
    "        plt.savefig((plotSavingPath+name+\"_PCA_i-\"+str(iterCount)+\".png\").encode('utf-8'), dpi=100, bbox_extra_artists=(legend,), bbox_inches='tight')\n",
    "        plt.close(\"all\")\n",
    "        # plt.show()\n",
    "        \n",
    "\n",
    "    def fit(self, dataView1, dataView2, labels, dv1_test, dv2_test, label_test, plot_save_name=None, plot_save_path=None):\n",
    "        # index of positive labeled samples\n",
    "        self.new_labeled_pos = []\n",
    "        # index of negative labeled samples\n",
    "        self.new_labeled_neg = []\n",
    "        # when fit co-train, we collect f1 on test samples wrt each iteration\n",
    "        self.f1_on_test_dv1 = []\n",
    "        self.f1_on_test_dv2 = []\n",
    "        \n",
    "        labels = np.asarray(labels, dtype='int32')\n",
    "        print(\"P value: \", self.p, \" N value: \", self.n)\n",
    "        \n",
    "        L, U, U_prime = self.init_L_U_U_prime(labels)\n",
    "        \n",
    "        iterCount = 0\n",
    "        #loop until we have assigned labels to every sample in U and U_prime or we hit our iteration break condition\n",
    "        while iterCount < self.k and U_prime:\n",
    "            # print(\"step\",iterCount, \" L: \",L)\n",
    "            # print(\"step\",iterCount, \" U_prime: \",U_prime)\n",
    "            iter_train_d1 = dataView1.iloc[L]\n",
    "            iter_train_d2 = dataView2.iloc[L]\n",
    "            iter_train_label = labels[L]\n",
    "            iter_clf1 = self.copy.deepcopy(self.clf1) \n",
    "            iter_clf2 = self.copy.deepcopy(self.clf2)\n",
    "            # print(iter_train_label)\n",
    "            # ----------- plot the co-training process -------------- #\n",
    "            if plot_save_name != None:\n",
    "                last_iter_labeled_pos_idx = []\n",
    "                last_iter_labeled_neg_idx = []\n",
    "                # start on second iteration\n",
    "                if iterCount != 0:\n",
    "                    last_iter_labeled_pos_idx = self.new_labeled_pos[-1]\n",
    "                    last_iter_labeled_neg_idx = self.new_labeled_neg[-1]\n",
    "                # ----- save pca reduced plot for dv1 ------ #\n",
    "                plot_save_dv1_name = plot_save_name+\"_dv1\"\n",
    "                self.plot_co_training_process(iterCount, iter_train_d1, iter_train_label, last_iter_labeled_pos_idx,\n",
    "                                              last_iter_labeled_neg_idx, plot_save_path, plot_save_dv1_name)\n",
    "                # ----- dv2 -------- #\n",
    "                plot_save_dv2_name = plot_save_name+\"_dv2\"\n",
    "                self.plot_co_training_process(iterCount, iter_train_d2, iter_train_label, last_iter_labeled_pos_idx,\n",
    "                                              last_iter_labeled_neg_idx, plot_save_path, plot_save_dv2_name)\n",
    "            \n",
    "            iter_clf1.fit(iter_train_d1, iter_train_label)\n",
    "            iter_clf2.fit(iter_train_d2, iter_train_label)\n",
    "            # --------- test error on test data --------------------- #\n",
    "            # make prediction on test data\n",
    "            y1 = iter_clf1.predict(dv1_test)\n",
    "            y2 = iter_clf2.predict(dv2_test)\n",
    "            # f1 score on each iteration\n",
    "            f1_dv1 = f1_score(label_test, y1, average='macro')\n",
    "            f1_dv2 = f1_score(label_test, y2, average='macro')\n",
    "            # collect f1 for current iteration\n",
    "            self.f1_on_test_dv1.append(f1_dv1)\n",
    "            self.f1_on_test_dv2.append(f1_dv2)\n",
    "            # ---------- get U_prime sample to be label at ---------- #\n",
    "            iter_labeling_d1 = dataView1.iloc[U_prime]\n",
    "            iter_labeling_d2 = dataView2.iloc[U_prime]\n",
    "            # rank class probabilities for unlabeled sample for it's confidence measure\n",
    "            dv1_proba = iter_clf1.predict_proba(iter_labeling_d1)\n",
    "            dv2_proba = iter_clf2.predict_proba(iter_labeling_d2)\n",
    "            dv1_proba_rank = []\n",
    "            dv2_proba_rank = []\n",
    "            # proba1_rank[i] is label i's confidence measure\n",
    "            for class_proba in dv1_proba.T:\n",
    "                dv1_proba_rank.append((-class_proba).argsort())\n",
    "            for class_proba in dv2_proba.T:\n",
    "                dv2_proba_rank.append((-class_proba).argsort())\n",
    "            # print(dv1_proba)\n",
    "            # print(dv1_proba_rank)\n",
    "            # print(dv2_proba)\n",
    "            # print(dv2_proba_rank)\n",
    "            # h1 classifier\n",
    "            p1,n1 = self.label_p_n_samples(dv1_proba, dv1_proba_rank)\n",
    "            # h2 classifier\n",
    "            p2,n2 = self.label_p_n_samples(dv2_proba, dv2_proba_rank)\n",
    "            roundP = set(p1+p2)\n",
    "            roundN = set(n1+n2)\n",
    "            print(\"P: \", len(roundP), \" N: \", len(roundN))\n",
    "            print(roundP, roundN)\n",
    "            # auto label the samples and remove it from U_prime\n",
    "            auto_labeled_pos_idx = [U_prime[x] for x in roundP]\n",
    "            auto_labeled_neg_idx = [U_prime[x] for x in roundN]\n",
    "            auto_labeled_samples_idx = auto_labeled_pos_idx+auto_labeled_neg_idx\n",
    "            # ---------- collect index of auto_labeled_samples ------------ #\n",
    "            self.new_labeled_pos.append(auto_labeled_pos_idx)\n",
    "            self.new_labeled_neg.append(auto_labeled_neg_idx)\n",
    "            \n",
    "            labels[auto_labeled_pos_idx] = 0\n",
    "            labels[auto_labeled_neg_idx] = 1\n",
    "            # extend the labeled sample\n",
    "            L.extend(auto_labeled_pos_idx)\n",
    "            L.extend(auto_labeled_neg_idx)\n",
    "            # remove the labeled sample from U_prime\n",
    "            U_prime = [x for x in U_prime if x not in auto_labeled_samples_idx]\n",
    "            #print(U_prime)\n",
    "            # randomly choice 2p+2n examples from u to replenish u_prime\n",
    "            replenishItem = U[-(2*self.p+2*self.n):]\n",
    "            U_prime.extend(replenishItem)\n",
    "            U = U[:-len(replenishItem)]\n",
    "            iterCount +=1\n",
    "            \n",
    "        print(\"Total Labeled number: \", len(L), \" Still unlabeled number: \", len(U_prime))\n",
    "        # final train\n",
    "        newtrain_d1 = dataView1.iloc[L]\n",
    "        newtrain_d2 = dataView2.iloc[L]\n",
    "        self.clf1.fit(newtrain_d1, labels[L])\n",
    "        self.clf2.fit(newtrain_d2, labels[L])\n",
    "        # ------ save f1 vs number of iteration plot ------- #\n",
    "        if plot_save_name != None:\n",
    "            default_text_based = [self.f1_on_test_dv1[0]] * iterCount\n",
    "            default_citation_based = [self.f1_on_test_dv2[0]] * iterCount\n",
    "            default_step = np.arange(0,iterCount)\n",
    "            co_train_text_based = self.f1_on_test_dv1[1:]\n",
    "            co_train_citation_based = self.f1_on_test_dv2[1:]\n",
    "            co_training_step = np.arange(1,iterCount)\n",
    "\n",
    "            fig = plt.figure()\n",
    "            ax = plt.axes()\n",
    "            plt.plot(default_step, default_text_based, linestyle='dashed', label=\"Text based default\")\n",
    "            plt.plot(default_step, default_citation_based, linestyle='dashdot', label=\"Citation based default\")\n",
    "            plt.plot(co_training_step, co_train_text_based, linestyle='solid', marker = \"*\", label=\"Text based\")\n",
    "            plt.plot(co_training_step, co_train_citation_based, linestyle='dotted', marker = \"+\", label=\"Citation based\")\n",
    "            ax.autoscale_view()\n",
    "            legend = ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.2), ncol=2)\n",
    "            plt.xlabel('Co-Training Iterations')\n",
    "            plt.ylabel('F1 score')\n",
    "            plt.savefig((plot_save_path+plot_save_name+\"_diff_iter_f1.png\"), dpi=300, bbox_extra_artists=(legend,), bbox_inches='tight')\n",
    "            # plt.show()\n",
    "            plt.close(\"all\")\n",
    "    \n",
    "    def supports_proba(self, clf, x):\n",
    "        try:\n",
    "            clf.predict_proba([x])\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def predict(self, dataView1, dataView2):\n",
    "        y1 = self.clf1.predict(dataView1)\n",
    "        y2 = self.clf2.predict(dataView2)\n",
    "        proba_supported = self.supports_proba(self.clf1, dataView1.iloc[0]) and self.supports_proba(self.clf2, dataView2.iloc[0])\n",
    "        #fill pred with -1 so we can identify the samples in which sample classifiers failed to agree\n",
    "        y_pred = np.asarray([-1] * dataView1.shape[0])\n",
    "        for i, (y1_i, y2_i) in enumerate(zip(y1, y2)):\n",
    "            # if both agree on label\n",
    "            if y1_i == y2_i:\n",
    "                y_pred[i] = y1_i\n",
    "            # if disagree on label, times probability together, choice the class have higher probabilities\n",
    "            elif proba_supported:\n",
    "                y1_probas = self.clf1.predict_proba([dataView1.iloc[i]])[0]\n",
    "                y2_probas = self.clf2.predict_proba([dataView2.iloc[i]])[0]\n",
    "                print(\"y1 disagree on\",i, \" Proba: \",y1_probas)\n",
    "                print(\"y2 not aggreed on \",i, \"Proba: \", y2_probas)\n",
    "                prod_y_probas = [proba_y1 * proba_y2 for (proba_y1, proba_y2) in zip(y1_probas, y2_probas)]\n",
    "                print(\"product probas:\",prod_y_probas)\n",
    "                y_pred[i] = prod_y_probas.index(max(prod_y_probas))\n",
    "                print(\"result\",y_pred[i])\n",
    "            else:\n",
    "                #the classifiers disagree and don't support probability, so we guess\n",
    "                warnings.warn(\"classifiers disagree with label, result may not accurate\")\n",
    "                print(\"sample at: \", i, \" c1: \", y1_i, \" c2: \", y2_i)\n",
    "                y_pred[i] = random.randint(0, 1)\n",
    "        #check if predict works\n",
    "        assert not (-1 in y_pred)\n",
    "        return y_pred\n",
    "\n",
    "    def predict_proba(self, dataView1, dataView2):\n",
    "        # the predicted probabilities is simply a product (*) of probabilities given from each classifier trained\n",
    "        y1_probas = self.clf1.predict_proba(dataView1)\n",
    "        y2_probas = self.clf2.predict_proba(dataView2)\n",
    "        \n",
    "        proba = (y1_probas*y2_probas)\n",
    "        return proba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-10T04:44:40.426296Z",
     "start_time": "2019-02-10T04:44:37.889536Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_score, recall_score,f1_score,accuracy_score\n",
    "import seaborn as sns\n",
    "\n",
    "# cross validation\n",
    "def k_fold_cv_co_train_binary(dataview1, dataview2, unlabeled_dv1, unlabeled_dv2, label,\n",
    "                              clf, k=10, plot_save_name=None, plot_save_path=None):\n",
    "    kf = StratifiedKFold(n_splits=k)\n",
    "    allTrueLabel = []\n",
    "    allPredLabel = []\n",
    "    \n",
    "    all_fold_statistic = []\n",
    "    fold = 0\n",
    "    \n",
    "    for train_index, test_index in kf.split(dataview1, label):\n",
    "        fold +=1\n",
    "        # print(\"TRAIN:\", train_index, \" \\n TEST:\", test_index)\n",
    "        # ---------------split train and test -------------------- #\n",
    "        dv1_train, dv1_test = dataview1.iloc[train_index], dataview1.iloc[test_index]\n",
    "        dv2_train, dv2_test = dataview2.iloc[train_index], dataview2.iloc[test_index]\n",
    "        _, label_test = label.iloc[train_index], label.iloc[test_index]\n",
    "        # -------------- add unlabeled to train ------------------ #\n",
    "        final_dv1 = pd.concat([dv1_train,unlabeled_dv1], ignore_index=True)\n",
    "        final_dv2 = pd.concat([dv2_train,unlabeled_dv2], ignore_index=True)\n",
    "        # ----------------extract label for training ---------------- #\n",
    "        label_train = final_dv1[\"label\"]\n",
    "        final_dv1.drop([\"label\", \"paperID\"], axis=1, inplace = True)\n",
    "        final_dv2.drop([\"label\", \"paperID\"], axis=1, inplace = True)\n",
    "        # ----------------- extract data for test ------------------------ #\n",
    "        dv1_test = dv1_test.drop([\"label\", \"paperID\"], axis=1)\n",
    "        dv2_test = dv2_test.drop([\"label\", \"paperID\"], axis=1)\n",
    "        # -------------- train binary co-training ------------------- #\n",
    "        per_fold_clf = copy.deepcopy(clf)\n",
    "        detailed_plot_path = plot_save_path+plot_save_name+\"/fold\"+str(fold)+\"/\"\n",
    "        per_fold_clf.fit(final_dv1, final_dv2, label_train, dv1_test, dv2_test, label_test, plot_save_name, detailed_plot_path)\n",
    "        # -------------- get self-labeled sample index -------------- #\n",
    "        self_labeled_pos_index, self_labeled_neg_index = per_fold_clf.get_self_labeled_sample()\n",
    "        self_labeled_pos_index = [j for i in self_labeled_pos_index for j in i]\n",
    "        self_labeled_neg_index = [j for i in self_labeled_neg_index for j in i]\n",
    "        self_labeled_sample_size = len(self_labeled_pos_index)+len(self_labeled_neg_index)\n",
    "        self_labeled_pos_sample = final_dv1.iloc[self_labeled_pos_index]\n",
    "        self_labeled_neg_sample = final_dv1.iloc[self_labeled_neg_index]\n",
    "        print(\"Self labeled pos size: \", len(self_labeled_pos_index))\n",
    "        print(\"Self labeled neg size: \", len(self_labeled_neg_index))\n",
    "        # get predicted label\n",
    "        co_lr_label_predict = per_fold_clf.predict(dv1_test, dv2_test)\n",
    "        allTrueLabel.extend(label_test[\"label\"].values.tolist())\n",
    "        allPredLabel.extend(co_lr_label_predict)\n",
    "        # collect per fold statistic\n",
    "        curr_fold_statistic = {'author': plot_save_name, 'fold':fold, 'train_size': dv1_train.shape[0], \n",
    "                               'self_labeled_train': self_labeled_sample_size, 'test_size': dv1_test.shape[0],\n",
    "                               'f1': f1_score(label_test[\"label\"].values.tolist(), co_lr_label_predict,average='macro')}\n",
    "        all_fold_statistic.append(curr_fold_statistic)\n",
    "        # print(allTrueLabel)\n",
    "        # print(allPredLabel)\n",
    "        # print(\"True positive: {tp}, False positive: {fp}, False negative: {fn}, True negative: {tn}\"\n",
    "        # .format(tp=round_tp, fp=round_fp, fn=round_fn, tn=round_tn))\n",
    "\n",
    "    accuracy = accuracy_score(allTrueLabel, allPredLabel)\n",
    "    f1 = f1_score(allTrueLabel, allPredLabel,average='macro')\n",
    "    \n",
    "    print(metrics.classification_report(allTrueLabel, allPredLabel))\n",
    "    print(metrics.confusion_matrix(allTrueLabel, allPredLabel).ravel())\n",
    "    # ------------ plot result variance for each author ----------------------- #\n",
    "    all_fold_statistic_plot = pd.DataFrame(all_fold_statistic)\n",
    "    ax = sns.boxplot(x=\"author\", y=\"f1\", data=all_fold_statistic_plot)\n",
    "    ax = sns.swarmplot(x=\"author\", y=\"f1\", data=all_fold_statistic_plot, color=\".25\")\n",
    "    plt.savefig(plot_save_path+plot_save_name+\"/\"+plot_save_name+\"_result_variance.png\", dpi=100)\n",
    "    # plt.show()\n",
    "    \n",
    "    return accuracy, f1, all_fold_statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-11T01:34:53.902096Z",
     "start_time": "2019-02-10T04:44:40.456164Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "# loop through all files in directory add name to name list\n",
    "fileDir = \"../../Data/\"+Dataset+\"/canopies/\"\n",
    "listfiles = os.listdir(fileDir)\n",
    "\n",
    "co_lr_diff_embedding_result = []\n",
    "\n",
    "# ------------ view two citation is fix, so move out to save time ------- #\n",
    "# read viewtwo embedding\n",
    "print(\"Load citation embedding: \", pp_citation)\n",
    "viewtwo_citation_embedding = com_func.read_all_citation_embedding_sorted(emb_type = pp_citation)\n",
    "\n",
    "#---------------- load different embeddings for view one ---------------#\n",
    "for select_emb in pp_textual:\n",
    "    print(\"Load textual embedding: \", select_emb)\n",
    "    # read viewone embeddings\n",
    "    viewone_textual_emb = com_func.read_all_textual_embedding_sorted(emb_type=select_emb, training_size = \"3m\")\n",
    "    \n",
    "    # print(viewone_textual_emb[0])\n",
    "    # print(viewtwo_citation_embedding[0])\n",
    "    \n",
    "    threshold_change_all_co_lr_f1s = []\n",
    "    threshold_change = []\n",
    "    \n",
    "    # -------------- different threshold (step by 10) -----------------------#\n",
    "    for step_threshold in range(threshold_lower, threshold_upper, 10):\n",
    "        plot_save_path = \"../../plot/co_train_detail_plots/binary_sample=3m/\"+select_emb+\"/threshold=\"+str(step_threshold)+\"/\"\n",
    "        threshold_change.append(step_threshold)\n",
    "        # collect statistic to output\n",
    "        allname, positive_sample_size, negative_sample_size  = ([] for i in range(3))\n",
    "        all_labeled_count, unlabeled_count = ([] for i in range(2))\n",
    "\n",
    "        all_co_LR_accuracy, all_co_LR_f1 = ([] for i in range(2))\n",
    "        all_per_fold_f1_score_variance = []\n",
    "\n",
    "        total_selected_group = 0\n",
    "\n",
    "        # ------- different name group in all name group --------------------#\n",
    "        for file in listfiles:\n",
    "            per_name_per_fold_f1_variance = []\n",
    "            # group name\n",
    "            temp = file.split(\"_\")\n",
    "            name = temp[1]+\"_\"+temp[-1]\n",
    "            print(\"For name: \",name)\n",
    "            # read pid and aid from file\n",
    "            data = com_func.read_pid_aid(fileDir+file)\n",
    "            labeled_mask = data[\"authorID\"] != \"-1\"\n",
    "            labeled_data = data[labeled_mask]\n",
    "            unlabeled_mask = data[\"authorID\"] == \"-1\"\n",
    "            ublabeled_data = data[unlabeled_mask]\n",
    "            unlabeled_pid = ublabeled_data[\"paperID\"].tolist()\n",
    "            print(labeled_data.shape)\n",
    "            # ---------------- collect all labeled sample -------------------- #\n",
    "            # ---------------- if use all samples as negative --------------- #\n",
    "            all_labeled_samples = labeled_data[\"paperID\"].tolist()\n",
    "            authorCounter = com_func.select_productive_groups(labeled_data, threshold_select_name_group)\n",
    "            # if only have one class or no class pass the threshold, not applicable\n",
    "            if(len(authorCounter)==0) or (len(authorCounter)==1):\n",
    "                print(name,\" pass\")\n",
    "            else:\n",
    "                total_selected_group+= 1\n",
    "                # --------------for each name group---------------- #\n",
    "                if apply_threshold_to_sample == True:\n",
    "                    # ---------- only use sample pass threshold ------- #\n",
    "                    #-------- only select authors in name group are very productive (more than threshold)---------#\n",
    "                    labeled_data, author_list, _= com_func.only_select_productive_authors(labeled_data, step_threshold)\n",
    "                    # ----------------- if use filtered samples as negative  --------- #\n",
    "                    filtered_all_labeled_samples = labeled_data[\"paperID\"].tolist()\n",
    "                else:\n",
    "                    # ----------- use all sample in name group --------- #\n",
    "                    author_list = com_func.productive_authors_list(labeled_data, step_threshold)\n",
    "                    print(name, \" name group sample size: \",labeled_data.shape)\n",
    "                    \n",
    "                # -------------- extract all samples for name group -------------- #\n",
    "                # for each name group\n",
    "                # read in labeled data\n",
    "                labeled_viewone_textual = com_func.extract_sorted_embedding(viewone_textual_emb, labeled_data[\"paperID\"])\n",
    "                print(labeled_viewone_textual.shape)\n",
    "                labeled_viewtwo_citation = com_func.extract_sorted_embedding(viewtwo_citation_embedding, labeled_data[\"paperID\"])\n",
    "                print(labeled_viewtwo_citation.shape)\n",
    "                print(\"Labeled: \",len(labeled_viewone_textual), \" : \", len(labeled_viewtwo_citation))\n",
    "\n",
    "                # read in unlabeled data\n",
    "                unlabeled_viewone_textual = com_func.extract_unlabeled_embedding(viewone_textual_emb, unlabeled_pid)\n",
    "                print(unlabeled_viewone_textual.shape)\n",
    "                unlabeled_viewtwo_citation = com_func.extract_unlabeled_embedding(viewtwo_citation_embedding, unlabeled_pid)\n",
    "                print(unlabeled_viewtwo_citation.shape)\n",
    "                print(\"Unlabeled: \",len(unlabeled_viewone_textual), \" : \", len(unlabeled_viewtwo_citation))\n",
    "                \n",
    "                # remove samples that have no citation link from ublabeled data\n",
    "                noCitationPids_unlabeled = set(unlabeled_viewone_textual['paperID'])-set(unlabeled_viewtwo_citation['paperID'])\n",
    "                print(\"Unlabeled no citation link size: \", len(noCitationPids_unlabeled))\n",
    "                # process unlabeled data\n",
    "                unlabeled_dv1 = unlabeled_viewone_textual[~unlabeled_viewone_textual['paperID'].isin(noCitationPids_unlabeled)].reset_index(drop=True)\n",
    "                unlabeled_dv2 = unlabeled_viewtwo_citation\n",
    "                \n",
    "                # ---------------- shuffle the data ----------------- #\n",
    "                labeled_data = labeled_data.sample(frac=1).reset_index(drop=True)\n",
    "                # ------------------ alignment ---------------------- #\n",
    "                labeled_viewone_textual = pd.merge(labeled_data, labeled_viewone_textual, left_on=\"paperID\", right_on = [0], how = \"left\")\n",
    "                labeled_viewtwo_citation = pd.merge(labeled_data, labeled_viewtwo_citation, left_on=\"paperID\", right_on = [0], how = \"left\")\n",
    "                labeled_viewtwo_citation.fillna(0, inplace=True)\n",
    "                \n",
    "                print(labeled_viewone_textual.shape)\n",
    "                print(labeled_viewtwo_citation.shape)\n",
    "                print(unlabeled_dv1.shape)\n",
    "                print(unlabeled_dv2.shape)\n",
    "                counter = 0\n",
    "                # loop through each author\n",
    "                for author in author_list:\n",
    "                    all_labeled_count.append(len(labeled_data))\n",
    "                    unlabeled_count.append(len(unlabeled_dv1))\n",
    "                    author_name = name+'_'+str(counter)\n",
    "                    allname.append(author_name)\n",
    "                    print(author_name, \" : \", author)\n",
    "                    mask = labeled_data[\"authorID\"] == author\n",
    "                    temp = labeled_data[mask]\n",
    "                    positive_sample_pid = temp[\"paperID\"].tolist()\n",
    "                    negative_sample_pid = com_func.extractNegativeSample(positive_sample_pid, filtered_all_labeled_samples)\n",
    "                    \n",
    "                    # save number of positive and negative samples\n",
    "                    positive_sample_size.append(len(positive_sample_pid))\n",
    "                    negative_sample_size.append(len(negative_sample_pid))\n",
    "                    \n",
    "                    # ----------------- generate binary label ------------------ #\n",
    "                    # form positive and negative (negative class come from similar name group)\n",
    "                    all_authors = []\n",
    "                    all_authors.append(positive_sample_pid)\n",
    "                    all_authors.append(negative_sample_pid)\n",
    "                    appended_data = []\n",
    "                    for label, pid in enumerate(all_authors):\n",
    "                        # create df save one author data \n",
    "                        authordf = pd.DataFrame({\"paperID\":pid})\n",
    "                        authordf['label'] = label\n",
    "                        appended_data.append(authordf)\n",
    "                    label_pid = pd.concat(appended_data, axis=0,ignore_index=True)\n",
    "                    # ----------- alignment of label with input data ------------ #\n",
    "                    label_pid = pd.merge(labeled_viewone_textual[\"paperID\"].to_frame(), label_pid, on = \"paperID\")\n",
    "                    #------------- process data for k-fold cv ------------------- #\n",
    "                    # throw away some column for labeled data\n",
    "                    labeled_dv1 = labeled_viewone_textual.drop([\"authorID\", 0], axis=1)\n",
    "                    labeled_dv2 = labeled_viewtwo_citation.drop([\"authorID\", 0], axis=1)\n",
    "                    # merge label into data\n",
    "                    labeled_dv1 = pd.merge(labeled_dv1, label_pid, on = \"paperID\")\n",
    "                    labeled_dv2 = pd.merge(labeled_dv2, label_pid, on = \"paperID\")\n",
    "                    label = label_pid.drop([\"paperID\"], axis=1)\n",
    "                    # ----------- check the final inputs------------------ #\n",
    "                    # print(labeled_dv1.head())\n",
    "                    # print(unlabeled_dv1.head())\n",
    "                    # ------------ fit co-training model with k-fold ------------------------ #\n",
    "                    co_logistic_clf = Co_training_clf(clf1=LogisticRegression(solver= \"liblinear\"),p=1,n=1, k=30)\n",
    "                    co_lr_accuracy, co_lr_f1, author_per_fold_f1_score= k_fold_cv_co_train_binary(labeled_dv1, labeled_dv2, \n",
    "                                                                         unlabeled_dv1, unlabeled_dv2, label,\n",
    "                                                                         co_logistic_clf, 10, author_name, plot_save_path)\n",
    "                    # f1 variance on different fold for different author\n",
    "                    all_co_LR_accuracy.append(co_lr_accuracy)\n",
    "                    all_co_LR_f1.append(co_lr_f1)\n",
    "                    per_name_per_fold_f1_variance.extend(author_per_fold_f1_score)\n",
    "                    all_per_fold_f1_score_variance.extend(author_per_fold_f1_score)\n",
    "                    counter+=1\n",
    "                \n",
    "                # ---------- plot per name group classifier f1 variance -------------- #\n",
    "                per_name_per_fold_f1_variance_plot = pd.DataFrame(per_name_per_fold_f1_variance)\n",
    "                ax = sns.boxplot(x=\"author\", y=\"f1\", data=per_name_per_fold_f1_variance_plot)\n",
    "                ax = sns.swarmplot(x=\"author\", y=\"f1\", data=per_name_per_fold_f1_variance_plot, color=\".25\")\n",
    "                plt.savefig(plot_save_path+name+\"_group_result_variance.png\", dpi=300)\n",
    "        \n",
    "#         # write evaluation result to excel\n",
    "#         output = pd.DataFrame({'Author Name':allname, \"positive sample size\":positive_sample_size,\"negative sample size\":negative_sample_size, \n",
    "#                                \"labeled sample size\": all_labeled_count, \"unlabeled sample size\": unlabeled_count, \n",
    "#                                \"co_logisticRegression Accuracy\":all_co_LR_accuracy, \"co_logisticRegression F1\": all_co_LR_f1})\n",
    "#         savePath = \"../../result/\"+Dataset+\"/co_train_binary/\"\n",
    "#         filename = \"(Global emb sample 3m) viewone_textual=\"+select_emb+\"_viewtwo_citation=\"+pp_citation+\"_threshold=\"+str(step_threshold)+\"_namegroupcount=\"+str(total_selected_group)+\".csv\"\n",
    "#         com_func.write_csv_df(savePath, filename, output)\n",
    "#         print(\"Done\")\n",
    "\n",
    "        \n",
    "#         threshold_change_all_co_lr_f1s.append(all_co_LR_f1)\n",
    "        \n",
    "#     co_lr_diff_embedding_result.append(threshold_change_all_co_lr_f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-10T04:41:47.426Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from statistics import mean \n",
    "\n",
    "print(threshold_change_all_co_lr_f1s)\n",
    "print(co_lr_diff_embedding_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#         # --------------- plot overall result f1 variance --------------- #\n",
    "#         all_per_fold_f1_score_variance_plot = pd.DataFrame(all_per_fold_f1_score_variance)\n",
    "#         ax = sns.boxplot(x=\"author\", y=\"f1\", data=all_per_fold_f1_score_variance_plot)\n",
    "#         ax = sns.swarmplot(x=\"author\", y=\"f1\", data=all_per_fold_f1_score_variance_plot, color=\".25\")\n",
    "#         plt.savefig(plot_save_path+\"all_result_variance.png\", dpi=300)\n",
    "#         # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-23T03:52:46.851Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %whos\n",
    "del viewtwo_citation_embedding\n",
    "del viewone_textual_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
