{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train each author as classifier (Different textual embeddings)\n",
    "1. Either apply threshold to samples in each name group so each author classifier only trained with samples pass threshold\n",
    "\n",
    "2. Or not apply threshold to samples so each author classifier trained with all samples (include samples can't pass threshold) \n",
    "\n",
    "Example: k-kim name group have 1111 samples for 57 author.\n",
    "\n",
    "With threshold of 100:\n",
    "\n",
    "method 1 have 504 sample for training 3 author classifier.\n",
    "\n",
    "method 2 have 1111 sample for training 3 author classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-16T01:25:49.205614Z",
     "start_time": "2019-02-16T01:25:47.149188Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import com_func\n",
    "\n",
    "# parameters\n",
    "#----- threshold for selecting set of name group -----------#\n",
    "threshold_select_name_group = 100\n",
    "#----- threshold for selecting min sample in name group ----#\n",
    "threshold_lower = 100\n",
    "threshold_upper = 110\n",
    "\n",
    "pp_textual = [\"lsa\", \"pv_dm\", \"pv_dbow\"]\n",
    "apply_threshold_to_sample = True\n",
    "\n",
    "Dataset = \"pubmed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-16T01:26:38.829912Z",
     "start_time": "2019-02-16T01:26:23.368837Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load embedding:  pv_dbow\n",
      "Total textual vector records: 135796\n",
      "Vector dimension:  100\n",
      "For name:  j_read\n",
      "j_read  pass\n",
      "For name:  f_esteves\n",
      "f_esteves  pass\n",
      "For name:  c_miller\n",
      "c_miller  pass\n",
      "For name:  r_jha\n",
      "r_jha  pass\n",
      "For name:  a_lowe\n",
      "a_lowe  pass\n",
      "For name:  a_vega\n",
      "a_vega  pass\n",
      "For name:  k_smith\n",
      "k_smith  pass\n",
      "For name:  j_gordon\n",
      "j_gordon  pass\n",
      "For name:  s_liao\n",
      "s_liao  pass\n",
      "For name:  j_qian\n",
      "j_qian  pass\n",
      "For name:  s_bernardi\n",
      "s_bernardi  pass\n",
      "For name:  t_hill\n",
      "t_hill  pass\n",
      "For name:  s_schindler\n",
      "s_schindler  pass\n",
      "For name:  j_williams\n",
      "j_williams  pass\n",
      "For name:  s_jacobson\n",
      "s_jacobson  pass\n",
      "For name:  e_andrade\n",
      "e_andrade  pass\n",
      "For name:  t_santos\n",
      "t_santos  pass\n",
      "For name:  k_kim\n",
      "Total sample size before apply threshold:  1111\n",
      "Counter({'0000-0002-6929-5359': 211, '0000-0001-9498-284X': 154, '0000-0002-5878-8895': 139, '0000-0002-1864-3392': 92, '0000-0002-7045-8004': 57, '0000-0001-7896-6751': 57, '0000-0002-7991-9428': 55, '0000-0002-4010-1063': 45, '0000-0002-2186-3484': 28, '0000-0002-4899-1929': 25, '0000-0003-0487-4242': 24, '0000-0002-3642-1486': 22, '0000-0001-9965-3535': 17, '0000-0002-4168-757X': 17, '0000-0001-6525-3744': 14, '0000-0002-3897-0278': 14, '0000-0002-1181-5112': 12, '0000-0003-1447-9385': 11, '0000-0002-7305-8786': 11, '0000-0002-2655-7806': 10, '0000-0003-3466-5353': 9, '0000-0002-7359-663X': 8, '0000-0003-4600-8668': 6, '0000-0002-1382-7088': 5, '0000-0002-9505-4882': 5, '0000-0003-3667-9900': 4, '0000-0001-9714-6038': 4, '0000-0002-4760-0228': 3, '0000-0003-4188-7915': 3, '0000-0001-9454-0427': 3, '0000-0002-0333-6808': 3, '0000-0003-2134-4964': 3, '0000-0002-6658-047X': 3, '0000-0003-1273-379X': 3, '0000-0002-7047-3183': 3, '0000-0002-1814-9546': 3, '0000-0003-4812-6297': 2, '0000-0001-6597-578X': 2, '0000-0002-5285-9138': 2, '0000-0002-6796-7844': 2, '0000-0002-1130-8698': 2, '0000-0001-8518-8150': 2, '0000-0002-7103-924X': 2, '0000-0002-5407-0202': 1, '0000-0001-6220-8411': 1, '0000-0002-7440-6703': 1, '0000-0002-1603-7559': 1, '0000-0003-0257-1707': 1, '0000-0001-8532-6517': 1, '0000-0001-6626-316X': 1, '0000-0002-3246-9861': 1, '0000-0002-7207-4389': 1, '0000-0001-9682-9654': 1, '0000-0002-0196-3832': 1, '0000-0001-8063-6081': 1, '0000-0003-2037-3333': 1, '0000-0001-8872-6751': 1})\n",
      "Total author before apply threshoid:  57\n",
      "Total author after apply threshoid:  3\n",
      "Total sample size after apply threshold:  504\n",
      "Total missing sample:  0\n",
      "(504, 100)\n",
      "1\n",
      "(504, 100)\n",
      "k_kim_0  :  0000-0002-6929-5359\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       211\n",
      "           1       0.99      0.99      0.99       293\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       504\n",
      "   macro avg       0.99      0.99      0.99       504\n",
      "weighted avg       0.99      0.99      0.99       504\n",
      "\n",
      "[209   2   2 291]\n",
      "LR Accuracy:  0.9920634920634921\n",
      "LR F1:  0.9918476942238326\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98       211\n",
      "           1       0.99      0.97      0.98       293\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       504\n",
      "   macro avg       0.98      0.98      0.98       504\n",
      "weighted avg       0.98      0.98      0.98       504\n",
      "\n",
      "[209   2   8 285]\n",
      "svc Accuracy:  0.9801587301587301\n",
      "svc F1:  0.9796970673541734\n",
      "k_kim_1  :  0000-0002-5878-8895\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94       139\n",
      "           1       0.98      0.98      0.98       365\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       504\n",
      "   macro avg       0.96      0.96      0.96       504\n",
      "weighted avg       0.97      0.97      0.97       504\n",
      "\n",
      "[131   8   8 357]\n",
      "LR Accuracy:  0.9682539682539683\n",
      "LR F1:  0.9602641174731448\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94       139\n",
      "           1       0.98      0.98      0.98       365\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       504\n",
      "   macro avg       0.96      0.96      0.96       504\n",
      "weighted avg       0.96      0.96      0.96       504\n",
      "\n",
      "[130   9   9 356]\n",
      "svc Accuracy:  0.9642857142857143\n",
      "svc F1:  0.9552971321572878\n",
      "k_kim_2  :  0000-0001-9498-284X\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.91       154\n",
      "           1       0.96      0.96      0.96       350\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       504\n",
      "   macro avg       0.94      0.94      0.94       504\n",
      "weighted avg       0.95      0.95      0.95       504\n",
      "\n",
      "[140  14  13 337]\n",
      "LR Accuracy:  0.9464285714285714\n",
      "LR F1:  0.9367678560641615\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.91      0.89       154\n",
      "           1       0.96      0.94      0.95       350\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       504\n",
      "   macro avg       0.92      0.93      0.92       504\n",
      "weighted avg       0.93      0.93      0.93       504\n",
      "\n",
      "[140  14  20 330]\n",
      "svc Accuracy:  0.9325396825396826\n",
      "svc F1:  0.9213641953780356\n"
     ]
    }
   ],
   "source": [
    "# load the file\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from statistics import mean \n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "fileDir = \"../../Data/\"+Dataset+\"/canopies_labeled/\"\n",
    "listfiles = os.listdir(fileDir)\n",
    "\n",
    "lr_diff_emb_f1_result = []\n",
    "svm_diff_emb_f1_result = []\n",
    "\n",
    "\n",
    "for select_emb in pp_textual:\n",
    "    print(\"Load embedding: \", select_emb)\n",
    "    # read pretrained embeddings\n",
    "    emb_all_sample, emb_all_pid = com_func.read_textual_embedding(emb_type=select_emb, training_size = \"3m\")\n",
    "    \n",
    "    threshold_change_all_lr_f1s = []\n",
    "    threshold_change_all_svm_f1s = []\n",
    "    threshold_change = []\n",
    "    \n",
    "    # -------------- different threshold (step by 10) -----------------------#\n",
    "    for step_threshold in range(threshold_lower, threshold_upper, 10):\n",
    "        threshold_change.append(step_threshold)\n",
    "        # collect statistic to output\n",
    "        allname, positive_sample_size, negative_sample_size, total_sample_size= ([] for i in range(4))\n",
    "        all_LR_accuracy, all_LR_f1, all_svcLinear_accuracy, all_svcLinear_f1 = ([] for i in range(4))\n",
    "        \n",
    "        total_selected_group = 0\n",
    "\n",
    "        # read all file in labeled group\n",
    "        for file in listfiles:\n",
    "            # group name\n",
    "            temp = file.split(\"_\")\n",
    "            name = temp[1]+\"_\"+temp[-1]\n",
    "            print(\"For name: \",name)\n",
    "            # read needed content in labeled file\n",
    "            labeled_data = com_func.read_pid_aid(fileDir+file)\n",
    "            # ---------------- collect all labeled sample -------------------- #\n",
    "            # ---------------- if use all samples as negative --------------- #\n",
    "            all_labeled_samples = labeled_data[\"paperID\"].tolist()\n",
    "            #----------- select name group contain productive author------------------------------------#\n",
    "            #----------- (contain pair of author write more than 100 papers) ---------------------------#\n",
    "            authorCounter = com_func.select_productive_groups(labeled_data, threshold_select_name_group)\n",
    "            # if only have one class or no class pass the threshold, not applicable\n",
    "            if(len(authorCounter)==0) or (len(authorCounter)==1):\n",
    "                print(name, \" pass\")\n",
    "            else:\n",
    "                total_selected_group+= 1\n",
    "                # --------------for each name group---------------- #\n",
    "                if apply_threshold_to_sample == True:\n",
    "                    # ---------- only use sample pass threshold ------- #\n",
    "                    #-------- only select authors in name group are very productive (more than threshold)---------#\n",
    "                    labeled_data, author_list,_= com_func.only_select_productive_authors(labeled_data, step_threshold)\n",
    "                    # ----------------- if use filtered samples as negative --------- #\n",
    "                    filtered_all_labeled_samples = labeled_data[\"paperID\"].tolist()\n",
    "                else:\n",
    "                    # ----------- use all sample in name group --------- #\n",
    "                    author_list = com_func.productive_authors_list(labeled_data, step_threshold)\n",
    "                    print(name, \" name group sample size: \",labeled_data.shape)\n",
    "                # shuffle the data\n",
    "                labeled_data = labeled_data.sample(frac=1).reset_index(drop=True)\n",
    "                # list of different data field\n",
    "                part_collection = []\n",
    "                # select feature wanted to fit to clustering/classification algorithm\n",
    "                data_textual = com_func.extract_embedding(emb_all_sample, emb_all_pid, labeled_data[\"paperID\"])\n",
    "                print(data_textual.shape)\n",
    "                part_collection.append(data_textual)\n",
    "                # merge different part of data data together by concatenate it all together\n",
    "                combinedata = com_func.merge_data_parts(part_collection)\n",
    "                print(combinedata.shape)\n",
    "                # ------------- index tracker -------------------- #\n",
    "                group_pid = labeled_data[\"paperID\"].to_frame()\n",
    "                counter = 0\n",
    "                # loop through each author have label, one vs rest\n",
    "                for author in author_list:\n",
    "                    total_sample_size.append(len(labeled_data))\n",
    "                    author_name = name+'_'+str(counter)\n",
    "                    allname.append(author_name)\n",
    "                    print(author_name, \" : \", author)\n",
    "                    mask = labeled_data[\"authorID\"] == author\n",
    "                    temp = labeled_data[mask]\n",
    "                    positive_sample_pid = temp[\"paperID\"].tolist()\n",
    "                    negative_sample_pid = com_func.extractNegativeSample(positive_sample_pid, filtered_all_labeled_samples)\n",
    "                    # append to statistic collection\n",
    "                    positive_sample_size.append(len(positive_sample_pid))\n",
    "                    negative_sample_size.append(len(negative_sample_pid))\n",
    "                    # form positive and negative (negative class come from similar name group)\n",
    "                    all_authors = []\n",
    "                    all_authors.append(positive_sample_pid)\n",
    "                    all_authors.append(negative_sample_pid)\n",
    "                    appended_data = []\n",
    "                    for label, pid in enumerate(all_authors):\n",
    "                        # create df save one author data \n",
    "                        authordf = pd.DataFrame({\"paperID\":pid})\n",
    "                        authordf['label'] = label\n",
    "                        appended_data.append(authordf)\n",
    "                    processed_data = pd.concat(appended_data, axis=0,ignore_index=True)\n",
    "\n",
    "                    # alignment \n",
    "                    processed_data = pd.merge(group_pid, processed_data, on=\"paperID\")\n",
    "\n",
    "                    # extract true label and it's corresponeding pid for matching\n",
    "                    label = processed_data[\"label\"]\n",
    "                    pid = processed_data[\"paperID\"]\n",
    "\n",
    "                    # using converted feature vector to train classifier\n",
    "                    # using logistic regression\n",
    "                    clf = LogisticRegression()\n",
    "                    LRaccuracy, LRmarcof1 = com_func.k_fold_cv(combinedata, label, clf, k=10)\n",
    "                    print(\"LR Accuracy: \",LRaccuracy)\n",
    "                    print(\"LR F1: \", LRmarcof1)\n",
    "                    all_LR_accuracy.append(LRaccuracy)\n",
    "                    all_LR_f1.append(LRmarcof1)\n",
    "                    # using SVM with linear kernal\n",
    "                    clf = SVC(kernel='linear')\n",
    "                    svcaccuracy, svcmarcof1 = com_func.k_fold_cv(combinedata, label, clf, k=10)\n",
    "                    print(\"svc Accuracy: \",svcaccuracy)\n",
    "                    print(\"svc F1: \", svcmarcof1)\n",
    "                    all_svcLinear_accuracy.append(svcaccuracy)\n",
    "                    all_svcLinear_f1.append(svcmarcof1)\n",
    "                    counter+=1\n",
    "                break\n",
    "#         # write evaluation result to excel\n",
    "#         output = pd.DataFrame({'Author ':allname, \"positive sample size\":positive_sample_size,\n",
    "#                                \"negative sample size\":negative_sample_size, \"Name group sample size\": total_sample_size,\n",
    "#                                 \"logistic regression accuracy\":all_LR_accuracy, \"logistic regression f1\": all_LR_f1,\n",
    "#                                \"svc(linear) accuracy\":all_svcLinear_accuracy, \"svc(linear) f1\": all_svcLinear_f1})\n",
    "#         savePath = \"../../result/\"+Dataset+\"/binary_global_emb_sample=3m/\"\n",
    "#         filename = \"(Global emb sample 3m) textual=\"+select_emb+\"_threshold=\"+str(step_threshold)+\"_namegroupcount=\"+str(total_selected_group)+\".csv\"\n",
    "#         com_func.write_csv_df(savePath, filename, output)\n",
    "#         print(\"Done\")\n",
    "        \n",
    "#         threshold_change_all_lr_f1s.append(all_LR_f1)\n",
    "#         threshold_change_all_svm_f1s.append(all_svcLinear_f1)\n",
    "        \n",
    "#     lr_diff_emb_f1_result.append(threshold_change_all_lr_f1s)\n",
    "#     svm_diff_emb_f1_result.append(threshold_change_all_lr_f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-13T01:42:36.149Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(train_feature)\n",
    "print(mnb_diff_feature_average_f1_result)\n",
    "print(lr_diff_feature_average_f1_result)\n",
    "print(svm_diff_freature_average_f1_result)\n",
    "print(threshold_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "threshold_change = np.array(threshold_change)\n",
    "#--------------   logistic regression --------------------------#\n",
    "# process result into np array\n",
    "logistic_regression_result = np.array(combined_lr_result)\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "for emb_type, result in zip(combined_emb, logistic_regression_result):\n",
    "    plt.plot(threshold_change, result, label=emb_type)\n",
    "plt.legend()\n",
    "plt.title('Average f1 for different feature in logistic regression')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('marco f1 score')\n",
    "\n",
    "# plt.savefig('diff_embedding_sample=3m_clf=logistic regression.eps', format='eps', dpi=300)\n",
    "\n",
    "\n",
    "# -------------------- svm -------------------------------------#\n",
    "svm_result = np.array(combined_svm_result)\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "for emb_type, result in zip(combined_emb, svm_result):\n",
    "    plt.plot(threshold_change, result, label=emb_type)\n",
    "plt.legend()\n",
    "plt.title('Average f1 for different feature in SVM')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('marco f1 score')\n",
    "\n",
    "# plt.savefig('diff_embedding_sample=3m_clf=SVM.eps', format='eps', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
