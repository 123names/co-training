{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract from textual files that are useful\n",
    "\n",
    "Many of the data in raw files are useless, thus we need to extract data that are useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T23:36:45.940458Z",
     "start_time": "2019-02-17T23:35:50.394993Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "Total records: 4163772 records\n"
     ]
    }
   ],
   "source": [
    "# load the file\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "\n",
    "Dataset = \"pubmed\"\n",
    "\n",
    "fileDir = \"Data/\"+Dataset+\"/canopies/\"\n",
    "listfiles = os.listdir(fileDir)\n",
    "\n",
    "allpaper_need_extract = []\n",
    "\n",
    "for file in listfiles:\n",
    "    if not file.startswith('.'):\n",
    "        with open(fileDir+file, 'r', encoding = 'utf8') as f:\n",
    "            for line in f:\n",
    "                read_data = line.split(\"\\t\")\n",
    "                # some record's doi contain \\r or \\n character in which creating issue, since we do not use those, ignore it\n",
    "                if(len(read_data)==13 or len(read_data)==12):\n",
    "                    paper_detail = {\"paperID\": read_data[0], \"mesh\": read_data[8].lower().strip(),\n",
    "                                    \"keywords\": read_data[9].lower().strip()}\n",
    "                    allpaper_need_extract.append(paper_detail)\n",
    "                else:\n",
    "                    print(len(read_data))\n",
    "        f.close()\n",
    "        \n",
    "print(\"Total records:\",len(allpaper_need_extract), \"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T23:36:57.139290Z",
     "start_time": "2019-02-17T23:36:45.943599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique paper count:  3151504\n"
     ]
    }
   ],
   "source": [
    "# find unique paper id\n",
    "# sort paperID\n",
    "paperIDs = []\n",
    "for paper in allpaper_need_extract:\n",
    "    paperIDs.append(int(paper[\"paperID\"]))\n",
    "# the paper id is not unique, we need to extract text info with unique paper id \n",
    "paperIDs = sorted(set(paperIDs))\n",
    "num_need_extract = len(paperIDs)\n",
    "print(\"Total unique paper count: \",len(paperIDs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T23:36:57.181486Z",
     "start_time": "2019-02-17T23:36:57.142566Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28355772"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paperIDs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T00:19:13.009571Z",
     "start_time": "2019-02-17T23:45:15.171597Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reache point:  3\n",
      "8362786  :  8362787\n",
      "reache point:  8608573\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a7e2cd938739>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0mpaper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"paperID\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"title\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"abstract\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mpaperID_title_abstract\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                 \u001b[0mpaperIDs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaperIDs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpaperIDs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0;31m# remove paper that not in all dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# extract text info based on paperID and generate new smaller file with id, title, abstract, keyword, and mesh\n",
    "filePath = \"Data/\"+Dataset+\"/allAdditional/id_title_abstract.txt\"\n",
    "paperID_title_abstract = []\n",
    "num_fail_extract = 0\n",
    "with open(filePath, 'r', encoding = 'utf8') as f:\n",
    "    for line in f:\n",
    "        if(len(paperIDs)==0):\n",
    "            break\n",
    "        read_data = line.split(\"\\t\")\n",
    "        # if pid > allpid, pass\n",
    "        while (paperIDs[0]<=int(read_data[0])):\n",
    "            if(paperIDs[0]==int(read_data[0])):\n",
    "                if(len(paperID_title_abstract)%500000==0):\n",
    "                    print(\"reache point: \",read_data[0])\n",
    "                paper = {\"paperID\": read_data[0], \"title\": read_data[1], \"abstract\": read_data[2]}\n",
    "                paperID_title_abstract.append(paper)\n",
    "                paperIDs.remove(paperIDs[0])\n",
    "            elif (paperIDs[0]<int(read_data[0])):\n",
    "                # remove paper that not in all dataset\n",
    "                print(paperIDs[0], \" : \",read_data[0])\n",
    "                paperIDs.remove(paperIDs[0])\n",
    "                num_fail_extract+=1\n",
    "            if len(paperIDs)==0:\n",
    "                break\n",
    "                \n",
    "f.close()\n",
    "# this means some paper are missing from all text information\n",
    "print(\"Total extracted title and abstract info: \",len(paperID_title_abstract))\n",
    "print(\"Total unique paper need to extract: \",num_need_extract)\n",
    "print(\"Total paper fail to extract: \",num_fail_extract)\n",
    "print(paperID_title_abstract[-1][\"paperID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-18T00:19:13.011474Z",
     "start_time": "2019-02-17T23:45:20.460Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(paperID_title_abstract))\n",
    "print(num_need_extract)\n",
    "print(paperID_title_abstract[-1])\n",
    "print(allpaper_need_extract[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-01T04:09:36.954047Z",
     "start_time": "2018-12-01T04:09:17.024887Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Some recent papers not in old database do not have abstract and title, only pid and keyword+mesh\n",
    "Code below removed paper do not have abstract and title.\n",
    "'''\n",
    "# # write content to file\n",
    "# newfile = open(\"Data/\"+Dataset+\"/allAdditional/id_title_abstract_extracted.txt\", \"w\",encoding='utf8')\n",
    "# for paper in paperID_title_abstract:\n",
    "#     newfile.write((paper[\"paperID\"]+\"\\t\"+paper[\"title\"]+\"\\t\"+paper[\"abstract\"]).strip('\\n')+\"\\n\")\n",
    "# newfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract from canopies folder\n",
    "\n",
    "We have 4,163,772 records, but we only extract keyword and mesh. When we drop the duplicate items, we should have only unique paper(pid) and it's keywords and mesh\n",
    "\n",
    "But some of records are not consist with keyword and mesh even it's same paper (Different records gives different keyword and mesh). Thus we fix it by select the longer length keyword+mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T10:26:27.446209Z",
     "start_time": "2018-11-21T10:26:03.187427Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract keyword and mesh, drop duplicate\n",
    "keywords_mesh = pd.DataFrame(allpaper_need_extract).drop_duplicates()\n",
    "print(keywords_mesh.shape)\n",
    "# show some case failed\n",
    "idcol = keywords_mesh[\"paperID\"]\n",
    "duplicatedset = keywords_mesh[idcol.isin(idcol[idcol.duplicated()])].sort_values(\"paperID\")\n",
    "print(duplicatedset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T10:26:43.215768Z",
     "start_time": "2018-11-21T10:26:27.451178Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keep keywords + mesh length longer one\n",
    "unique_pid = sorted(set(duplicatedset[\"paperID\"]))\n",
    "remove_dup_idx = []\n",
    "for pid in unique_pid:\n",
    "    lenlist = []\n",
    "    idxlist = []\n",
    "    pidlist = []\n",
    "    for index, row in duplicatedset.iterrows():\n",
    "        if pid == row[\"paperID\"]:\n",
    "            item_char_length = len(row[\"keywords\"]+row[\"mesh\"])\n",
    "            lenlist.append(item_char_length)\n",
    "            idxlist.append(index)\n",
    "            pidlist.append(pid)\n",
    "    print(lenlist)\n",
    "    print(idxlist)\n",
    "    print(pidlist)\n",
    "    keep_idx = lenlist.index(max(lenlist))\n",
    "    del idxlist[keep_idx]\n",
    "    print(\" idx: \", idxlist)\n",
    "    remove_dup_idx.extend(idxlist)\n",
    "    \n",
    "print(sorted(remove_dup_idx))\n",
    "print(len(remove_dup_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T10:26:44.042829Z",
     "start_time": "2018-11-21T10:26:43.218048Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_dup_keywords_mesh = keywords_mesh.drop(remove_dup_idx)\n",
    "print(no_dup_keywords_mesh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T10:47:36.692779Z",
     "start_time": "2018-11-21T10:47:27.536959Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "title_abstract = pd.DataFrame(paperID_title_abstract)\n",
    "print(title_abstract.shape)\n",
    "# merge information together\n",
    "text_data = no_dup_keywords_mesh.merge(title_abstract, how='outer', left_on='paperID', right_on='paperID')\n",
    "text_data.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T10:47:45.368631Z",
     "start_time": "2018-11-21T10:47:45.361575Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_data.shape\n",
    "text_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T10:47:50.767397Z",
     "start_time": "2018-11-21T10:47:50.735281Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T10:48:08.129992Z",
     "start_time": "2018-11-21T10:48:08.098233Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T11:12:46.798620Z",
     "start_time": "2018-11-21T11:03:15.082935Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write content to file\n",
    "newf = open(\"Data/\"+Dataset+\"/id_text_combined.txt\", \"w\",encoding='utf8')\n",
    "for idx, paper in text_data.iterrows():\n",
    "    try:\n",
    "        newf.write((paper[\"paperID\"]+\"\\t\"+paper[\"title\"]+\"\\t\"+str(paper[\"keywords\"])+\"\\t\"+str(paper[\"mesh\"])+\"\\t\"+paper[\"abstract\"]).strip('\\n')+\"\\n\")\n",
    "    except:\n",
    "        print(paper[\"paperID\"],\"\\t\",paper[\"title\"],\"\\t\",str(paper[\"keywords\"]),\"\\t\",str(paper[\"mesh\"]),\"\\t\",paper[\"abstract\"])\n",
    "        print(type(paper[\"paperID\"]))\n",
    "        print(type(paper[\"title\"]))\n",
    "        print(type(paper[\"keywords\"]))\n",
    "        print(type(paper[\"mesh\"]))\n",
    "        print(type(paper[\"abstract\"]))\n",
    "    #print(\"Currently on row: {}; Currently iterrated {}% of rows\".format(idx, (idx + 1)/len(text_data.index) * 100))\n",
    "newf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T11:00:15.700048Z",
     "start_time": "2018-11-21T10:54:40.026374Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write pid to file\n",
    "file = open(\"Data/\"+Dataset+\"/pids.txt\", \"w\",encoding='utf8')\n",
    "for idx, paper in text_data.iterrows():\n",
    "    file.write(paper[\"paperID\"]+\"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
