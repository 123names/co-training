{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of feature importance\n",
    "1. Only use Term Frequency (TF) for different features (\"title\", \"keywords_mesh\", \"abstract\", \"text\")\n",
    "2. Apply filter to filter out less productive authors\n",
    "\n",
    "Example: k-kim name group have 1111 samples for 57 authors.\n",
    "\n",
    "With filter of 100:\n",
    "\n",
    "k-kim name group have 504 paper for 3 authors now, each author write more than 100 papers. Other paper are omitted\n",
    "\n",
    "This script is to evaluate different feature with tf vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T06:41:29.980536Z",
     "start_time": "2020-05-25T06:41:27.964465Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import com_func\n",
    "\n",
    "# parameters\n",
    "#----- filter for selecting set of name group -----------#\n",
    "filter_select_name_group = 100\n",
    "#----- filter for selecting min paper write for authors in name group ----#\n",
    "filter_lower = 10\n",
    "filter_upper = 100\n",
    "\n",
    "apply_filter_to_sample = True\n",
    "\n",
    "Dataset = \"pubmed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T06:44:18.307125Z",
     "start_time": "2020-05-25T06:44:18.254473Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gensim\n",
    "# read tf embedding with different feature\n",
    "def read_text_embedding_feature_wise(Dataset = \"pubmed\", feature = \"off\"):\n",
    "    text_emb = []\n",
    "    emb_pid = []\n",
    "    while True:\n",
    "        if feature == \"off\":\n",
    "            break\n",
    "        else:\n",
    "            modelSaveDir = \"../models/\"+Dataset+\"/tf/\"+feature+\"_sample=140k/\"\n",
    "            with open(modelSaveDir+'tf_features.pickle', \"rb\") as input_file:\n",
    "                vec = pickle.load(input_file)\n",
    "            with open(modelSaveDir+'feature_pid.pickle', \"rb\") as input_file:\n",
    "                allPaperid = pickle.load(input_file)\n",
    "            text_emb = vec.toarray()\n",
    "            emb_pid = allPaperid\n",
    "            print(\"Total text embedding size: \",len(text_emb))\n",
    "            print(\"Vector dimension: \", len(text_emb[0]))\n",
    "            break\n",
    "    return text_emb, emb_pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T06:47:35.495417Z",
     "start_time": "2020-05-25T06:45:18.264682Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the file\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from statistics import mean \n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "fileDir = \"../Data/\"+Dataset+\"/canopies_labeled/\"\n",
    "listfiles = os.listdir(fileDir)\n",
    "\n",
    "mnb_diff_feature_average_f1_result = []\n",
    "lr_diff_feature_average_f1_result = []\n",
    "svm_diff_freature_average_f1_result = []\n",
    "\n",
    "# ----------------------- different feature ---------------------------- #\n",
    "train_feature = [\"title\", \"keywords_mesh\", \"abstract\", \"text\"]\n",
    "# train_feature = [\"text\"]\n",
    "for select_feature in train_feature:\n",
    "    print(\"Load feature: \", select_feature)\n",
    "    # read pretrained embeddings\n",
    "    tf_all_sample, all_tf_pid = read_text_embedding_feature_wise(feature=select_feature)\n",
    "    \n",
    "    filter_change_all_average_mnb_f1s = []\n",
    "    filter_change_all_average_lr_f1s = []\n",
    "    filter_change_all_average_svm_f1s = []\n",
    "    filter_change = []\n",
    "    \n",
    "    # -------------- different filter (step by 10) -----------------------#\n",
    "    for step_filter in range(filter_lower, filter_upper, 10):\n",
    "        filter_change.append(step_filter)\n",
    "        # collect statistic to output\n",
    "        allname, all_author_id, positive_sample_size, negative_sample_size = ([] for i in range(4))\n",
    "        orginal_sample_size, selected_sample_size= ([] for i in range(2))\n",
    "        \n",
    "        all_mnb_accuracy, all_mnb_f1, all_mnb_details = ([] for i in range(3))\n",
    "        all_LR_accuracy, all_LR_f1, all_LR_details = ([] for i in range(3))\n",
    "        all_svcLinear_accuracy, all_svcLinear_f1, all_svc_details= ([] for i in range(3))\n",
    "        \n",
    "        total_selected_group = 0\n",
    "\n",
    "        # read all file in labeled group\n",
    "        for file in listfiles:\n",
    "            # group name\n",
    "            temp = file.split(\"_\")\n",
    "            name = temp[1]+\"_\"+temp[-1]\n",
    "            print(\"For name: \",name)\n",
    "            # read needed content in labeled file\n",
    "            labeled_data = com_func.read_pid_aid(fileDir+file)\n",
    "            # collect all labeled sample\n",
    "            all_labeled_sample = labeled_data[\"paperID\"].tolist()\n",
    "            #----------- select name group contain productive author------------------------------------#\n",
    "            #----------- (contain pair of author write more than 100 papers) ---------------------------#\n",
    "            # count number of paper each author write based on author ID\n",
    "            authorCounter = collections.Counter(labeled_data[\"authorID\"])\n",
    "            # remove name group that do not contain pair of author write more than 100 papers\n",
    "            for k in list(authorCounter):\n",
    "                if authorCounter[k] < filter_select_name_group:\n",
    "                    del authorCounter[k]\n",
    "            # if only have one author or no author pass the filter, not applicable\n",
    "            if(len(authorCounter)==0) or (len(authorCounter)==1):\n",
    "                print(name, \" pass\")\n",
    "            else:\n",
    "                total_selected_group+= 1\n",
    "                # --------------for each name group---------------- #\n",
    "                if apply_filter_to_sample == True:\n",
    "                    # ---------- only use sample pass filter ------- #\n",
    "                    #--------select authors in name group are very productive (more than filter)---------#\n",
    "                    print(\"Total sample size before apply filter: \",len(labeled_data))\n",
    "                    # count number of paper each author write based on author ID\n",
    "                    paperCounter = collections.Counter(labeled_data[\"authorID\"])\n",
    "                    print(paperCounter)\n",
    "                    print(\"Total author before apply threshoid: \", len(paperCounter))\n",
    "                    # collect per class statistic\n",
    "                    for k in list(paperCounter):\n",
    "                        if paperCounter[k] < step_filter:\n",
    "                            del paperCounter[k]\n",
    "                    temp =list(paperCounter.keys())\n",
    "                    print(temp)\n",
    "                    print(\"Total author after apply threshoid: \", len(temp))\n",
    "                    # remove samples that are smaller than filter\n",
    "                    labeled_data = labeled_data[labeled_data.authorID.isin(temp)]\n",
    "                    author_list = set(temp)\n",
    "                    print(\"Total sample size after apply filter: \",len(labeled_data))\n",
    "                    print(name, \" name group sample size: \",labeled_data.shape)\n",
    "                else:\n",
    "                    print(name, \" name group sample size: \",labeled_data.shape)\n",
    "                # ----------- use all sample in name group --------- #\n",
    "                # shuffle the data\n",
    "                labeled_data = labeled_data.sample(frac=1).reset_index(drop=True)\n",
    "                # list of different data field\n",
    "                part_collection = []\n",
    "                # select feature wanted to fit to clustering/classification algorithm\n",
    "                data_text = com_func.extract_embedding(tf_all_sample, all_tf_pid, labeled_data[\"paperID\"])\n",
    "                print(data_text.shape)\n",
    "                part_collection.append(data_text)\n",
    "                # merge different part of data data together by concatenate it all together\n",
    "                # remove empty emb (when emb set off)\n",
    "                part_collection = [part for part in part_collection if len(part)!=0]\n",
    "                print(len(part_collection))\n",
    "                if len(part_collection)>1:\n",
    "                    combinedata = np.concatenate(part_collection,axis=1)\n",
    "                elif len(part_collection)==1:\n",
    "                    if isinstance(part_collection[0], pd.DataFrame):\n",
    "                        combinedata = part_collection[0].values\n",
    "                    else:\n",
    "                        combinedata = part_collection[0]\n",
    "                else:\n",
    "                    print(\"No data available\")\n",
    "                    break\n",
    "                print(combinedata.shape)\n",
    "                print(combinedata[0])\n",
    "\n",
    "                group_pid = labeled_data[\"paperID\"].to_frame()\n",
    "                \n",
    "                if len(author_list)==2:\n",
    "                    author_list = list(author_list)\n",
    "                    print(\"Binary case\", name)\n",
    "                    label = labeled_data[\"authorID\"].values\n",
    "                    # append to statistic collection\n",
    "                    allname.append(name)\n",
    "                    all_author_id.append(author_list)\n",
    "                    orginal_sample_size.append(len(all_labeled_sample))\n",
    "                    selected_sample_size.append(len(labeled_data))\n",
    "                    positive_sample_size.append(np.count_nonzero(label == author_list[0]))\n",
    "                    negative_sample_size.append(np.count_nonzero(label == author_list[1]))\n",
    "                    \n",
    "                    # using converted feature vector to train classifier\n",
    "                    # using multinomial naive bayes\n",
    "                    mnbclf = MultinomialNB()\n",
    "                    mnbaccuracy, mnbmarcof1, mnbtp, mnbtn, mnbfp, mnbfn= com_func.k_fold_cv_with_accumulate_statistic(combinedata, label, mnbclf, k=10)\n",
    "                    print(\"MNB Accuracy: \",mnbaccuracy)\n",
    "                    print(\"MNB F1: \", mnbmarcof1)\n",
    "                    all_mnb_accuracy.append(mnbaccuracy)\n",
    "                    all_mnb_f1.append(mnbmarcof1)\n",
    "                    all_mnb_details.append({\"TP\": mnbtp, \"TN\": mnbtn, \"FP\": mnbfp, \"FN\": mnbfn})\n",
    "                    # using logistic regression\n",
    "                    lrclf = LogisticRegression()\n",
    "                    LRaccuracy, LRmarcof1, LRtp, LRtn, LRfp, LRfn = com_func.k_fold_cv_with_accumulate_statistic(combinedata, label, lrclf, k=10)\n",
    "                    print(\"LR Accuracy: \",LRaccuracy)\n",
    "                    print(\"LR F1: \", LRmarcof1)\n",
    "                    all_LR_accuracy.append(LRaccuracy)\n",
    "                    all_LR_f1.append(LRmarcof1)\n",
    "                    all_LR_details.append({\"TP\": LRtp, \"TN\": LRtn, \"FP\": LRfp, \"FN\": LRfn})\n",
    "                    # using SVM with linear kernal\n",
    "                    svmclf = SVC(kernel='linear')\n",
    "                    svcaccuracy, svcmarcof1, svmtp, svmtn, svmfp, svmfn= com_func.k_fold_cv_with_accumulate_statistic(combinedata, label, svmclf, k=10)\n",
    "                    print(\"svc Accuracy: \",svcaccuracy)\n",
    "                    print(\"svc F1: \", svcmarcof1)\n",
    "                    all_svcLinear_accuracy.append(svcaccuracy)\n",
    "                    all_svcLinear_f1.append(svcmarcof1)\n",
    "                    all_svc_details.append({\"TP\": svmtp, \"TN\": svmtn, \"FP\": svmfp, \"FN\": svmfn})\n",
    "                    \n",
    "                else:\n",
    "                    print(\"Muti-class case\")\n",
    "                    counter = 0\n",
    "                    # loop through each author have label, one vs rest\n",
    "                    for author in author_list:\n",
    "                        author_name = name+'_'+str(counter)\n",
    "                        print(author_name)\n",
    "                        selected_labeled_samples = labeled_data[\"paperID\"].tolist()\n",
    "                        mask = labeled_data[\"authorID\"] == author\n",
    "                        temp = labeled_data[mask]\n",
    "                        positive_sample_pid = temp[\"paperID\"].tolist()\n",
    "                        negative_sample_pid = com_func.extractNegativeSample(positive_sample_pid, selected_labeled_samples)\n",
    "                        # append to statistic collection\n",
    "                        allname.append(author_name)\n",
    "                        all_author_id.append(author)\n",
    "                        orginal_sample_size.append(len(all_labeled_sample))\n",
    "                        selected_sample_size.append(len(labeled_data))\n",
    "                        positive_sample_size.append(len(positive_sample_pid))\n",
    "                        negative_sample_size.append(len(negative_sample_pid))\n",
    "                        # form positive and negative (negative class come from similar name group)\n",
    "                        all_authors = []\n",
    "                        all_authors.append(positive_sample_pid)\n",
    "                        all_authors.append(negative_sample_pid)\n",
    "                        appended_data = []\n",
    "                        for label, pid in enumerate(all_authors):\n",
    "                            # create df save one author data \n",
    "                            authordf = pd.DataFrame({\"paperID\":pid})\n",
    "                            authordf['label'] = label\n",
    "                            appended_data.append(authordf)\n",
    "                        processed_data = pd.concat(appended_data, axis=0,ignore_index=True)\n",
    "\n",
    "                        # alignment \n",
    "                        processed_data = pd.merge(group_pid, processed_data, on=\"paperID\")\n",
    "\n",
    "                        # extract true label and it's corresponeding pid for matching\n",
    "                        label = processed_data[\"label\"]\n",
    "                        pid = processed_data[\"paperID\"]\n",
    "\n",
    "                        # using converted feature vector to train classifier\n",
    "                        # using multinomial naive bayes\n",
    "                        mnbclf = MultinomialNB()\n",
    "                        mnbaccuracy, mnbmarcof1, mnbtp, mnbtn, mnbfp, mnbfn= com_func.k_fold_cv_with_accumulate_statistic(combinedata, label, mnbclf, k=10)\n",
    "                        print(\"MNB Accuracy: \",mnbaccuracy)\n",
    "                        print(\"MNB F1: \", mnbmarcof1)\n",
    "                        all_mnb_accuracy.append(mnbaccuracy)\n",
    "                        all_mnb_f1.append(mnbmarcof1)\n",
    "                        all_mnb_details.append({\"TP\": mnbtp, \"TN\": mnbtn, \"FP\": mnbfp, \"FN\": mnbfn})\n",
    "                        # using logistic regression\n",
    "                        lrclf = LogisticRegression()\n",
    "                        LRaccuracy, LRmarcof1, LRtp, LRtn, LRfp, LRfn = com_func.k_fold_cv_with_accumulate_statistic(combinedata, label, lrclf, k=10)\n",
    "                        print(\"LR Accuracy: \",LRaccuracy)\n",
    "                        print(\"LR F1: \", LRmarcof1)\n",
    "                        all_LR_accuracy.append(LRaccuracy)\n",
    "                        all_LR_f1.append(LRmarcof1)\n",
    "                        all_LR_details.append({\"TP\": LRtp, \"TN\": LRtn, \"FP\": LRfp, \"FN\": LRfn})\n",
    "                        # using SVM with linear kernal\n",
    "                        svmclf = SVC(kernel='linear')\n",
    "                        svcaccuracy, svcmarcof1, svmtp, svmtn, svmfp, svmfn= com_func.k_fold_cv_with_accumulate_statistic(combinedata, label, svmclf, k=10)\n",
    "                        print(\"svc Accuracy: \",svcaccuracy)\n",
    "                        print(\"svc F1: \", svcmarcof1)\n",
    "                        all_svcLinear_accuracy.append(svcaccuracy)\n",
    "                        all_svcLinear_f1.append(svcmarcof1)\n",
    "                        all_svc_details.append({\"TP\": svmtp, \"TN\": svmtn, \"FP\": svmfp, \"FN\": svmfn})\n",
    "                        counter+=1\n",
    "                    \n",
    "        # write evaluation result to excel\n",
    "        output = pd.DataFrame({'Author':allname, \"Author ID\":all_author_id, \"positive sample size\":positive_sample_size, \"negative sample size\":negative_sample_size, \n",
    "                               \"Orginal Name group sample size\": orginal_sample_size, \"Selected Name group sample size\": selected_sample_size,\n",
    "                               \"MNB Accuracy\":all_mnb_accuracy, \"MNB F1\": all_mnb_f1, \"MNB details\": all_mnb_details,\n",
    "                               \"SVM(linear) accuracy\":all_svcLinear_accuracy, \"SVM f1\": all_svcLinear_f1, \"SVM details\": all_svc_details,\n",
    "                               \"LR accuracy\":all_LR_accuracy, \"LR f1\": all_LR_f1,\"LR details\": all_LR_details})\n",
    "        #savePath = \"../../result/\"+Dataset+\"/binary_global_emb_sample=140k/\"\n",
    "        #filename = \"(Global emb sample 140k) textEmb=tf_feature=\"+select_feature+\"_filter=\"+str(step_filter)+\"_namegroupcount=\"+str(total_selected_group)+\".csv\"\n",
    "        #com_func.write_csv_df(savePath, filename, output)\n",
    "        print(\"Done\")\n",
    "        \n",
    "        # --------------------------- mean f1 score over all groups ------------------ #\n",
    "        mnb_f1 = [x for x in all_mnb_f1 if isinstance(x, float)]\n",
    "        lr_f1 = [x for x in all_LR_f1 if isinstance(x, float)]\n",
    "        svcLinear_f1 = [x for x in all_svcLinear_f1 if isinstance(x, float)]\n",
    "        average_mnb_f1 = mean(mnb_f1)\n",
    "        average_lr_f1 = mean(lr_f1)\n",
    "        average_svcLinear_f1 = mean(svcLinear_f1)\n",
    "        filter_change_all_average_mnb_f1s.append(average_mnb_f1)\n",
    "        filter_change_all_average_lr_f1s.append(average_lr_f1)\n",
    "        filter_change_all_average_svm_f1s.append(average_svcLinear_f1)\n",
    "        \n",
    "    mnb_diff_feature_average_f1_result.append(filter_change_all_average_mnb_f1s)\n",
    "    lr_diff_feature_average_f1_result.append(filter_change_all_average_lr_f1s)\n",
    "    svm_diff_freature_average_f1_result.append(filter_change_all_average_svm_f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
