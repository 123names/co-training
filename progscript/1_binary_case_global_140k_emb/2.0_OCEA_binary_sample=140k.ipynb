{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T08:47:03.466120Z",
     "start_time": "2020-05-25T08:47:03.456947Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scikit-learn version is 0.23.1.\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One classifier each author (OCEA)\n",
    "1. This method throw away the authors write less than 100 papers\n",
    "2. This method will convert muti-class case to mutiple OVR binary classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T10:26:29.198373Z",
     "start_time": "2020-05-25T10:26:27.286180Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import com_func\n",
    "\n",
    "# parameters\n",
    "#----- filter for selecting set of name group -----------#\n",
    "filter_select_name_group = 100\n",
    "#----- filter for selecting productive authors ----#\n",
    "filter_lower = 100\n",
    "filter_upper = 110\n",
    "\n",
    "Dataset = \"pubmed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T08:47:06.028138Z",
     "start_time": "2020-05-25T08:47:06.022129Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# text embedding only\n",
    "pp_text_emb = [\"tf\", \"tf_idf\", \"lsa\", \"pv_dm\", \"pv_dbow\"]\n",
    "pp_citation_emb = [\"off\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T08:47:06.645827Z",
     "start_time": "2020-05-25T08:47:06.638321Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# citation embedding only\n",
    "pp_text_emb = [\"off\"]\n",
    "pp_citation_emb = [\"n2v\",\"node2vec\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T10:26:35.676672Z",
     "start_time": "2020-05-25T10:26:35.652307Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combined embedding\n",
    "pp_text_emb = [\"lsa\", \"pv_dm\", \"pv_dbow\"]\n",
    "pp_citation_emb = [\"n2v\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T10:26:36.333934Z",
     "start_time": "2020-05-25T10:26:36.325831Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lsa', 'pv_dm', 'pv_dbow']\n",
      "['n2v']\n"
     ]
    }
   ],
   "source": [
    "print(pp_text_emb)\n",
    "print(pp_citation_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T10:34:21.421169Z",
     "start_time": "2020-05-25T10:33:48.111244Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load text embedding:  lsa\n",
      "Total text vector records: 135796\n",
      "Vector dimension:  100\n",
      "Load citation embedding:  n2v\n",
      "Total citation vector records: 124922\n",
      "Vector dimension:  100\n",
      "For name:  j_read\n",
      "j_read  pass\n",
      "For name:  f_esteves\n",
      "f_esteves  pass\n",
      "For name:  c_miller\n",
      "c_miller  pass\n",
      "For name:  r_jha\n",
      "r_jha  pass\n",
      "For name:  a_lowe\n",
      "a_lowe  pass\n",
      "For name:  a_vega\n",
      "a_vega  pass\n",
      "For name:  k_smith\n",
      "k_smith  pass\n",
      "For name:  j_gordon\n",
      "j_gordon  pass\n",
      "For name:  s_liao\n",
      "s_liao  pass\n",
      "For name:  j_qian\n",
      "j_qian  pass\n",
      "For name:  s_bernardi\n",
      "s_bernardi  pass\n",
      "For name:  t_hill\n",
      "t_hill  pass\n",
      "For name:  s_schindler\n",
      "s_schindler  pass\n",
      "For name:  j_williams\n",
      "j_williams  pass\n",
      "For name:  s_jacobson\n",
      "s_jacobson  pass\n",
      "For name:  e_andrade\n",
      "e_andrade  pass\n",
      "For name:  t_santos\n",
      "t_santos  pass\n",
      "For name:  k_kim\n",
      "Muti-class case, convert to mutiple binary case.\n",
      "For name:  d_ricci\n",
      "d_ricci  pass\n",
      "For name:  s_cameron\n",
      "s_cameron  pass\n",
      "For name:  t_wright\n",
      "t_wright  pass\n",
      "For name:  r_cunha\n",
      "r_cunha  pass\n",
      "For name:  s_fuchs\n",
      "s_fuchs  pass\n",
      "For name:  m_nawaz\n",
      "m_nawaz  pass\n",
      "For name:  k_harris\n",
      "k_harris  pass\n",
      "For name:  r_daniel\n",
      "r_daniel  pass\n",
      "For name:  k_xu\n",
      "k_xu  pass\n",
      "For name:  s_antunes\n",
      "s_antunes  pass\n",
      "For name:  k_cho\n",
      "k_cho  pass\n",
      "For name:  j_sanderson\n",
      "j_sanderson  pass\n",
      "For name:  s_uddin\n",
      "s_uddin  pass\n",
      "For name:  a_batista\n",
      "a_batista  pass\n",
      "For name:  h_pereira\n",
      "h_pereira  pass\n",
      "For name:  a_patel\n",
      "a_patel  pass\n",
      "For name:  r_graham\n",
      "r_graham  pass\n",
      "For name:  a_nilsson\n",
      "a_nilsson  pass\n",
      "For name:  m_soto\n",
      "m_soto  pass\n",
      "For name:  g_guidi\n",
      "g_guidi  pass\n",
      "For name:  e_andersson\n",
      "e_andersson  pass\n",
      "For name:  s_reid\n",
      "s_reid  pass\n",
      "For name:  a_maleki\n",
      "a_maleki  pass\n",
      "For name:  j_moon\n",
      "j_moon  pass\n",
      "For name:  t_abe\n",
      "t_abe  pass\n",
      "For name:  x_fu\n",
      "x_fu  pass\n",
      "For name:  f_ortega\n",
      "f_ortega  pass\n",
      "For name:  r_morris\n",
      "r_morris  pass\n",
      "For name:  w_fang\n",
      "w_fang  pass\n",
      "For name:  m_amaral\n",
      "m_amaral  pass\n",
      "For name:  h_song\n",
      "h_song  pass\n",
      "For name:  h_dai\n",
      "h_dai  pass\n",
      "For name:  y_nakajima\n",
      "y_nakajima  pass\n",
      "For name:  t_warner\n",
      "t_warner  pass\n",
      "For name:  s_saha\n",
      "s_saha  pass\n",
      "For name:  j_fernandez\n",
      "j_fernandez  pass\n",
      "For name:  m_pan\n",
      "m_pan  pass\n",
      "For name:  a_simon\n",
      "a_simon  pass\n",
      "For name:  r_freitas\n",
      "r_freitas  pass\n",
      "For name:  c_yun\n",
      "c_yun  pass\n",
      "For name:  j_huang\n",
      "j_huang  pass\n",
      "For name:  p_santos\n",
      "p_santos  pass\n",
      "For name:  n_young\n",
      "n_young  pass\n",
      "For name:  d_ross\n",
      "d_ross  pass\n",
      "For name:  q_wang\n",
      "q_wang  pass\n",
      "For name:  c_cardoso\n",
      "c_cardoso  pass\n",
      "For name:  j_matthews\n",
      "j_matthews  pass\n",
      "For name:  g_lee\n",
      "g_lee  pass\n",
      "For name:  m_salem\n",
      "m_salem  pass\n",
      "For name:  h_lai\n",
      "h_lai  pass\n",
      "For name:  r_harris\n",
      "r_harris  pass\n",
      "For name:  c_vaughan\n",
      "c_vaughan  pass\n",
      "For name:  e_thompson\n",
      "e_thompson  pass\n",
      "For name:  r_gomes\n",
      "r_gomes  pass\n",
      "For name:  r_bennett\n",
      "r_bennett  pass\n",
      "For name:  m_collins\n",
      "m_collins  pass\n",
      "For name:  m_cowley\n",
      "m_cowley  pass\n",
      "For name:  p_teixeira\n",
      "p_teixeira  pass\n",
      "For name:  c_cox\n",
      "c_cox  pass\n",
      "For name:  s_hsu\n",
      "s_hsu  pass\n",
      "For name:  f_williams\n",
      "f_williams  pass\n",
      "For name:  d_parsons\n",
      "d_parsons  pass\n",
      "For name:  a_choudhury\n",
      "a_choudhury  pass\n",
      "For name:  c_richter\n",
      "c_richter  pass\n",
      "For name:  m_hossain\n",
      "m_hossain  pass\n",
      "For name:  v_alves\n",
      "v_alves  pass\n",
      "For name:  j_becker\n",
      "j_becker  pass\n",
      "For name:  m_soares\n",
      "m_soares  pass\n",
      "For name:  j_yi\n",
      "j_yi  pass\n",
      "For name:  s_khan\n",
      "s_khan  pass\n",
      "For name:  a_rao\n",
      "a_rao  pass\n",
      "For name:  d_cameron\n",
      "d_cameron  pass\n",
      "For name:  c_morgan\n",
      "c_morgan  pass\n",
      "For name:  h_cui\n",
      "h_cui  pass\n",
      "For name:  p_zhang\n",
      "p_zhang  pass\n",
      "For name:  j_fernandes\n",
      "j_fernandes  pass\n",
      "For name:  a_jain\n",
      "a_jain  pass\n",
      "For name:  d_zhang\n",
      "d_zhang  pass\n",
      "For name:  b_huang\n",
      "b_huang  pass\n",
      "For name:  m_chong\n",
      "m_chong  pass\n",
      "For name:  m_cerqueira\n",
      "m_cerqueira  pass\n",
      "For name:  p_yang\n",
      "p_yang  pass\n",
      "For name:  j_marques\n",
      "j_marques  pass\n",
      "For name:  n_ali\n",
      "n_ali  pass\n",
      "For name:  h_ng\n",
      "h_ng  pass\n",
      "For name:  m_viana\n",
      "m_viana  pass\n",
      "For name:  t_inoue\n",
      "t_inoue  pass\n",
      "For name:  b_meyer\n",
      "b_meyer  pass\n",
      "For name:  c_liao\n",
      "c_liao  pass\n",
      "For name:  k_wheeler\n",
      "k_wheeler  pass\n",
      "For name:  m_rizzo\n",
      "m_rizzo  pass\n",
      "For name:  y_shi\n",
      "y_shi  pass\n",
      "For name:  c_luo\n",
      "c_luo  pass\n",
      "For name:  j_arthur\n",
      "j_arthur  pass\n",
      "For name:  m_ansari\n",
      "m_ansari  pass\n",
      "For name:  g_anderson\n",
      "g_anderson  pass\n",
      "For name:  m_hidalgo\n",
      "m_hidalgo  pass\n",
      "For name:  k_jacobsen\n",
      "k_jacobsen  pass\n",
      "For name:  s_kelly\n",
      "s_kelly  pass\n",
      "For name:  s_james\n",
      "s_james  pass\n",
      "For name:  p_persson\n",
      "p_persson  pass\n",
      "For name:  y_tanaka\n",
      "y_tanaka  pass\n",
      "For name:  c_gao\n",
      "c_gao  pass\n",
      "For name:  w_jung\n",
      "w_jung  pass\n",
      "For name:  s_lewis\n",
      "s_lewis  pass\n",
      "For name:  w_han\n",
      "w_han  pass\n",
      "For name:  m_shah\n",
      "m_shah  pass\n",
      "For name:  c_arango\n",
      "c_arango  pass\n",
      "For name:  r_young\n",
      "r_young  pass\n",
      "For name:  r_coleman\n",
      "r_coleman  pass\n",
      "For name:  b_kang\n",
      "b_kang  pass\n",
      "For name:  s_carter\n",
      "s_carter  pass\n",
      "For name:  c_thomas\n",
      "c_thomas  pass\n",
      "For name:  m_gutierrez\n",
      "m_gutierrez  pass\n",
      "For name:  s_moon\n",
      "s_moon  pass\n",
      "For name:  r_pereira\n",
      "r_pereira  pass\n",
      "For name:  a_nielsen\n",
      "a_nielsen  pass\n",
      "For name:  j_conde\n",
      "j_conde  pass\n",
      "For name:  k_wright\n",
      "k_wright  pass\n",
      "For name:  m_parker\n",
      "m_parker  pass\n",
      "For name:  h_huang\n",
      "h_huang  pass\n",
      "For name:  j_terry\n",
      "j_terry  pass\n",
      "For name:  y_xu\n",
      "y_xu  pass\n",
      "For name:  a_melo\n",
      "a_melo  pass\n",
      "For name:  r_doyle\n",
      "r_doyle  pass\n",
      "For name:  m_bernardo\n",
      "m_bernardo  pass\n",
      "For name:  j_soares\n",
      "j_soares  pass\n",
      "For name:  j_richard\n",
      "j_richard  pass\n",
      "For name:  p_robinson\n",
      "Total sample size before apply filter:  275\n",
      "Counter({'0000-0002-7878-0313': 133, '0000-0002-0736-9199': 119, '0000-0002-3156-3418': 19, '0000-0002-0577-3147': 4})\n",
      "Total author before apply threshoid:  4\n",
      "['0000-0002-0736-9199', '0000-0002-7878-0313']\n",
      "Total author after apply threshoid:  2\n",
      "Total sample size after apply filter:  252\n",
      "Total missing sample:  0\n",
      "Text embedding shape:  (252, 100)\n",
      "Missing Sample:  10929617\n",
      "Missing Sample:  23920735\n",
      "Missing Sample:  18283030\n",
      "Missing Sample:  25355889\n",
      "Missing Sample:  21857152\n",
      "Missing Sample:  12192611\n",
      "Total missing sample:  6\n",
      "Citation embedding shape:  (252, 100)\n",
      "Final feature (combined embedding) shape:  (252, 200)\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "0000-0002-0736-9199       0.94      0.94      0.94       119\n",
      "0000-0002-7878-0313       0.95      0.95      0.95       133\n",
      "\n",
      "           accuracy                           0.94       252\n",
      "          macro avg       0.94      0.94      0.94       252\n",
      "       weighted avg       0.94      0.94      0.94       252\n",
      "\n",
      "[112   7   7 126]\n",
      "LR F1:  0.9442724458204335\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "0000-0002-0736-9199       0.98      0.93      0.96       119\n",
      "0000-0002-7878-0313       0.94      0.98      0.96       133\n",
      "\n",
      "           accuracy                           0.96       252\n",
      "          macro avg       0.96      0.96      0.96       252\n",
      "       weighted avg       0.96      0.96      0.96       252\n",
      "\n",
      "[111   8   2 131]\n",
      "SVM F1:  0.9600659229208925\n",
      "For name:  c_zou\n",
      "c_zou  pass\n",
      "For name:  s_rana\n",
      "s_rana  pass\n",
      "For name:  a_nunes\n",
      "a_nunes  pass\n",
      "For name:  s_jeong\n",
      "s_jeong  pass\n",
      "For name:  b_olsen\n",
      "b_olsen  pass\n",
      "For name:  m_reilly\n",
      "m_reilly  pass\n",
      "For name:  d_nguyen\n",
      "d_nguyen  pass\n",
      "For name:  r_santos\n",
      "r_santos  pass\n",
      "For name:  f_ferreira\n",
      "f_ferreira  pass\n",
      "For name:  y_ng\n",
      "y_ng  pass\n",
      "For name:  j_madsen\n",
      "j_madsen  pass\n",
      "For name:  d_collins\n",
      "d_collins  pass\n",
      "For name:  l_davies\n",
      "l_davies  pass\n",
      "For name:  m_mora\n",
      "m_mora  pass\n",
      "For name:  a_fontana\n",
      "a_fontana  pass\n",
      "For name:  r_chen\n",
      "r_chen  pass\n",
      "For name:  s_krause\n",
      "s_krause  pass\n",
      "For name:  t_smith\n",
      "Total sample size before apply filter:  603\n",
      "Counter({'0000-0002-3650-9381': 154, '0000-0003-1673-2954': 113, '0000-0002-2120-2766': 85, '0000-0002-6279-9685': 84, '0000-0003-3528-6793': 65, '0000-0003-4453-9713': 32, '0000-0002-5197-5030': 26, '0000-0002-3945-630X': 10, '0000-0001-7894-6814': 9, '0000-0002-5750-0706': 6, '0000-0002-5495-8906': 4, '0000-0003-3762-6253': 4, '0000-0002-0479-4261': 3, '0000-0003-2389-461X': 2, '0000-0001-6272-8871': 2, '0000-0001-7683-2653': 1, '0000-0002-2104-2264': 1, '0000-0001-9068-4642': 1, '0000-0002-1881-2766': 1})\n",
      "Total author before apply threshoid:  19\n",
      "['0000-0003-1673-2954', '0000-0002-3650-9381']\n",
      "Total author after apply threshoid:  2\n",
      "Total sample size after apply filter:  267\n",
      "Total missing sample:  0\n",
      "Text embedding shape:  (267, 100)\n",
      "Missing Sample:  18713802\n",
      "Missing Sample:  19681312\n",
      "Missing Sample:  26370268\n",
      "Missing Sample:  19943576\n",
      "Missing Sample:  25716704\n",
      "Missing Sample:  19876883\n",
      "Missing Sample:  21948040\n",
      "Missing Sample:  24563973\n",
      "Missing Sample:  21467087\n",
      "Missing Sample:  20698447\n",
      "Missing Sample:  26645724\n",
      "Missing Sample:  20880486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing sample:  12\n",
      "Citation embedding shape:  (267, 100)\n",
      "Final feature (combined embedding) shape:  (267, 200)\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "0000-0002-3650-9381       1.00      0.99      1.00       154\n",
      "0000-0003-1673-2954       0.99      1.00      1.00       113\n",
      "\n",
      "           accuracy                           1.00       267\n",
      "          macro avg       1.00      1.00      1.00       267\n",
      "       weighted avg       1.00      1.00      1.00       267\n",
      "\n",
      "[153   1   0 113]\n",
      "LR F1:  0.9961686923330799\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "0000-0002-3650-9381       1.00      0.98      0.99       154\n",
      "0000-0003-1673-2954       0.97      1.00      0.99       113\n",
      "\n",
      "           accuracy                           0.99       267\n",
      "          macro avg       0.99      0.99      0.99       267\n",
      "       weighted avg       0.99      0.99      0.99       267\n",
      "\n",
      "[151   3   0 113]\n",
      "SVM F1:  0.9885317488725034\n",
      "For name:  a_biswas\n",
      "a_biswas  pass\n",
      "For name:  j_day\n",
      "j_day  pass\n",
      "For name:  d_truong\n",
      "d_truong  pass\n",
      "For name:  s_pan\n",
      "s_pan  pass\n",
      "For name:  a_andrade\n",
      "a_andrade  pass\n",
      "For name:  t_oliveira\n",
      "t_oliveira  pass\n",
      "For name:  n_romano\n",
      "n_romano  pass\n",
      "For name:  t_hara\n",
      "t_hara  pass\n",
      "For name:  t_wong\n",
      "t_wong  pass\n",
      "For name:  s_ross\n",
      "s_ross  pass\n",
      "For name:  d_richardson\n",
      "Total sample size before apply filter:  456\n",
      "Counter({'0000-0003-0960-6415': 231, '0000-0002-7751-1058': 167, '0000-0002-3992-8610': 22, '0000-0003-0247-9118': 17, '0000-0002-3189-2190': 12, '0000-0002-0054-6850': 7})\n",
      "Total author before apply threshoid:  6\n",
      "['0000-0003-0960-6415', '0000-0002-7751-1058']\n",
      "Total author after apply threshoid:  2\n",
      "Total sample size after apply filter:  398\n",
      "Total missing sample:  0\n",
      "Text embedding shape:  (398, 100)\n",
      "Missing Sample:  17873940\n",
      "Missing Sample:  19529600\n",
      "Missing Sample:  23287991\n",
      "Missing Sample:  18542692\n",
      "Missing Sample:  12237336\n",
      "Missing Sample:  25836529\n",
      "Missing Sample:  19532619\n",
      "Missing Sample:  19529514\n",
      "Missing Sample:  18084478\n",
      "Missing Sample:  18183207\n",
      "Missing Sample:  19532763\n",
      "Missing Sample:  19529351\n",
      "Missing Sample:  18542306\n",
      "Missing Sample:  18071456\n",
      "Missing Sample:  19495279\n",
      "Missing Sample:  18091853\n",
      "Missing Sample:  26490424\n",
      "Missing Sample:  19127267\n",
      "Missing Sample:  19483871\n",
      "Missing Sample:  19532177\n",
      "Missing Sample:  18071574\n",
      "Missing Sample:  19475030\n",
      "Missing Sample:  19498947\n",
      "Missing Sample:  19859161\n",
      "Missing Sample:  18071515\n",
      "Missing Sample:  21214214\n",
      "Missing Sample:  18542531\n",
      "Missing Sample:  18084446\n",
      "Missing Sample:  11222108\n",
      "Missing Sample:  19516597\n",
      "Missing Sample:  16373528\n",
      "Missing Sample:  19436371\n",
      "Missing Sample:  17099764\n",
      "Missing Sample:  18091883\n",
      "Missing Sample:  19532311\n",
      "Missing Sample:  19475054\n",
      "Missing Sample:  24515200\n",
      "Missing Sample:  21418014\n",
      "Missing Sample:  19529432\n",
      "Missing Sample:  20048773\n",
      "Missing Sample:  15483720\n",
      "Missing Sample:  25321569\n",
      "Missing Sample:  18049570\n",
      "Missing Sample:  19532760\n",
      "Missing Sample:  14605412\n",
      "Missing Sample:  19516457\n",
      "Missing Sample:  22274243\n",
      "Missing Sample:  18084583\n",
      "Missing Sample:  19471492\n",
      "Missing Sample:  23455143\n",
      "Missing Sample:  19529129\n",
      "Missing Sample:  15455784\n",
      "Missing Sample:  19516664\n",
      "Missing Sample:  21189130\n",
      "Total missing sample:  54\n",
      "Citation embedding shape:  (398, 100)\n",
      "Final feature (combined embedding) shape:  (398, 200)\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "0000-0002-7751-1058       0.92      1.00      0.96       167\n",
      "0000-0003-0960-6415       1.00      0.94      0.97       231\n",
      "\n",
      "           accuracy                           0.96       398\n",
      "          macro avg       0.96      0.97      0.96       398\n",
      "       weighted avg       0.97      0.96      0.96       398\n",
      "\n",
      "[167   0  15 216]\n",
      "LR F1:  0.9617315051633624\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-30f2ca2db434>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0;31m# using SVM with linear kernal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                     \u001b[0msvcaccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvcmarcof1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_fold_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombinedata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SVM F1: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvcmarcof1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                     \u001b[0mstatistic_detail\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"SVM(linear) accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvcaccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/progscript/com_func.py\u001b[0m in \u001b[0;36mk_fold_cv\u001b[0;34m(data, label, clf, k, verbose)\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0;31m# fit data to clf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0mper_fold_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m         \u001b[0mper_fold_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m         \u001b[0;31m# get predicted label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[0mlabel_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mper_fold_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/intel/intelpython3/lib/python3.6/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    160\u001b[0m             X, y = self._validate_data(X, y, dtype=np.float64,\n\u001b[1;32m    161\u001b[0m                                        \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                                        accept_large_sparse=False)\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/intel/intelpython3/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/intel/intelpython3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/intel/intelpython3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    801\u001b[0m                     \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    804\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         y = check_array(y, accept_sparse='csr', force_all_finite=True,\n",
      "\u001b[0;32m~/intel/intelpython3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/intel/intelpython3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    597\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m~/intel/intelpython3/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \"\"\"\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load the file\n",
    "import io\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from statistics import mean \n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "fileDir = \"../Data/\"+Dataset+\"/canopies_labeled/\"\n",
    "listfiles = os.listdir(fileDir)\n",
    "\n",
    "diff_embedding_result = collections.defaultdict(list)\n",
    "\n",
    "# ----------------------- different text embedding ----------------------#\n",
    "for text_emb in pp_text_emb:\n",
    "    print(\"Load text embedding: \", text_emb)\n",
    "    # read pretrained embeddings\n",
    "    all_text_embedding = com_func.read_text_embedding(emb_type=text_emb, training_size=\"140k\")\n",
    "    all_text_emb_pid = [emb[0] for emb in all_text_embedding]\n",
    "    all_text_embedding = [emb[1:] for emb in all_text_embedding]\n",
    "    \n",
    "    for citation_emb in pp_citation_emb:\n",
    "        print(\"Load citation embedding: \", citation_emb)\n",
    "        all_citation_embedding = com_func.read_citation_embedding_sorted(emb_type = citation_emb)\n",
    "        all_citation_emb_pid = [emb[0] for emb in all_citation_embedding]\n",
    "        all_citation_embedding = [emb[1:] for emb in all_citation_embedding]\n",
    "        \n",
    "        diff_threshold_result = collections.defaultdict(list)\n",
    "\n",
    "        # -------------- different filter (step by 10) -----------------------#\n",
    "        for step_filter in range(filter_lower, filter_upper, 10):\n",
    "            # collect statistic to output\n",
    "            statistic_detail = collections.defaultdict(list)\n",
    "            \n",
    "            # ------- select useful name group in all name group --------------------#\n",
    "            for file in listfiles:\n",
    "                # group name\n",
    "                temp = file.split(\"_\")\n",
    "                name = temp[1]+\"_\"+temp[-1]\n",
    "                print(\"For name: \",name)\n",
    "                # read needed content in labeled file\n",
    "                labeled_data = com_func.read_pid_aid(fileDir+file)\n",
    "                #----------- select name group contain productive author------------------------------------#\n",
    "                #----------- (contain pair of author write more than 100 papers) ---------------------------#\n",
    "                # count number of paper each author write based on author ID\n",
    "                authorCounter = collections.Counter(labeled_data[\"authorID\"])\n",
    "                # remove name group that do not contain pair of author write more than 100 papers\n",
    "                for k in list(authorCounter):\n",
    "                    if authorCounter[k] < filter_select_name_group:\n",
    "                        del authorCounter[k]\n",
    "                # if only have one class or no class pass the filter, not applicable\n",
    "                if(len(authorCounter)==0) or (len(authorCounter)==1):\n",
    "                    print(name, \" pass\")\n",
    "                elif len(authorCounter)>2:\n",
    "                    print(\"Muti-class case, convert to mutiple binary case.\")\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                    temp_orginal_sample_size = len(labeled_data)\n",
    "                    #--------select authors in name group are very productive (more than filter)---------#\n",
    "                    print(\"Total sample size before apply filter: \",len(labeled_data))\n",
    "                    # count number of paper each author write based on author ID\n",
    "                    paperCounter = collections.Counter(labeled_data[\"authorID\"])\n",
    "                    print(paperCounter)\n",
    "                    print(\"Total author before apply threshoid: \", len(paperCounter))\n",
    "                    # collect per class statistic\n",
    "                    for k in list(paperCounter):\n",
    "                        if paperCounter[k] < step_filter:\n",
    "                            del paperCounter[k]\n",
    "                    temp =list(paperCounter.keys())\n",
    "                    print(temp)\n",
    "                    print(\"Total author after apply threshoid: \", len(temp))\n",
    "                    # remove samples that are smaller than filter\n",
    "                    labeled_data = labeled_data[labeled_data.authorID.isin(temp)]\n",
    "                    print(\"Total sample size after apply filter: \",len(labeled_data))\n",
    "                    #------------ extract paper representation -------------------------------------------#\n",
    "                    # shuffle the data\n",
    "                    labeled_data = labeled_data.sample(frac=1).reset_index(drop=True)\n",
    "                    # extract true label and pid\n",
    "                    label = labeled_data[\"authorID\"]\n",
    "                    pid = labeled_data[\"paperID\"]\n",
    "                    # list of different data field\n",
    "                    part_collection = []\n",
    "                    # select feature wanted to fit to clustering/classification algorithm\n",
    "                    # data part, text information\n",
    "                    data_part_text = com_func.extract_embedding(all_text_embedding, all_text_emb_pid, pid)\n",
    "                    print(\"Text embedding shape: \", data_part_text.shape)\n",
    "                    part_collection.append(data_part_text)\n",
    "                    # data part, citation information\n",
    "                    data_part_citation = com_func.extract_embedding(all_citation_embedding, all_citation_emb_pid, pid)\n",
    "                    data_part_citation.fillna(0, inplace=True)\n",
    "                    print(\"Citation embedding shape: \", data_part_citation.shape)\n",
    "                    part_collection.append(data_part_citation)\n",
    "                    # merge different part of data data together by concatenate it all together\n",
    "                    # remove empty emb (when emb set off)\n",
    "                    part_collection = [part for part in part_collection if len(part)!=0]\n",
    "                    if len(part_collection)>1:\n",
    "                        combinedata = np.concatenate(part_collection,axis=1)\n",
    "                    elif len(part_collection)==1:\n",
    "                        if isinstance(part_collection[0], pd.DataFrame):\n",
    "                            combinedata = part_collection[0].values\n",
    "                        else:\n",
    "                            combinedata = part_collection[0]\n",
    "                    else:\n",
    "                        print(\"No data available\")\n",
    "                        break\n",
    "                    print(\"Final feature (combined embedding) shape: \", combinedata.shape)\n",
    "                    statistic_detail[\"Name group\"].append(name)\n",
    "                    statistic_detail[\"Class number\"].append(len(paperCounter))\n",
    "                    statistic_detail[\"Per class size\"].append(paperCounter)\n",
    "                    statistic_detail[\"Orginal sample size\"].append(temp_orginal_sample_size)\n",
    "                    statistic_detail[\"Total selected sample size\"].append(len(labeled_data))\n",
    "                    # -------------- using converted feature vector to train classifier-------------------#\n",
    "                    if text_emb == \"tf\":\n",
    "                        # using multinomial naive bayes\n",
    "                        clf = MultinomialNB()    \n",
    "                        mnbaccuracy, mnbmarcof1= com_func.k_fold_cv(combinedata, label, clf, k=10, verbose=True)\n",
    "                        print(\"MNB F1: \", mnbmarcof1)\n",
    "                        statistic_detail['MNB Accuracy'].append(mnbaccuracy)\n",
    "                        statistic_detail['MNB macro F1'].append(mnbmarcof1)\n",
    "                    # using logistic regression\n",
    "                    clf = LogisticRegression(solver= \"liblinear\")\n",
    "                    LRaccuracy, LRmarcof1 = com_func.k_fold_cv(combinedata, label, clf, k=10, verbose=True)\n",
    "                    print(\"LR F1: \", LRmarcof1)\n",
    "                    statistic_detail[\"LR accuracy\"].append(LRaccuracy)\n",
    "                    statistic_detail[\"LR macro f1\"].append(LRmarcof1)\n",
    "                    # using SVM with linear kernal\n",
    "                    clf = SVC(gamma=\"auto\", kernel='linear')\n",
    "                    svcaccuracy, svcmarcof1 = com_func.k_fold_cv(combinedata, label, clf, k=10, verbose=True)\n",
    "                    print(\"SVM F1: \", svcmarcof1)\n",
    "                    statistic_detail[\"SVM(linear) accuracy\"].append(svcaccuracy)\n",
    "                    statistic_detail[\"SVM(linear) macro f1\"].append(svcmarcof1)\n",
    "\n",
    "            # write evaluation result to excel\n",
    "            output = pd.DataFrame(statistic_detail)\n",
    "            print(output)\n",
    "\n",
    "            #savePath = \"../../result/\"+Dataset+\"/3_OCEA_sample=140k/\"\n",
    "            #filename = \"citation=\"+citation_emb+\"_textual=\"+text_emb+\"_threshold=\"+str(step_filter)+\".csv\"\n",
    "            #com_func.write_csv_df(savePath, filename, output)\n",
    "            print(\"Done\")\n",
    "            \n",
    "            diff_threshold_result[step_filter].append(statistic_detail)\n",
    "        \n",
    "        diff_embedding_result[\"text=\"+text_emb+\"citation=\"+citation_emb]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T16:40:50.867327Z",
     "start_time": "2019-01-10T09:58:57.655751Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T16:40:50.910860Z",
     "start_time": "2019-01-10T16:40:50.871136Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%who"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
