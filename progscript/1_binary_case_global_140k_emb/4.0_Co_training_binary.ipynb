{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T06:30:49.379339Z",
     "start_time": "2020-05-12T06:30:49.366003Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import logging\n",
    "# logging.basicConfig(filename='./test.log', level=logging.DEBUG, \n",
    "#                     format='%(asctime)s %(levelname)s %(name)s %(message)s')\n",
    "# logger=logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T06:30:51.498974Z",
     "start_time": "2020-05-12T06:30:51.481204Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from IPython.core.magic import register_cell_magic\n",
    "\n",
    "# @register_cell_magic('handle')\n",
    "# def handle(line, cell):\n",
    "#     try:\n",
    "#         exec(cell)\n",
    "#     except Exception as e:\n",
    "#         logger.error(e)\n",
    "#         raise # if you want the full trace-back in the notebook\n",
    "\n",
    "\n",
    "# use %%handle when want to output error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co-training\n",
    "\n",
    "For visualization of co-training process, we apply PCA to feature before training. This will make co-training process clear, but the result will be not accuracy because apply PCA will loss lots of information.\n",
    "\n",
    "1. We assume only part of label exist\n",
    "\n",
    "2. We only select binary case (Only when one name indicate two and only two author)\n",
    "\n",
    "3. When we apply 10 fold with co-training, each fold of first iteration will be baseline compare to co-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T05:56:19.905522Z",
     "start_time": "2020-05-13T05:56:17.749816Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "#warnings.filterwarnings('error')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import com_func\n",
    "\n",
    "#----- threshold for selecting set of name group -----------#\n",
    "threshold_select_name_group = 100\n",
    "#----- threshold for selecting min sample in name group ----#\n",
    "threshold_lower = 100\n",
    "threshold_upper = 110\n",
    "\n",
    "apply_threshold_to_name_group_samples = True\n",
    "\n",
    "pp_text = [\"pv_dbow\"]\n",
    "pp_citation = \"n2v\"\n",
    "\n",
    "Dataset = \"pubmed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Co-training details\n",
    "1. Basic co-training algorithm required parameter p,n,k,u. Since we have 15 different dataset, we will assume p and n is 1, k is 30. (We are simulate real world situation where we do not know the distribution of unlabeled data amount 15 different dataset)\n",
    "2. We set the parameter u as size of input train data (labeled+unlabeled) since our ublabeled data is not that large.\n",
    "3. During co-training process, the confidence measure is using probability as confident score to evaluate whether unlabel sample should be label or not, and which class belone to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T05:56:27.939194Z",
     "start_time": "2020-05-13T05:56:19.908052Z"
    },
    "code_folding": [
     12,
     29,
     33,
     36,
     52,
     64,
     65,
     91,
     134,
     142,
     188,
     368,
     371,
     374,
     418
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import warnings\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "from sklearn.metrics import precision_score, recall_score,f1_score,accuracy_score\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# create co training classifier\n",
    "class Co_training_clf(object):\n",
    "    \n",
    "    def __init__(self, clf1, clf2=None, p=1, n=1, k=30, u = 75):\n",
    "        \n",
    "        self.clf1 = clf1\n",
    "        # assume co_training on one classifier\n",
    "        if clf2 == None:\n",
    "            self.clf2 = copy.deepcopy(clf1)\n",
    "        else:\n",
    "            self.clf2 = clf2\n",
    "        # take p example from most confidently positive labels to example\n",
    "        self.p = p\n",
    "        # take n example from most confidently negative label to example\n",
    "        self.n = n\n",
    "        # number of iteration\n",
    "        self.k = k\n",
    "        # size of pool of unlabeled samples\n",
    "        self.u = u\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=1)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return np.around(1 / (1 + np.exp(-x)), decimals=4).item()\n",
    "\n",
    "    def init_L_U_U_prime(self, labels):\n",
    "        # index of the samples that are initially labeled\n",
    "        L = labels.index[labels != -1].tolist()\n",
    "        # index of unlabeled samples\n",
    "        U = labels.index[labels == -1].tolist()\n",
    "        print(\"Initial L size: \", len(L))\n",
    "        print(\"Initial U size: \", len(U))\n",
    "        # random drawing sample from U\n",
    "        random.shuffle(U)\n",
    "        U_prime = U[-min(len(U), self.u):]\n",
    "        # remove the samples in U_prime from U\n",
    "        U = U[:-len(U_prime)]\n",
    "        print(\"U size after drawing sample to U prime:\",len(U))\n",
    "        print(\"Initial U prime size: \", len(U_prime))\n",
    "        return L, U, U_prime\n",
    "    \n",
    "    def check_iter_label_mapping(self, iter_clf1, iter_clf2):\n",
    "        '''\n",
    "        In theory, it shouldn't occur that label not mapping since it trained on same dataset but different view\n",
    "        But add a check to make sure it won't occur and save the class mapping for later label unlabeled sample\n",
    "        '''\n",
    "        dv1_class_label = iter_clf1.classes_\n",
    "        dv2_class_label = iter_clf2.classes_\n",
    "        if all(dv1_class_label == dv2_class_label):\n",
    "            self.classes_ = dv1_class_label\n",
    "        else:\n",
    "            sys.exit(\"Two view classifier label not mapping\")\n",
    "\n",
    "    def get_confidence_score(self, clf_h1, clf_h2, dv1, dv2):\n",
    "        if hasattr(clf_h1, \"predict_proba\"):\n",
    "            dv1_proba = clf_h1.predict_proba(dv1)\n",
    "            dv2_proba = clf_h2.predict_proba(dv2)\n",
    "        elif hasattr(clf_h1, \"decision_function\"):    # use decision function\n",
    "            dv1_distance = np.array(clf_h1.decision_function(dv1))\n",
    "            dv2_distance = np.array(clf_h2.decision_function(dv2))\n",
    "            # if binary case, use sigmoid function to calculate probability\n",
    "            if iter_train_label.nunique()==2:\n",
    "                dv1_proba = []\n",
    "                dv2_proba = []\n",
    "                for distance in dv1_distance:\n",
    "                    dv1_proba.append([1-self.sigmoid(distance), self.sigmoid(distance)])\n",
    "                for distance in dv2_distance:\n",
    "                    dv2_proba.append([1-self.sigmoid(distance), self.sigmoid(distance)])\n",
    "            else:\n",
    "                dv1_proba = self.softmax(dv1_distance)\n",
    "                dv2_proba = self.softmax(dv2_distance)\n",
    "            dv1_proba = np.array(dv1_proba)\n",
    "            dv2_proba = np.array(dv2_proba)\n",
    "            #print(\"Distance to hyperplane (dv1): \",dv1_distance)\n",
    "            #print(\"Probability (dv1): \",dv1_proba)\n",
    "        else:\n",
    "            sys.exit(\"No confident score for process\")\n",
    "        \n",
    "        return dv1_proba, dv2_proba\n",
    "\n",
    "    def label_p_n_samples(self, proba, rank, proba_sample_idx_map):\n",
    "        U_prime_size = len(proba)\n",
    "        self_trained_sample_idx = []\n",
    "        self_trained_confident = []\n",
    "        for label, confident_rank in enumerate(rank):\n",
    "            # 0 positive sample\n",
    "            if label==0:\n",
    "                p = []\n",
    "                p_confident = []\n",
    "                index = 0\n",
    "                while(len(p) < self.p):\n",
    "                    max_conf_sample_index = confident_rank[index]\n",
    "                    # ---- if positive predict proba is more than 50% ------- #\n",
    "                    if (proba[max_conf_sample_index][label] > 0.5):\n",
    "                        #print('Sample idx: P: ', proba_sample_idx_map[max_conf_sample_index], \" : \", proba[max_conf_sample_index])\n",
    "                        p.append(proba_sample_idx_map[max_conf_sample_index])\n",
    "                        p_confident.append(proba[max_conf_sample_index][label])\n",
    "                    index +=1\n",
    "                    if (index>=U_prime_size):\n",
    "                        break\n",
    "                self_trained_sample_idx.append(p)\n",
    "                self_trained_confident.append(p_confident)\n",
    "            # 1 negative sample\n",
    "            elif label == 1:\n",
    "                n = []\n",
    "                n_confident = []\n",
    "                index = 0\n",
    "                while(len(n) < self.n):\n",
    "                    max_conf_sample_index = confident_rank[index]\n",
    "                    # ---- if negative predict proba is more than 50% ------- #\n",
    "                    if (proba[max_conf_sample_index][label] > 0.5):\n",
    "                        #print('Sample idx: N: ', proba_sample_idx_map[max_conf_sample_index], \" : \", proba[max_conf_sample_index])\n",
    "                        n.append(proba_sample_idx_map[max_conf_sample_index])\n",
    "                        n_confident.append(proba[max_conf_sample_index][label])\n",
    "                    index +=1\n",
    "                    if (index>=U_prime_size):\n",
    "                        break\n",
    "                self_trained_sample_idx.append(n)\n",
    "                self_trained_confident.append(n_confident)\n",
    "            else:\n",
    "                print(\"Class label error\")\n",
    "        return self_trained_sample_idx, self_trained_confident\n",
    "\n",
    "    def get_self_labeled_sample(self):\n",
    "        '''\n",
    "        return:\n",
    "            self-labeled new positive, self-labeled new negative (Index)\n",
    "        '''\n",
    "        \n",
    "        return self.new_labeled_idx\n",
    "\n",
    "    def plot_co_training_process(self, iterCount, data, iter_train_label, unlabeled_idx, h1_new = [], h2_new = [],\n",
    "                                 h1_new_prob = [], h2_new_prob = [], plotSavingPath=None, name=None):\n",
    "        if not os.path.exists(plotSavingPath):\n",
    "            os.makedirs(plotSavingPath)\n",
    "        pca_one = data[:,0]\n",
    "        pca_two = data[:,1]\n",
    "        # Layer 1. plot unlabel samples in u_prime\n",
    "        fig, ax = plt.subplots(figsize=(9,7))\n",
    "        ax.scatter(pca_one[unlabeled_idx], pca_two[unlabeled_idx], color='grey', label = \"unlabeled\", s = 50, alpha = 0.5)\n",
    "        # Layer 2. plot the labeled samples\n",
    "        for author in np.unique(iter_train_label):\n",
    "            ix = iter_train_label.index[iter_train_label == author].tolist()\n",
    "            # print(ix)\n",
    "            ax.scatter(pca_one[ix], pca_two[ix], cmap='viridis', label = author, s = 50, alpha = 0.5)\n",
    "        # layer 3. mark self labeled samples\n",
    "        if iterCount != 0:\n",
    "            all_h1_new = list(itertools.chain(*h1_new))\n",
    "            all_h2_new = list(itertools.chain(*h2_new))\n",
    "            temp_h1 = ax.scatter(pca_one[all_h1_new], pca_two[all_h1_new], edgecolor='black', linewidth=1, s=50)\n",
    "            temp_h1.set_facecolor(\"none\")\n",
    "            temp_h1.set_label(\"h1 self-labeled\")\n",
    "            temp_h2 = ax.scatter(pca_one[all_h2_new], pca_two[all_h2_new], edgecolor='red', linewidth=1, s=50)\n",
    "            temp_h2.set_facecolor(\"none\")\n",
    "            temp_h2.set_label(\"h2 self-labeled\")\n",
    "            # layer 4. mark new samples confidence and which view produce it\n",
    "            last_iter_h1_new = h1_new[-1]\n",
    "            last_iter_h2_new = h2_new[-1]\n",
    "            last_iter_h1_new_prob = h1_new_prob[-1]\n",
    "            last_iter_h2_new_prob = h2_new_prob[-1]\n",
    "            text = []\n",
    "            for i, idx in enumerate(last_iter_h1_new):\n",
    "                text.append(plt.text(pca_one[idx], pca_two[idx], \"{:.2f}\".format(last_iter_h1_new_prob[i]), color='black'))\n",
    "            for i, idx in enumerate(last_iter_h2_new):\n",
    "                text.append(plt.text(pca_one[idx], pca_two[idx], \"{:.2f}\".format(last_iter_h2_new_prob[i]), color='red'))\n",
    "            #print(text)\n",
    "            adjust_text(text, x=pca_one, y=pca_two, force_points=0.3, force_text=0.3, expand_points=(2, 2), \n",
    "                        expand_text=(2, 2), arrowprops=dict(arrowstyle='Simple', color='red'))\n",
    "        legend = ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.25), ncol=3,prop={'size': 13})\n",
    "        plt.title('Co-training iteration: '+ str(iterCount), fontsize=14)\n",
    "        plt.xlabel(\"First principal component\",fontsize=14)\n",
    "        plt.ylabel(\"Second principal component\",fontsize=14)\n",
    "        plt.savefig(fname=plotSavingPath+name+\"_PCA_i-\"+str(iterCount)+\".png\", dpi=100, bbox_extra_artists=(legend,), bbox_inches='tight')\n",
    "        #plt.show()\n",
    "        plt.close(\"all\")\n",
    "        \n",
    "\n",
    "    def fit(self, dv1, dv2, labels, dv1_validation, dv2_validation, label_validation, dataset_name=None, plot_save_path=None):        \n",
    "        # using all unlabeled sample instead of pool of unlabeled sample\n",
    "        self.u = len(labels)\n",
    "        # sync input datatype\n",
    "        if not all(isinstance(i, pd.DataFrame) for i in [dv1, dv2, labels]):\n",
    "            if not isinstance(dv1, pd.DataFrame):\n",
    "                dv1 = pd.DataFrame(dv1)\n",
    "            if not isinstance(dv2, pd.DataFrame):\n",
    "                dv2 = pd.DataFrame(dv2)\n",
    "            if not isinstance(labels, pd.DataFrame):\n",
    "                labels = pd.DataFrame(labels, index = dv1.index.values)\n",
    "        labels = pd.Series(labels[0].values, index=dv1.index.values) \n",
    "        \n",
    "        L, U, U_prime = self.init_L_U_U_prime(labels)\n",
    "        #print(\"All data index: \",dv1.index.values)\n",
    "        #print(\"L: \", L)\n",
    "        #print(\"U: \", U)\n",
    "        #print(\"U_prime: \", U_prime)\n",
    "        print(\"P value: \", self.p, \" N value: \", self.n)\n",
    "        \n",
    "        # index of self labeled samples\n",
    "        self.new_labeled_idx = defaultdict(list)\n",
    "        self.h1_new_idx = defaultdict(list)\n",
    "        self.h2_new_idx = defaultdict(list)\n",
    "        # when fit co-train, we collect f1 on validation samples wrt each iteration\n",
    "        self.f1_on_validation_dv1 = []\n",
    "        self.f1_on_validation_dv2 = []\n",
    "        \n",
    "        self.iterCounter = 0\n",
    "        # --------- plot PCA reduced plot for initial stage of train -------------- #\n",
    "        init_train_label = labels[L]\n",
    "        pca = PCA(n_components=2)\n",
    "        '''Notice pca will change input datatype dataframe to datatype list, keep it's order,\n",
    "           or re-assign dv1.index.values to the output'''\n",
    "        pca_dv1 = pca.fit_transform(X=dv1)\n",
    "        pca_dv2 = pca.fit_transform(X=dv2)\n",
    "        if dataset_name != None:\n",
    "            for pca_view,v_name in [(pca_dv1,\"dv1\"),(pca_dv2,\"dv2\")]:\n",
    "                self.plot_co_training_process(self.iterCounter, pca_view, init_train_label, U_prime,\n",
    "                                              plotSavingPath = plot_save_path, name = dataset_name+\"_\"+v_name)\n",
    "        #loop until we have assigned labels to every sample in U and U_prime or we hit our iteration break condition\n",
    "        while self.iterCounter < self.k and U_prime:\n",
    "            '''\n",
    "            Extract labeled training sample from training sample and train classifier, then make prediction\n",
    "            Evaluate performance of current iteration classifier\n",
    "            '''\n",
    "            # print(\"Iteration:\",self.iterCounter, \" L: \",L)\n",
    "            # print(\"Iteration:\",self.iterCounter, \" U_prime: \",U_prime)\n",
    "            # ------------- get labeled samples for train ----------- # \n",
    "            iter_train_d1 = dv1.iloc[L]\n",
    "            iter_train_d2 = dv2.iloc[L]\n",
    "            iter_train_label = labels.iloc[L]\n",
    "            # ----------- get U_prime unlabeled samples  ------------ #\n",
    "            iter_unlabeled_d1 = dv1.iloc[U_prime]\n",
    "            iter_unlabeled_d2 = dv2.iloc[U_prime]\n",
    "            # ------------ train different view with classifier ----------- #\n",
    "            iter_clf1 = copy.deepcopy(self.clf1) \n",
    "            iter_clf2 = copy.deepcopy(self.clf2)\n",
    "            iter_clf1.fit(iter_train_d1, iter_train_label)\n",
    "            iter_clf2.fit(iter_train_d2, iter_train_label)\n",
    "            self.check_iter_label_mapping(iter_clf1, iter_clf2)\n",
    "            # --------- test error on validation data --------------------- #\n",
    "            # make prediction on validation data\n",
    "            y1 = iter_clf1.predict(dv1_validation)\n",
    "            y2 = iter_clf2.predict(dv2_validation)\n",
    "            # f1 score on each iteration\n",
    "            f1_dv1 = f1_dv2 = 0\n",
    "            f1_dv1 = f1_score(label_validation, y1, average='macro')\n",
    "            f1_dv2 = f1_score(label_validation, y2, average='macro')\n",
    "            # collect f1 for current iteration\n",
    "            self.f1_on_validation_dv1.append(f1_dv1)\n",
    "            self.f1_on_validation_dv2.append(f1_dv2)\n",
    "            ''' \n",
    "            Notice here dv1_proba and dv2_proba's index is index for u' (Unlabeled data only)\n",
    "            We use index of u' to find index (position) of data in U where U and L is all data index\n",
    "            '''\n",
    "            # rank class probabilities for unlabeled sample for it's confidence measure\n",
    "            dv1_proba, dv2_proba = self.get_confidence_score(clf_h1=iter_clf1, clf_h2=iter_clf2, \n",
    "                                                             dv1=iter_unlabeled_d1, dv2=iter_unlabeled_d2)\n",
    "            proba_sample_idx_map = iter_unlabeled_d1.index\n",
    "            dv1_proba_rank = []\n",
    "            dv2_proba_rank = []\n",
    "            # proba1_rank[i] is label i's confidence measure\n",
    "            for class_proba in dv1_proba.T:\n",
    "                dv1_proba_rank.append((-class_proba).argsort())\n",
    "            for class_proba in dv2_proba.T:\n",
    "                dv2_proba_rank.append((-class_proba).argsort())\n",
    "            # print(dv1_proba)\n",
    "            # print(dv1_proba_rank)\n",
    "            # print(dv2_proba)\n",
    "            # print(dv2_proba_rank)\n",
    "            # h1 classifier self label data\n",
    "            h1_new_sample, h1_new_sample_probs = self.label_p_n_samples(dv1_proba, dv1_proba_rank, proba_sample_idx_map)\n",
    "            # h2 classifier\n",
    "            h2_new_sample, h2_new_sample_probs = self.label_p_n_samples(dv2_proba, dv2_proba_rank, proba_sample_idx_map)\n",
    "            \n",
    "            # collect statistic for plot only (before remove self-labeled sample from u')\n",
    "            iter_h1_for_plot = list(itertools.chain(*h1_new_sample))\n",
    "            iter_h2_for_plot = list(itertools.chain(*h2_new_sample))\n",
    "            iter_h1_prob = list(itertools.chain(*h1_new_sample_probs))\n",
    "            iter_h2_prob = list(itertools.chain(*h2_new_sample_probs))\n",
    "            self.h1_new_idx[\"index\"].append(iter_h1_for_plot)\n",
    "            self.h1_new_idx[\"confident\"].append(iter_h1_prob)\n",
    "            self.h2_new_idx[\"index\"].append(iter_h2_for_plot)\n",
    "            self.h2_new_idx[\"confident\"].append(iter_h2_prob)\n",
    "            '''\n",
    "            Add most confidence samples as new training samples, auto label the samples and remove it from U_prime\n",
    "            Special Case: if two view classifier give same most confident sample and p is 1, only 1 datapoint self-labeled\n",
    "            '''\n",
    "            roundNew = list(zip(h1_new_sample, h2_new_sample))\n",
    "            roundNew_flatten_unique = []\n",
    "            for label, round_new in enumerate(roundNew):\n",
    "                round_new = set([item for sublist in round_new for item in sublist])\n",
    "                round_new = [idx for idx in round_new]\n",
    "                self.new_labeled_idx[self.classes_[label]].append(round_new)\n",
    "                roundNew_flatten_unique.extend(round_new)\n",
    "                # add label to those new samples\n",
    "                #print(labels[round_new])\n",
    "                labels[round_new] = self.classes_[label]\n",
    "                #print(labels[round_new])\n",
    "                #print(self.classes_[label],\" (u' idx): \",round_new)\n",
    "            #print(roundNew_flatten_unique)\n",
    "            # extend the labeled sample\n",
    "            L.extend(roundNew_flatten_unique)\n",
    "            # remove the labeled sample from U_prime\n",
    "            U_prime = [x for x in U_prime if x not in roundNew_flatten_unique]\n",
    "            #print(U_prime)\n",
    "            # randomly choice 2p+2n examples from u to replenish u_prime\n",
    "            replenishItem = U[-(2*self.p+2*self.n):]\n",
    "            U_prime.extend(replenishItem)\n",
    "            U = U[:-len(replenishItem)]\n",
    "            self.iterCounter +=1\n",
    "            \n",
    "            # ----------- plot the co-training process -------------- #\n",
    "            if dataset_name != None:\n",
    "                new_train_label = labels[L]\n",
    "                # self_labeled_idx = [val for sublist in self_labeled_idx_temp for subsublist in sublist for val in subsublist]\n",
    "                print(\"Iteration\",self.iterCounter,\" h1 new: \", iter_h1_for_plot, \" probs: \", iter_h1_prob)\n",
    "                print(\"Iteration\",self.iterCounter,\" h2 new: \", iter_h2_for_plot, \" probs: \", iter_h2_prob)\n",
    "                # ----- save pca reduced plot ------ #\n",
    "                for pca_view,v_name in [(pca_dv1,\"dv1\"),(pca_dv2,\"dv2\")]:\n",
    "                    self.plot_co_training_process(self.iterCounter, pca_view, new_train_label, U_prime,\n",
    "                                                  self.h1_new_idx[\"index\"], self.h2_new_idx[\"index\"],\n",
    "                                                  self.h1_new_idx[\"confident\"], self.h2_new_idx[\"confident\"],\n",
    "                                                  plotSavingPath = plot_save_path, name = dataset_name+\"_\"+v_name)\n",
    "            \n",
    "        print(\"Total Labeled number: \", len(L), \" Still unlabeled number: \", len(U_prime))\n",
    "        #print(self.f1_on_validation_dv1)\n",
    "        #print(self.f1_on_validation_dv2)\n",
    "        # final train\n",
    "        newtrain_d1 = dv1.iloc[L]\n",
    "        newtrain_d2 = dv2.iloc[L]\n",
    "        self.clf1.fit(newtrain_d1, labels.iloc[L])\n",
    "        self.clf2.fit(newtrain_d2, labels.iloc[L])\n",
    "        '''\n",
    "        Evalutation plot for co-training process, save f1 score vs number of iteration plot\n",
    "        '''\n",
    "        if dataset_name != None:\n",
    "            default_text_based = [self.f1_on_validation_dv1[0]] * self.iterCounter\n",
    "            default_citation_based = [self.f1_on_validation_dv2[0]] * self.iterCounter\n",
    "            default_step = np.arange(0,self.iterCounter)\n",
    "            co_train_text_based = self.f1_on_validation_dv1[1:]\n",
    "            co_train_citation_based = self.f1_on_validation_dv2[1:]\n",
    "            co_training_step = np.arange(1,self.iterCounter)\n",
    "\n",
    "            fig = plt.figure()\n",
    "            ax = plt.axes()\n",
    "            plt.plot(default_step, default_text_based, linestyle='dashed', label=\"Text based default\")\n",
    "            plt.plot(default_step, default_citation_based, linestyle='dashdot', label=\"Citation based default\")\n",
    "            plt.plot(co_training_step, co_train_text_based, linestyle='solid', marker = \"*\", label=\"Text based\")\n",
    "            plt.plot(co_training_step, co_train_citation_based, linestyle='dotted', marker = \"+\", label=\"Citation based\")\n",
    "            ax.autoscale_view()\n",
    "            legend = ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.2), ncol=2)\n",
    "            plt.xlabel('Co-Training Iterations')\n",
    "            plt.ylabel('F1 score')\n",
    "            plt.savefig((plot_save_path+dataset_name+\"_co_train_iteration_f1.png\"), dpi=150, bbox_extra_artists=(legend,), bbox_inches='tight')\n",
    "            # plt.show()\n",
    "            plt.close(\"all\")\n",
    "        return \"Training Done\"\n",
    "\n",
    "    def co_train_process_f1(self):\n",
    "        return self.f1_on_validation_dv1, self.f1_on_validation_dv2\n",
    "\n",
    "    def get_iter_count(self):\n",
    "        return self.iterCounter\n",
    "\n",
    "    def predict(self, dv1, dv2):\n",
    "        dv1_predict = self.clf1.predict(dv1)\n",
    "        dv2_predict = self.clf2.predict(dv2)\n",
    "        #fill pred with -1 so we can identify the samples in which error occur\n",
    "        y_pred = [\"-1\"] * dv1.shape[0]\n",
    "        for i, (dv1_y, dv2_y) in enumerate(zip(dv1_predict, dv2_predict)):\n",
    "            # if both agree on label\n",
    "            if dv1_y == dv2_y:\n",
    "                y_pred[i] = dv1_y\n",
    "            # If disagree on label and support proba method: We times probability together, choice class have higher probabilities\n",
    "            elif hasattr(self.clf1, \"predict_proba\") and hasattr(self.clf2, \"predict_proba\"):\n",
    "                h1_probas = self.clf1.predict_proba([dv1.iloc[i]])[0]\n",
    "                h2_probas = self.clf2.predict_proba([dv2.iloc[i]])[0]\n",
    "                final_y_probas = [proba_y1 * proba_y2 for (proba_y1, proba_y2) in zip(h1_probas, h2_probas)]\n",
    "                #print(\"h1 and h2 disagree on\",i, \" h1 Proba : \",h1_probas, \" h2 Proba: \", h2_probas)\n",
    "                #print(\"product probas:\",final_y_probas)\n",
    "                max_prob_idx = final_y_probas.index(max(final_y_probas))\n",
    "                y_pred[i] = self.classes_[max_prob_idx]\n",
    "                #print(\"result idx: \", max_prob_idx, \" result: \",y_pred[i])\n",
    "            # If disagree on label and not support proba method: We use decision_function to first calculate probability,\n",
    "            # then we times calculate probability together, choice class have higher probabilities\n",
    "            elif hasattr(self.clf1, \"decision_function\") and hasattr(self.clf2, \"decision_function\"):\n",
    "                dv1_distance = self.clf1.decision_function([dv1.iloc[i]])\n",
    "                dv2_distance = self.clf2.decision_function([dv2.iloc[i]])\n",
    "                if len(self.clf1.classes_)==2:\n",
    "                    h1_probas = np.array([1-self.sigmoid(dv1_distance), self.sigmoid(dv1_distance)])\n",
    "                    h2_probas = np.array([1-self.sigmoid(dv2_distance), self.sigmoid(dv2_distance)])\n",
    "                else:\n",
    "                    h1_probas = self.softmax(dv1_distance)\n",
    "                    h2_probas = self.softmax(dv2_distance)\n",
    "                final_y_probas = [proba_y1 * proba_y2 for (proba_y1, proba_y2) in zip(h1_probas, h2_probas)]\n",
    "                #print(\"h1 and h2 disagree on\",i, \" h1 Proba : \",h1_probas, \" h2 Proba: \", h2_probas)\n",
    "                #print(\"product probas:\",final_y_probas)\n",
    "                max_prob_idx = final_y_probas.index(max(final_y_probas))\n",
    "                y_pred[i] = self.classes_[max_prob_idx]\n",
    "                #print(\"result idx: \", max_prob_idx, \" result: \",y_pred[i])\n",
    "            # disagree and not support any confidence measure\n",
    "            else:\n",
    "                sys.exit(\"Error occur\")\n",
    "\n",
    "        # convert final result to np array\n",
    "        y_pred_np_array = np.asarray(y_pred)\n",
    "        return y_pred_np_array\n",
    "\n",
    "    def predict_proba(self, dv1, dv2):\n",
    "        # the predicted probabilities is simply a product (*) of probabilities given from each classifier trained\n",
    "        h1_probas = self.clf1.predict_proba(dv1)\n",
    "        h2_probas = self.clf2.predict_proba(dv2)\n",
    "        \n",
    "        proba = (h1_probas*h2_probas)\n",
    "        return proba\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved co-training\n",
    "\n",
    "Possible improvement:\n",
    "\n",
    "Part 1: Parameter input\n",
    "1. Using two different algorithm for view one and view two.\n",
    "2. Instead of using parameter k to control number of iteration, using a stop criterion where if unlabeled sample can't get more than 95% of confidence on their confidence score, then stop iteration and finish co-training.\n",
    "3. Parameter p, n are not needed, use input data class ratio replace it (unlabeled distribution may different from labeled, this is unlikely works in reality).\n",
    "4. u prime not needed, use all unlabeled data.\n",
    "\n",
    "Part 2: Change confident measure method\n",
    "1. Changes method used when determine which class unlabeled sample belones to, instead of using confident score, we could use classifiers saved during each iteration of training to perform an majority voting. (Need to careful about number of iteration in co-training, since adding bad unlabel data as label data is easy to occur) (Source: 2004_Co-training and Self-training for Word Sense Disambiguation)\n",
    "\n",
    "Part 3: Change details in confident score method\n",
    "1. Add an third classifier and only train with original labeled samples(no-self-labeled sample), then use third classifier to evaluate self-labeled samples and get it's confidence score. Combine three classifier's confidence score together to get final decision.\n",
    "2. COTRADE method: construct undirected neighborhood graph using all samples, after finding most confident sample with it's corresponding class. Then, calculate distance between all labeled samples in this class and most confident sample in this class. This distance from graph + probability from supervised algorithm will be used together as confidence score.\n",
    "\n",
    "Part 4: Add checking to reduce the chance of adding mislabeled data\n",
    "1. Add an check on validation after new label sample is added, if not improving h1/h2, remove new labeled sample.\n",
    "\n",
    "Part 5: Change algorithm and make it work for muti-class\n",
    "1. Using same concept of muti-class SVM, train many OVR binary classifier. (To expensive)\n",
    "2. Allow algorithm directly take muti-class case and using same concept as binary co-training.\n",
    "\n",
    "\n",
    "My idea:\n",
    "1. Use one classifier (check)\n",
    "2. Have parameter k as default, but can be stoped early (check)\n",
    "3. Parameter p, n are replace with sl_base which set as input data class ratio*sl_base. (Allow muti-class case and will maintaining the class distribution in L. However, unlabeled distribution may different from labeled, this is unlikely works in reality) (check)\n",
    "4. u prime not needed, use all unlabeled data. (check)\n",
    "5. Adopt smooth co-training method and add an evaluation to all classifier saved during smooth co-training. This evaluation will accumulate the probability of each past iteration as score (check)\n",
    "6. Adopt COTRADE method, their assumption where that a correctly labeled example should be very close to samples in L with corresponding label same to be very useful. Thus a k-mean clustering is train with labeled, then transform unlabel data into same instance space as labeled, then calculate distance to each cluster's centroid, then use distance perfrom an softmax to get reverse of probability of sample belone to class. After that 1- reversed probability get probability and add it as part of co-train evaluation.\n",
    "7. Beside smooth co-training and COTRADE method, we also check current iteration probability, if most confidence sample is less than 80% for both view, we stop iteration no matter what."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T05:56:38.605087Z",
     "start_time": "2020-05-13T05:56:27.941638Z"
    },
    "code_folding": [
     15,
     31,
     35,
     38,
     54,
     66,
     67,
     93,
     126,
     155,
     163,
     208,
     440,
     443,
     446,
     490
    ]
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import warnings\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import precision_score, recall_score,f1_score,accuracy_score\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "\n",
    "# create co training classifier\n",
    "class Improved_co_training_clf(object):\n",
    "    \n",
    "    def __init__(self, clf1, clf2=None, sl_total=2, k=30, **args):\n",
    "        \n",
    "        self.clf1 = clf1\n",
    "        # assume co_training on one classifier\n",
    "        if clf2 == None:\n",
    "            self.clf2 = copy.deepcopy(clf1)\n",
    "        else:\n",
    "            self.clf2 = clf2\n",
    "        # self-label step for each iteration\n",
    "        if sl_total<2:\n",
    "            sl_total = 2\n",
    "        self.sl_total = sl_total\n",
    "        self.iter_new_size =[]\n",
    "        # number of iteration\n",
    "        self.k = k\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return np.around(1 / (1 + np.exp(-x)), decimals=4).item()\n",
    "\n",
    "    def init_L_U_U_prime(self, labels):\n",
    "        # index of the samples that are initially labeled\n",
    "        L = labels.index[labels != -1].tolist()\n",
    "        # index of unlabeled samples\n",
    "        U = labels.index[labels == -1].tolist()\n",
    "        print(\"Initial L size: \", len(L))\n",
    "        print(\"Initial U size: \", len(U))\n",
    "        # random drawing sample from U\n",
    "        random.shuffle(U)\n",
    "        U_prime = U[-min(len(U), self.u):]\n",
    "        # remove the samples in U_prime from U\n",
    "        U = U[:-len(U_prime)]\n",
    "        print(\"U size after drawing sample to U prime:\",len(U))\n",
    "        print(\"Initial U prime size: \", len(U_prime))\n",
    "        return L, U, U_prime\n",
    "    \n",
    "    def check_iter_label_mapping(self, iter_clf1, iter_clf2):\n",
    "        '''\n",
    "        In theory, it shouldn't occur that label not mapping since it trained on same dataset but different view\n",
    "        But add a check to make sure it won't occur and save the class mapping for later label unlabeled sample\n",
    "        '''\n",
    "        dv1_class_label = iter_clf1.classes_\n",
    "        dv2_class_label = iter_clf2.classes_\n",
    "        if all(dv1_class_label == dv2_class_label):\n",
    "            self.classes_ = dv1_class_label\n",
    "        else:\n",
    "            sys.exit(\"Two view classifier label not mapping\")\n",
    "\n",
    "    def get_confidence_score(self, clf_h1, clf_h2, dv1, dv2):\n",
    "        if hasattr(clf_h1, \"predict_proba\"):\n",
    "            dv1_proba = clf_h1.predict_proba(dv1)\n",
    "            dv2_proba = clf_h2.predict_proba(dv2)\n",
    "        elif hasattr(clf_h1, \"decision_function\"):    # use decision function\n",
    "            dv1_distance = np.array(clf_h1.decision_function(dv1))\n",
    "            dv2_distance = np.array(clf_h2.decision_function(dv2))\n",
    "            # if binary case, use sigmoid function to calculate probability\n",
    "            if iter_train_label.nunique()==2:\n",
    "                dv1_proba = []\n",
    "                dv2_proba = []\n",
    "                for distance in dv1_distance:\n",
    "                    dv1_proba.append([1-self.sigmoid(distance), self.sigmoid(distance)])\n",
    "                for distance in dv2_distance:\n",
    "                    dv2_proba.append([1-self.sigmoid(distance), self.sigmoid(distance)])\n",
    "            else:\n",
    "                dv1_proba = self.softmax(dv1_distance)\n",
    "                dv2_proba = self.softmax(dv2_distance)\n",
    "            dv1_proba = np.array(dv1_proba)\n",
    "            dv2_proba = np.array(dv2_proba)\n",
    "            #print(\"Distance to hyperplane (dv1): \",dv1_distance)\n",
    "            #print(\"Probability (dv1): \",dv1_proba)\n",
    "        else:\n",
    "            sys.exit(\"No confident score for process\")\n",
    "        \n",
    "        return dv1_proba, dv2_proba\n",
    "\n",
    "    def get_cluster_distance_as_proba(self, label_train, iter_train_label, unlabeled):\n",
    "        kmeans = KMeans(n_clusters=len(self.classes_)).fit(label_train)\n",
    "        # map correct idx to label\n",
    "        kmeans_predict = kmeans.labels_\n",
    "        label_mapping = list(zip(iter_train_label, kmeans_predict))\n",
    "        lmc = Counter(label_mapping)\n",
    "        final_label_mapping = []\n",
    "        for label in np.unique(iter_train_label):\n",
    "            for (item,index),count in lmc.most_common():\n",
    "                if item == label:\n",
    "                    #print(\"label:\",label, \" idx:\",index, \" count:\",count)\n",
    "                    final_label_mapping.append((item,index))\n",
    "                    break\n",
    "        #print(final_label_mapping)\n",
    "        \n",
    "        # convert distance to centroid to probability of belone to class\n",
    "        dist_to_centroid = kmeans.transform(unlabeled)\n",
    "        cluster_proba = []\n",
    "        for distance in dist_to_centroid:\n",
    "            reverse_proba = self.softmax(distance)\n",
    "            proba = [1-x for x in reverse_proba]\n",
    "            cluster_proba.append(proba)\n",
    "        cluster_proba = np.array(cluster_proba)\n",
    "        \n",
    "        # map the proba same order as train in supervised learning\n",
    "        new_permutation = [idx for label, idx in final_label_mapping]\n",
    "        #print(new_permutation)\n",
    "        idx = np.empty_like(new_permutation)\n",
    "        idx[new_permutation] = np.arange(len(new_permutation))\n",
    "        cluster_proba[:] = cluster_proba[:, idx]\n",
    "        \n",
    "        return cluster_proba\n",
    "\n",
    "    def label_samples(self, score, rank, score_sample_idx_map,curr_iter_proba):\n",
    "        U_prime_size = len(score)\n",
    "        self_trained_sample_idx = []\n",
    "        self_trained_confident = []\n",
    "        self_trained_iter_proba = []\n",
    "        for label, confident_rank in enumerate(rank):\n",
    "            new_label_sample_size = [size for item,size in self.iter_new_size if self.classes_[label] in item][0]\n",
    "            new_sample_idx = []\n",
    "            new_sample_confident_score = []\n",
    "            new_sample_proba = []\n",
    "            index = 0\n",
    "            while(len(new_sample_idx) < new_label_sample_size):\n",
    "                max_conf_sample_index = confident_rank[index]\n",
    "                if curr_iter_proba[max_conf_sample_index][label]<0.8:\n",
    "                    #print('Most confident idx: ', max_conf_sample_index,\":\",score_sample_idx_map[max_conf_sample_index],\" have score: \",score[max_conf_sample_index], \"have probability \",curr_iter_proba[max_conf_sample_index], \" less than 0.8 thus not used\")\n",
    "                    index +=1\n",
    "                else:\n",
    "                    #print('Class:', self.classes_[label],' idx: ',max_conf_sample_index,\":\",score_sample_idx_map[max_conf_sample_index], \" score: \", score[max_conf_sample_index], \" iter proba: \", curr_iter_proba[max_conf_sample_index])\n",
    "                    new_sample_idx.append(score_sample_idx_map[max_conf_sample_index])\n",
    "                    new_sample_confident_score.append(score[max_conf_sample_index][label])\n",
    "                    new_sample_proba.append(curr_iter_proba[max_conf_sample_index][label])\n",
    "                    index +=1\n",
    "                if (index>=U_prime_size) :\n",
    "                    break\n",
    "            self_trained_sample_idx.append(new_sample_idx)\n",
    "            self_trained_confident.append(new_sample_confident_score)\n",
    "            self_trained_iter_proba.append(new_sample_proba)\n",
    "        return self_trained_sample_idx, self_trained_confident, self_trained_iter_proba\n",
    "\n",
    "    def get_self_labeled_sample(self):\n",
    "        '''\n",
    "        return:\n",
    "            self-labeled new positive, self-labeled new negative (Index)\n",
    "        '''\n",
    "        \n",
    "        return self.new_labeled_idx\n",
    "\n",
    "    def plot_co_training_process(self, iterCount, data, iter_train_label, unlabeled_idx, h1_new = [], h2_new = [],\n",
    "                                 h1_new_prob = [], h2_new_prob = [], plotSavingPath=None, name=None):\n",
    "        if not os.path.exists(plotSavingPath):\n",
    "            os.makedirs(plotSavingPath)\n",
    "        pca_one = data[:,0]\n",
    "        pca_two = data[:,1]\n",
    "        # Layer 1. plot unlabel samples in u_prime\n",
    "        fig, ax = plt.subplots(figsize=(9,7))\n",
    "        ax.scatter(pca_one[unlabeled_idx], pca_two[unlabeled_idx], color='grey', label = \"unlabeled\", s = 50, alpha = 0.5)\n",
    "        # Layer 2. plot the labeled samples\n",
    "        for author in np.unique(iter_train_label):\n",
    "            ix = iter_train_label.index[iter_train_label == author].tolist()\n",
    "            # print(ix)\n",
    "            ax.scatter(pca_one[ix], pca_two[ix], cmap='viridis', label = author, s = 50, alpha = 0.5)\n",
    "        # layer 3. mark self labeled samples\n",
    "        if iterCount != 0:\n",
    "            all_h1_new = list(itertools.chain(*h1_new))\n",
    "            all_h2_new = list(itertools.chain(*h2_new))\n",
    "            temp_h1 = ax.scatter(pca_one[all_h1_new], pca_two[all_h1_new], edgecolor='black', linewidth=1, s=50)\n",
    "            temp_h1.set_facecolor(\"none\")\n",
    "            temp_h1.set_label(\"h1 self-labeled\")\n",
    "            temp_h2 = ax.scatter(pca_one[all_h2_new], pca_two[all_h2_new], edgecolor='red', linewidth=1, s=50)\n",
    "            temp_h2.set_facecolor(\"none\")\n",
    "            temp_h2.set_label(\"h2 self-labeled\")\n",
    "            # layer 4. mark new samples confidence and which view produce it\n",
    "            last_iter_h1_new = h1_new[-1]\n",
    "            last_iter_h2_new = h2_new[-1]\n",
    "            last_iter_h1_new_prob = h1_new_prob[-1]\n",
    "            last_iter_h2_new_prob = h2_new_prob[-1]\n",
    "            text = []\n",
    "            for i, idx in enumerate(last_iter_h1_new):\n",
    "                text.append(plt.text(pca_one[idx], pca_two[idx], \"{:.2f}\".format(last_iter_h1_new_prob[i]), color='black'))\n",
    "            for i, idx in enumerate(last_iter_h2_new):\n",
    "                text.append(plt.text(pca_one[idx], pca_two[idx], \"{:.2f}\".format(last_iter_h2_new_prob[i]), color='red'))\n",
    "            adjust_text(text, x=pca_one, y=pca_two, force_points=0.3, force_text=0.3, expand_points=(2, 2), \n",
    "                        expand_text=(2, 2), arrowprops=dict(arrowstyle='Simple', color='red'))\n",
    "        legend = ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.25), ncol=3,prop={'size': 13})\n",
    "        plt.title('Co-training iteration: '+ str(iterCount), fontsize=14)\n",
    "        plt.xlabel(\"First principal component\",fontsize=14)\n",
    "        plt.ylabel(\"Second principal component\",fontsize=14)\n",
    "        plt.savefig(fname=plotSavingPath+name+\"_PCA_i-\"+str(iterCount)+\".png\", dpi=100, bbox_extra_artists=(legend,), bbox_inches='tight')\n",
    "        # plt.show()\n",
    "        plt.close(\"all\")\n",
    "        \n",
    "\n",
    "    def fit(self, dv1, dv2, labels, dv1_validation, dv2_validation, label_validation, dataset_name=None, plot_save_path=None):        \n",
    "        # using all unlabeled sample instead of pool of unlabeled sample\n",
    "        self.u = len(labels)\n",
    "        # sync input datatype\n",
    "        if not all(isinstance(i, pd.DataFrame) for i in [dv1, dv2, labels]):\n",
    "            if not isinstance(dv1, pd.DataFrame):\n",
    "                dv1 = pd.DataFrame(dv1)\n",
    "            if not isinstance(dv2, pd.DataFrame):\n",
    "                dv2 = pd.DataFrame(dv2)\n",
    "            if not isinstance(labels, pd.DataFrame):\n",
    "                labels = pd.DataFrame(labels, index = dv1.index.values)\n",
    "        labels = pd.Series(labels[0].values, index=dv1.index.values) \n",
    "        \n",
    "        L, U, U_prime = self.init_L_U_U_prime(labels)\n",
    "        #print(\"All data index: \",dv1.index.values)\n",
    "        #print(\"L: \", L)\n",
    "        #print(\"U: \", U)\n",
    "        #print(\"U_prime: \", U_prime)\n",
    "        \n",
    "        # get class ratio\n",
    "        c = Counter(labels)\n",
    "        class_ratio = [(i, c[i] / (len(L))) for i in c if i != -1]\n",
    "        for label, ratio in class_ratio:\n",
    "            self.iter_new_size.append((label,np.ceil(self.sl_total*ratio)))\n",
    "        print(self.iter_new_size)\n",
    "        \n",
    "        # index of self labeled samples\n",
    "        self.new_labeled_idx = defaultdict(list)\n",
    "        self.h1_new_idx = defaultdict(list)\n",
    "        self.h2_new_idx = defaultdict(list)\n",
    "        # save clf in each iteration\n",
    "        self.iter_clf = []\n",
    "        # when fit co-train, we collect f1 on validation samples wrt each iteration\n",
    "        self.f1_on_validation_dv1 = []\n",
    "        self.f1_on_validation_dv2 = []\n",
    "        \n",
    "        self.iterCounter = 0\n",
    "        # --------- plot PCA reduced plot for initial stage of train -------------- #\n",
    "        init_train_label = labels[L]\n",
    "        pca = PCA(n_components=2)\n",
    "        '''Notice pca will change input datatype dataframe to datatype list, keep it's order,\n",
    "           or re-assign dv1.index.values to the output'''\n",
    "        pca_dv1 = pca.fit_transform(X=dv1)\n",
    "        pca_dv2 = pca.fit_transform(X=dv2)\n",
    "        if dataset_name != None:\n",
    "            for pca_view,v_name in [(pca_dv1,\"dv1\"),(pca_dv2,\"dv2\")]:\n",
    "                self.plot_co_training_process(self.iterCounter, pca_view, init_train_label, U_prime,\n",
    "                                              plotSavingPath = plot_save_path, name = dataset_name+\"_\"+v_name)\n",
    "        #loop until we have assigned labels to every sample in U and U_prime or we hit our iteration break condition\n",
    "        while self.iterCounter < self.k and U_prime:\n",
    "            '''\n",
    "            Extract labeled training sample from training sample and train classifier, then make prediction\n",
    "            Evaluate performance of current iteration classifier\n",
    "            '''\n",
    "            # print(\"Iteration:\",self.iterCounter, \" L: \",L)\n",
    "            # print(\"Iteration:\",self.iterCounter, \" U_prime: \",U_prime)\n",
    "            # get labeled samples and their label\n",
    "            iter_train_d1 = dv1.iloc[L]\n",
    "            iter_train_d2 = dv2.iloc[L]\n",
    "            iter_train_label = labels.iloc[L]\n",
    "            # get U_prime unlabeled samples\n",
    "            iter_unlabeled_d1 = dv1.iloc[U_prime]\n",
    "            iter_unlabeled_d2 = dv2.iloc[U_prime]\n",
    "            # ------------ train different view with classifier ----------- #\n",
    "            iter_clf1 = copy.deepcopy(self.clf1) \n",
    "            iter_clf2 = copy.deepcopy(self.clf2)\n",
    "            iter_clf1.fit(iter_train_d1, iter_train_label)\n",
    "            iter_clf2.fit(iter_train_d2, iter_train_label)\n",
    "            self.check_iter_label_mapping(iter_clf1, iter_clf2)\n",
    "            # --------- test error on validation data --------------------- #\n",
    "            # make prediction on validation data\n",
    "            y1 = iter_clf1.predict(dv1_validation)\n",
    "            y2 = iter_clf2.predict(dv2_validation)\n",
    "            # f1 score on each iteration\n",
    "            f1_dv1 = f1_dv2 = 0\n",
    "            f1_dv1 = f1_score(label_validation, y1, average='macro')\n",
    "            f1_dv2 = f1_score(label_validation, y2, average='macro')\n",
    "            \n",
    "            # collect f1 for current iteration\n",
    "            self.f1_on_validation_dv1.append(f1_dv1)\n",
    "            self.f1_on_validation_dv2.append(f1_dv2)\n",
    "            self.iter_clf.append((self.iterCounter,iter_clf1,iter_clf2))\n",
    "            ''' \n",
    "            Confidence measure:\n",
    "            Part 1: Accumulate predicted probability on each iteration as score\n",
    "            Case 1: Two view agree with large confidence score\n",
    "            Case 2: Two view disagree, but one have large confidence socre\n",
    "            Case 3: Two view disagree and both have large confidence socre\n",
    "            Case 4: Two view agree/disagree with small confidence score\n",
    "            For case 1,2,3, add it's score to final proba for rank and this accumulation of score show smooth problem \n",
    "            created by case 2 and 3. \n",
    "            For case 4, give score [0,0] to final proba for this iteration since clf not confident on this sample\n",
    "            '''\n",
    "            dv1_diff_iter_score = []\n",
    "            dv2_diff_iter_score = []\n",
    "            for iteration, h1_clf, h2_clf in self.iter_clf:\n",
    "                iter_dv1_proba, iter_dv2_proba = self.get_confidence_score(clf_h1=h1_clf, clf_h2=h2_clf, \n",
    "                                                                           dv1=iter_unlabeled_d1, dv2=iter_unlabeled_d2)\n",
    "                dv1_diff_iter_score.append(iter_dv1_proba)\n",
    "                dv2_diff_iter_score.append(iter_dv2_proba)\n",
    "            curr_iter_dv1_score = dv1_diff_iter_score[-1]\n",
    "            curr_iter_dv2_score = dv2_diff_iter_score[-1]\n",
    "            # add score in each iteration\n",
    "            dv1_final_score = np.array([sum(x) for x in zip(*dv1_diff_iter_score)])\n",
    "            dv2_final_score = np.array([sum(x) for x in zip(*dv2_diff_iter_score)])\n",
    "            '''\n",
    "            Part 2: Incorporate clustering distance as part of score\n",
    "            '''\n",
    "            dv1_cluster_proba = self.get_cluster_distance_as_proba(label_train=iter_train_d1, iter_train_label=iter_train_label,\n",
    "                                                                   unlabeled=iter_unlabeled_d1)\n",
    "            dv2_cluster_proba = self.get_cluster_distance_as_proba(label_train=iter_train_d2, iter_train_label=iter_train_label,\n",
    "                                                                   unlabeled=iter_unlabeled_d2)\n",
    "            dv1_final_score = np.array([sum(x) for x in zip(dv1_final_score,dv1_cluster_proba)])\n",
    "            dv2_final_score = np.array([sum(x) for x in zip(dv2_final_score,dv2_cluster_proba)])\n",
    "            '''\n",
    "            Part 3: Rank confidence and start self labeling\n",
    "            '''\n",
    "            proba_sample_idx_map=iter_unlabeled_d1.index\n",
    "            dv1_proba_rank = []\n",
    "            dv2_proba_rank = []\n",
    "            # proba1_rank[i] is label i's confidence measure\n",
    "            for class_proba in dv1_final_score.T:\n",
    "                dv1_proba_rank.append((-class_proba).argsort())\n",
    "            for class_proba in dv2_final_score.T:\n",
    "                dv2_proba_rank.append((-class_proba).argsort())\n",
    "            #print(dv1_final_score)\n",
    "            #print(dv1_proba_rank)\n",
    "            #print(dv2_final_score)\n",
    "            #print(dv2_proba_rank)\n",
    "            # h1 classifier self label data\n",
    "            h1_new_sample, h1_new_sample_score, h1_new_sample_proba = self.label_samples(dv1_final_score, \n",
    "                                                                                         dv1_proba_rank, \n",
    "                                                                                         proba_sample_idx_map, \n",
    "                                                                                         curr_iter_dv1_score)\n",
    "            # h2 classifier\n",
    "            h2_new_sample, h2_new_sample_score, h2_new_sample_proba = self.label_samples(dv2_final_score, \n",
    "                                                                                         dv2_proba_rank, \n",
    "                                                                                         proba_sample_idx_map, \n",
    "                                                                                         curr_iter_dv2_score)\n",
    "            # collect statistic for plot only (before remove self-labeled sample from u')\n",
    "            iter_h1_new_idx = list(itertools.chain(*h1_new_sample))\n",
    "            iter_h2_new_idx = list(itertools.chain(*h2_new_sample))\n",
    "            # if no new label added to L, stop iteration\n",
    "            if (len(iter_h1_new_idx)==0) and (len(iter_h2_new_idx)==0):\n",
    "                self.iterCounter += 1\n",
    "                print(\"Remaining unlabel sample is uncertain.\")\n",
    "                break\n",
    "            iter_h1_score= list(itertools.chain(*h1_new_sample_score))\n",
    "            iter_h2_score = list(itertools.chain(*h2_new_sample_score))\n",
    "            iter_h1_proba= list(itertools.chain(*h1_new_sample_proba))\n",
    "            iter_h2_proba = list(itertools.chain(*h2_new_sample_proba))\n",
    "            self.h1_new_idx[\"index\"].append(iter_h1_new_idx)\n",
    "            self.h1_new_idx[\"score\"].append(iter_h1_score)\n",
    "            self.h1_new_idx[\"last_iter_proba\"].append(iter_h1_proba)\n",
    "            self.h2_new_idx[\"index\"].append(iter_h2_new_idx)\n",
    "            self.h2_new_idx[\"score\"].append(iter_h2_score)\n",
    "            self.h2_new_idx[\"last_iter_proba\"].append(iter_h2_proba)\n",
    "            '''\n",
    "            Add most confidence samples as new training samples, auto label the samples and remove it from U_prime\n",
    "            Special Case: if two view classifier give same most confident sample, only 1 datapoint self-labeled\n",
    "            '''\n",
    "            roundNew = list(zip(h1_new_sample, h2_new_sample))\n",
    "            roundNew_flatten_unique = []\n",
    "            for label, round_new in enumerate(roundNew):\n",
    "                round_new = set([item for sublist in round_new for item in sublist])\n",
    "                round_new = [idx for idx in round_new]\n",
    "                self.new_labeled_idx[self.classes_[label]].append(round_new)\n",
    "                roundNew_flatten_unique.extend(round_new)\n",
    "                # add label to those new samples\n",
    "                #print(labels[round_new])\n",
    "                labels[round_new] = self.classes_[label]\n",
    "                #print(labels[round_new])\n",
    "                #print(self.classes_[label],\" (u' idx): \",round_new)\n",
    "            #print(roundNew_flatten_unique)\n",
    "            # extend the labeled sample\n",
    "            L.extend(roundNew_flatten_unique)\n",
    "            # remove the labeled sample from U_prime\n",
    "            U_prime = [x for x in U_prime if x not in roundNew_flatten_unique]\n",
    "            #print(U_prime)\n",
    "            # randomly choice 2p+2n examples from u to replenish u_prime\n",
    "            replenishItem = U[-(self.sl_total):]\n",
    "            U_prime.extend(replenishItem)\n",
    "            U = U[:-len(replenishItem)]\n",
    "            self.iterCounter +=1\n",
    "            \n",
    "            # ----------- plot the co-training process -------------- #\n",
    "            if dataset_name != None:\n",
    "                new_train_label = labels[L]\n",
    "                # self_labeled_idx = [val for sublist in self_labeled_idx_temp for subsublist in sublist for val in subsublist]\n",
    "                print(\"Iteration\",self.iterCounter,\" h1 new: \", iter_h1_new_idx, \" score: \", iter_h1_score)\n",
    "                print(\"Iteration\",self.iterCounter,\" h2 new: \", iter_h2_new_idx, \" score: \", iter_h2_score)\n",
    "                # ----- save pca reduced plot ------ #\n",
    "                for pca_view,v_name in [(pca_dv1,\"dv1\"),(pca_dv2,\"dv2\")]:\n",
    "                    self.plot_co_training_process(self.iterCounter, pca_view, new_train_label, U_prime,\n",
    "                                                  self.h1_new_idx[\"index\"], self.h2_new_idx[\"index\"],\n",
    "                                                  self.h1_new_idx[\"last_iter_proba\"], self.h2_new_idx[\"last_iter_proba\"],\n",
    "                                                  plotSavingPath = plot_save_path, name = dataset_name+\"_\"+v_name)\n",
    "            \n",
    "        print(\"Total Labeled number: \", len(L), \" Still unlabeled number: \", len(U_prime))\n",
    "        #print(self.f1_on_validation_dv1)\n",
    "        #print(self.f1_on_validation_dv2)\n",
    "        # final train\n",
    "        newtrain_d1 = dv1.iloc[L]\n",
    "        newtrain_d2 = dv2.iloc[L]\n",
    "        self.clf1.fit(newtrain_d1, labels.iloc[L])\n",
    "        self.clf2.fit(newtrain_d2, labels.iloc[L])\n",
    "        '''\n",
    "        Evalutation plot for co-training process, save f1 score vs number of iteration plot\n",
    "        '''\n",
    "        if dataset_name != None:\n",
    "            default_text_based = [self.f1_on_validation_dv1[0]] * self.iterCounter\n",
    "            default_citation_based = [self.f1_on_validation_dv2[0]] * self.iterCounter\n",
    "            default_step = np.arange(0,self.iterCounter)\n",
    "            co_train_text_based = self.f1_on_validation_dv1[1:]\n",
    "            co_train_citation_based = self.f1_on_validation_dv2[1:]\n",
    "            co_training_step = np.arange(1,self.iterCounter)\n",
    "\n",
    "            fig = plt.figure()\n",
    "            ax = plt.axes()\n",
    "            plt.plot(default_step, default_text_based, linestyle='dashed', label=\"Text based default\")\n",
    "            plt.plot(default_step, default_citation_based, linestyle='dashdot', label=\"Citation based default\")\n",
    "            plt.plot(co_training_step, co_train_text_based, linestyle='solid', marker = \"*\", label=\"Text based\")\n",
    "            plt.plot(co_training_step, co_train_citation_based, linestyle='dotted', marker = \"+\", label=\"Citation based\")\n",
    "            ax.autoscale_view()\n",
    "            legend = ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.2), ncol=2)\n",
    "            plt.xlabel('Co-Training Iterations')\n",
    "            plt.ylabel('F1 score')\n",
    "            plt.savefig((plot_save_path+dataset_name+\"_co_train_iteration_f1.png\"), dpi=150, bbox_extra_artists=(legend,), bbox_inches='tight')\n",
    "            # plt.show()\n",
    "            plt.close(\"all\")\n",
    "        return \"Training Done\"\n",
    "\n",
    "    def co_train_process_f1(self):\n",
    "        return self.f1_on_validation_dv1, self.f1_on_validation_dv2\n",
    "\n",
    "    def get_iter_count(self):\n",
    "        return self.iterCounter\n",
    "\n",
    "    def predict(self, dv1, dv2):\n",
    "        dv1_predict = self.clf1.predict(dv1)\n",
    "        dv2_predict = self.clf2.predict(dv2)\n",
    "        #fill pred with -1 so we can identify the samples in which error occur\n",
    "        y_pred = [\"-1\"] * dv1.shape[0]\n",
    "        for i, (dv1_y, dv2_y) in enumerate(zip(dv1_predict, dv2_predict)):\n",
    "            # if both agree on label\n",
    "            if dv1_y == dv2_y:\n",
    "                y_pred[i] = dv1_y\n",
    "            # If disagree on label and support proba method: We times probability together, choice class have higher probabilities\n",
    "            elif hasattr(self.clf1, \"predict_proba\") and hasattr(self.clf2, \"predict_proba\"):\n",
    "                h1_probas = self.clf1.predict_proba([dv1.iloc[i]])[0]\n",
    "                h2_probas = self.clf2.predict_proba([dv2.iloc[i]])[0]\n",
    "                final_y_probas = [proba_y1 * proba_y2 for (proba_y1, proba_y2) in zip(h1_probas, h2_probas)]\n",
    "                #print(\"h1 and h2 disagree on\",i, \" h1 Proba : \",h1_probas, \" h2 Proba: \", h2_probas)\n",
    "                #print(\"product probas:\",final_y_probas)\n",
    "                max_prob_idx = final_y_probas.index(max(final_y_probas))\n",
    "                y_pred[i] = self.classes_[max_prob_idx]\n",
    "                #print(\"result idx: \", max_prob_idx, \" result: \",y_pred[i])\n",
    "            # If disagree on label and not support proba method: We use decision_function to first calculate probability,\n",
    "            # then we times calculate probability together, choice class have higher probabilities\n",
    "            elif hasattr(self.clf1, \"decision_function\") and hasattr(self.clf2, \"decision_function\"):\n",
    "                dv1_distance = self.clf1.decision_function([dv1.iloc[i]])\n",
    "                dv2_distance = self.clf2.decision_function([dv2.iloc[i]])\n",
    "                if len(self.clf1.classes_)==2:\n",
    "                    h1_probas = np.array([1-self.sigmoid(dv1_distance), self.sigmoid(dv1_distance)])\n",
    "                    h2_probas = np.array([1-self.sigmoid(dv2_distance), self.sigmoid(dv2_distance)])\n",
    "                else:\n",
    "                    h1_probas = self.softmax(dv1_distance)\n",
    "                    h2_probas = self.softmax(dv2_distance)\n",
    "                final_y_probas = [proba_y1 * proba_y2 for (proba_y1, proba_y2) in zip(h1_probas, h2_probas)]\n",
    "                #print(\"h1 and h2 disagree on\",i, \" h1 Proba : \",h1_probas, \" h2 Proba: \", h2_probas)\n",
    "                #print(\"product probas:\",final_y_probas)\n",
    "                max_prob_idx = final_y_probas.index(max(final_y_probas))\n",
    "                y_pred[i] = self.classes_[max_prob_idx]\n",
    "                #print(\"result idx: \", max_prob_idx, \" result: \",y_pred[i])\n",
    "            # disagree and not support any confidence measure\n",
    "            else:\n",
    "                sys.exit(\"Error occur\")\n",
    "\n",
    "        # convert final result to np array\n",
    "        y_pred_np_array = np.asarray(y_pred)\n",
    "        return y_pred_np_array\n",
    "\n",
    "    def predict_proba(self, dv1, dv2):\n",
    "        # the predicted probabilities is simply a product (*) of probabilities given from each classifier trained\n",
    "        h1_probas = self.clf1.predict_proba(dv1)\n",
    "        h2_probas = self.clf2.predict_proba(dv2)\n",
    "        \n",
    "        proba = (h1_probas*h2_probas)\n",
    "        return proba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-13T05:56:20.293Z"
    },
    "code_folding": [
     13,
     50,
     83,
     104,
     164,
     177,
     195,
     230
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import collections\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score,f1_score,accuracy_score\n",
    "\n",
    "# cross validation\n",
    "def k_fold_cv_all_algorithm(dv1, dv2, label, init_labeled_size, muti_view_clf=[], combined_clf=[],\n",
    "                            num_fold=10, dataset_name=None, plot_save_path=None, validation=True):\n",
    "    # set validation dataset as 10% of all data\n",
    "    if validation:\n",
    "        init_validation_size = len(label)*0.1\n",
    "    else:\n",
    "        init_validation_size = 0\n",
    "    kf = StratifiedKFold(n_splits=num_fold)\n",
    "    allTrueLabel = []\n",
    "    co_train_algorithm = [name for clf,name in muti_view_clf]\n",
    "    baseline_algorithm = [name for clf,name in combined_clf]\n",
    "    allPredLabel = collections.defaultdict(list)\n",
    "    all_fold_coTrain_diff_iteration = collections.defaultdict(list)\n",
    "    co_train_iteration = collections.defaultdict(list)\n",
    "    \n",
    "    all_fold_statistic = []\n",
    "    fold = 0\n",
    "    # convert different input type to dataframe for consistency\n",
    "    dv1 = pd.DataFrame(dv1)\n",
    "    dv2 = pd.DataFrame(dv2)\n",
    "    \n",
    "    for train_index, test_index in kf.split(dv1, label):\n",
    "        fold +=1\n",
    "        # print(\"TRAIN:\", train_index, \" \\n TEST:\", test_index)\n",
    "        # ---------------split train and test -------------------- #\n",
    "        dv1_train, dv1_test = dv1.iloc[train_index], dv1.iloc[test_index]\n",
    "        dv2_train, dv2_test = dv2.iloc[train_index], dv2.iloc[test_index]\n",
    "        all_label_train, label_test = label.iloc[train_index], label.iloc[test_index]\n",
    "        \n",
    "        # -------plot true labeled result for different view on each fold ------- #\n",
    "        '''Only for visualization of co-training process, use PCA to reduce views to 2d\n",
    "           Real training process will not using PCA to reduce it's dimension '''\n",
    "        detailed_plot_path = plot_save_path+dataset_name+\"/fold\"+str(fold)+\"/\"\n",
    "        if not os.path.exists(detailed_plot_path):\n",
    "            os.makedirs(detailed_plot_path)\n",
    "        # 1. apply PCA to different views\n",
    "        pca = PCA(n_components=2)\n",
    "        for view,name in [(dv1,\"dv1\"),(dv2,\"dv2\")]:\n",
    "            pca_view = pca.fit_transform(X=view)\n",
    "            first_principal_component = pca_view[:,0]\n",
    "            second_principal_component = pca_view[:,1]\n",
    "            fig, ax = plt.subplots(figsize=(9,7))\n",
    "            for author in np.unique(label):\n",
    "                ix = label.index[label == author].tolist()\n",
    "                ax.scatter(x=first_principal_component[ix], y=second_principal_component[ix],\n",
    "                           cmap='viridis', label = author, s = 50, alpha = 0.5)\n",
    "            legend = ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.2), ncol=2,prop={'size': 13})\n",
    "            plt.title('True label', fontsize=14)\n",
    "            plt.xlabel(\"First principal component\",fontsize=14)\n",
    "            plt.ylabel(\"Second principal component\",fontsize=14)\n",
    "            plt.savefig(fname=detailed_plot_path+dataset_name+\"_PCA_true_label_\"+name+\".png\",\n",
    "                        dpi=100, bbox_extra_artists=(legend,), bbox_inches='tight')\n",
    "            #plt.show()\n",
    "            plt.close(\"all\")\n",
    "\n",
    "        # ----------- set some labeled data as unlabeled ------------ #\n",
    "        # 1. obtain data ratio\n",
    "        c = Counter(all_label_train)\n",
    "        data_ratio = [(i, c[i] / len(all_label_train)) for i in c]\n",
    "        print(data_ratio)\n",
    "        # 2. calculate per class size \n",
    "        # co_train_per_class_size contain (label,initial train size for each class, initial validation data for each class)\n",
    "        co_train_per_class_size = [(label, round(ratio*init_labeled_size),round(ratio*init_validation_size)) for label, ratio in data_ratio]\n",
    "        print(co_train_per_class_size)\n",
    "        # 3.Initialize train and validation sample index list save it for later use\n",
    "        temp_train_label = all_label_train.tolist()\n",
    "        train_sample_idx = []\n",
    "        validation_sample_idx = []\n",
    "        # 4. random draw both train labeled samples and validation samples, mark other as unlabeled\n",
    "        # we could also use validation samples to improve performance\n",
    "        for unique_label, training_size, validation_size in co_train_per_class_size:\n",
    "            curr_label_idx = [i for i, x in enumerate(temp_train_label) if x == unique_label]\n",
    "            curr_label_size = len(curr_label_idx)\n",
    "            # 1. get train sample idx\n",
    "            temp_train_sample_idx = random.sample(curr_label_idx, training_size)\n",
    "            train_sample_idx += temp_train_sample_idx\n",
    "            curr_label_idx_no_train = [x for x in curr_label_idx if x not in temp_train_sample_idx]\n",
    "            # 2. get validation sample idx\n",
    "            temp_validation_sample_idx = random.sample(curr_label_idx_no_train, validation_size)\n",
    "            validation_sample_idx += temp_validation_sample_idx\n",
    "            # 3. set other samples to -1 as unlabeled\n",
    "            unlabel_item_idx = [x for x in curr_label_idx_no_train if x not in temp_validation_sample_idx]\n",
    "            for unlabel_idx in unlabel_item_idx:\n",
    "                temp_train_label[unlabel_idx]=-1\n",
    "\n",
    "        #print(temp_train_label)\n",
    "        ''' Case 1: Using validation dataset for each fold of train\n",
    "            Case 2: Not Using validation dataset for each fold of train'''\n",
    "        # collect per fold statistic\n",
    "        curr_fold_statistic = {'author': dataset_name, 'fold':fold, 'train_size': len(train_sample_idx),\n",
    "                               'validation_size':len(validation_sample_idx), 'test_size': dv1_test.shape[0]} \n",
    "        for in_clf, clf_name in muti_view_clf: \n",
    "            per_clf_plot_save_path = detailed_plot_path+clf_name+\"/\"\n",
    "            if not os.path.exists(per_clf_plot_save_path):\n",
    "                os.makedirs(per_clf_plot_save_path)\n",
    "            if validation:\n",
    "                unlabeled_sample_size = len(temp_train_label)-len(train_sample_idx)-len(validation_sample_idx)\n",
    "                # extract validation data\n",
    "                dv1_validation = dv1_train.iloc[validation_sample_idx,:]\n",
    "                dv2_validation = dv2_train.iloc[validation_sample_idx,:]\n",
    "                validation_label = [temp_train_label[idx] for idx in validation_sample_idx]\n",
    "                # use other data as train\n",
    "                final_dv1_train = dv1_train[~dv1_train.isin(dv1_validation)].dropna(axis=0, how=\"all\").reset_index(drop=True)\n",
    "                final_dv2_train = dv2_train[~dv2_train.isin(dv2_validation)].dropna(axis=0, how=\"all\").reset_index(drop=True)\n",
    "                final_train_label = [x for i, x in enumerate(temp_train_label) if i not in validation_sample_idx]\n",
    "                # check data sync\n",
    "                # print(validation_label)\n",
    "                # print(all_label_train[dv1_validation.index])\n",
    "                per_fold_clf = copy.deepcopy(in_clf)\n",
    "                per_fold_clf.fit(dv1=final_dv1_train, dv2=final_dv2_train, labels=final_train_label, \n",
    "                                 dv1_validation=dv1_validation, dv2_validation=dv2_validation, label_validation=validation_label,\n",
    "                                 dataset_name=dataset_name, plot_save_path=per_clf_plot_save_path)\n",
    "            else:\n",
    "                unlabeled_sample_size = len(temp_train_label)-len(train_sample_idx)\n",
    "                final_dv1_train = dv1_train.reset_index(drop=True)\n",
    "                final_dv2_train = dv2_train.reset_index(drop=True)\n",
    "                final_train_label=temp_train_label\n",
    "                per_fold_clf = copy.deepcopy(in_clf)\n",
    "                per_fold_clf.fit(dv1=final_dv1_train, dv2=final_dv2_train, labels=final_train_label, \n",
    "                                 dv1_validation=dv1_test, dv2_validation=dv2_test, label_validation=label_test,\n",
    "                                 dataset_name=dataset_name, plot_save_path=per_clf_plot_save_path)\n",
    "            curr_fold_statistic[\"unlabel_size\"]=unlabeled_sample_size\n",
    "            curr_fold_statistic[\"total_iteration\"]= per_fold_clf.get_iter_count()\n",
    "            # -------------- get self-labeled sample index --------------- #\n",
    "            self_labeled_index = per_fold_clf.get_self_labeled_sample()\n",
    "            #print(\"Self labeled sample index: \", self_labeled_index)\n",
    "            self_labeled_idx_temp = [idx for idx in self_labeled_index.values()]\n",
    "            all_self_labeled_index = [val for sublist in self_labeled_idx_temp for subsublist in sublist for val in subsublist]\n",
    "            curr_fold_statistic[clf_name+'_total_self_labeled']= len(all_self_labeled_index)\n",
    "            # ------------- get predicted label for test set ------------- #\n",
    "            co_train_predict = per_fold_clf.predict(dv1_test, dv2_test)\n",
    "            print(clf_name+\" f1: \", metrics.classification_report(label_test, co_train_predict))\n",
    "            print(metrics.confusion_matrix(label_test, co_train_predict).ravel())\n",
    "            curr_fold_statistic[clf_name+\" f1\"] = f1_score(label_test.values.tolist(), co_train_predict,average='macro')\n",
    "            allPredLabel[clf_name+\" predict label\"].append(co_train_predict)\n",
    "            # ------------- get co-training iterations f1 score ---------- #\n",
    "            co_train_iteration[clf_name].append(per_fold_clf.get_iter_count())\n",
    "            coTrain_diff_iter_on_test_dv1, coTrain_diff_iter_on_test_dv2 = per_fold_clf.co_train_process_f1()\n",
    "            all_fold_coTrain_diff_iteration[clf_name+\"_dv1\"].append(coTrain_diff_iter_on_test_dv1)\n",
    "            all_fold_coTrain_diff_iteration[clf_name+\"_dv2\"].append(coTrain_diff_iter_on_test_dv2)\n",
    "        ''' \n",
    "        Comparison: Use two view concatenated features and train with supervise learning \n",
    "        Notice we only train supervise learning with labeled train, data mark as unlabeled is not used\n",
    "        Case 1: logistic regression\n",
    "        Case 2: SVM\n",
    "        '''\n",
    "        # ----------- generate concatenated train dataset ------------ #\n",
    "        concatenated_train = pd.concat([dv1_train.iloc[train_sample_idx],dv2_train.iloc[train_sample_idx]], axis=1, ignore_index=True)\n",
    "        train_label = [temp_train_label[i] for i in train_sample_idx]\n",
    "        # ------------ generate concatenated test dataset ------------ #\n",
    "        concatenated_test = pd.concat([dv1_test,dv2_test], axis=1, ignore_index=True)\n",
    "        for in_clf, clf_name in combined_clf:\n",
    "            in_clf.fit(concatenated_train, train_label)\n",
    "            # ------------- get predicted label for test set ------------- #\n",
    "            predict_label = in_clf.predict(concatenated_test)\n",
    "            print(clf_name+\" f1: \", metrics.classification_report(label_test, predict_label))\n",
    "            print(metrics.confusion_matrix(label_test, predict_label).ravel())\n",
    "            curr_fold_statistic[clf_name+\" f1\"] = f1_score(label_test.values.tolist(), predict_label,average='macro')\n",
    "            allPredLabel[clf_name+\" predict label\"].append(predict_label)\n",
    "        \n",
    "        allTrueLabel.extend(label_test.values.tolist())\n",
    "        all_fold_statistic.append(curr_fold_statistic)\n",
    "\n",
    "    ''' # --------------- plot per fold result f1 variance --------------- # '''\n",
    "    if plot_save_path !=None:\n",
    "        all_per_fold_f1_score_variance_plot = pd.DataFrame(all_fold_statistic)\n",
    "        #print(all_per_fold_f1_score_variance_plot)\n",
    "        sns.set(rc={'figure.figsize':(10,8)})\n",
    "        plot_temp_f1 = pd.DataFrame()\n",
    "        for clf_name in co_train_algorithm+baseline_algorithm:\n",
    "            clf_temp_f1 = all_per_fold_f1_score_variance_plot[[clf_name+' f1']].values\n",
    "            plot_temp_f1[clf_name]=clf_temp_f1.flatten()\n",
    "        plot_temp_f1 = pd.melt(plot_temp_f1, var_name='methods', value_name='f1')\n",
    "        #print(plot_temp_f1)\n",
    "        ax = sns.boxplot(x=\"methods\", y=\"f1\", data=plot_temp_f1)\n",
    "        ax = sns.swarmplot(x=\"methods\", y=\"f1\", data=plot_temp_f1, color=\".25\")\n",
    "        ax.set_title(dataset_name+\" result variance within \"+str(num_fold)+\" fold\")\n",
    "        plt.savefig(plot_save_path+dataset_name+\"/all_method_result_variance.png\", dpi=150)\n",
    "        plt.show()\n",
    "        plt.close(\"all\")\n",
    "\n",
    "    '''  plot averaged f1 score wrt different fold in different iterations in co-training process '''\n",
    "    for clf_name in co_train_algorithm:\n",
    "        print(clf_name,\" per fold iteration: \", co_train_iteration[clf_name])\n",
    "        # each fold have different iteration, find minimum of iteration and \n",
    "        co_train_min_iteration_num = min(co_train_iteration[clf_name])\n",
    "        for sublist in all_fold_coTrain_diff_iteration[clf_name+\"_dv1\"]:\n",
    "            sublist[:] = sublist[:co_train_min_iteration_num]\n",
    "        for sublist in all_fold_coTrain_diff_iteration[clf_name+\"_dv2\"]:\n",
    "            sublist[:] = sublist[:co_train_min_iteration_num]\n",
    "        # -------- mean with respect to all fold --------- #\n",
    "        averaged_coTrain_diff_iter_dv1 = np.mean(all_fold_coTrain_diff_iteration[clf_name+\"_dv1\"], axis=0)\n",
    "        averaged_coTrain_diff_iter_dv2 = np.mean(all_fold_coTrain_diff_iteration[clf_name+\"_dv2\"], axis=0)\n",
    "        # -------- initial variables for plot ------------ #\n",
    "        default_text_based = [averaged_coTrain_diff_iter_dv1[0]] * co_train_min_iteration_num\n",
    "        default_citation_based = [averaged_coTrain_diff_iter_dv2[0]] * co_train_min_iteration_num\n",
    "        default_step = np.arange(0, co_train_min_iteration_num)\n",
    "        co_train_text_based = averaged_coTrain_diff_iter_dv1[1:]\n",
    "        co_train_citation_based = averaged_coTrain_diff_iter_dv2[1:]\n",
    "        co_training_step = np.arange(1, co_train_min_iteration_num)\n",
    "        # ----------- plot details ----------------------- #\n",
    "        fig = plt.figure()\n",
    "        ax = plt.axes()\n",
    "        plt.plot(default_step, default_text_based, linestyle='dashed', label=\"Text based default\")\n",
    "        plt.plot(default_step, default_citation_based, linestyle='dashdot', label=\"Citation based default\")\n",
    "        plt.plot(co_training_step, co_train_text_based, linestyle='solid', marker = \"*\", label=\"Text based\")\n",
    "        plt.plot(co_training_step, co_train_citation_based, linestyle='dotted', marker = \"+\", label=\"Citation based\")\n",
    "        ax.autoscale_view()\n",
    "        legend = ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.2), ncol=2)\n",
    "        plt.xlabel('Co-Training Iterations')\n",
    "        plt.ylabel('F1 score')\n",
    "        plt.savefig((plot_save_path+dataset_name+\"/\"+clf_name+\"_mean_diff_iter_f1.png\"), dpi=150, bbox_extra_artists=(legend,), bbox_inches='tight')\n",
    "        plt.show()\n",
    "        plt.close(\"all\")\n",
    "    \n",
    "    ''' The results of a k-fold cross-validation run are often summarized with the mean of the model scores.'''\n",
    "    final_f1_score = []\n",
    "    for clf_name in co_train_algorithm+baseline_algorithm:\n",
    "        clf_all_fold_f1 =[]\n",
    "        for per_fold_statistic in all_fold_statistic:\n",
    "            clf_per_fold_f1 = per_fold_statistic[clf_name+\" f1\"]\n",
    "            clf_all_fold_f1.append(clf_per_fold_f1)\n",
    "        clf_mean_f1 = np.mean(clf_all_fold_f1, axis=0)\n",
    "        #print(clf_all_fold_f1)\n",
    "        #print(clf_mean_f1)\n",
    "        final_f1_score.append((clf_name,clf_mean_f1))\n",
    "    \n",
    "    return final_f1_score, all_fold_statistic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-13T05:56:21.047Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load text embedding:  pv_dbow\n",
      "Total text vector records: 135796\n",
      "Vector dimension:  100\n",
      "Load citation embedding:  n2v\n",
      "Total citation vector records: 124922\n",
      "Vector dimension:  100\n",
      "For name:  j_read\n",
      "(136, 2)\n",
      "j_read  pass\n",
      "For name:  f_esteves\n",
      "(34, 2)\n",
      "f_esteves  pass\n",
      "For name:  c_miller\n",
      "(252, 2)\n",
      "c_miller  pass\n",
      "For name:  r_jha\n",
      "(11, 2)\n",
      "r_jha  pass\n",
      "For name:  a_lowe\n",
      "(102, 2)\n",
      "a_lowe  pass\n",
      "For name:  a_vega\n",
      "(20, 2)\n",
      "a_vega  pass\n",
      "For name:  k_smith\n",
      "(338, 2)\n",
      "k_smith  pass\n",
      "For name:  j_gordon\n",
      "(19, 2)\n",
      "j_gordon  pass\n",
      "For name:  s_liao\n",
      "(104, 2)\n",
      "s_liao  pass\n",
      "For name:  j_qian\n",
      "(17, 2)\n",
      "j_qian  pass\n",
      "For name:  s_bernardi\n",
      "(91, 2)\n",
      "s_bernardi  pass\n",
      "For name:  t_hill\n",
      "(15, 2)\n",
      "t_hill  pass\n",
      "For name:  s_schindler\n",
      "(51, 2)\n",
      "s_schindler  pass\n",
      "For name:  j_williams\n",
      "(625, 2)\n",
      "j_williams  pass\n",
      "For name:  s_jacobson\n",
      "(28, 2)\n",
      "s_jacobson  pass\n",
      "For name:  e_andrade\n",
      "(17, 2)\n",
      "e_andrade  pass\n",
      "For name:  t_santos\n",
      "(45, 2)\n",
      "t_santos  pass\n",
      "For name:  k_kim\n",
      "(1111, 2)\n",
      "Total sample size before apply threshold:  1111\n",
      "Counter({'0000-0002-6929-5359': 211, '0000-0001-9498-284X': 154, '0000-0002-5878-8895': 139, '0000-0002-1864-3392': 92, '0000-0002-7045-8004': 57, '0000-0001-7896-6751': 57, '0000-0002-7991-9428': 55, '0000-0002-4010-1063': 45, '0000-0002-2186-3484': 28, '0000-0002-4899-1929': 25, '0000-0003-0487-4242': 24, '0000-0002-3642-1486': 22, '0000-0001-9965-3535': 17, '0000-0002-4168-757X': 17, '0000-0001-6525-3744': 14, '0000-0002-3897-0278': 14, '0000-0002-1181-5112': 12, '0000-0003-1447-9385': 11, '0000-0002-7305-8786': 11, '0000-0002-2655-7806': 10, '0000-0003-3466-5353': 9, '0000-0002-7359-663X': 8, '0000-0003-4600-8668': 6, '0000-0002-1382-7088': 5, '0000-0002-9505-4882': 5, '0000-0003-3667-9900': 4, '0000-0001-9714-6038': 4, '0000-0002-4760-0228': 3, '0000-0003-4188-7915': 3, '0000-0001-9454-0427': 3, '0000-0002-0333-6808': 3, '0000-0003-2134-4964': 3, '0000-0002-6658-047X': 3, '0000-0003-1273-379X': 3, '0000-0002-7047-3183': 3, '0000-0002-1814-9546': 3, '0000-0003-4812-6297': 2, '0000-0001-6597-578X': 2, '0000-0002-5285-9138': 2, '0000-0002-6796-7844': 2, '0000-0002-1130-8698': 2, '0000-0001-8518-8150': 2, '0000-0002-7103-924X': 2, '0000-0002-5407-0202': 1, '0000-0001-6220-8411': 1, '0000-0002-7440-6703': 1, '0000-0002-1603-7559': 1, '0000-0003-0257-1707': 1, '0000-0001-8532-6517': 1, '0000-0001-6626-316X': 1, '0000-0002-3246-9861': 1, '0000-0002-7207-4389': 1, '0000-0001-9682-9654': 1, '0000-0002-0196-3832': 1, '0000-0001-8063-6081': 1, '0000-0003-2037-3333': 1, '0000-0001-8872-6751': 1})\n",
      "Total author before apply threshoid:  57\n",
      "Total author after apply threshoid:  3\n",
      "Total sample size after apply threshold:  504\n",
      "Total missing sample:  0\n",
      "(504, 101)\n",
      "Total missing sample:  47\n",
      "(504, 101)\n",
      "Labeled:  504  :  504\n",
      "(504, 103)\n",
      "(504, 103)\n",
      "k_kim is multi-class case, ignored\n",
      "For name:  d_ricci\n",
      "(40, 2)\n",
      "d_ricci  pass\n",
      "For name:  s_cameron\n",
      "(66, 2)\n",
      "s_cameron  pass\n",
      "For name:  t_wright\n",
      "(31, 2)\n",
      "t_wright  pass\n",
      "For name:  r_cunha\n",
      "(209, 2)\n",
      "r_cunha  pass\n",
      "For name:  s_fuchs\n",
      "(32, 2)\n",
      "s_fuchs  pass\n",
      "For name:  m_nawaz\n",
      "(9, 2)\n",
      "m_nawaz  pass\n",
      "For name:  k_harris\n",
      "(47, 2)\n",
      "k_harris  pass\n",
      "For name:  r_daniel\n",
      "(173, 2)\n",
      "r_daniel  pass\n",
      "For name:  k_xu\n",
      "(37, 2)\n",
      "k_xu  pass\n",
      "For name:  s_antunes\n",
      "(54, 2)\n",
      "s_antunes  pass\n",
      "For name:  k_cho\n",
      "(126, 2)\n",
      "k_cho  pass\n",
      "For name:  j_sanderson\n",
      "(31, 2)\n",
      "j_sanderson  pass\n",
      "For name:  s_uddin\n",
      "(39, 2)\n",
      "s_uddin  pass\n",
      "For name:  a_batista\n",
      "(48, 2)\n",
      "a_batista  pass\n",
      "For name:  h_pereira\n",
      "(70, 2)\n",
      "h_pereira  pass\n",
      "For name:  a_patel\n",
      "(262, 2)\n",
      "a_patel  pass\n",
      "For name:  r_graham\n",
      "(52, 2)\n",
      "r_graham  pass\n",
      "For name:  a_nilsson\n",
      "(42, 2)\n",
      "a_nilsson  pass\n",
      "For name:  m_soto\n",
      "(97, 2)\n",
      "m_soto  pass\n",
      "For name:  g_guidi\n",
      "(37, 2)\n",
      "g_guidi  pass\n",
      "For name:  e_andersson\n",
      "(138, 2)\n",
      "e_andersson  pass\n",
      "For name:  s_reid\n",
      "(132, 2)\n",
      "s_reid  pass\n",
      "For name:  a_maleki\n",
      "(25, 2)\n",
      "a_maleki  pass\n",
      "For name:  j_moon\n",
      "(203, 2)\n",
      "j_moon  pass\n",
      "For name:  t_abe\n",
      "(50, 2)\n",
      "t_abe  pass\n",
      "For name:  x_fu\n",
      "(16, 2)\n",
      "x_fu  pass\n",
      "For name:  f_ortega\n",
      "(368, 2)\n",
      "f_ortega  pass\n",
      "For name:  r_morris\n",
      "(409, 2)\n",
      "r_morris  pass\n",
      "For name:  w_fang\n",
      "(43, 2)\n",
      "w_fang  pass\n",
      "For name:  m_amaral\n",
      "(134, 2)\n",
      "m_amaral  pass\n",
      "For name:  h_song\n",
      "(210, 2)\n",
      "h_song  pass\n",
      "For name:  h_dai\n",
      "(6, 2)\n",
      "h_dai  pass\n",
      "For name:  y_nakajima\n",
      "(12, 2)\n",
      "y_nakajima  pass\n",
      "For name:  t_warner\n",
      "(68, 2)\n",
      "t_warner  pass\n",
      "For name:  s_saha\n",
      "(111, 2)\n",
      "s_saha  pass\n",
      "For name:  j_fernandez\n",
      "(28, 2)\n",
      "j_fernandez  pass\n",
      "For name:  m_pan\n",
      "(146, 2)\n",
      "m_pan  pass\n",
      "For name:  a_simon\n",
      "(117, 2)\n",
      "a_simon  pass\n",
      "For name:  r_freitas\n",
      "(73, 2)\n",
      "r_freitas  pass\n",
      "For name:  c_yun\n",
      "(284, 2)\n",
      "c_yun  pass\n",
      "For name:  j_huang\n",
      "(443, 2)\n",
      "j_huang  pass\n",
      "For name:  p_santos\n",
      "(92, 2)\n",
      "p_santos  pass\n",
      "For name:  n_young\n",
      "(182, 2)\n",
      "n_young  pass\n",
      "For name:  d_ross\n",
      "(25, 2)\n",
      "d_ross  pass\n",
      "For name:  q_wang\n",
      "(348, 2)\n",
      "q_wang  pass\n",
      "For name:  c_cardoso\n",
      "(52, 2)\n",
      "c_cardoso  pass\n",
      "For name:  j_matthews\n",
      "(65, 2)\n",
      "j_matthews  pass\n",
      "For name:  g_lee\n",
      "(202, 2)\n",
      "g_lee  pass\n",
      "For name:  m_salem\n",
      "(25, 2)\n",
      "m_salem  pass\n",
      "For name:  h_lai\n",
      "(165, 2)\n",
      "h_lai  pass\n",
      "For name:  r_harris\n",
      "(50, 2)\n",
      "r_harris  pass\n",
      "For name:  c_vaughan\n",
      "(83, 2)\n",
      "c_vaughan  pass\n",
      "For name:  e_thompson\n",
      "(181, 2)\n",
      "e_thompson  pass\n",
      "For name:  r_gomes\n",
      "(52, 2)\n",
      "r_gomes  pass\n",
      "For name:  r_bennett\n",
      "(93, 2)\n",
      "r_bennett  pass\n",
      "For name:  m_collins\n",
      "(57, 2)\n",
      "m_collins  pass\n",
      "For name:  m_cowley\n",
      "(132, 2)\n",
      "m_cowley  pass\n",
      "For name:  p_teixeira\n",
      "(213, 2)\n",
      "p_teixeira  pass\n",
      "For name:  c_cox\n",
      "(48, 2)\n",
      "c_cox  pass\n",
      "For name:  s_hsu\n",
      "(204, 2)\n",
      "s_hsu  pass\n",
      "For name:  f_williams\n",
      "(149, 2)\n",
      "f_williams  pass\n",
      "For name:  d_parsons\n",
      "(30, 2)\n",
      "d_parsons  pass\n",
      "For name:  a_choudhury\n",
      "(56, 2)\n",
      "a_choudhury  pass\n",
      "For name:  c_richter\n",
      "(11, 2)\n",
      "c_richter  pass\n",
      "For name:  m_hossain\n",
      "(102, 2)\n",
      "m_hossain  pass\n",
      "For name:  v_alves\n",
      "(24, 2)\n",
      "v_alves  pass\n",
      "For name:  j_becker\n",
      "(177, 2)\n",
      "j_becker  pass\n",
      "For name:  m_soares\n",
      "(247, 2)\n",
      "m_soares  pass\n",
      "For name:  j_yi\n",
      "(29, 2)\n",
      "j_yi  pass\n",
      "For name:  s_khan\n",
      "(193, 2)\n",
      "s_khan  pass\n",
      "For name:  a_rao\n",
      "(93, 2)\n",
      "a_rao  pass\n",
      "For name:  d_cameron\n",
      "(49, 2)\n",
      "d_cameron  pass\n",
      "For name:  c_morgan\n",
      "(43, 2)\n",
      "c_morgan  pass\n",
      "For name:  h_cui\n",
      "(40, 2)\n",
      "h_cui  pass\n",
      "For name:  p_zhang\n",
      "(137, 2)\n",
      "p_zhang  pass\n",
      "For name:  j_fernandes\n",
      "(208, 2)\n",
      "j_fernandes  pass\n",
      "For name:  a_jain\n",
      "(67, 2)\n",
      "a_jain  pass\n",
      "For name:  d_zhang\n",
      "(94, 2)\n",
      "d_zhang  pass\n",
      "For name:  b_huang\n",
      "(48, 2)\n",
      "b_huang  pass\n",
      "For name:  m_chong\n",
      "(43, 2)\n",
      "m_chong  pass\n",
      "For name:  m_cerqueira\n",
      "(41, 2)\n",
      "m_cerqueira  pass\n",
      "For name:  p_yang\n",
      "(227, 2)\n",
      "p_yang  pass\n",
      "For name:  j_marques\n",
      "(183, 2)\n",
      "j_marques  pass\n",
      "For name:  n_ali\n",
      "(14, 2)\n",
      "n_ali  pass\n",
      "For name:  h_ng\n",
      "(109, 2)\n",
      "h_ng  pass\n",
      "For name:  m_viana\n",
      "(139, 2)\n",
      "m_viana  pass\n",
      "For name:  t_inoue\n",
      "(70, 2)\n",
      "t_inoue  pass\n",
      "For name:  b_meyer\n",
      "(92, 2)\n",
      "b_meyer  pass\n",
      "For name:  c_liao\n",
      "(35, 2)\n",
      "c_liao  pass\n",
      "For name:  k_wheeler\n",
      "(28, 2)\n",
      "k_wheeler  pass\n",
      "For name:  m_rizzo\n",
      "(152, 2)\n",
      "m_rizzo  pass\n",
      "For name:  y_shi\n",
      "(67, 2)\n",
      "y_shi  pass\n",
      "For name:  c_luo\n",
      "(78, 2)\n",
      "c_luo  pass\n",
      "For name:  j_arthur\n",
      "(42, 2)\n",
      "j_arthur  pass\n",
      "For name:  m_ansari\n",
      "(34, 2)\n",
      "m_ansari  pass\n",
      "For name:  g_anderson\n",
      "(103, 2)\n",
      "g_anderson  pass\n",
      "For name:  m_hidalgo\n",
      "(279, 2)\n",
      "m_hidalgo  pass\n",
      "For name:  k_jacobsen\n",
      "(113, 2)\n",
      "k_jacobsen  pass\n",
      "For name:  s_kelly\n",
      "(102, 2)\n",
      "s_kelly  pass\n",
      "For name:  s_james\n",
      "(59, 2)\n",
      "s_james  pass\n",
      "For name:  p_persson\n",
      "(80, 2)\n",
      "p_persson  pass\n",
      "For name:  y_tanaka\n",
      "(20, 2)\n",
      "y_tanaka  pass\n",
      "For name:  c_gao\n",
      "(189, 2)\n",
      "c_gao  pass\n",
      "For name:  w_jung\n",
      "(33, 2)\n",
      "w_jung  pass\n",
      "For name:  s_lewis\n",
      "(306, 2)\n",
      "s_lewis  pass\n",
      "For name:  w_han\n",
      "(34, 2)\n",
      "w_han  pass\n",
      "For name:  m_shah\n",
      "(17, 2)\n",
      "m_shah  pass\n",
      "For name:  c_arango\n",
      "(185, 2)\n",
      "c_arango  pass\n",
      "For name:  r_young\n",
      "(361, 2)\n",
      "r_young  pass\n",
      "For name:  r_coleman\n",
      "(34, 2)\n",
      "r_coleman  pass\n",
      "For name:  b_kang\n",
      "(20, 2)\n",
      "b_kang  pass\n",
      "For name:  s_carter\n",
      "(205, 2)\n",
      "s_carter  pass\n",
      "For name:  c_thomas\n",
      "(102, 2)\n",
      "c_thomas  pass\n",
      "For name:  m_gutierrez\n",
      "(32, 2)\n",
      "m_gutierrez  pass\n",
      "For name:  s_moon\n",
      "(85, 2)\n",
      "s_moon  pass\n",
      "For name:  r_pereira\n",
      "(202, 2)\n",
      "r_pereira  pass\n",
      "For name:  a_nielsen\n",
      "(132, 2)\n",
      "a_nielsen  pass\n",
      "For name:  j_conde\n",
      "(84, 2)\n",
      "j_conde  pass\n",
      "For name:  k_wright\n",
      "(59, 2)\n",
      "k_wright  pass\n",
      "For name:  m_parker\n",
      "(280, 2)\n",
      "m_parker  pass\n",
      "For name:  h_huang\n",
      "(224, 2)\n",
      "h_huang  pass\n",
      "For name:  j_terry\n",
      "(57, 2)\n",
      "j_terry  pass\n",
      "For name:  y_xu\n",
      "(137, 2)\n",
      "y_xu  pass\n",
      "For name:  a_melo\n",
      "(48, 2)\n",
      "a_melo  pass\n",
      "For name:  r_doyle\n",
      "(11, 2)\n",
      "r_doyle  pass\n",
      "For name:  m_bernardo\n",
      "(250, 2)\n",
      "m_bernardo  pass\n",
      "For name:  j_soares\n",
      "(49, 2)\n",
      "j_soares  pass\n",
      "For name:  j_richard\n",
      "(179, 2)\n",
      "j_richard  pass\n",
      "For name:  p_robinson\n",
      "(275, 2)\n",
      "Total sample size before apply threshold:  275\n",
      "Counter({'0000-0002-7878-0313': 133, '0000-0002-0736-9199': 119, '0000-0002-3156-3418': 19, '0000-0002-0577-3147': 4})\n",
      "Total author before apply threshoid:  4\n",
      "Total author after apply threshoid:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sample size after apply threshold:  252\n",
      "Total missing sample:  0\n",
      "(252, 101)\n",
      "Total missing sample:  6\n",
      "(252, 101)\n",
      "Labeled:  252  :  252\n",
      "(252, 103)\n",
      "(252, 103)\n",
      "p_robinson is binary case\n",
      "[('p_robinson_0', 0.472636815920398), ('p_robinson_1', 0.527363184079602)]\n",
      "[('p_robinson_0', 5, 12), ('p_robinson_1', 5, 13)]\n",
      "Initial L size:  10\n",
      "Initial U size:  166\n",
      "U size after drawing sample to U prime: 0\n",
      "Initial U prime size:  166\n",
      "P value:  1  N value:  1\n",
      "Iteration 1  h1 new:  [6, 118]  probs:  [0.9123029495832661, 0.9287816726410726]\n",
      "Iteration 1  h2 new:  [168, 61]  probs:  [0.9409265858147919, 0.9715282803133084]\n",
      "Iteration 2  h1 new:  [158, 85]  probs:  [0.9343585073617872, 0.9531245227318873]\n",
      "Iteration 2  h2 new:  [18, 127]  probs:  [0.9525930973405065, 0.9730789256337102]\n",
      "Iteration 3  h1 new:  [96, 67]  probs:  [0.946570759455844, 0.951990145212511]\n",
      "Iteration 3  h2 new:  [82, 3]  probs:  [0.9587586383739428, 0.9816175576406857]\n",
      "Iteration 4  h1 new:  [106, 56]  probs:  [0.9395604788577714, 0.9621155960404545]\n",
      "Iteration 4  h2 new:  [152, 122]  probs:  [0.9582532376984767, 0.9712356265133096]\n",
      "Iteration 5  h1 new:  [98, 22]  probs:  [0.9438372837215199, 0.9667101101851048]\n",
      "Iteration 5  h2 new:  [163, 74]  probs:  [0.9589312650605961, 0.9736041769947104]\n",
      "Iteration 6  h1 new:  [52, 30]  probs:  [0.9500083903188328, 0.9709183538295196]\n",
      "Iteration 6  h2 new:  [2, 60]  probs:  [0.9647906873324142, 0.9752900888401532]\n",
      "Iteration 7  h1 new:  [114, 137]  probs:  [0.9455353127608012, 0.9746420760608868]\n",
      "Iteration 7  h2 new:  [37, 97]  probs:  [0.9636427184503762, 0.9776879622186486]\n",
      "Iteration 8  h1 new:  [27, 16]  probs:  [0.9533996876759068, 0.9718421617904907]\n",
      "Iteration 8  h2 new:  [94, 55]  probs:  [0.9711384076513898, 0.9791672289010881]\n",
      "Iteration 9  h1 new:  [54, 51]  probs:  [0.9627210245509377, 0.9659877880816875]\n",
      "Iteration 9  h2 new:  [19, 75]  probs:  [0.9742724820680474, 0.9778663741020687]\n",
      "Iteration 10  h1 new:  [124, 89]  probs:  [0.9572719602152635, 0.969207905078079]\n",
      "Iteration 10  h2 new:  [81, 169]  probs:  [0.9780562391899739, 0.9784924761979887]\n",
      "Iteration 11  h1 new:  [23, 145]  probs:  [0.963505207109888, 0.972089244704037]\n",
      "Iteration 11  h2 new:  [23, 49]  probs:  [0.9810791824849336, 0.9787258516794259]\n",
      "Iteration 12  h1 new:  [117, 110]  probs:  [0.9660278358562557, 0.9725623435647964]\n",
      "Iteration 12  h2 new:  [65, 69]  probs:  [0.9814961773366709, 0.9805136975219592]\n",
      "Iteration 13  h1 new:  [14, 73]  probs:  [0.973562110099313, 0.9732831761163082]\n",
      "Iteration 13  h2 new:  [14, 88]  probs:  [0.9827094650914996, 0.9758104958516546]\n",
      "Iteration 14  h1 new:  [21, 44]  probs:  [0.9678097323357489, 0.9768718135584636]\n",
      "Iteration 14  h2 new:  [126, 113]  probs:  [0.9807932568129931, 0.9764974156618251]\n",
      "Iteration 15  h1 new:  [162, 157]  probs:  [0.9690251893039531, 0.9739297023343347]\n",
      "Iteration 15  h2 new:  [11, 121]  probs:  [0.9813624917576638, 0.9700685505518639]\n",
      "Iteration 16  h1 new:  [141, 132]  probs:  [0.9722203899768156, 0.9740976983501293]\n",
      "Iteration 16  h2 new:  [92, 160]  probs:  [0.9832304413559194, 0.9707579497823727]\n",
      "Iteration 17  h1 new:  [25, 15]  probs:  [0.9699025024944634, 0.9741251299572141]\n",
      "Iteration 17  h2 new:  [87, 7]  probs:  [0.9784713159541324, 0.9652349673946519]\n",
      "Iteration 18  h1 new:  [116, 104]  probs:  [0.9754992197865838, 0.9728981010990045]\n",
      "Iteration 18  h2 new:  [165, 62]  probs:  [0.9793874373878128, 0.9658652390973658]\n",
      "Iteration 19  h1 new:  [107, 174]  probs:  [0.9687744311426714, 0.9751544039783564]\n",
      "Iteration 19  h2 new:  [139, 148]  probs:  [0.9808186713811702, 0.9673286694038635]\n",
      "Iteration 20  h1 new:  [29, 77]  probs:  [0.9705907688831914, 0.9748742404705015]\n",
      "Iteration 20  h2 new:  [57, 79]  probs:  [0.9770083612404105, 0.9670253511937119]\n",
      "Iteration 21  h1 new:  [63, 5]  probs:  [0.9693382600046125, 0.9705139858126637]\n",
      "Iteration 21  h2 new:  [76, 83]  probs:  [0.9767006988248851, 0.966278584900272]\n",
      "Iteration 22  h1 new:  [149, 46]  probs:  [0.9674604569856811, 0.9699081242273855]\n",
      "Iteration 22  h2 new:  [138, 47]  probs:  [0.9747529764380654, 0.9701613623961526]\n",
      "Iteration 23  h1 new:  [20, 130]  probs:  [0.9700910365834018, 0.9691697998696128]\n",
      "Iteration 23  h2 new:  [66, 170]  probs:  [0.9758977625291992, 0.9659253540928151]\n",
      "Iteration 24  h1 new:  [50, 28]  probs:  [0.9639637065977611, 0.9671893443933764]\n",
      "Iteration 24  h2 new:  [159, 26]  probs:  [0.9741426340383408, 0.959103823008853]\n",
      "Iteration 25  h1 new:  [70, 101]  probs:  [0.9699008481037106, 0.9696780768420341]\n",
      "Iteration 25  h2 new:  [155, 172]  probs:  [0.9615480765464446, 0.9614164642516413]\n",
      "Iteration 26  h1 new:  [43, 146]  probs:  [0.9697769580083068, 0.9750044155951997]\n",
      "Iteration 26  h2 new:  [1, 58]  probs:  [0.9549739287584345, 0.9643508704944794]\n",
      "Iteration 27  h1 new:  [45, 24]  probs:  [0.978153212820501, 0.9688810891660469]\n",
      "Iteration 27  h2 new:  [35, 144]  probs:  [0.9559278373014578, 0.9668951955084649]\n",
      "Iteration 28  h1 new:  [39, 153]  probs:  [0.9734412671960322, 0.9673489522762067]\n",
      "Iteration 28  h2 new:  [100, 105]  probs:  [0.9564397864624354, 0.9565907031532939]\n",
      "Iteration 29  h1 new:  [78, 175]  probs:  [0.9554560377284119, 0.9692722115820623]\n",
      "Iteration 29  h2 new:  [91, 48]  probs:  [0.9587140530925038, 0.957578887192011]\n",
      "Iteration 30  h1 new:  [173, 164]  probs:  [0.9584657692023797, 0.9691735560121261]\n",
      "Iteration 30  h2 new:  [41, 164]  probs:  [0.9539432709656547, 0.9581842087180227]\n",
      "Total Labeled number:  127  Still unlabeled number:  49\n",
      "co_LR f1:                precision    recall  f1-score   support\n",
      "\n",
      "p_robinson_0       0.96      1.00      0.98        24\n",
      "p_robinson_1       1.00      0.96      0.98        27\n",
      "\n",
      " avg / total       0.98      0.98      0.98        51\n",
      "\n",
      "[24  0  1 26]\n",
      "Initial L size:  10\n",
      "Initial U size:  166\n",
      "U size after drawing sample to U prime: 0\n",
      "Initial U prime size:  166\n",
      "P value:  1  N value:  1\n",
      "Iteration 1  h1 new:  [6, 85]  probs:  [0.9238795470246968, 0.9484912084995063]\n",
      "Iteration 1  h2 new:  [168, 3]  probs:  [0.8068391343735689, 0.9162622169365621]\n",
      "Iteration 2  h1 new:  [52, 56]  probs:  [0.9561326812489491, 0.957214605755582]\n",
      "Iteration 2  h2 new:  [18, 60]  probs:  [0.8585456633123923, 0.943492631657573]\n",
      "Iteration 3  h1 new:  [158, 145]  probs:  [0.9678839070152149, 0.9677043942908605]\n",
      "Iteration 3  h2 new:  [19, 127]  probs:  [0.9231474396223445, 0.97520607333826]\n",
      "Iteration 4  h1 new:  [23, 67]  probs:  [0.9728962213635033, 0.9703783519128952]\n",
      "Iteration 4  h2 new:  [65, 61]  probs:  [0.9288243209683242, 0.98472823842233]\n",
      "Iteration 5  h1 new:  [27, 22]  probs:  [0.9805325274706634, 0.975567156337189]\n",
      "Iteration 5  h2 new:  [57, 97]  probs:  [0.9460678936975179, 0.9878048119034257]\n",
      "Iteration 6  h1 new:  [141, 118]  probs:  [0.9812386346763067, 0.969918921992059]\n",
      "Iteration 6  h2 new:  [94, 122]  probs:  [0.9593395623663008, 0.9823224665113028]\n",
      "Iteration 7  h1 new:  [54, 89]  probs:  [0.986271553705854, 0.9750831790772044]\n",
      "Iteration 7  h2 new:  [163, 88]  probs:  [0.9560062274467411, 0.9863197913170436]\n",
      "Iteration 8  h1 new:  [21, 137]  probs:  [0.984078743174142, 0.9842384793735686]\n",
      "Iteration 8  h2 new:  [2, 55]  probs:  [0.9434621331242083, 0.9822996414093177]\n",
      "Iteration 9  h1 new:  [14, 16]  probs:  [0.9823679661837837, 0.9733971693497597]\n",
      "Iteration 9  h2 new:  [152, 74]  probs:  [0.9723144378236346, 0.9894667219332755]\n",
      "Iteration 10  h1 new:  [116, 30]  probs:  [0.9814214532525135, 0.976465688654349]\n",
      "Iteration 10  h2 new:  [82, 69]  probs:  [0.95669330934805, 0.9896949304451229]\n",
      "Iteration 11  h1 new:  [96, 51]  probs:  [0.9852012310637311, 0.972199642086107]\n",
      "Iteration 11  h2 new:  [11, 174]  probs:  [0.9723504379250565, 0.9923223390086483]\n",
      "Iteration 12  h1 new:  [117, 121]  probs:  [0.9855743103814977, 0.9710046205966887]\n",
      "Iteration 12  h2 new:  [155, 121]  probs:  [0.9721279489953745, 0.9889876242860962]\n",
      "Iteration 13  h1 new:  [124, 44]  probs:  [0.9856694957129571, 0.9772649399746531]\n",
      "Iteration 13  h2 new:  [138, 169]  probs:  [0.9691483782125168, 0.9896333731515037]\n",
      "Iteration 14  h1 new:  [25, 157]  probs:  [0.986068243521438, 0.9756242799502888]\n",
      "Iteration 14  h2 new:  [98, 75]  probs:  [0.9668225940763475, 0.9894558464479621]\n",
      "Iteration 15  h1 new:  [114, 7]  probs:  [0.9844818972290679, 0.9785621496950639]\n",
      "Iteration 15  h2 new:  [92, 49]  probs:  [0.9699378569620093, 0.9873894005611287]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16  h1 new:  [162, 132]  probs:  [0.9863609383053593, 0.9787731160221937]\n",
      "Iteration 16  h2 new:  [63, 113]  probs:  [0.9698220130313425, 0.9903373420135442]\n",
      "Iteration 17  h1 new:  [81, 71]  probs:  [0.9852827057037132, 0.9771630233889773]\n",
      "Iteration 17  h2 new:  [139, 62]  probs:  [0.9711900300174172, 0.9855968257055313]\n",
      "Iteration 18  h1 new:  [35, 101]  probs:  [0.9933243470250972, 0.9925418720855358]\n",
      "Iteration 18  h2 new:  [106, 83]  probs:  [0.9831208808577824, 0.9816015895656041]\n",
      "Iteration 19  h1 new:  [126, 102]  probs:  [0.9919567428090418, 0.994305809588999]\n",
      "Iteration 19  h2 new:  [100, 46]  probs:  [0.9793278420099971, 0.9758851043641736]\n",
      "Iteration 20  h1 new:  [29, 148]  probs:  [0.9856874301664857, 0.990761564765872]\n",
      "Iteration 20  h2 new:  [103, 26]  probs:  [0.9916026128538391, 0.9684452503693093]\n",
      "Iteration 21  h1 new:  [123, 15]  probs:  [0.9902452376154512, 0.9896210140714524]\n",
      "Iteration 21  h2 new:  [91, 110]  probs:  [0.9845777731329955, 0.9659905717704995]\n",
      "Iteration 22  h1 new:  [20, 175]  probs:  [0.9765472477697714, 0.9921991477014527]\n",
      "Iteration 22  h2 new:  [66, 170]  probs:  [0.9857020534421307, 0.9784986688658582]\n",
      "Iteration 23  h1 new:  [50, 28]  probs:  [0.9799441620890695, 0.9932007239501723]\n",
      "Iteration 23  h2 new:  [40, 79]  probs:  [0.9757087851922678, 0.9811546573985674]\n",
      "Iteration 24  h1 new:  [107, 164]  probs:  [0.9740674032376708, 0.982134450532337]\n",
      "Iteration 24  h2 new:  [131, 160]  probs:  [0.9565331548359733, 0.9686945396091267]\n",
      "Iteration 25  h1 new:  [39, 73]  probs:  [0.9789560389791327, 0.9900289028891487]\n",
      "Iteration 25  h2 new:  [165, 73]  probs:  [0.9554043461009378, 0.9616303064470388]\n",
      "Iteration 26  h1 new:  [70, 24]  probs:  [0.9797084153969937, 0.9921711579838891]\n",
      "Iteration 26  h2 new:  [37, 47]  probs:  [0.9732383334412642, 0.9550431057821481]\n",
      "Iteration 27  h1 new:  [34, 153]  probs:  [0.980191444026355, 0.9908935865244299]\n",
      "Iteration 27  h2 new:  [159, 58]  probs:  [0.97086265245226, 0.9810900552454929]\n",
      "Iteration 28  h1 new:  [173, 146]  probs:  [0.983425681064187, 0.9926081937694078]\n",
      "Iteration 28  h2 new:  [154, 144]  probs:  [0.9879410280387422, 0.9902419681089943]\n",
      "Iteration 29  h1 new:  [78, 104]  probs:  [0.977907673766942, 0.9899486663202289]\n",
      "Iteration 29  h2 new:  [149, 172]  probs:  [0.9522534677238605, 0.9887538305485554]\n",
      "Iteration 30  h1 new:  [9, 115]  probs:  [0.9798918584613097, 0.990467851209853]\n",
      "Iteration 30  h2 new:  [43, 48]  probs:  [0.9454005476787224, 0.9922108569951086]\n",
      "Total Labeled number:  128  Still unlabeled number:  48\n",
      "co_SVM f1:                precision    recall  f1-score   support\n",
      "\n",
      "p_robinson_0       1.00      1.00      1.00        24\n",
      "p_robinson_1       1.00      1.00      1.00        27\n",
      "\n",
      " avg / total       1.00      1.00      1.00        51\n",
      "\n",
      "[24  0  0 27]\n",
      "Initial L size:  10\n",
      "Initial U size:  166\n",
      "U size after drawing sample to U prime: 0\n",
      "Initial U prime size:  166\n",
      "P value:  1  N value:  1\n",
      "Iteration 1  h1 new:  [6, 118]  probs:  [0.9123029495832661, 0.9287816726410726]\n",
      "Iteration 1  h2 new:  [168, 3]  probs:  [0.8879816205486011, 0.9685586633948884]\n",
      "Iteration 2  h1 new:  [158, 85]  probs:  [0.9329891137215927, 0.9511659847643564]\n",
      "Iteration 2  h2 new:  [18, 60]  probs:  [0.8756295735395767, 0.9444818967906449]\n",
      "Iteration 3  h1 new:  [96, 22]  probs:  [0.9381770965794141, 0.9566526700634533]\n",
      "Iteration 3  h2 new:  [19, 127]  probs:  [0.9142806916792078, 0.9683191182329611]\n",
      "Iteration 4  h1 new:  [52, 56]  probs:  [0.9554070124212771, 0.9598720199593858]\n",
      "Iteration 4  h2 new:  [163, 61]  probs:  [0.9307425167140218, 0.9855956900825563]\n",
      "Iteration 5  h1 new:  [27, 67]  probs:  [0.9562865111737571, 0.9678715780412145]\n",
      "Iteration 5  h2 new:  [65, 97]  probs:  [0.9571518370008498, 0.9882331353703974]\n",
      "Iteration 6  h1 new:  [54, 30]  probs:  [0.9628408697894236, 0.9639152207209061]\n",
      "Iteration 6  h2 new:  [57, 122]  probs:  [0.9479834359138085, 0.9861417270559135]\n",
      "Iteration 7  h1 new:  [23, 137]  probs:  [0.9636754429692623, 0.9665632175392362]\n",
      "Iteration 7  h2 new:  [94, 88]  probs:  [0.9533795145269922, 0.9893334800971267]\n",
      "Iteration 8  h1 new:  [14, 89]  probs:  [0.9679604752345585, 0.9643420227215934]\n",
      "Iteration 8  h2 new:  [21, 55]  probs:  [0.9516133797556887, 0.99265360062183]\n",
      "Iteration 9  h1 new:  [141, 145]  probs:  [0.9718036308602004, 0.9649753530696527]\n",
      "Iteration 9  h2 new:  [2, 74]  probs:  [0.9583880912864584, 0.990938253634582]\n",
      "Iteration 10  h1 new:  [162, 44]  probs:  [0.9692543114364749, 0.9654844928779952]\n",
      "Iteration 10  h2 new:  [152, 69]  probs:  [0.9599459771025182, 0.9910525835390078]\n",
      "Iteration 11  h1 new:  [124, 16]  probs:  [0.972549333135489, 0.9705469464213553]\n",
      "Iteration 11  h2 new:  [82, 174]  probs:  [0.9519479828482017, 0.9938312716236761]\n",
      "Iteration 12  h1 new:  [117, 51]  probs:  [0.9699182256859427, 0.9677428334700474]\n",
      "Iteration 12  h2 new:  [155, 169]  probs:  [0.9614131746229573, 0.9906851901895054]\n",
      "Iteration 13  h1 new:  [116, 121]  probs:  [0.9684307242997455, 0.9669946998486606]\n",
      "Iteration 13  h2 new:  [11, 75]  probs:  [0.9637345282165042, 0.9932749634398267]\n",
      "Iteration 14  h1 new:  [81, 157]  probs:  [0.9685450754338024, 0.9699491123907602]\n",
      "Iteration 14  h2 new:  [138, 49]  probs:  [0.9747152249144843, 0.9869998113763253]\n",
      "Iteration 15  h1 new:  [126, 73]  probs:  [0.9699109702140287, 0.9701634390140169]\n",
      "Iteration 15  h2 new:  [98, 113]  probs:  [0.9761734925539706, 0.9888213711696869]\n",
      "Iteration 16  h1 new:  [106, 132]  probs:  [0.9706715069014825, 0.970631744168178]\n",
      "Iteration 16  h2 new:  [92, 62]  probs:  [0.9814703319847522, 0.98588741134341]\n",
      "Iteration 17  h1 new:  [29, 79]  probs:  [0.96923090096217, 0.9736901461218921]\n",
      "Iteration 17  h2 new:  [63, 7]  probs:  [0.9747215249197759, 0.9865434648421619]\n",
      "Iteration 18  h1 new:  [25, 110]  probs:  [0.9691754559524175, 0.971978351268092]\n",
      "Iteration 18  h2 new:  [165, 83]  probs:  [0.975208132247278, 0.9820578406052867]\n",
      "Iteration 19  h1 new:  [107, 15]  probs:  [0.9677890288501333, 0.9759919173118761]\n",
      "Iteration 19  h2 new:  [139, 46]  probs:  [0.9784368784931038, 0.9807826187756411]\n",
      "Iteration 20  h1 new:  [114, 28]  probs:  [0.9693971276883603, 0.9733400506913465]\n",
      "Iteration 20  h2 new:  [100, 47]  probs:  [0.9751184242214286, 0.9784994604982387]\n",
      "Iteration 21  h1 new:  [20, 77]  probs:  [0.9628257068582357, 0.9729430229907916]\n",
      "Iteration 21  h2 new:  [40, 33]  probs:  [0.9624032043283485, 0.9698360407136487]\n",
      "Iteration 22  h1 new:  [149, 104]  probs:  [0.9637886929442658, 0.97290157712217]\n",
      "Iteration 22  h2 new:  [64, 160]  probs:  [0.9601180802569044, 0.9807854748537048]\n",
      "Iteration 23  h1 new:  [37, 5]  probs:  [0.9700383533804011, 0.9751004725734448]\n",
      "Iteration 23  h2 new:  [66, 148]  probs:  [0.970725063373077, 0.9869216280838089]\n",
      "Iteration 24  h1 new:  [43, 146]  probs:  [0.9680015119920762, 0.9738139329275142]\n",
      "Iteration 24  h2 new:  [103, 26]  probs:  [0.9809023168049158, 0.9814851079209341]\n",
      "Iteration 25  h1 new:  [50, 130]  probs:  [0.9674956630915117, 0.9731173726983977]\n",
      "Iteration 25  h2 new:  [91, 170]  probs:  [0.9775237404390389, 0.9821958772909912]\n",
      "Iteration 26  h1 new:  [39, 24]  probs:  [0.9691564262592681, 0.9772868040648499]\n",
      "Iteration 26  h2 new:  [76, 34]  probs:  [0.9750832611178182, 0.9796398161881555]\n",
      "Iteration 27  h1 new:  [70, 175]  probs:  [0.9593658616167636, 0.9776722579470507]\n",
      "Iteration 27  h2 new:  [41, 172]  probs:  [0.9792058002778269, 0.9887955330709829]\n",
      "Iteration 28  h1 new:  [35, 101]  probs:  [0.9690849560175264, 0.9745184382210633]\n",
      "Iteration 28  h2 new:  [87, 58]  probs:  [0.997244172474996, 0.981852768928125]\n",
      "Iteration 29  h1 new:  [45, 164]  probs:  [0.9724399840530608, 0.9761525780202771]\n",
      "Iteration 29  h2 new:  [159, 144]  probs:  [0.9889499965479094, 0.9877438182997522]\n",
      "Iteration 30  h1 new:  [173, 153]  probs:  [0.9649449417141508, 0.9709169976323371]\n",
      "Iteration 30  h2 new:  [1, 105]  probs:  [0.9835508684416885, 0.9559580897547164]\n",
      "Total Labeled number:  130  Still unlabeled number:  46\n",
      "co_LR_SVM f1:                precision    recall  f1-score   support\n",
      "\n",
      "p_robinson_0       1.00      1.00      1.00        24\n",
      "p_robinson_1       1.00      1.00      1.00        27\n",
      "\n",
      " avg / total       1.00      1.00      1.00        51\n",
      "\n",
      "[24  0  0 27]\n",
      "Initial L size:  10\n",
      "Initial U size:  166\n",
      "U size after drawing sample to U prime: 0\n",
      "Initial U prime size:  166\n",
      "[('p_robinson_1', 1.0), ('p_robinson_0', 1.0)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1  h1 new:  [158, 118]  score:  [1.7398725723381163, 1.7913475248276463]\n",
      "Iteration 1  h2 new:  [18, 61]  score:  [1.8512885069697724, 1.8470363598497306]\n",
      "Iteration 2  h1 new:  [6, 16]  score:  [2.657215830093708, 2.6740964644278757]\n",
      "Iteration 2  h2 new:  [82, 127]  score:  [2.7847249565127323, 2.760142062988702]\n",
      "Iteration 3  h1 new:  [96, 5]  score:  [3.5749463424656676, 3.6378315555322254]\n",
      "Iteration 3  h2 new:  [168, 3]  score:  [3.7089655745650445, 3.7825671927509203]\n",
      "Iteration 4  h1 new:  [106, 22]  score:  [4.467106106729376, 4.588088162570864]\n",
      "Iteration 4  h2 new:  [152, 74]  score:  [4.630089095795172, 4.737212050916302]\n",
      "Iteration 5  h1 new:  [98, 85]  score:  [5.420295804873435, 5.562065304938385]\n",
      "Iteration 5  h2 new:  [163, 122]  score:  [5.580533196418106, 5.704586995227914]\n",
      "Iteration 6  h1 new:  [2, 56]  score:  [6.237540517419103, 6.493467725459009]\n",
      "Iteration 6  h2 new:  [2, 56]  score:  [6.5281817069351495, 6.642195177013361]\n",
      "Iteration 7  h1 new:  [52, 30]  score:  [7.170546441936777, 7.451016986968607]\n",
      "Iteration 7  h2 new:  [138, 55]  score:  [7.366293953877383, 7.579680427895448]\n",
      "Iteration 8  h1 new:  [29, 67]  score:  [8.113609361356852, 8.418843264445188]\n",
      "Iteration 8  h2 new:  [139, 60]  score:  [8.282386337274305, 8.557005866105182]\n",
      "Iteration 9  h1 new:  [20, 145]  score:  [9.062260392715972, 9.34205416438826]\n",
      "Iteration 9  h2 new:  [66, 49]  score:  [9.097911119942289, 9.511785585152012]\n",
      "Iteration 10  h1 new:  [43, 132]  score:  [9.90262265156084, 10.301649987366732]\n",
      "Iteration 10  h2 new:  [114, 75]  score:  [9.99550469988356, 10.464877013621999]\n",
      "Iteration 11  h1 new:  [149, 137]  score:  [10.80273437429264, 11.269095085070417]\n",
      "Iteration 11  h2 new:  [19, 97]  score:  [10.92112052972418, 11.428707226594636]\n",
      "Iteration 12  h1 new:  [27, 89]  score:  [11.721351478519406, 12.136140814900731]\n",
      "Iteration 12  h2 new:  [94, 113]  score:  [11.850498152430022, 12.361337938210731]\n",
      "Iteration 13  h1 new:  [50, 44]  score:  [12.569903286698299, 13.09073356219611]\n",
      "Iteration 13  h2 new:  [37, 69]  score:  [12.837040599001199, 13.318279408030802]\n",
      "Iteration 14  h1 new:  [126, 31]  score:  [13.47953248253824, 13.939833098031443]\n",
      "Iteration 14  h2 new:  [23, 169]  score:  [13.789900376248946, 14.282275588757653]\n",
      "Iteration 15  h1 new:  [54, 110]  score:  [14.442023997057461, 14.913704186984033]\n",
      "Iteration 15  h2 new:  [14, 88]  score:  [14.764257988492973, 15.25864388861862]\n",
      "Iteration 16  h1 new:  [117, 51]  score:  [15.177669841966875, 15.852753509459875]\n",
      "Iteration 16  h2 new:  [155, 121]  score:  [15.659377184591673, 16.016008743632792]\n",
      "Iteration 17  h1 new:  [90, 15]  score:  [15.99750126678232, 16.819608395895294]\n",
      "Iteration 17  h2 new:  [81, 160]  score:  [16.62155912476237, 16.943320616352047]\n",
      "Iteration 18  h1 new:  [124, 170]  score:  [16.957268354498858, 17.77083353374414]\n",
      "Iteration 18  h2 new:  [65, 157]  score:  [17.531721621630833, 17.88532885570783]\n",
      "Iteration 19  h1 new:  [116, 174]  score:  [17.78341101125616, 18.723359240357713]\n",
      "Iteration 19  h2 new:  [21, 62]  score:  [18.456609129908898, 18.747227234004484]\n",
      "Iteration 20  h1 new:  [78, 144]  score:  [18.705088873559003, 19.67899906461659]\n",
      "Iteration 20  h2 new:  [11, 7]  score:  [19.41945291689452, 19.556773504060438]\n",
      "Iteration 21  h1 new:  [25, 146]  score:  [19.557723111358936, 20.5700878649215]\n",
      "Iteration 21  h2 new:  [165, 148]  score:  [20.368749256581445, 20.37658919561874]\n",
      "Iteration 22  h1 new:  [107, 104]  score:  [20.46003381708877, 21.381382614819177]\n",
      "Iteration 22  h2 new:  [92, 79]  score:  [21.307008494726897, 21.32350147847055]\n",
      "Iteration 23  h1 new:  [162, 73]  score:  [21.3122708573962, 22.298197224411453]\n",
      "Iteration 23  h2 new:  [141, 46]  score:  [21.813110189653887, 22.213178097250708]\n",
      "Iteration 24  h1 new:  [39, 58]  score:  [22.058790767194672, 23.170776988380286]\n",
      "Iteration 24  h2 new:  [159, 83]  score:  [22.77091947889282, 23.124283538918053]\n",
      "Iteration 25  h1 new:  [40, 77]  score:  [22.955927515291616, 24.103946751463464]\n",
      "Iteration 25  h2 new:  [100, 172]  score:  [23.689497977931055, 23.943859933966742]\n",
      "Iteration 26  h1 new:  [63, 28]  score:  [23.909809828438124, 25.010116758025095]\n",
      "Iteration 26  h2 new:  [91, 47]  score:  [24.567748760724456, 24.902119746806186]\n",
      "Iteration 27  h1 new:  [45, 101]  score:  [24.605391200316532, 25.970003056799094]\n",
      "Iteration 27  h2 new:  [57, 26]  score:  [25.440663127528413, 25.40560937739128]\n",
      "Iteration 28  h1 new:  [70, 164]  score:  [25.11672658210623, 26.8887971974355]\n",
      "Iteration 28  h2 new:  [87, 164]  score:  [26.219660524894, 25.945980614883165]\n",
      "Iteration 29  h1 new:  [123, 153]  score:  [25.183964064317905, 27.77456883511373]\n",
      "Iteration 29  h2 new:  [76, 48]  score:  [27.14208314902745, 26.79011767516306]\n",
      "Iteration 30  h1 new:  [173, 130]  score:  [26.12665315561319, 28.72566365757544]\n",
      "Iteration 30  h2 new:  [131, 105]  score:  [27.977681734629233, 27.44503097097037]\n",
      "Total Labeled number:  127  Still unlabeled number:  49\n",
      "Improved_co_LR f1:                precision    recall  f1-score   support\n",
      "\n",
      "p_robinson_0       1.00      1.00      1.00        24\n",
      "p_robinson_1       1.00      1.00      1.00        27\n",
      "\n",
      " avg / total       1.00      1.00      1.00        51\n",
      "\n",
      "[24  0  0 27]\n",
      "LR f1:                precision    recall  f1-score   support\n",
      "\n",
      "p_robinson_0       1.00      1.00      1.00        24\n",
      "p_robinson_1       1.00      1.00      1.00        27\n",
      "\n",
      " avg / total       1.00      1.00      1.00        51\n",
      "\n",
      "[24  0  0 27]\n",
      "SVM f1:                precision    recall  f1-score   support\n",
      "\n",
      "p_robinson_0       1.00      1.00      1.00        24\n",
      "p_robinson_1       1.00      1.00      1.00        27\n",
      "\n",
      " avg / total       1.00      1.00      1.00        51\n",
      "\n",
      "[24  0  0 27]\n",
      "[('p_robinson_0', 0.472636815920398), ('p_robinson_1', 0.527363184079602)]\n",
      "[('p_robinson_0', 5, 12), ('p_robinson_1', 5, 13)]\n",
      "Initial L size:  10\n",
      "Initial U size:  166\n",
      "U size after drawing sample to U prime: 0\n",
      "Initial U prime size:  166\n",
      "P value:  1  N value:  1\n",
      "Iteration 1  h1 new:  [32, 158]  probs:  [0.9337618957874046, 0.9159921513084285]\n",
      "Iteration 1  h2 new:  [100, 63]  probs:  [0.940708907737115, 0.9751795096910192]\n",
      "Iteration 2  h1 new:  [72, 87]  probs:  [0.9496303052457944, 0.9411634077056124]\n",
      "Iteration 2  h2 new:  [17, 128]  probs:  [0.9503436848998187, 0.9736624755633908]\n",
      "Iteration 3  h1 new:  [55, 68]  probs:  [0.9483948691369016, 0.9598645875842844]\n",
      "Iteration 3  h2 new:  [34, 77]  probs:  [0.961691920394569, 0.9602288169807176]\n",
      "Iteration 4  h1 new:  [56, 58]  probs:  [0.9620569374986929, 0.9651645601448698]\n",
      "Iteration 4  h2 new:  [96, 4]  probs:  [0.9755404728044439, 0.9680521991200025]\n",
      "Iteration 5  h1 new:  [119, 5]  probs:  [0.9613295362303296, 0.9577461489016336]\n",
      "Iteration 5  h2 new:  [127, 122]  probs:  [0.9745123500453948, 0.9706811528916706]\n",
      "Iteration 6  h1 new:  [116, 28]  probs:  [0.956736528376513, 0.9582327048607543]\n",
      "Iteration 6  h2 new:  [66, 57]  probs:  [0.9736657694561938, 0.9752451912892922]\n",
      "Iteration 7  h1 new:  [124, 23]  probs:  [0.9603976573641373, 0.9625252525303303]\n",
      "Iteration 7  h2 new:  [89, 76]  probs:  [0.976296207413622, 0.9748430677000148]\n",
      "Iteration 8  h1 new:  [108, 75]  probs:  [0.9644983893068391, 0.9670224029115652]\n",
      "Iteration 8  h2 new:  [94, 137]  probs:  [0.9772748267712071, 0.9760044524644556]\n",
      "Iteration 9  h1 new:  [156, 27]  probs:  [0.9670308498872242, 0.9622010718917823]\n",
      "Iteration 9  h2 new:  [79, 62]  probs:  [0.9737839863895185, 0.9775119069684499]\n",
      "Iteration 10  h1 new:  [142, 83]  probs:  [0.9708503402076316, 0.9652565428136587]\n",
      "Iteration 10  h2 new:  [142, 70]  probs:  [0.9731204600637254, 0.9765301087564309]\n",
      "Iteration 11  h1 new:  [118, 120]  probs:  [0.9697348361143523, 0.9666926416200201]\n",
      "Iteration 11  h2 new:  [12, 52]  probs:  [0.9682769777996335, 0.9786382449382954]\n",
      "Iteration 12  h1 new:  [9, 132]  probs:  [0.9734880661897538, 0.9670150832435699]\n",
      "Iteration 12  h2 new:  [7, 125]  probs:  [0.9704305775125355, 0.9790349175500335]\n",
      "Iteration 13  h1 new:  [163, 10]  probs:  [0.9709168640145629, 0.9686477270935501]\n",
      "Iteration 13  h2 new:  [59, 13]  probs:  [0.9731443765649483, 0.982501532163594]\n",
      "Iteration 14  h1 new:  [48, 98]  probs:  [0.9727116256454296, 0.9697039088952594]\n",
      "Iteration 14  h2 new:  [22, 19]  probs:  [0.9758358476284305, 0.9792675640770633]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15  h1 new:  [159, 106]  probs:  [0.9749343926388865, 0.9681901736711941]\n",
      "Iteration 15  h2 new:  [0, 115]  probs:  [0.9730507668550514, 0.9781711489014573]\n",
      "Iteration 16  h1 new:  [2, 157]  probs:  [0.9742165892247793, 0.9680661155887708]\n",
      "Iteration 16  h2 new:  [65, 161]  probs:  [0.9701070574201797, 0.9804042753486396]\n",
      "Iteration 17  h1 new:  [78, 111]  probs:  [0.9755840245100271, 0.9644094091198201]\n",
      "Iteration 17  h2 new:  [139, 126]  probs:  [0.9708197935021537, 0.9803747893196381]\n",
      "Iteration 18  h1 new:  [3, 80]  probs:  [0.983191569895413, 0.9658068204073097]\n",
      "Iteration 18  h2 new:  [152, 141]  probs:  [0.9682632811979256, 0.9806219984282537]\n",
      "Iteration 19  h1 new:  [51, 174]  probs:  [0.9703305873095677, 0.9645685004007174]\n",
      "Iteration 19  h2 new:  [160, 90]  probs:  [0.9658401526951041, 0.9780540578624272]\n",
      "Iteration 20  h1 new:  [71, 101]  probs:  [0.9712208991861748, 0.9604356454747983]\n",
      "Iteration 20  h2 new:  [49, 43]  probs:  [0.9690375368233993, 0.9757912458880794]\n",
      "Iteration 21  h1 new:  [149, 54]  probs:  [0.9692989737383029, 0.9596358945742319]\n",
      "Iteration 21  h2 new:  [67, 148]  probs:  [0.9715951558619832, 0.9712191600389527]\n",
      "Iteration 22  h1 new:  [107, 29]  probs:  [0.9735697338330802, 0.9547332722794041]\n",
      "Iteration 22  h2 new:  [74, 29]  probs:  [0.971862221363476, 0.9623613439191026]\n",
      "Iteration 23  h1 new:  [24, 114]  probs:  [0.9697642732528969, 0.9543011575322022]\n",
      "Iteration 23  h2 new:  [138, 85]  probs:  [0.9732728083455446, 0.9625391506030733]\n",
      "Iteration 24  h1 new:  [50, 165]  probs:  [0.9705185080832512, 0.9553216071513301]\n",
      "Iteration 24  h2 new:  [6, 45]  probs:  [0.9682809427892543, 0.9658714220795953]\n",
      "Iteration 25  h1 new:  [8, 170]  probs:  [0.9721263084374004, 0.9555410953473358]\n",
      "Iteration 25  h2 new:  [155, 64]  probs:  [0.9610773487024276, 0.965836942810672]\n",
      "Iteration 26  h1 new:  [53, 104]  probs:  [0.9668501376256465, 0.9656984231073179]\n",
      "Iteration 26  h2 new:  [25, 60]  probs:  [0.9518724476975506, 0.9504108877930142]\n",
      "Iteration 27  h1 new:  [16, 146]  probs:  [0.9752220190196412, 0.9709458771070311]\n",
      "Iteration 27  h2 new:  [93, 171]  probs:  [0.9423767478482755, 0.9616240859638516]\n",
      "Iteration 28  h1 new:  [172, 113]  probs:  [0.9737817921250753, 0.9657135788626815]\n",
      "Iteration 28  h2 new:  [131, 145]  probs:  [0.9436849968674658, 0.9683317283950336]\n",
      "Iteration 29  h1 new:  [164, 153]  probs:  [0.9711718195217617, 0.9654068590572131]\n",
      "Iteration 29  h2 new:  [47, 166]  probs:  [0.936177478397211, 0.9453354462531167]\n",
      "Iteration 30  h1 new:  [92, 130]  probs:  [0.9643209900799411, 0.957604115343724]\n",
      "Iteration 30  h2 new:  [154, 40]  probs:  [0.9237604236724424, 0.9474263106939638]\n",
      "Total Labeled number:  128  Still unlabeled number:  48\n",
      "co_LR f1:                precision    recall  f1-score   support\n",
      "\n",
      "p_robinson_0       0.89      1.00      0.94        24\n",
      "p_robinson_1       1.00      0.89      0.94        27\n",
      "\n",
      " avg / total       0.95      0.94      0.94        51\n",
      "\n",
      "[24  0  3 24]\n",
      "Initial L size:  10\n",
      "Initial U size:  166\n",
      "U size after drawing sample to U prime: 0\n",
      "Initial U prime size:  166\n",
      "P value:  1  N value:  1\n",
      "Iteration 1  h1 new:  [72, 158]  probs:  [0.9872249725711179, 0.9714509174912983]\n",
      "Iteration 1  h2 new:  [100, 128]  probs:  [0.970778186477602, 0.936686816309707]\n",
      "Iteration 2  h1 new:  [32, 87]  probs:  [0.9788824080558516, 0.9750188020888851]\n",
      "Iteration 2  h2 new:  [152, 63]  probs:  [0.9652095204018875, 0.93882092454205]\n",
      "Iteration 3  h1 new:  [163, 68]  probs:  [0.9728175078755618, 0.9710502733127756]\n",
      "Iteration 3  h2 new:  [127, 77]  probs:  [0.9826872481822344, 0.9312348675331888]\n",
      "Iteration 4  h1 new:  [116, 58]  probs:  [0.9663555248265282, 0.9626621953166117]\n",
      "Iteration 4  h2 new:  [12, 111]  probs:  [0.9965164907826332, 0.9510617565291755]\n",
      "Iteration 5  h1 new:  [119, 170]  probs:  [0.9763953903687346, 0.9730516189649918]\n",
      "Iteration 5  h2 new:  [2, 170]  probs:  [0.996656649067246, 0.9494658154802849]\n",
      "Iteration 6  h1 new:  [55, 10]  probs:  [0.9694529736267261, 0.9651458624128074]\n",
      "Iteration 6  h2 new:  [159, 125]  probs:  [0.9940591134195957, 0.9544127083114897]\n",
      "Iteration 7  h1 new:  [24, 23]  probs:  [0.972918757377198, 0.9726665771573063]\n",
      "Iteration 7  h2 new:  [34, 57]  probs:  [0.9931484689958484, 0.9369784764945048]\n",
      "Iteration 8  h1 new:  [56, 5]  probs:  [0.9733014650613676, 0.9702091285017073]\n",
      "Iteration 8  h2 new:  [17, 43]  probs:  [0.9942213464822565, 0.9203790992439966]\n",
      "Iteration 9  h1 new:  [9, 83]  probs:  [0.9690058869164417, 0.9708409208852933]\n",
      "Iteration 9  h2 new:  [96, 83]  probs:  [0.9955769995031066, 0.9203340737985924]\n",
      "Iteration 10  h1 new:  [156, 62]  probs:  [0.9699739154886959, 0.9716133720422822]\n",
      "Iteration 10  h2 new:  [89, 4]  probs:  [0.9874305480358674, 0.9599229237853195]\n",
      "Iteration 11  h1 new:  [108, 28]  probs:  [0.9749337757752808, 0.9813092391943377]\n",
      "Iteration 11  h2 new:  [66, 76]  probs:  [0.9741214966439619, 0.9935174056128427]\n",
      "Iteration 12  h1 new:  [51, 75]  probs:  [0.9826280149223792, 0.9815894147466566]\n",
      "Iteration 12  h2 new:  [22, 13]  probs:  [0.9727892465103598, 0.9845179904649052]\n",
      "Iteration 13  h1 new:  [48, 120]  probs:  [0.9824415082708229, 0.9814348952266689]\n",
      "Iteration 13  h2 new:  [139, 52]  probs:  [0.9808019969644316, 0.9899175493047975]\n",
      "Iteration 14  h1 new:  [3, 106]  probs:  [0.9946739735855985, 0.9807608373896081]\n",
      "Iteration 14  h2 new:  [0, 122]  probs:  [0.9836168558385526, 0.9843330561835992]\n",
      "Iteration 15  h1 new:  [78, 98]  probs:  [0.9901750778908047, 0.9699051338511289]\n",
      "Iteration 15  h2 new:  [94, 27]  probs:  [0.9818321811240676, 0.9901339323311986]\n",
      "Iteration 16  h1 new:  [53, 132]  probs:  [0.9911815900713589, 0.9701428181356669]\n",
      "Iteration 16  h2 new:  [79, 70]  probs:  [0.9704961523825525, 0.985178834609875]\n",
      "Iteration 17  h1 new:  [118, 101]  probs:  [0.987711089773271, 0.971986593975652]\n",
      "Iteration 17  h2 new:  [160, 137]  probs:  [0.9803469394463306, 0.9865833726346575]\n",
      "Iteration 18  h1 new:  [142, 54]  probs:  [0.9913490985302947, 0.9661400216161568]\n",
      "Iteration 18  h2 new:  [59, 126]  probs:  [0.9844866846697916, 0.9857749052442805]\n",
      "Iteration 19  h1 new:  [149, 174]  probs:  [0.990417811776414, 0.9793955053610456]\n",
      "Iteration 19  h2 new:  [7, 157]  probs:  [0.980764173762945, 0.9788045508658912]\n",
      "Iteration 20  h1 new:  [124, 115]  probs:  [0.9896701539049144, 0.9701097231889211]\n",
      "Iteration 20  h2 new:  [65, 90]  probs:  [0.9729645205602067, 0.9790807455058022]\n",
      "Iteration 21  h1 new:  [107, 80]  probs:  [0.9896723821721229, 0.9727037668629305]\n",
      "Iteration 21  h2 new:  [67, 19]  probs:  [0.9741891098883534, 0.9802044193272736]\n",
      "Iteration 22  h1 new:  [172, 173]  probs:  [0.98893668992049, 0.9760592259052023]\n",
      "Iteration 22  h2 new:  [49, 141]  probs:  [0.9803425779113885, 0.9770074308608553]\n",
      "Iteration 23  h1 new:  [71, 45]  probs:  [0.9901890653506357, 0.9708303206961881]\n",
      "Iteration 23  h2 new:  [74, 161]  probs:  [0.9804504091384333, 0.9834298506923308]\n",
      "Iteration 24  h1 new:  [50, 148]  probs:  [0.9913373177553448, 0.9724801441295575]\n",
      "Iteration 24  h2 new:  [138, 85]  probs:  [0.9811744787912136, 0.9757445718242032]\n",
      "Iteration 25  h1 new:  [6, 18]  probs:  [0.9880240144370198, 0.9814399480419592]\n",
      "Iteration 25  h2 new:  [6, 29]  probs:  [0.9793883902308896, 0.9673188300115392]\n",
      "Iteration 26  h1 new:  [16, 153]  probs:  [0.9882596772367366, 0.9763308412781517]\n",
      "Iteration 26  h2 new:  [155, 165]  probs:  [0.981178193990393, 0.9713823507778699]\n",
      "Iteration 27  h1 new:  [123, 113]  probs:  [0.9864510338375303, 0.9738652548726333]\n",
      "Iteration 27  h2 new:  [93, 40]  probs:  [0.9599503547479057, 0.9772311172030442]\n",
      "Iteration 28  h1 new:  [8, 130]  probs:  [0.9914748977223852, 0.982944918048521]\n",
      "Iteration 28  h2 new:  [47, 64]  probs:  [0.9565821674027984, 0.9802506966621536]\n",
      "Iteration 29  h1 new:  [112, 104]  probs:  [0.9768714456133262, 0.9962228790085973]\n",
      "Iteration 29  h2 new:  [25, 60]  probs:  [0.970601383050545, 0.9421874248689934]\n",
      "Iteration 30  h1 new:  [97, 146]  probs:  [0.9742112689031209, 0.9864549542531614]\n",
      "Iteration 30  h2 new:  [131, 171]  probs:  [0.9589391538360097, 0.9573394276552424]\n",
      "Total Labeled number:  127  Still unlabeled number:  49\n",
      "co_SVM f1:                precision    recall  f1-score   support\n",
      "\n",
      "p_robinson_0       0.92      1.00      0.96        24\n",
      "p_robinson_1       1.00      0.93      0.96        27\n",
      "\n",
      " avg / total       0.96      0.96      0.96        51\n",
      "\n",
      "[24  0  2 25]\n",
      "Initial L size:  10\n",
      "Initial U size:  166\n",
      "U size after drawing sample to U prime: 0\n",
      "Initial U prime size:  166\n",
      "P value:  1  N value:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1  h1 new:  [32, 158]  probs:  [0.9337618957874046, 0.9159921513084285]\n",
      "Iteration 1  h2 new:  [100, 128]  probs:  [0.9716534059945301, 0.9388190700315308]\n",
      "Iteration 2  h1 new:  [72, 87]  probs:  [0.9478101301505946, 0.940587558170878]\n",
      "Iteration 2  h2 new:  [127, 63]  probs:  [0.9669028440778795, 0.9422738007579013]\n",
      "Iteration 3  h1 new:  [55, 68]  probs:  [0.9511905056433885, 0.9568043412197318]\n",
      "Iteration 3  h2 new:  [34, 77]  probs:  [0.9713731416236516, 0.9698769936056048]\n",
      "Iteration 4  h1 new:  [56, 58]  probs:  [0.9592050989693153, 0.9653038605992529]\n",
      "Iteration 4  h2 new:  [17, 58]  probs:  [0.9880972333333128, 0.8615265679752262]\n",
      "Iteration 5  h1 new:  [119, 5]  probs:  [0.9597297851269231, 0.953183652512511]\n",
      "Iteration 5  h2 new:  [96, 57]  probs:  [0.9859204687331012, 0.8670850645578277]\n",
      "Iteration 6  h1 new:  [116, 23]  probs:  [0.9600294574192371, 0.9566904354987937]\n",
      "Iteration 6  h2 new:  [89, 137]  probs:  [0.9784200509654599, 0.9100219589381427]\n",
      "Iteration 7  h1 new:  [124, 28]  probs:  [0.9620564214562229, 0.9621392307375649]\n",
      "Iteration 7  h2 new:  [48, 83]  probs:  [0.968301244455253, 0.9243215663714724]\n",
      "Iteration 8  h1 new:  [108, 75]  probs:  [0.9681923775575642, 0.9665734504643841]\n",
      "Iteration 8  h2 new:  [94, 75]  probs:  [0.9758848565060643, 0.9325000519685968]\n",
      "Iteration 9  h1 new:  [156, 27]  probs:  [0.9694679045724041, 0.9597748365145434]\n",
      "Iteration 9  h2 new:  [66, 122]  probs:  [0.9738847847398528, 0.9210268714272619]\n",
      "Iteration 10  h1 new:  [9, 62]  probs:  [0.9697400620706919, 0.9570190242469037]\n",
      "Iteration 10  h2 new:  [79, 125]  probs:  [0.9827551824033228, 0.9163314545000819]\n",
      "Iteration 11  h1 new:  [142, 157]  probs:  [0.9753285668260394, 0.9582972410441636]\n",
      "Iteration 11  h2 new:  [22, 4]  probs:  [0.9718665602727238, 0.9889391282203773]\n",
      "Iteration 12  h1 new:  [163, 120]  probs:  [0.9722539513957138, 0.9652750093956151]\n",
      "Iteration 12  h2 new:  [12, 76]  probs:  [0.9624169601725305, 0.9746063739638884]\n",
      "Iteration 13  h1 new:  [159, 132]  probs:  [0.975613138688804, 0.9618949526320275]\n",
      "Iteration 13  h2 new:  [59, 52]  probs:  [0.9673030542500772, 0.97935808194747]\n",
      "Iteration 14  h1 new:  [118, 10]  probs:  [0.9766522640483671, 0.9615110152007675]\n",
      "Iteration 14  h2 new:  [7, 70]  probs:  [0.9351214459981732, 0.990957803278265]\n",
      "Iteration 15  h1 new:  [65, 98]  probs:  [0.9693474568368688, 0.9677054704234431]\n",
      "Iteration 15  h2 new:  [65, 13]  probs:  [0.9414326654763556, 0.9935574024170253]\n",
      "Iteration 16  h1 new:  [2, 106]  probs:  [0.9686342614446086, 0.9676211815661044]\n",
      "Iteration 16  h2 new:  [74, 19]  probs:  [0.9427074252359592, 0.9941805991842796]\n",
      "Iteration 17  h1 new:  [3, 80]  probs:  [0.9716986522057777, 0.967019356009493]\n",
      "Iteration 17  h2 new:  [139, 126]  probs:  [0.9442440512055137, 0.9928631535491408]\n",
      "Iteration 18  h1 new:  [107, 115]  probs:  [0.9722330342849377, 0.9649729343214583]\n",
      "Iteration 18  h2 new:  [49, 115]  probs:  [0.9402247968997473, 0.9921951780031526]\n",
      "Iteration 19  h1 new:  [149, 43]  probs:  [0.9704044205571125, 0.9609641831304868]\n",
      "Iteration 19  h2 new:  [149, 90]  probs:  [0.9432312241381817, 0.986794563848834]\n",
      "Iteration 20  h1 new:  [78, 174]  probs:  [0.9666985236590611, 0.9670762163468549]\n",
      "Iteration 20  h2 new:  [0, 174]  probs:  [0.9404736508076359, 0.9873413963726803]\n",
      "Iteration 21  h1 new:  [152, 54]  probs:  [0.9716319612825487, 0.9625553581218546]\n",
      "Iteration 21  h2 new:  [138, 141]  probs:  [0.9393746569380867, 0.9851349487564883]\n",
      "Iteration 22  h1 new:  [8, 29]  probs:  [0.969621415863959, 0.9606115220458002]\n",
      "Iteration 22  h2 new:  [155, 161]  probs:  [0.9385424828317936, 0.9885665819520745]\n",
      "Iteration 23  h1 new:  [51, 148]  probs:  [0.9755864126118171, 0.9566422121643722]\n",
      "Iteration 23  h2 new:  [6, 148]  probs:  [0.932057651165122, 0.9859933150397912]\n",
      "Iteration 24  h1 new:  [24, 101]  probs:  [0.9722548023407677, 0.9559845184497843]\n",
      "Iteration 24  h2 new:  [67, 85]  probs:  [0.9808280641212499, 0.9794823852347173]\n",
      "Iteration 25  h1 new:  [71, 165]  probs:  [0.9747889366194069, 0.9565927939253017]\n",
      "Iteration 25  h2 new:  [160, 45]  probs:  [0.9857119834800766, 0.9675885031612483]\n",
      "Iteration 26  h1 new:  [50, 111]  probs:  [0.9759463547615461, 0.9558108466411148]\n",
      "Iteration 26  h2 new:  [93, 40]  probs:  [0.9592927235399239, 0.879459006910525]\n",
      "Iteration 27  h1 new:  [53, 114]  probs:  [0.9711038027591218, 0.96096143609858]\n",
      "Iteration 27  h2 new:  [25, 170]  probs:  [0.9606767202307444, 0.8902354167506901]\n",
      "Iteration 28  h1 new:  [16, 60]  probs:  [0.9759595863015952, 0.9619763913355779]\n",
      "Iteration 28  h2 new:  [47, 64]  probs:  [0.9618090664039143, 0.8777631848009455]\n",
      "Iteration 29  h1 new:  [172, 104]  probs:  [0.9755079615530466, 0.9711023669820746]\n",
      "Iteration 29  h2 new:  [129, 61]  probs:  [0.9422431705815036, 0.8903771924168381]\n",
      "Iteration 30  h1 new:  [164, 113]  probs:  [0.9631956608757044, 0.9822823593068178]\n",
      "Iteration 30  h2 new:  [46, 113]  probs:  [0.9410168811887993, 0.9096515871471956]\n",
      "Total Labeled number:  122  Still unlabeled number:  54\n",
      "co_LR_SVM f1:                precision    recall  f1-score   support\n",
      "\n",
      "p_robinson_0       0.96      1.00      0.98        24\n",
      "p_robinson_1       1.00      0.96      0.98        27\n",
      "\n",
      " avg / total       0.98      0.98      0.98        51\n",
      "\n",
      "[24  0  1 26]\n",
      "Initial L size:  10\n",
      "Initial U size:  166\n",
      "U size after drawing sample to U prime: 0\n",
      "Initial U prime size:  166\n",
      "[('p_robinson_1', 1.0), ('p_robinson_0', 1.0)]\n",
      "Iteration 1  h1 new:  [116, 87]  score:  [1.4036918902309394, 1.5839558712138109]\n",
      "Iteration 1  h2 new:  [22, 128]  score:  [1.7976984397693638, 1.8542028918275284]\n",
      "Iteration 2  h1 new:  [12, 68]  score:  [2.5885197913006635, 2.705147004388549]\n",
      "Iteration 2  h2 new:  [12, 63]  score:  [2.787320012532689, 2.7999922511354987]\n",
      "Iteration 3  h1 new:  [163, 58]  score:  [3.559940594462561, 3.643342818864073]\n",
      "Iteration 3  h2 new:  [2, 77]  score:  [3.6740850948008488, 3.749842345828344]\n",
      "Iteration 4  h1 new:  [32, 158]  score:  [4.5311118228013445, 4.593791526720877]\n",
      "Iteration 4  h2 new:  [100, 122]  score:  [4.620762605252992, 4.691682143530272]\n",
      "Iteration 5  h1 new:  [72, 28]  score:  [5.507985426512956, 5.4939647009981405]\n",
      "Iteration 5  h2 new:  [152, 4]  score:  [5.544037542536653, 5.648781049777703]\n",
      "Iteration 6  h1 new:  [55, 5]  score:  [6.4518166647194155, 6.4054481419783915]\n",
      "Iteration 6  h2 new:  [17, 57]  score:  [6.4992473812976845, 6.629415750791056]\n",
      "Iteration 7  h1 new:  [9, 83]  score:  [7.4014477005829145, 7.344498259371711]\n",
      "Iteration 7  h2 new:  [96, 76]  score:  [7.447459434229256, 7.601164421280625]\n",
      "Iteration 8  h1 new:  [156, 120]  score:  [8.363633142938259, 8.282374456875198]\n",
      "Iteration 8  h2 new:  [34, 125]  score:  [8.451504083648276, 8.524066824333485]\n",
      "Iteration 9  h1 new:  [159, 23]  score:  [9.301491661253603, 9.222648714439249]\n",
      "Iteration 9  h2 new:  [139, 62]  score:  [9.269893933344648, 9.486882332297457]\n",
      "Iteration 10  h1 new:  [3, 75]  score:  [10.232479559966828, 10.175895632388444]\n",
      "Iteration 10  h2 new:  [127, 27]  score:  [10.191859285232475, 10.47708567998649]\n",
      "Iteration 11  h1 new:  [107, 170]  score:  [11.150585627817176, 11.088429902959948]\n",
      "Iteration 11  h2 new:  [66, 52]  score:  [11.086583301875049, 11.429785636948695]\n",
      "Iteration 12  h1 new:  [56, 10]  score:  [12.097261063149196, 12.03734382154787]\n",
      "Iteration 12  h2 new:  [94, 137]  score:  [12.042575804500984, 12.382730996085186]\n",
      "Iteration 13  h1 new:  [119, 132]  score:  [13.025496228682314, 12.94856555245789]\n",
      "Iteration 13  h2 new:  [48, 115]  score:  [12.967916189652447, 13.231917795261165]\n",
      "Iteration 14  h1 new:  [149, 106]  score:  [13.947311022727504, 13.875117019424124]\n",
      "Iteration 14  h2 new:  [78, 13]  score:  [13.869950056235426, 14.184376029852837]\n",
      "Iteration 15  h1 new:  [24, 54]  score:  [14.761023760305918, 14.748174110627744]\n",
      "Iteration 15  h2 new:  [89, 19]  score:  [14.804460310512377, 15.159851275034216]\n",
      "Iteration 16  h1 new:  [53, 111]  score:  [15.721190835068803, 15.619011779878559]\n",
      "Iteration 16  h2 new:  [0, 70]  score:  [15.77847402208944, 16.1100925946917]\n",
      "Iteration 17  h1 new:  [74, 174]  score:  [16.67420151368872, 16.56834596698635]\n",
      "Iteration 17  h2 new:  [74, 161]  score:  [16.58682483357326, 17.039020865382582]\n",
      "Iteration 18  h1 new:  [124, 80]  score:  [17.492249096170795, 17.52859028794634]\n",
      "Iteration 18  h2 new:  [67, 98]  score:  [17.554031610413528, 18.018323191659398]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19  h1 new:  [118, 157]  score:  [18.467453087073665, 18.459321763026043]\n",
      "Iteration 19  h2 new:  [6, 90]  score:  [18.47069301387178, 18.94733563775546]\n",
      "Iteration 20  h1 new:  [108, 43]  score:  [19.415926735880188, 19.303021472191492]\n",
      "Iteration 20  h2 new:  [138, 43]  score:  [19.40596628927409, 19.922148798877394]\n",
      "Iteration 21  h1 new:  [8, 165]  score:  [20.34737703368185, 20.18306037800181]\n",
      "Iteration 21  h2 new:  [79, 126]  score:  [20.361050496467367, 20.730453226758584]\n",
      "Iteration 22  h1 new:  [142, 60]  score:  [21.06931422802652, 21.064319292913442]\n",
      "Iteration 22  h2 new:  [49, 141]  score:  [21.321737425507905, 21.69397357862645]\n",
      "Iteration 23  h1 new:  [50, 29]  score:  [21.974305638115556, 21.992998305502077]\n",
      "Iteration 23  h2 new:  [160, 64]  score:  [22.19457276129873, 22.482592354460575]\n",
      "Iteration 24  h1 new:  [51, 101]  score:  [22.837836214415994, 22.909221047759083]\n",
      "Iteration 24  h2 new:  [59, 148]  score:  [22.929183362528576, 23.321556323119452]\n",
      "Iteration 25  h1 new:  [164, 146]  score:  [23.69256689991053, 23.748293620828214]\n",
      "Iteration 25  h2 new:  [7, 45]  score:  [23.82363412175118, 24.022704791613265]\n",
      "Iteration 26  h1 new:  [81, 153]  score:  [24.294755343871252, 24.821851199198097]\n",
      "Iteration 26  h2 new:  [155, 85]  score:  [24.54938568535717, 24.92635554408567]\n",
      "Iteration 27  h1 new:  [16, 130]  score:  [25.61474146134393, 25.52702525808857]\n",
      "Iteration 27  h2 new:  [65, 113]  score:  [25.36451046299298, 25.685269329817892]\n",
      "Iteration 28  h1 new:  [71, 145]  score:  [26.039187441407766, 26.415136825907712]\n",
      "Iteration 28  h2 new:  [25, 171]  score:  [26.00128931355374, 26.65597820864647]\n",
      "Iteration 29  h1 new:  [46, 104]  score:  [26.81451104012946, 27.105316335263364]\n",
      "Iteration 29  h2 new:  [93, 40]  score:  [26.336143693062517, 26.63924908956489]\n",
      "Iteration 30  h1 new:  [92, 173]  score:  [27.623078922917916, 28.03416110031095]\n",
      "Iteration 30  h2 new:  [131, 166]  score:  [26.94104651139314, 27.510550941706274]\n",
      "Total Labeled number:  127  Still unlabeled number:  49\n",
      "Improved_co_LR f1:                precision    recall  f1-score   support\n",
      "\n",
      "p_robinson_0       0.89      1.00      0.94        24\n",
      "p_robinson_1       1.00      0.89      0.94        27\n",
      "\n",
      " avg / total       0.95      0.94      0.94        51\n",
      "\n",
      "[24  0  3 24]\n",
      "LR f1:                precision    recall  f1-score   support\n",
      "\n",
      "p_robinson_0       0.92      1.00      0.96        24\n",
      "p_robinson_1       1.00      0.93      0.96        27\n",
      "\n",
      " avg / total       0.96      0.96      0.96        51\n",
      "\n",
      "[24  0  2 25]\n",
      "SVM f1:                precision    recall  f1-score   support\n",
      "\n",
      "p_robinson_0       0.92      0.96      0.94        24\n",
      "p_robinson_1       0.96      0.93      0.94        27\n",
      "\n",
      " avg / total       0.94      0.94      0.94        51\n",
      "\n",
      "[23  1  2 25]\n",
      "[('p_robinson_0', 0.472636815920398), ('p_robinson_1', 0.527363184079602)]\n",
      "[('p_robinson_0', 5, 12), ('p_robinson_1', 5, 13)]\n",
      "Initial L size:  10\n",
      "Initial U size:  166\n",
      "U size after drawing sample to U prime: 0\n",
      "Initial U prime size:  166\n",
      "P value:  1  N value:  1\n",
      "Iteration 1  h1 new:  [60, 23]  probs:  [0.9192125635381129, 0.8727033461897773]\n",
      "Iteration 1  h2 new:  [83, 47]  probs:  [0.9913440489242197, 0.9522009371844551]\n",
      "Iteration 2  h1 new:  [65, 131]  probs:  [0.9373259123230877, 0.9147739489623885]\n",
      "Iteration 2  h2 new:  [95, 98]  probs:  [0.9731476088231739, 0.940201792792572]\n",
      "Iteration 3  h1 new:  [73, 6]  probs:  [0.9520758875703883, 0.940633135927385]\n",
      "Iteration 3  h2 new:  [33, 5]  probs:  [0.9743861120298382, 0.9561407834223998]\n",
      "Iteration 4  h1 new:  [31, 137]  probs:  [0.9603066920263376, 0.9493706705215926]\n",
      "Iteration 4  h2 new:  [126, 121]  probs:  [0.976249535776738, 0.9603621656768411]\n",
      "Iteration 5  h1 new:  [107, 110]  probs:  [0.9656594278883369, 0.9543583982235925]\n",
      "Iteration 5  h2 new:  [90, 169]  probs:  [0.9768744445888187, 0.9638348751990818]\n",
      "Iteration 6  h1 new:  [123, 155]  probs:  [0.9612986437448369, 0.9583969670438357]\n",
      "Iteration 6  h2 new:  [69, 62]  probs:  [0.9793013112724825, 0.9640136168884752]\n",
      "Iteration 7  h1 new:  [67, 68]  probs:  [0.9579721219759333, 0.958970327598998]\n",
      "Iteration 7  h2 new:  [17, 158]  probs:  [0.9818224101189457, 0.9620702575016657]\n",
      "Iteration 8  h1 new:  [141, 76]  probs:  [0.9588361391004894, 0.9557368802572568]\n",
      "Iteration 8  h2 new:  [93, 124]  probs:  [0.9802022421910686, 0.9649156352815466]\n",
      "Iteration 9  h1 new:  [56, 170]  probs:  [0.963919495683107, 0.9624917058118397]\n",
      "Iteration 9  h2 new:  [56, 114]  probs:  [0.978967199558205, 0.9679519928476877]\n",
      "Iteration 10  h1 new:  [10, 132]  probs:  [0.954725617686508, 0.9673525732787697]\n",
      "Iteration 10  h2 new:  [1, 97]  probs:  [0.9673980060809669, 0.9692335563772467]\n",
      "Iteration 11  h1 new:  [165, 117]  probs:  [0.954808965631494, 0.969410898582735]\n",
      "Iteration 11  h2 new:  [165, 125]  probs:  [0.969962753538563, 0.9679638044835845]\n",
      "Iteration 12  h1 new:  [154, 61]  probs:  [0.9518456843572627, 0.9686569318654162]\n",
      "Iteration 12  h2 new:  [8, 140]  probs:  [0.9693010927747137, 0.9717193844422576]\n",
      "Iteration 13  h1 new:  [156, 11]  probs:  [0.9563110380993008, 0.9712051220680082]\n",
      "Iteration 13  h2 new:  [168, 86]  probs:  [0.9744662210980183, 0.9723672327371902]\n",
      "Iteration 14  h1 new:  [116, 127]  probs:  [0.9586408218362863, 0.9719898756634575]\n",
      "Iteration 14  h2 new:  [94, 127]  probs:  [0.9839131887361524, 0.9728376625365308]\n",
      "Iteration 15  h1 new:  [64, 57]  probs:  [0.9662622853165249, 0.9680634053077168]\n",
      "Iteration 15  h2 new:  [13, 29]  probs:  [0.9762147867814369, 0.9711925461074358]\n",
      "Iteration 16  h1 new:  [3, 42]  probs:  [0.9655207752866274, 0.9698762336546488]\n",
      "Iteration 16  h2 new:  [138, 172]  probs:  [0.9693672765979509, 0.9733339522337041]\n",
      "Iteration 17  h1 new:  [160, 50]  probs:  [0.9800167816423514, 0.9705689756677259]\n",
      "Iteration 17  h2 new:  [71, 88]  probs:  [0.9639606448402546, 0.9693250286050735]\n",
      "Iteration 18  h1 new:  [96, 104]  probs:  [0.9797406067895241, 0.9734070972958171]\n",
      "Iteration 18  h2 new:  [150, 43]  probs:  [0.9625232583650488, 0.9681051829493923]\n",
      "Iteration 19  h1 new:  [163, 145]  probs:  [0.9797453149139558, 0.9712907842119382]\n",
      "Iteration 19  h2 new:  [163, 52]  probs:  [0.9602947229876596, 0.9710753387482913]\n",
      "Iteration 20  h1 new:  [51, 146]  probs:  [0.974698625087639, 0.9695356574359743]\n",
      "Iteration 20  h2 new:  [45, 55]  probs:  [0.9631133103128917, 0.9723805712143447]\n",
      "Iteration 21  h1 new:  [4, 112]  probs:  [0.9761383544641687, 0.9684088360439352]\n",
      "Iteration 21  h2 new:  [147, 28]  probs:  [0.9655358927180284, 0.9723681404972647]\n",
      "Iteration 22  h1 new:  [66, 70]  probs:  [0.9726322211226174, 0.963324917748807]\n",
      "Iteration 22  h2 new:  [157, 120]  probs:  [0.9658641500548161, 0.9662604776443772]\n",
      "Iteration 23  h1 new:  [7, 148]  probs:  [0.9725907876998482, 0.9597782309640697]\n",
      "Iteration 23  h2 new:  [7, 89]  probs:  [0.9601390756797782, 0.9669586850766613]\n",
      "Iteration 24  h1 new:  [173, 174]  probs:  [0.9722023817459801, 0.966023039140666]\n",
      "Iteration 24  h2 new:  [106, 46]  probs:  [0.9562199022580761, 0.9685910905353511]\n",
      "Iteration 25  h1 new:  [75, 101]  probs:  [0.9730820232218893, 0.9630761745271462]\n",
      "Iteration 25  h2 new:  [41, 105]  probs:  [0.9555462958835929, 0.9654081332382569]\n",
      "Iteration 26  h1 new:  [9, 164]  probs:  [0.963233515635424, 0.9698541821158473]\n",
      "Iteration 26  h2 new:  [153, 32]  probs:  [0.9542765262159034, 0.9657959948597481]\n",
      "Iteration 27  h1 new:  [84, 151]  probs:  [0.96515715765471, 0.9684143037154862]\n",
      "Iteration 27  h2 new:  [24, 85]  probs:  [0.9503808715994896, 0.9673175667334306]\n",
      "Iteration 28  h1 new:  [82, 166]  probs:  [0.9556682003779408, 0.9695002414280582]\n",
      "Iteration 28  h2 new:  [81, 2]  probs:  [0.94668473788124, 0.967113386914138]\n",
      "Iteration 29  h1 new:  [162, 74]  probs:  [0.9511735148352746, 0.9743912435364481]\n",
      "Iteration 29  h2 new:  [18, 87]  probs:  [0.9414979939127692, 0.9711128662819225]\n",
      "Iteration 30  h1 new:  [122, 102]  probs:  [0.9342635668245018, 0.9628606173446523]\n",
      "Iteration 30  h2 new:  [22, 77]  probs:  [0.9444825509012093, 0.9608333809494022]\n",
      "Total Labeled number:  125  Still unlabeled number:  51\n",
      "co_LR f1:                precision    recall  f1-score   support\n",
      "\n",
      "p_robinson_0       0.92      0.92      0.92        24\n",
      "p_robinson_1       0.93      0.93      0.93        27\n",
      "\n",
      " avg / total       0.92      0.92      0.92        51\n",
      "\n",
      "[22  2  2 25]\n",
      "Initial L size:  10\n",
      "Initial U size:  166\n",
      "U size after drawing sample to U prime: 0\n",
      "Initial U prime size:  166\n",
      "P value:  1  N value:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1  h1 new:  [60, 110]  probs:  [0.9826172880677093, 0.9534528427237884]\n",
      "Iteration 1  h2 new:  [83, 47]  probs:  [0.9982243139259359, 0.9521933331312702]\n",
      "Iteration 2  h1 new:  [107, 23]  probs:  [0.9583638634492693, 0.9741509440857713]\n",
      "Iteration 2  h2 new:  [33, 170]  probs:  [0.9620294259081289, 0.9161584579937827]\n",
      "Iteration 3  h1 new:  [31, 131]  probs:  [0.9397638047789544, 0.9572542677032798]\n",
      "Iteration 3  h2 new:  [126, 55]  probs:  [0.9746993962510857, 0.9183292804892504]\n",
      "Iteration 4  h1 new:  [73, 137]  probs:  [0.9786798080874932, 0.9461472829568807]\n",
      "Iteration 4  h2 new:  [95, 137]  probs:  [0.981387140983341, 0.9316015592551143]\n",
      "Iteration 5  h1 new:  [10, 145]  probs:  [0.957111476780523, 0.9550388992615753]\n",
      "Iteration 5  h2 new:  [1, 112]  probs:  [0.9815872797229194, 0.9185756842541787]\n",
      "Iteration 6  h1 new:  [82, 6]  probs:  [0.9698412225438346, 0.968264424749293]\n",
      "Iteration 6  h2 new:  [3, 98]  probs:  [0.9809993555134433, 0.9595850030628328]\n",
      "Iteration 7  h1 new:  [123, 76]  probs:  [0.9502940818342774, 0.9671101348283878]\n",
      "Iteration 7  h2 new:  [94, 169]  probs:  [0.9844865187770707, 0.9631314960003565]\n",
      "Iteration 8  h1 new:  [156, 155]  probs:  [0.9677842368658648, 0.971420495975826]\n",
      "Iteration 8  h2 new:  [64, 121]  probs:  [0.9906430192243479, 0.9645824037919885]\n",
      "Iteration 9  h1 new:  [65, 68]  probs:  [0.9638505929185547, 0.9748246516033723]\n",
      "Iteration 9  h2 new:  [138, 43]  probs:  [0.9906918268325889, 0.9522867272496268]\n",
      "Iteration 10  h1 new:  [67, 104]  probs:  [0.9857620703253942, 0.9615376276155231]\n",
      "Iteration 10  h2 new:  [13, 86]  probs:  [0.9921380732067822, 0.9589745497866774]\n",
      "Iteration 11  h1 new:  [160, 62]  probs:  [0.9865281445671572, 0.9593591388038892]\n",
      "Iteration 11  h2 new:  [69, 52]  probs:  [0.9922834154712884, 0.9561597700946315]\n",
      "Iteration 12  h1 new:  [154, 5]  probs:  [0.9831208763833318, 0.9657406948137841]\n",
      "Iteration 12  h2 new:  [168, 124]  probs:  [0.9962289082248563, 0.9615053621013528]\n",
      "Iteration 13  h1 new:  [4, 57]  probs:  [0.9844376816093502, 0.9542689459762453]\n",
      "Iteration 13  h2 new:  [150, 125]  probs:  [0.9865459634801268, 0.9391288704811772]\n",
      "Iteration 14  h1 new:  [96, 132]  probs:  [0.9882081812096479, 0.9832936901168757]\n",
      "Iteration 14  h2 new:  [96, 114]  probs:  [0.9908311842020964, 0.9726775715222971]\n",
      "Iteration 15  h1 new:  [163, 11]  probs:  [0.9868333374063745, 0.97368802631588]\n",
      "Iteration 15  h2 new:  [17, 120]  probs:  [0.9866506731538234, 0.9712812590386106]\n",
      "Iteration 16  h1 new:  [173, 117]  probs:  [0.9834233702935166, 0.9762249460467329]\n",
      "Iteration 16  h2 new:  [147, 97]  probs:  [0.9838899921339297, 0.9576154992148063]\n",
      "Iteration 17  h1 new:  [84, 146]  probs:  [0.9703313095340017, 0.9715702479961148]\n",
      "Iteration 17  h2 new:  [45, 127]  probs:  [0.9751843604442011, 0.9768031559183443]\n",
      "Iteration 18  h1 new:  [51, 164]  probs:  [0.9800971059995477, 0.9730028013188584]\n",
      "Iteration 18  h2 new:  [7, 158]  probs:  [0.9828476256755792, 0.9814533625372603]\n",
      "Iteration 19  h1 new:  [66, 50]  probs:  [0.9766074559353788, 0.9729351678816365]\n",
      "Iteration 19  h2 new:  [153, 42]  probs:  [0.9728930133380819, 0.9751957957037333]\n",
      "Iteration 20  h1 new:  [116, 101]  probs:  [0.9746034333286764, 0.9773633295342442]\n",
      "Iteration 20  h2 new:  [165, 88]  probs:  [0.9743331714235223, 0.996898447346935]\n",
      "Iteration 21  h1 new:  [106, 28]  probs:  [0.9785097166759604, 0.9812604829684901]\n",
      "Iteration 21  h2 new:  [93, 140]  probs:  [0.9787710088756781, 0.9919529057423631]\n",
      "Iteration 22  h1 new:  [71, 70]  probs:  [0.9802132076110911, 0.97797307143217]\n",
      "Iteration 22  h2 new:  [56, 29]  probs:  [0.9814549336375185, 0.9917959348667306]\n",
      "Iteration 23  h1 new:  [9, 61]  probs:  [0.9843943856489404, 0.978975518247139]\n",
      "Iteration 23  h2 new:  [78, 172]  probs:  [0.9641220818847754, 0.9879494685477406]\n",
      "Iteration 24  h1 new:  [141, 166]  probs:  [0.9813630357336146, 0.9774998595740569]\n",
      "Iteration 24  h2 new:  [41, 87]  probs:  [0.967607431228904, 0.9902816495114279]\n",
      "Iteration 25  h1 new:  [162, 46]  probs:  [0.9866636530252211, 0.9663086056832737]\n",
      "Iteration 25  h2 new:  [8, 46]  probs:  [0.961871634051145, 0.9932960404802598]\n",
      "Iteration 26  h1 new:  [15, 21]  probs:  [0.9826183412038774, 0.9621067426188205]\n",
      "Iteration 26  h2 new:  [59, 38]  probs:  [0.9573126842526173, 0.9904037214941344]\n",
      "Iteration 27  h1 new:  [75, 174]  probs:  [0.9927878406620229, 0.9802274814905563]\n",
      "Iteration 27  h2 new:  [90, 105]  probs:  [0.9697481985671181, 0.990242204481607]\n",
      "Iteration 28  h1 new:  [81, 102]  probs:  [0.9780827405877796, 0.9960326765295417]\n",
      "Iteration 28  h2 new:  [81, 89]  probs:  [0.9514389719942916, 0.9824083607292301]\n",
      "Iteration 29  h1 new:  [99, 167]  probs:  [0.9688767062475948, 0.9966233849124204]\n",
      "Iteration 29  h2 new:  [24, 72]  probs:  [0.9646586796289215, 0.9728990764503732]\n",
      "Iteration 30  h1 new:  [157, 32]  probs:  [0.9671858844572525, 0.9899216868851902]\n",
      "Iteration 30  h2 new:  [18, 85]  probs:  [0.9480832686251898, 0.9896289680443014]\n",
      "Total Labeled number:  126  Still unlabeled number:  50\n",
      "co_SVM f1:                precision    recall  f1-score   support\n",
      "\n",
      "p_robinson_0       0.92      0.92      0.92        24\n",
      "p_robinson_1       0.93      0.93      0.93        27\n",
      "\n",
      " avg / total       0.92      0.92      0.92        51\n",
      "\n",
      "[22  2  2 25]\n",
      "Initial L size:  10\n",
      "Initial U size:  166\n",
      "U size after drawing sample to U prime: 0\n",
      "Initial U prime size:  166\n",
      "P value:  1  N value:  1\n",
      "Iteration 1  h1 new:  [60, 23]  probs:  [0.9192125635381129, 0.8727033461897773]\n",
      "Iteration 1  h2 new:  [83, 47]  probs:  [0.9993691906669423, 0.9575890689750982]\n",
      "Iteration 2  h1 new:  [65, 131]  probs:  [0.9373259123230877, 0.9147739489623885]\n",
      "Iteration 2  h2 new:  [33, 170]  probs:  [0.9617392396048681, 0.9186191502870223]\n",
      "Iteration 3  h1 new:  [73, 110]  probs:  [0.9526919691075049, 0.9351454576432605]\n",
      "Iteration 3  h2 new:  [126, 55]  probs:  [0.944557180620284, 0.9253362508147298]\n",
      "Iteration 4  h1 new:  [31, 6]  probs:  [0.9598888490155906, 0.9426705198111583]\n",
      "Iteration 4  h2 new:  [95, 137]  probs:  [0.9724536491867533, 0.9307015888922332]\n",
      "Iteration 5  h1 new:  [107, 121]  probs:  [0.9611164249705915, 0.9464357131705703]\n",
      "Iteration 5  h2 new:  [1, 112]  probs:  [0.9683141682459492, 0.9471092678411308]\n",
      "Iteration 6  h1 new:  [67, 155]  probs:  [0.9562471919537353, 0.9487744938902497]\n",
      "Iteration 6  h2 new:  [69, 145]  probs:  [0.982089618677308, 0.9392764598854056]\n",
      "Iteration 7  h1 new:  [141, 76]  probs:  [0.9580162223753785, 0.9487389311319424]\n",
      "Iteration 7  h2 new:  [90, 172]  probs:  [0.9834900431500436, 0.9485163208752379]\n",
      "Iteration 8  h1 new:  [123, 61]  probs:  [0.9639350658034573, 0.9529018944083785]\n",
      "Iteration 8  h2 new:  [138, 98]  probs:  [0.9853518355006312, 0.9529047969604486]\n",
      "Iteration 9  h1 new:  [10, 43]  probs:  [0.9706007557405066, 0.9511001379464887]\n",
      "Iteration 9  h2 new:  [3, 85]  probs:  [0.9852440624469014, 0.9690433521971847]\n",
      "Iteration 10  h1 new:  [156, 68]  probs:  [0.9682037933789083, 0.9535587920977365]\n",
      "Iteration 10  h2 new:  [94, 169]  probs:  [0.9940810017326885, 0.9814549953753622]\n",
      "Iteration 11  h1 new:  [64, 132]  probs:  [0.9730835614924738, 0.9585856656019865]\n",
      "Iteration 11  h2 new:  [64, 86]  probs:  [0.9859773602865954, 0.9656271466731543]\n",
      "Iteration 12  h1 new:  [154, 5]  probs:  [0.9753181634395345, 0.9623588552434323]\n",
      "Iteration 12  h2 new:  [13, 52]  probs:  [0.994828370150664, 0.958810980894644]\n",
      "Iteration 13  h1 new:  [160, 117]  probs:  [0.9706774494836561, 0.9687558192957866]\n",
      "Iteration 13  h2 new:  [168, 124]  probs:  [0.9912095529979864, 0.987092211458549]\n",
      "Iteration 14  h1 new:  [96, 62]  probs:  [0.9680740610035153, 0.970657355718031]\n",
      "Iteration 14  h2 new:  [150, 62]  probs:  [0.9689557340593244, 0.978650980228651]\n",
      "Iteration 15  h1 new:  [4, 11]  probs:  [0.9693078450380703, 0.9692600122466042]\n",
      "Iteration 15  h2 new:  [147, 114]  probs:  [0.9782534835394, 0.9787239078634238]\n",
      "Iteration 16  h1 new:  [51, 57]  probs:  [0.9718792145311613, 0.967359703700539]\n",
      "Iteration 16  h2 new:  [163, 158]  probs:  [0.9692190604845425, 0.9592344269203471]\n",
      "Iteration 17  h1 new:  [84, 50]  probs:  [0.9649270634736204, 0.9655580406834456]\n",
      "Iteration 17  h2 new:  [17, 97]  probs:  [0.9728175379270139, 0.9812006938510714]\n",
      "Iteration 18  h1 new:  [165, 104]  probs:  [0.9634461566868997, 0.9676609764342007]\n",
      "Iteration 18  h2 new:  [93, 89]  probs:  [0.9891141001574332, 0.9834685994663354]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19  h1 new:  [7, 70]  probs:  [0.9667169509534037, 0.9685007922449073]\n",
      "Iteration 19  h2 new:  [56, 29]  probs:  [0.9858830510598954, 0.9741745354948415]\n",
      "Iteration 20  h1 new:  [66, 164]  probs:  [0.9678701205236095, 0.9629700161648599]\n",
      "Iteration 20  h2 new:  [157, 125]  probs:  [0.9763200385723557, 0.9806776151395589]\n",
      "Iteration 21  h1 new:  [71, 42]  probs:  [0.9740255683226697, 0.9708711997879846]\n",
      "Iteration 21  h2 new:  [153, 127]  probs:  [0.9537690862542056, 0.9999844319954085]\n",
      "Iteration 22  h1 new:  [173, 146]  probs:  [0.9726321484517968, 0.9638244036116365]\n",
      "Iteration 22  h2 new:  [45, 88]  probs:  [0.9670910176719876, 0.9966560773235099]\n",
      "Iteration 23  h1 new:  [9, 140]  probs:  [0.9673316532645031, 0.9667571574075667]\n",
      "Iteration 23  h2 new:  [78, 120]  probs:  [0.9615473458481442, 0.9963563867514603]\n",
      "Iteration 24  h1 new:  [116, 166]  probs:  [0.9697141022480401, 0.9676466755210902]\n",
      "Iteration 24  h2 new:  [106, 46]  probs:  [0.9685375992332959, 0.9897724023667056]\n",
      "Iteration 25  h1 new:  [162, 101]  probs:  [0.9643427842747655, 0.964271781454485]\n",
      "Iteration 25  h2 new:  [8, 28]  probs:  [0.9668864855641408, 0.9853292786469992]\n",
      "Iteration 26  h1 new:  [75, 148]  probs:  [0.9661221406810714, 0.9608842663181947]\n",
      "Iteration 26  h2 new:  [41, 87]  probs:  [0.9634308560907795, 0.9937347748757831]\n",
      "Iteration 27  h1 new:  [24, 151]  probs:  [0.9606343084613768, 0.9577663226399882]\n",
      "Iteration 27  h2 new:  [81, 38]  probs:  [0.9427699510492706, 0.9878923908763496]\n",
      "Iteration 28  h1 new:  [82, 174]  probs:  [0.9450928869021803, 0.9588172916614975]\n",
      "Iteration 28  h2 new:  [18, 79]  probs:  [0.9381042339410997, 0.9942157685586885]\n",
      "Iteration 29  h1 new:  [122, 21]  probs:  [0.9463945127394572, 0.952208077101322]\n",
      "Iteration 29  h2 new:  [59, 80]  probs:  [0.9658367267473584, 0.9911267303025516]\n",
      "Iteration 30  h1 new:  [22, 74]  probs:  [0.9569015932209778, 0.9562960405113934]\n",
      "Iteration 30  h2 new:  [100, 105]  probs:  [0.9499713228187209, 0.9944756874820192]\n",
      "Total Labeled number:  128  Still unlabeled number:  48\n",
      "co_LR_SVM f1:                precision    recall  f1-score   support\n",
      "\n",
      "p_robinson_0       0.96      0.92      0.94        24\n",
      "p_robinson_1       0.93      0.96      0.95        27\n",
      "\n",
      " avg / total       0.94      0.94      0.94        51\n",
      "\n",
      "[22  2  1 26]\n",
      "Initial L size:  10\n",
      "Initial U size:  166\n",
      "U size after drawing sample to U prime: 0\n",
      "Initial U prime size:  166\n",
      "[('p_robinson_1', 1.0), ('p_robinson_0', 1.0)]\n",
      "Iteration 1  h1 new:  [3, 110]  score:  [1.3657318937018008, 1.5060331135090488]\n",
      "Iteration 1  h2 new:  [83, 145]  score:  [1.239345578075401, 1.5799593191619827]\n",
      "Iteration 2  h1 new:  [95, 170]  score:  [2.3128347502781748, 2.452959534664476]\n",
      "Iteration 2  h2 new:  [126, 55]  score:  [2.5263837053256415, 2.549740389274487]\n",
      "Iteration 3  h1 new:  [73, 131]  score:  [3.5869669596725275, 3.3221614441567526]\n",
      "Iteration 3  h2 new:  [33, 47]  score:  [3.8203876737549236, 3.6876415125796727]\n",
      "Iteration 4  h1 new:  [65, 23]  score:  [4.513402793372313, 4.460631290952992]\n",
      "Iteration 4  h2 new:  [69, 5]  score:  [4.770957627635644, 4.616949925301623]\n",
      "Iteration 5  h1 new:  [31, 137]  score:  [5.435129704736073, 5.406697610833676]\n",
      "Iteration 5  h2 new:  [60, 86]  score:  [5.72601216764148, 5.545190487519775]\n",
      "Iteration 6  h1 new:  [107, 6]  score:  [6.430378422549026, 6.327144483711556]\n",
      "Iteration 6  h2 new:  [17, 121]  score:  [6.697592535112658, 6.495973788655747]\n",
      "Iteration 7  h1 new:  [56, 61]  score:  [7.366029172613878, 7.145473437663904]\n",
      "Iteration 7  h2 new:  [90, 62]  score:  [7.667040099239919, 7.457819210611825]\n",
      "Iteration 8  h1 new:  [141, 155]  score:  [8.28055042918495, 8.03926605742908]\n",
      "Iteration 8  h2 new:  [93, 114]  score:  [8.625523682277134, 8.429456314518418]\n",
      "Iteration 9  h1 new:  [10, 132]  score:  [8.822344477945258, 8.995362092290172]\n",
      "Iteration 9  h2 new:  [67, 158]  score:  [9.510146659253051, 9.390407093031444]\n",
      "Iteration 10  h1 new:  [123, 76]  score:  [10.195394778134702, 9.955796698084313]\n",
      "Iteration 10  h2 new:  [1, 97]  score:  [10.456365214074845, 10.325763403517382]\n",
      "Iteration 11  h1 new:  [156, 117]  score:  [10.828238808317778, 10.921753460158198]\n",
      "Iteration 11  h2 new:  [94, 98]  score:  [11.339963980398664, 11.292530709205254]\n",
      "Iteration 12  h1 new:  [165, 68]  score:  [11.792302395167368, 11.856514296755295]\n",
      "Iteration 12  h2 new:  [165, 169]  score:  [12.309253416692222, 12.251824308313683]\n",
      "Iteration 13  h1 new:  [64, 104]  score:  [12.740692359868405, 12.758042037852277]\n",
      "Iteration 13  h2 new:  [168, 124]  score:  [13.11621250781032, 13.19478687533725]\n",
      "Iteration 14  h1 new:  [154, 57]  score:  [13.708979998092431, 13.70145949184725]\n",
      "Iteration 14  h2 new:  [13, 29]  score:  [14.084020303638475, 14.144432628012225]\n",
      "Iteration 15  h1 new:  [18, 11]  score:  [14.470932791497429, 14.484673789052938]\n",
      "Iteration 15  h2 new:  [138, 172]  score:  [15.051838697506147, 15.084860067738612]\n",
      "Iteration 16  h1 new:  [116, 43]  score:  [15.408850593155059, 15.362395723641267]\n",
      "Iteration 16  h2 new:  [8, 112]  score:  [15.917767821305047, 16.019612483786087]\n",
      "Iteration 17  h1 new:  [71, 50]  score:  [16.31076093028367, 16.258762981291053]\n",
      "Iteration 17  h2 new:  [71, 127]  score:  [16.88077332663015, 16.744979169107534]\n",
      "Iteration 18  h1 new:  [160, 70]  score:  [17.266760688778415, 17.069856353062903]\n",
      "Iteration 18  h2 new:  [157, 140]  score:  [17.841464190414015, 17.724142601927486]\n",
      "Iteration 19  h1 new:  [163, 101]  score:  [18.218539005256773, 17.92817264360843]\n",
      "Iteration 19  h2 new:  [150, 28]  score:  [18.502110976189847, 18.533921089624027]\n",
      "Iteration 20  h1 new:  [96, 42]  score:  [19.171569505055672, 18.846546914955077]\n",
      "Iteration 20  h2 new:  [41, 52]  score:  [19.087048760190303, 19.457996096603708]\n",
      "Iteration 21  h1 new:  [173, 148]  score:  [20.078642433396706, 19.622980064610648]\n",
      "Iteration 21  h2 new:  [81, 125]  score:  [19.801695077725892, 20.422974176188568]\n",
      "Iteration 22  h1 new:  [82, 164]  score:  [20.990470149725066, 20.52150888525585]\n",
      "Iteration 22  h2 new:  [147, 85]  score:  [20.717283253902718, 21.21146327271403]\n",
      "Iteration 23  h1 new:  [84, 146]  score:  [21.691737734576414, 21.485934683835914]\n",
      "Iteration 23  h2 new:  [45, 89]  score:  [21.572833888313365, 22.119051566259213]\n",
      "Iteration 24  h1 new:  [4, 166]  score:  [22.4464376059955, 22.317471155067917]\n",
      "Iteration 24  h2 new:  [7, 88]  score:  [21.99794742937928, 23.049770258871895]\n",
      "Iteration 25  h1 new:  [51, 115]  score:  [23.345234328593033, 23.24660044638731]\n",
      "Iteration 25  h2 new:  [24, 32]  score:  [22.930038841764933, 24.01614031855612]\n",
      "Iteration 26  h1 new:  [75, 46]  score:  [23.88591528360143, 23.89375997262061]\n",
      "Iteration 26  h2 new:  [106, 105]  score:  [23.66425509269792, 24.764478221901665]\n",
      "Iteration 27  h1 new:  [66, 74]  score:  [24.83945099454555, 24.85792770673665]\n",
      "Iteration 27  h2 new:  [153, 120]  score:  [24.649125311452444, 25.481340244185116]\n",
      "Iteration 28  h1 new:  [80, 39]  score:  [25.425790274175263, 25.655872409155894]\n",
      "Iteration 28  h2 new:  [25, 40]  score:  [24.255355012106456, 26.413835991441584]\n",
      "Iteration 29  h1 new:  [118, 77]  score:  [26.26851425194728, 26.473570575522285]\n",
      "Iteration 29  h2 new:  [100, 77]  score:  [24.968775850608537, 27.341978705432588]\n",
      "Iteration 30  h1 new:  [134, 102]  score:  [27.091713301922873, 27.386398736783775]\n",
      "Iteration 30  h2 new:  [92, 2]  score:  [25.567856643175872, 28.077421083174666]\n",
      "Total Labeled number:  127  Still unlabeled number:  49\n",
      "Improved_co_LR f1:                precision    recall  f1-score   support\n",
      "\n",
      "p_robinson_0       0.92      0.92      0.92        24\n",
      "p_robinson_1       0.93      0.93      0.93        27\n",
      "\n",
      " avg / total       0.92      0.92      0.92        51\n",
      "\n",
      "[22  2  2 25]\n",
      "LR f1:                precision    recall  f1-score   support\n",
      "\n",
      "p_robinson_0       0.96      0.92      0.94        24\n",
      "p_robinson_1       0.93      0.96      0.95        27\n",
      "\n",
      " avg / total       0.94      0.94      0.94        51\n",
      "\n",
      "[22  2  1 26]\n",
      "SVM f1:                precision    recall  f1-score   support\n",
      "\n",
      "p_robinson_0       0.96      0.92      0.94        24\n",
      "p_robinson_1       0.93      0.96      0.95        27\n",
      "\n",
      " avg / total       0.94      0.94      0.94        51\n",
      "\n",
      "[22  2  1 26]\n",
      "[('p_robinson_0', 0.47029702970297027), ('p_robinson_1', 0.5297029702970297)]\n",
      "[('p_robinson_0', 5, 12), ('p_robinson_1', 5, 13)]\n",
      "Initial L size:  10\n",
      "Initial U size:  167\n",
      "U size after drawing sample to U prime: 0\n",
      "Initial U prime size:  167\n",
      "P value:  1  N value:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1  h1 new:  [74, 159]  probs:  [0.8972854165118189, 0.8564149322890179]\n",
      "Iteration 1  h2 new:  [85, 108]  probs:  [0.9673141476097851, 0.9797632920040386]\n",
      "Iteration 2  h1 new:  [157, 172]  probs:  [0.9313892632636801, 0.918538040335469]\n",
      "Iteration 2  h2 new:  [170, 50]  probs:  [0.964759517012996, 0.978713711386099]\n",
      "Iteration 3  h1 new:  [65, 107]  probs:  [0.9220993602816795, 0.9446752261634448]\n",
      "Iteration 3  h2 new:  [19, 5]  probs:  [0.9453896191940361, 0.9773864268155726]\n",
      "Iteration 4  h1 new:  [117, 138]  probs:  [0.9349564194709546, 0.9507456933425521]\n",
      "Iteration 4  h2 new:  [64, 121]  probs:  [0.9418937173497611, 0.977784231825222]\n",
      "Iteration 5  h1 new:  [101, 114]  probs:  [0.9579506230109665, 0.957747464044115]\n",
      "Iteration 5  h2 new:  [113, 14]  probs:  [0.959049787944254, 0.9790608115059586]\n",
      "Iteration 6  h1 new:  [33, 25]  probs:  [0.9657085171625328, 0.9652274288172921]\n",
      "Iteration 6  h2 new:  [39, 62]  probs:  [0.9700893833621164, 0.9832641371265829]\n",
      "Iteration 7  h1 new:  [10, 67]  probs:  [0.952074499726607, 0.9640586617758438]\n",
      "Iteration 7  h2 new:  [17, 103]  probs:  [0.9724569436462946, 0.9836883605552733]\n",
      "Iteration 8  h1 new:  [68, 6]  probs:  [0.9565548096184047, 0.9712185777072796]\n",
      "Iteration 8  h2 new:  [35, 98]  probs:  [0.9706389506208538, 0.9839550916768338]\n",
      "Iteration 9  h1 new:  [66, 30]  probs:  [0.9615292676404209, 0.9715498714434319]\n",
      "Iteration 9  h2 new:  [56, 171]  probs:  [0.9724946252978365, 0.9846272909700136]\n",
      "Iteration 10  h1 new:  [73, 46]  probs:  [0.957671266656175, 0.9711195614655586]\n",
      "Iteration 10  h2 new:  [129, 122]  probs:  [0.973273565149239, 0.9846926816641944]\n",
      "Iteration 11  h1 new:  [84, 104]  probs:  [0.9536735888973847, 0.971990329737493]\n",
      "Iteration 11  h2 new:  [60, 21]  probs:  [0.9758839631862127, 0.9837646776120875]\n",
      "Iteration 12  h1 new:  [167, 147]  probs:  [0.9592067714469361, 0.9742016177265141]\n",
      "Iteration 12  h2 new:  [143, 29]  probs:  [0.9751852717628084, 0.9831460572872618]\n",
      "Iteration 13  h1 new:  [24, 100]  probs:  [0.9587642894161046, 0.9735114347506055]\n",
      "Iteration 13  h2 new:  [71, 142]  probs:  [0.9729699097490858, 0.978440998024709]\n",
      "Iteration 14  h1 new:  [160, 120]  probs:  [0.9607335668963017, 0.9733302184283582]\n",
      "Iteration 14  h2 new:  [124, 96]  probs:  [0.9729529213754714, 0.9783874295856307]\n",
      "Iteration 15  h1 new:  [165, 11]  probs:  [0.9590993008554113, 0.9711353180605585]\n",
      "Iteration 15  h2 new:  [1, 176]  probs:  [0.9743929181189052, 0.9755951826030863]\n",
      "Iteration 16  h1 new:  [164, 76]  probs:  [0.9606577098183344, 0.9729803935246353]\n",
      "Iteration 16  h2 new:  [8, 162]  probs:  [0.9758082183755645, 0.9789426203986134]\n",
      "Iteration 17  h1 new:  [7, 61]  probs:  [0.9594333845411327, 0.9749881961437961]\n",
      "Iteration 17  h2 new:  [110, 158]  probs:  [0.9655783067547975, 0.9774636312459563]\n",
      "Iteration 18  h1 new:  [4, 52]  probs:  [0.9568222584041746, 0.9707930826025937]\n",
      "Iteration 18  h2 new:  [130, 150]  probs:  [0.9689541380126667, 0.975041670398654]\n",
      "Iteration 19  h1 new:  [3, 153]  probs:  [0.9648290975157944, 0.9717969462807987]\n",
      "Iteration 19  h2 new:  [140, 93]  probs:  [0.967826735311932, 0.9765665920189779]\n",
      "Iteration 20  h1 new:  [92, 148]  probs:  [0.9740930494511828, 0.9696615035478733]\n",
      "Iteration 20  h2 new:  [70, 112]  probs:  [0.9571511160227109, 0.9713373191137691]\n",
      "Iteration 21  h1 new:  [152, 53]  probs:  [0.9772470936806295, 0.9754890523852509]\n",
      "Iteration 21  h2 new:  [151, 31]  probs:  [0.9606480717606789, 0.9720738090686284]\n",
      "Iteration 22  h1 new:  [166, 75]  probs:  [0.9758838227761525, 0.9716902904645048]\n",
      "Iteration 22  h2 new:  [91, 49]  probs:  [0.9613011562792312, 0.9716525444842303]\n",
      "Iteration 23  h1 new:  [9, 146]  probs:  [0.980037768900486, 0.9726124971542603]\n",
      "Iteration 23  h2 new:  [89, 174]  probs:  [0.9584604980404259, 0.9742819095348036]\n",
      "Iteration 24  h1 new:  [87, 69]  probs:  [0.9805573771565728, 0.979678545578511]\n",
      "Iteration 24  h2 new:  [119, 105]  probs:  [0.9634264424086162, 0.977366145994636]\n",
      "Iteration 25  h1 new:  [154, 55]  probs:  [0.9823361488002366, 0.9786752850743999]\n",
      "Iteration 25  h2 new:  [48, 55]  probs:  [0.9643583954794153, 0.9816231150840011]\n",
      "Iteration 26  h1 new:  [126, 155]  probs:  [0.9757789994720719, 0.9734592092275816]\n",
      "Iteration 26  h2 new:  [139, 125]  probs:  [0.9684263666187463, 0.9702806278539808]\n",
      "Iteration 27  h1 new:  [99, 77]  probs:  [0.9727133647111076, 0.9732383331578127]\n",
      "Iteration 27  h2 new:  [45, 97]  probs:  [0.9572344364323502, 0.9715779818284143]\n",
      "Iteration 28  h1 new:  [94, 168]  probs:  [0.9660063548930982, 0.9711453700291163]\n",
      "Iteration 28  h2 new:  [82, 95]  probs:  [0.9424644636725764, 0.971085967607678]\n",
      "Iteration 29  h1 new:  [116, 57]  probs:  [0.9662699432963116, 0.9755761473369127]\n",
      "Iteration 29  h2 new:  [90, 131]  probs:  [0.9544828907896474, 0.9714994538596914]\n",
      "Iteration 30  h1 new:  [88, 175]  probs:  [0.9469542994180895, 0.955470534416005]\n",
      "Iteration 30  h2 new:  [26, 72]  probs:  [0.9445415860788843, 0.9677760148826221]\n",
      "Total Labeled number:  129  Still unlabeled number:  48\n",
      "co_LR f1:                precision    recall  f1-score   support\n",
      "\n",
      "p_robinson_0       0.96      1.00      0.98        24\n",
      "p_robinson_1       1.00      0.96      0.98        26\n",
      "\n",
      " avg / total       0.98      0.98      0.98        50\n",
      "\n",
      "[24  0  1 25]\n",
      "Initial L size:  10\n",
      "Initial U size:  167\n",
      "U size after drawing sample to U prime: 0\n",
      "Initial U prime size:  167\n",
      "P value:  1  N value:  1\n",
      "Iteration 1  h1 new:  [3, 148]  probs:  [0.5125368284851322, 0.5126944869006982]\n",
      "Iteration 1  h2 new:  [85, 122]  probs:  [0.9979760211174458, 0.9703349537778052]\n",
      "Iteration 2  h1 new:  [74, 61]  probs:  [0.8641054450951807, 0.8827209583888448]\n",
      "Iteration 2  h2 new:  [89, 171]  probs:  [0.9657764814979839, 0.9407244691182443]\n",
      "Iteration 3  h1 new:  [101, 25]  probs:  [0.9483152173922856, 0.9437026416032167]\n",
      "Iteration 3  h2 new:  [170, 50]  probs:  [0.9886329856996533, 0.9510666097058084]\n",
      "Iteration 4  h1 new:  [157, 138]  probs:  [0.9496168083433234, 0.9609964910040539]\n",
      "Iteration 4  h2 new:  [19, 108]  probs:  [0.9787259605608697, 0.9944860956748887]\n",
      "Iteration 5  h1 new:  [68, 159]  probs:  [0.9665055009679079, 0.966146274178055]\n",
      "Iteration 5  h2 new:  [39, 5]  probs:  [0.9666054599365754, 0.9947683951641906]\n",
      "Iteration 6  h1 new:  [117, 114]  probs:  [0.9724112355417421, 0.9763030326106749]\n",
      "Iteration 6  h2 new:  [113, 14]  probs:  [0.9725165674494197, 0.9950706066205653]\n",
      "Iteration 7  h1 new:  [66, 67]  probs:  [0.9801706996541316, 0.9718338744362831]\n",
      "Iteration 7  h2 new:  [64, 107]  probs:  [0.9646565541243435, 0.9954481878551699]\n",
      "Iteration 8  h1 new:  [35, 172]  probs:  [0.9613022481498247, 0.9596073245475452]\n",
      "Iteration 8  h2 new:  [17, 121]  probs:  [0.9597020344335724, 0.9952775100280932]\n",
      "Iteration 9  h1 new:  [87, 162]  probs:  [0.9646257805200328, 0.9738894452637166]\n",
      "Iteration 9  h2 new:  [1, 62]  probs:  [0.9540750022170497, 0.9955420183047367]\n",
      "Iteration 10  h1 new:  [33, 6]  probs:  [0.9684558179014187, 0.9697149990470273]\n",
      "Iteration 10  h2 new:  [56, 103]  probs:  [0.962718886894155, 0.9957683145871739]\n",
      "Iteration 11  h1 new:  [129, 38]  probs:  [0.9673218395217219, 0.974602681916093]\n",
      "Iteration 11  h2 new:  [143, 30]  probs:  [0.9613195148321481, 0.9957361775494312]\n",
      "Iteration 12  h1 new:  [92, 69]  probs:  [0.9754887165626606, 0.9771288137866955]\n",
      "Iteration 12  h2 new:  [124, 176]  probs:  [0.9798843306666041, 0.9943667762398901]\n",
      "Iteration 13  h1 new:  [45, 46]  probs:  [0.9709050091084948, 0.98359617832505]\n",
      "Iteration 13  h2 new:  [8, 96]  probs:  [0.974617950360676, 0.9972618156246649]\n",
      "Iteration 14  h1 new:  [60, 120]  probs:  [0.9684438735509784, 0.9837465132615978]\n",
      "Iteration 14  h2 new:  [60, 98]  probs:  [0.9740827923128813, 0.9999853906019003]\n",
      "Iteration 15  h1 new:  [165, 115]  probs:  [0.9626321111385152, 0.9839907868100326]\n",
      "Iteration 15  h2 new:  [140, 21]  probs:  [0.9636118701325719, 0.9968342251653918]\n",
      "Iteration 16  h1 new:  [84, 147]  probs:  [0.9850097315798223, 0.9935122861651655]\n",
      "Iteration 16  h2 new:  [24, 147]  probs:  [0.9662243582562753, 0.9999869276226079]\n",
      "Iteration 17  h1 new:  [73, 153]  probs:  [0.9816957677174871, 0.9792081347318703]\n",
      "Iteration 17  h2 new:  [110, 142]  probs:  [0.9662241324523723, 0.9952508749751056]\n",
      "Iteration 18  h1 new:  [7, 53]  probs:  [0.9779657247285104, 0.9808757725269408]\n",
      "Iteration 18  h2 new:  [71, 104]  probs:  [0.9641059763335268, 0.9941092152247325]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19  h1 new:  [154, 168]  probs:  [0.9765117607744552, 0.9830653387854241]\n",
      "Iteration 19  h2 new:  [78, 93]  probs:  [0.954190107564034, 0.9945720216376325]\n",
      "Iteration 20  h1 new:  [10, 52]  probs:  [0.986934489984115, 0.9843854745093024]\n",
      "Iteration 20  h2 new:  [70, 174]  probs:  [0.9681970204302266, 0.994391731173837]\n",
      "Iteration 21  h1 new:  [65, 11]  probs:  [0.9891671493686173, 0.9891712641913988]\n",
      "Iteration 21  h2 new:  [91, 29]  probs:  [0.9626212128471751, 0.9959159513335837]\n",
      "Iteration 22  h1 new:  [160, 55]  probs:  [0.9884093953516183, 0.9908047437520621]\n",
      "Iteration 22  h2 new:  [130, 150]  probs:  [0.96759894662844, 0.9933184656825578]\n",
      "Iteration 23  h1 new:  [9, 12]  probs:  [0.9897630723440138, 0.9905435034810194]\n",
      "Iteration 23  h2 new:  [111, 158]  probs:  [0.967650948943992, 0.995678105016545]\n",
      "Iteration 24  h1 new:  [164, 77]  probs:  [0.9750034341726088, 0.9790110814261109]\n",
      "Iteration 24  h2 new:  [151, 76]  probs:  [0.9629616943362291, 0.9897789785356216]\n",
      "Iteration 25  h1 new:  [167, 155]  probs:  [0.9821299535393344, 0.9844366543741966]\n",
      "Iteration 25  h2 new:  [82, 112]  probs:  [0.9468446230830385, 0.9931228209592492]\n",
      "Iteration 26  h1 new:  [126, 2]  probs:  [0.9917747391764778, 0.99127083932607]\n",
      "Iteration 26  h2 new:  [99, 2]  probs:  [0.952311158858567, 0.991247270687591]\n",
      "Iteration 27  h1 new:  [4, 75]  probs:  [0.9798866837505773, 0.9902479068338222]\n",
      "Iteration 27  h2 new:  [119, 105]  probs:  [0.9488990735777932, 0.9941800684083169]\n",
      "Iteration 28  h1 new:  [166, 146]  probs:  [0.9876660236564271, 0.9815317294821161]\n",
      "Iteration 28  h2 new:  [139, 146]  probs:  [0.9207366732047293, 0.9962228523515116]\n",
      "Iteration 29  h1 new:  [152, 44]  probs:  [0.9823165395796363, 0.9841640182803698]\n",
      "Iteration 29  h2 new:  [136, 97]  probs:  [0.9426060140915963, 0.9933788187598704]\n",
      "Iteration 30  h1 new:  [94, 86]  probs:  [0.986233850373541, 0.9947892364185836]\n",
      "Iteration 30  h2 new:  [59, 49]  probs:  [0.9327146773646641, 0.9941212162246296]\n",
      "Total Labeled number:  126  Still unlabeled number:  51\n",
      "co_SVM f1:                precision    recall  f1-score   support\n",
      "\n",
      "p_robinson_0       1.00      1.00      1.00        24\n",
      "p_robinson_1       1.00      1.00      1.00        26\n",
      "\n",
      " avg / total       1.00      1.00      1.00        50\n",
      "\n",
      "[24  0  0 26]\n",
      "Initial L size:  10\n",
      "Initial U size:  167\n",
      "U size after drawing sample to U prime: 0\n",
      "Initial U prime size:  167\n",
      "P value:  1  N value:  1\n",
      "Iteration 1  h1 new:  [74, 159]  probs:  [0.8972854165118189, 0.8564149322890179]\n",
      "Iteration 1  h2 new:  [85, 122]  probs:  [0.9996476098040563, 0.9779377460468005]\n",
      "Iteration 2  h1 new:  [157, 114]  probs:  [0.9114744083915758, 0.9228439336293048]\n",
      "Iteration 2  h2 new:  [170, 171]  probs:  [0.9718146308681354, 0.9109536694214521]\n",
      "Iteration 3  h1 new:  [3, 6]  probs:  [0.9316608407232316, 0.9385097379107881]\n",
      "Iteration 3  h2 new:  [113, 5]  probs:  [0.9513800388283988, 0.9896317127210094]\n",
      "Iteration 4  h1 new:  [101, 25]  probs:  [0.9466556927225043, 0.9348460353774992]\n",
      "Iteration 4  h2 new:  [39, 50]  probs:  [0.9674564464039359, 0.9882577930602152]\n",
      "Iteration 5  h1 new:  [117, 30]  probs:  [0.9410893809325807, 0.9491227850434311]\n",
      "Iteration 5  h2 new:  [19, 107]  probs:  [0.9627259898431632, 0.9921527130873391]\n",
      "Iteration 6  h1 new:  [33, 100]  probs:  [0.9559887293283581, 0.9579824731746838]\n",
      "Iteration 6  h2 new:  [64, 121]  probs:  [0.9659210396886045, 0.9919143817135284]\n",
      "Iteration 7  h1 new:  [68, 67]  probs:  [0.9587458909055241, 0.9600793103859488]\n",
      "Iteration 7  h2 new:  [17, 108]  probs:  [0.9595324789518458, 0.9949241690523225]\n",
      "Iteration 8  h1 new:  [66, 120]  probs:  [0.9558404322760335, 0.9625280446207304]\n",
      "Iteration 8  h2 new:  [143, 176]  probs:  [0.9644273749335637, 0.9946934230673697]\n",
      "Iteration 9  h1 new:  [84, 46]  probs:  [0.9543926749626703, 0.9636661097974402]\n",
      "Iteration 9  h2 new:  [89, 14]  probs:  [0.9647395178952511, 0.9844104390014705]\n",
      "Iteration 10  h1 new:  [10, 138]  probs:  [0.9575592561596621, 0.9747499155518832]\n",
      "Iteration 10  h2 new:  [1, 103]  probs:  [0.9732773579008417, 0.9926261745451936]\n",
      "Iteration 11  h1 new:  [92, 104]  probs:  [0.9619349533739492, 0.9715361418898586]\n",
      "Iteration 11  h2 new:  [124, 96]  probs:  [0.9642335932387923, 0.9928556426155007]\n",
      "Iteration 12  h1 new:  [4, 147]  probs:  [0.9642895864574768, 0.9642211088900743]\n",
      "Iteration 12  h2 new:  [56, 62]  probs:  [0.9653827344987924, 0.9941736783898067]\n",
      "Iteration 13  h1 new:  [152, 76]  probs:  [0.9600622371804819, 0.9635669597223385]\n",
      "Iteration 13  h2 new:  [8, 98]  probs:  [0.9582207228545788, 0.9923312616982617]\n",
      "Iteration 14  h1 new:  [65, 172]  probs:  [0.9608197161664447, 0.9669049965998254]\n",
      "Iteration 14  h2 new:  [60, 21]  probs:  [0.965703327128809, 0.9947098165217584]\n",
      "Iteration 15  h1 new:  [167, 52]  probs:  [0.9672426096722917, 0.9718208998350114]\n",
      "Iteration 15  h2 new:  [45, 29]  probs:  [0.9648582766558, 0.9935976362633545]\n",
      "Iteration 16  h1 new:  [73, 158]  probs:  [0.9667987874392928, 0.9675460162867237]\n",
      "Iteration 16  h2 new:  [78, 142]  probs:  [0.9613616144557336, 0.9941104490538919]\n",
      "Iteration 17  h1 new:  [87, 11]  probs:  [0.9754556225995299, 0.9713365197685181]\n",
      "Iteration 17  h2 new:  [24, 162]  probs:  [0.9658865921875512, 0.9872545465465198]\n",
      "Iteration 18  h1 new:  [165, 153]  probs:  [0.970748801073403, 0.9573470468245802]\n",
      "Iteration 18  h2 new:  [140, 150]  probs:  [0.9656233070616166, 0.9876803400789175]\n",
      "Iteration 19  h1 new:  [7, 125]  probs:  [0.9725111823314567, 0.9569997646276878]\n",
      "Iteration 19  h2 new:  [91, 93]  probs:  [0.9594081924309205, 0.9902475081800199]\n",
      "Iteration 20  h1 new:  [160, 61]  probs:  [0.9758320365575973, 0.9656284911084593]\n",
      "Iteration 20  h2 new:  [111, 112]  probs:  [0.9616327932177862, 0.9859432852316098]\n",
      "Iteration 21  h1 new:  [164, 53]  probs:  [0.9717866369900544, 0.9674974991653711]\n",
      "Iteration 21  h2 new:  [35, 49]  probs:  [0.9627855088951373, 0.9903798658062708]\n",
      "Iteration 22  h1 new:  [166, 55]  probs:  [0.9699703954533474, 0.9681785266610448]\n",
      "Iteration 22  h2 new:  [82, 79]  probs:  [0.9660522633251252, 0.9910206420922462]\n",
      "Iteration 23  h1 new:  [9, 148]  probs:  [0.9681612357854541, 0.9708328521593104]\n",
      "Iteration 23  h2 new:  [99, 131]  probs:  [0.9679038257104239, 0.9788495499683177]\n",
      "Iteration 24  h1 new:  [126, 155]  probs:  [0.96715656418749, 0.9740741866017718]\n",
      "Iteration 24  h2 new:  [71, 31]  probs:  [0.9657011059625155, 0.9860090990382774]\n",
      "Iteration 25  h1 new:  [154, 69]  probs:  [0.973064219097422, 0.9694607781161834]\n",
      "Iteration 25  h2 new:  [130, 174]  probs:  [0.9644107455882462, 0.9857764466800094]\n",
      "Iteration 26  h1 new:  [129, 105]  probs:  [0.9664042010400391, 0.9683463350740209]\n",
      "Iteration 26  h2 new:  [151, 105]  probs:  [0.9736799896832458, 0.9950411642235376]\n",
      "Iteration 27  h1 new:  [116, 146]  probs:  [0.9562463810789339, 0.9707370772564452]\n",
      "Iteration 27  h2 new:  [70, 2]  probs:  [0.9725257027614201, 0.9935955743337122]\n",
      "Iteration 28  h1 new:  [94, 75]  probs:  [0.9561040132442157, 0.9712583461062742]\n",
      "Iteration 28  h2 new:  [119, 95]  probs:  [0.9769968302327693, 0.9973808680141607]\n",
      "Iteration 29  h1 new:  [139, 168]  probs:  [0.9472101280808327, 0.9725559803941107]\n",
      "Iteration 29  h2 new:  [48, 97]  probs:  [0.9891119751263612, 0.9913629478588011]\n",
      "Iteration 30  h1 new:  [110, 57]  probs:  [0.9375094133392157, 0.972097634542526]\n",
      "Iteration 30  h2 new:  [59, 72]  probs:  [0.9752370386171434, 0.979269808304648]\n",
      "Total Labeled number:  129  Still unlabeled number:  48\n",
      "co_LR_SVM f1:                precision    recall  f1-score   support\n",
      "\n",
      "p_robinson_0       0.92      1.00      0.96        24\n",
      "p_robinson_1       1.00      0.92      0.96        26\n",
      "\n",
      " avg / total       0.96      0.96      0.96        50\n",
      "\n",
      "[24  0  2 24]\n",
      "Initial L size:  10\n",
      "Initial U size:  167\n",
      "U size after drawing sample to U prime: 0\n",
      "Initial U prime size:  167\n",
      "[('p_robinson_0', 1.0), ('p_robinson_1', 1.0)]\n",
      "Iteration 1  h1 new:  [65, 176]  score:  [1.6159819946564482, 1.536342909706105]\n",
      "Iteration 1  h2 new:  [130, 108]  score:  [1.726319065049289, 1.8042118788724533]\n",
      "Iteration 2  h1 new:  [3, 67]  score:  [2.6071077914087484, 2.4472300059725876]\n",
      "Iteration 2  h2 new:  [170, 5]  score:  [2.6993362281827182, 2.749792529474801]\n",
      "Iteration 3  h1 new:  [74, 46]  score:  [3.5747433070326378, 3.415192971558882]\n",
      "Iteration 3  h2 new:  [85, 50]  score:  [3.566684769202439, 3.78964964711293]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4  h1 new:  [157, 159]  score:  [4.170996010979457, 4.341638177523115]\n",
      "Iteration 4  h2 new:  [19, 121]  score:  [4.547030965674664, 4.760656632143223]\n",
      "Iteration 5  h1 new:  [152, 107]  score:  [5.324119583585144, 5.314390387511458]\n",
      "Iteration 5  h2 new:  [140, 62]  score:  [5.429518202219884, 5.702307803281332]\n",
      "Iteration 6  h1 new:  [7, 61]  score:  [6.299657440964732, 6.275931262934225]\n",
      "Iteration 6  h2 new:  [1, 103]  score:  [6.348417614777389, 6.677529861124842]\n",
      "Iteration 7  h1 new:  [9, 100]  score:  [7.235814044295772, 7.097269085623694]\n",
      "Iteration 7  h2 new:  [35, 122]  score:  [7.207545403387556, 7.677862014058799]\n",
      "Iteration 8  h1 new:  [4, 172]  score:  [8.126198548807267, 8.12116697744915]\n",
      "Iteration 8  h2 new:  [64, 98]  score:  [8.255516567733354, 8.643841788417955]\n",
      "Iteration 9  h1 new:  [10, 114]  score:  [8.779905261081353, 9.002056746987648]\n",
      "Iteration 9  h2 new:  [39, 104]  score:  [9.194792323066435, 9.605337709196375]\n",
      "Iteration 10  h1 new:  [92, 30]  score:  [9.549944809283271, 9.8366224088255]\n",
      "Iteration 10  h2 new:  [17, 14]  score:  [10.119672021283709, 10.588807221034417]\n",
      "Iteration 11  h1 new:  [167, 138]  score:  [11.06943778507325, 10.933988917948273]\n",
      "Iteration 11  h2 new:  [113, 171]  score:  [11.080158531439464, 11.552442092161009]\n",
      "Iteration 12  h1 new:  [24, 148]  score:  [11.956081182847981, 11.826806951692461]\n",
      "Iteration 12  h2 new:  [60, 25]  score:  [12.044826778118127, 12.506709549992818]\n",
      "Iteration 13  h1 new:  [160, 6]  score:  [12.932284858304131, 12.734630637485871]\n",
      "Iteration 13  h2 new:  [129, 21]  score:  [12.952854564364596, 13.463272971370564]\n",
      "Iteration 14  h1 new:  [166, 147]  score:  [13.86127761755119, 13.697656958936939]\n",
      "Iteration 14  h2 new:  [117, 29]  score:  [13.913101566835666, 14.448999992481454]\n",
      "Iteration 15  h1 new:  [164, 11]  score:  [14.83466593886162, 14.645678442483852]\n",
      "Iteration 15  h2 new:  [56, 158]  score:  [14.871667549038218, 15.195545104382825]\n",
      "Iteration 16  h1 new:  [84, 52]  score:  [15.791505244544965, 15.577382485054317]\n",
      "Iteration 16  h2 new:  [68, 142]  score:  [15.832048947396999, 16.1679144171244]\n",
      "Iteration 17  h1 new:  [154, 155]  score:  [16.627253000177394, 16.528240681908994]\n",
      "Iteration 17  h2 new:  [66, 162]  score:  [16.707586663086687, 17.103207732132734]\n",
      "Iteration 18  h1 new:  [101, 76]  score:  [17.724875331656815, 17.39685895443884]\n",
      "Iteration 18  h2 new:  [33, 96]  score:  [17.63328341013404, 18.0541711990883]\n",
      "Iteration 19  h1 new:  [151, 146]  score:  [18.306423532703285, 18.34791464764692]\n",
      "Iteration 19  h2 new:  [71, 93]  score:  [18.583710207437278, 18.84977149910138]\n",
      "Iteration 20  h1 new:  [99, 75]  score:  [19.266172260088794, 19.265591617529658]\n",
      "Iteration 20  h2 new:  [124, 150]  score:  [19.484692481913115, 19.665901986705453]\n",
      "Iteration 21  h1 new:  [126, 53]  score:  [20.12002670294806, 20.13042696855455]\n",
      "Iteration 21  h2 new:  [73, 120]  score:  [20.35851658679168, 20.58084209409566]\n",
      "Iteration 22  h1 new:  [119, 69]  score:  [20.872705185317468, 21.074377211675802]\n",
      "Iteration 22  h2 new:  [91, 31]  score:  [21.293425693407197, 21.534896628407164]\n",
      "Iteration 23  h1 new:  [48, 105]  score:  [21.716459926122276, 22.00030016632839]\n",
      "Iteration 23  h2 new:  [143, 112]  score:  [22.203634166438583, 22.43204626977318]\n",
      "Iteration 24  h1 new:  [139, 55]  score:  [22.460960925140547, 22.8909610143047]\n",
      "Iteration 24  h2 new:  [89, 49]  score:  [23.125290233723128, 23.378415552739146]\n",
      "Iteration 25  h1 new:  [87, 153]  score:  [23.269593458935628, 23.680957905402927]\n",
      "Iteration 25  h2 new:  [8, 125]  score:  [23.94345757046608, 24.246847317562366]\n",
      "Iteration 26  h1 new:  [88, 168]  score:  [23.631548235216457, 24.456709232755333]\n",
      "Iteration 26  h2 new:  [70, 131]  score:  [24.63877654601282, 25.0716763763918]\n",
      "Iteration 27  h1 new:  [94, 175]  score:  [24.449484844094542, 25.374762512318867]\n",
      "Iteration 27  h2 new:  [45, 174]  score:  [25.288755859724155, 26.04717643577554]\n",
      "Iteration 28  h1 new:  [165, 2]  score:  [25.187078071852568, 26.271719492384992]\n",
      "Iteration 28  h2 new:  [165, 95]  score:  [26.006564686608233, 26.87957967351388]\n",
      "Iteration 29  h1 new:  [90, 57]  score:  [25.748284110829214, 27.250292193576534]\n",
      "Iteration 29  h2 new:  [110, 72]  score:  [26.957670932430844, 27.799201145174468]\n",
      "Iteration 30  h1 new:  [116, 77]  score:  [26.571525064697344, 28.190212477364938]\n",
      "Iteration 30  h2 new:  [82, 97]  score:  [27.61574459394262, 28.19991167090852]\n",
      "Total Labeled number:  129  Still unlabeled number:  48\n",
      "Improved_co_LR f1:                precision    recall  f1-score   support\n",
      "\n",
      "p_robinson_0       0.96      0.96      0.96        24\n",
      "p_robinson_1       0.96      0.96      0.96        26\n",
      "\n",
      " avg / total       0.96      0.96      0.96        50\n",
      "\n",
      "[23  1  1 25]\n",
      "LR f1:                precision    recall  f1-score   support\n",
      "\n",
      "p_robinson_0       1.00      0.88      0.93        24\n",
      "p_robinson_1       0.90      1.00      0.95        26\n",
      "\n",
      " avg / total       0.95      0.94      0.94        50\n",
      "\n",
      "[21  3  0 26]\n",
      "SVM f1:                precision    recall  f1-score   support\n",
      "\n",
      "p_robinson_0       1.00      0.83      0.91        24\n",
      "p_robinson_1       0.87      1.00      0.93        26\n",
      "\n",
      " avg / total       0.93      0.92      0.92        50\n",
      "\n",
      "[20  4  0 26]\n",
      "[('p_robinson_0', 0.4729064039408867), ('p_robinson_1', 0.5270935960591133)]\n",
      "[('p_robinson_0', 5, 12), ('p_robinson_1', 5, 13)]\n",
      "Initial L size:  10\n",
      "Initial U size:  168\n",
      "U size after drawing sample to U prime: 0\n",
      "Initial U prime size:  168\n",
      "P value:  1  N value:  1\n",
      "Iteration 1  h1 new:  [100, 66]  probs:  [0.9254900341089003, 0.9119920562556033]\n",
      "Iteration 1  h2 new:  [157, 49]  probs:  [0.9510841143069046, 0.9625605236362211]\n",
      "Iteration 2  h1 new:  [32, 28]  probs:  [0.9456028937731009, 0.9343553539280568]\n",
      "Iteration 2  h2 new:  [84, 139]  probs:  [0.9703465726222896, 0.9699364678247712]\n",
      "Iteration 3  h1 new:  [141, 23]  probs:  [0.9595761224486052, 0.9352610799279214]\n",
      "Iteration 3  h2 new:  [141, 172]  probs:  [0.9733461595001978, 0.9577408853702017]\n",
      "Iteration 4  h1 new:  [150, 111]  probs:  [0.9477301956414974, 0.9515450915187049]\n",
      "Iteration 4  h2 new:  [135, 27]  probs:  [0.9717515317535123, 0.9552143522646203]\n",
      "Iteration 5  h1 new:  [71, 6]  probs:  [0.9588321485749581, 0.9642863792692775]\n",
      "Iteration 5  h2 new:  [16, 165]  probs:  [0.9808044829691104, 0.9643938666640556]\n",
      "Iteration 6  h1 new:  [125, 103]  probs:  [0.9653008363074966, 0.968211856384076]\n",
      "Iteration 6  h2 new:  [34, 118]  probs:  [0.9842649567627824, 0.9676809208483667]\n",
      "Iteration 7  h1 new:  [102, 127]  probs:  [0.9691819360771219, 0.9702218487182747]\n",
      "Iteration 7  h2 new:  [64, 5]  probs:  [0.9791435025623163, 0.9680767169540899]\n",
      "Iteration 8  h1 new:  [167, 117]  probs:  [0.9743356763293481, 0.9657635659873345]\n",
      "Iteration 8  h2 new:  [69, 113]  probs:  [0.9815312040894545, 0.9675329223944863]\n",
      "Iteration 9  h1 new:  [38, 130]  probs:  [0.9743603507301293, 0.9657985466728514]\n",
      "Iteration 9  h2 new:  [128, 61]  probs:  [0.9828016939532389, 0.96541252575368]\n",
      "Iteration 10  h1 new:  [60, 121]  probs:  [0.9755523820496124, 0.9647152755174978]\n",
      "Iteration 10  h2 new:  [110, 97]  probs:  [0.9838370292593338, 0.963187942659978]\n",
      "Iteration 11  h1 new:  [114, 101]  probs:  [0.977001565776408, 0.9669778050217078]\n",
      "Iteration 11  h2 new:  [133, 12]  probs:  [0.9828755320548745, 0.9687429865684338]\n",
      "Iteration 12  h1 new:  [160, 123]  probs:  [0.9789614016258498, 0.9677482223718671]\n",
      "Iteration 12  h2 new:  [170, 129]  probs:  [0.9823691869784266, 0.9692291259853014]\n",
      "Iteration 13  h1 new:  [159, 11]  probs:  [0.980702940997279, 0.9714551686228063]\n",
      "Iteration 13  h2 new:  [56, 108]  probs:  [0.9829902431623415, 0.9682975637575532]\n",
      "Iteration 14  h1 new:  [119, 147]  probs:  [0.9675899392795061, 0.9607194811162942]\n",
      "Iteration 14  h2 new:  [120, 18]  probs:  [0.9794806378533287, 0.970213793168418]\n",
      "Iteration 15  h1 new:  [68, 45]  probs:  [0.9604567053570798, 0.9661400825197135]\n",
      "Iteration 15  h2 new:  [134, 171]  probs:  [0.9778816999496788, 0.9733464111083009]\n",
      "Iteration 16  h1 new:  [10, 175]  probs:  [0.9680816868203538, 0.9656940740803615]\n",
      "Iteration 16  h2 new:  [50, 168]  probs:  [0.9785521420510291, 0.9714936947856161]\n",
      "Iteration 17  h1 new:  [88, 48]  probs:  [0.9576529595984135, 0.967805663805517]\n",
      "Iteration 17  h2 new:  [104, 164]  probs:  [0.9771909766265179, 0.9720420166780154]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18  h1 new:  [13, 161]  probs:  [0.966088795618813, 0.9710039087274199]\n",
      "Iteration 18  h2 new:  [8, 46]  probs:  [0.9788741338686829, 0.9732511993570898]\n",
      "Iteration 19  h1 new:  [15, 53]  probs:  [0.959882641765399, 0.9627092108294587]\n",
      "Iteration 19  h2 new:  [21, 53]  probs:  [0.9787546501580541, 0.9736211572731769]\n",
      "Iteration 20  h1 new:  [3, 29]  probs:  [0.9610332391729023, 0.9631102996746752]\n",
      "Iteration 20  h2 new:  [1, 126]  probs:  [0.9716193607792774, 0.9735555103077654]\n",
      "Iteration 21  h1 new:  [63, 138]  probs:  [0.9652764691960958, 0.9645818609478111]\n",
      "Iteration 21  h2 new:  [63, 138]  probs:  [0.9750469126244489, 0.972607635685835]\n",
      "Iteration 22  h1 new:  [4, 99]  probs:  [0.964606976712148, 0.96450224246244]\n",
      "Iteration 22  h2 new:  [81, 78]  probs:  [0.9685161885893656, 0.9577230204455087]\n",
      "Iteration 23  h1 new:  [137, 57]  probs:  [0.9668331515870151, 0.94558332037229]\n",
      "Iteration 23  h2 new:  [44, 95]  probs:  [0.963679187364289, 0.9563282408851099]\n",
      "Iteration 24  h1 new:  [24, 142]  probs:  [0.9662408764709121, 0.9509968481972839]\n",
      "Iteration 24  h2 new:  [90, 70]  probs:  [0.9636709808614295, 0.9564850278027547]\n",
      "Iteration 25  h1 new:  [80, 51]  probs:  [0.962576985203753, 0.9547093798039512]\n",
      "Iteration 25  h2 new:  [116, 30]  probs:  [0.9561882705313334, 0.9602875451882981]\n",
      "Iteration 26  h1 new:  [52, 19]  probs:  [0.9627420292082618, 0.9485206306897733]\n",
      "Iteration 26  h2 new:  [7, 92]  probs:  [0.9594175975596155, 0.955695590682194]\n",
      "Iteration 27  h1 new:  [98, 155]  probs:  [0.9650264239075285, 0.9412310267224827]\n",
      "Iteration 27  h2 new:  [140, 153]  probs:  [0.9601974843123058, 0.9573010145695064]\n",
      "Iteration 28  h1 new:  [83, 20]  probs:  [0.9633197083361402, 0.9415572596267919]\n",
      "Iteration 28  h2 new:  [83, 58]  probs:  [0.9690252189777377, 0.9520380181757917]\n",
      "Iteration 29  h1 new:  [149, 73]  probs:  [0.9634535471927023, 0.9441024010260956]\n",
      "Iteration 29  h2 new:  [25, 42]  probs:  [0.9626864825647841, 0.9498580492925838]\n",
      "Iteration 30  h1 new:  [65, 156]  probs:  [0.9637018527795587, 0.949027633677168]\n",
      "Iteration 30  h2 new:  [173, 94]  probs:  [0.9199864201038588, 0.9489887130802969]\n",
      "Total Labeled number:  125  Still unlabeled number:  53\n",
      "co_LR f1:                precision    recall  f1-score   support\n",
      "\n",
      "p_robinson_0       0.96      1.00      0.98        23\n",
      "p_robinson_1       1.00      0.96      0.98        26\n",
      "\n",
      " avg / total       0.98      0.98      0.98        49\n",
      "\n",
      "[23  0  1 25]\n",
      "Initial L size:  10\n",
      "Initial U size:  168\n",
      "U size after drawing sample to U prime: 0\n",
      "Initial U prime size:  168\n",
      "P value:  1  N value:  1\n",
      "Iteration 1  h1 new:  [141, 66]  probs:  [0.9972049998992126, 0.9864058897329466]\n",
      "Iteration 1  h2 new:  [157, 49]  probs:  [0.898354002008849, 0.965805262214142]\n",
      "Iteration 2  h1 new:  [32, 28]  probs:  [0.9866645753053211, 0.972174530265776]\n",
      "Iteration 2  h2 new:  [84, 139]  probs:  [0.9145984689032757, 0.9570084586739024]\n",
      "Iteration 3  h1 new:  [100, 23]  probs:  [0.9789565074525753, 0.9595435455900817]\n",
      "Iteration 3  h2 new:  [21, 172]  probs:  [0.8996480564229707, 0.948735780780192]\n",
      "Iteration 4  h1 new:  [38, 111]  probs:  [0.9818165415036418, 0.9503736776542828]\n",
      "Iteration 4  h2 new:  [34, 5]  probs:  [0.9592006420391461, 0.9704385729336515]\n",
      "Iteration 5  h1 new:  [125, 127]  probs:  [0.9831034854541444, 0.9504278907111926]\n",
      "Iteration 5  h2 new:  [64, 113]  probs:  [0.955367428298385, 0.9670597458113983]\n",
      "Iteration 6  h1 new:  [167, 101]  probs:  [0.9739107519008267, 0.9657469556180753]\n",
      "Iteration 6  h2 new:  [135, 12]  probs:  [0.9593209617502403, 0.969409488739789]\n",
      "Iteration 7  h1 new:  [159, 103]  probs:  [0.9801108222206298, 0.9700198174743145]\n",
      "Iteration 7  h2 new:  [16, 165]  probs:  [0.9715089622430578, 0.9820309122191048]\n",
      "Iteration 8  h1 new:  [60, 48]  probs:  [0.9852915596910559, 0.9747358936176015]\n",
      "Iteration 8  h2 new:  [110, 130]  probs:  [0.9601968838032626, 0.9834195229489335]\n",
      "Iteration 9  h1 new:  [160, 117]  probs:  [0.9897428226266154, 0.9643348795243203]\n",
      "Iteration 9  h2 new:  [104, 27]  probs:  [0.9553154142255381, 0.9911189833306903]\n",
      "Iteration 10  h1 new:  [13, 45]  probs:  [0.9912294960015515, 0.9636589776542852]\n",
      "Iteration 10  h2 new:  [114, 129]  probs:  [0.9572588935641497, 0.9868608996376772]\n",
      "Iteration 11  h1 new:  [102, 123]  probs:  [0.98530597810057, 0.9659813901929433]\n",
      "Iteration 11  h2 new:  [56, 108]  probs:  [0.9617876773562265, 0.9869858882124198]\n",
      "Iteration 12  h1 new:  [71, 121]  probs:  [0.9877623867770123, 0.9685234474442705]\n",
      "Iteration 12  h2 new:  [69, 61]  probs:  [0.95658261721711, 0.9887092867948556]\n",
      "Iteration 13  h1 new:  [81, 118]  probs:  [0.9843032855223182, 0.9636497272577779]\n",
      "Iteration 13  h2 new:  [133, 18]  probs:  [0.9535953753894851, 0.9903800494314249]\n",
      "Iteration 14  h1 new:  [54, 11]  probs:  [0.9870999920221329, 0.9720663607120753]\n",
      "Iteration 14  h2 new:  [90, 164]  probs:  [0.9634849003611279, 0.9895666232926063]\n",
      "Iteration 15  h1 new:  [4, 6]  probs:  [0.9837828708905773, 0.9639235954569497]\n",
      "Iteration 15  h2 new:  [134, 171]  probs:  [0.980000034068182, 0.9885738100546414]\n",
      "Iteration 16  h1 new:  [88, 53]  probs:  [0.9860885878365709, 0.9596301471062446]\n",
      "Iteration 16  h2 new:  [68, 97]  probs:  [0.9674227629217844, 0.9864131604090877]\n",
      "Iteration 17  h1 new:  [150, 168]  probs:  [0.9858588761924406, 0.9591012056754274]\n",
      "Iteration 17  h2 new:  [8, 126]  probs:  [0.9649689372160896, 0.9893540167639997]\n",
      "Iteration 18  h1 new:  [10, 29]  probs:  [0.983565375738282, 0.963555614704684]\n",
      "Iteration 18  h2 new:  [116, 78]  probs:  [0.9611959570564584, 0.9884657252476451]\n",
      "Iteration 19  h1 new:  [119, 115]  probs:  [0.9847784038815312, 0.9656571124790028]\n",
      "Iteration 19  h2 new:  [119, 161]  probs:  [0.9490706694531303, 0.9842915246058043]\n",
      "Iteration 20  h1 new:  [98, 20]  probs:  [0.9816170524002618, 0.9641872280940811]\n",
      "Iteration 20  h2 new:  [63, 138]  probs:  [0.9869485073016923, 0.976885467139035]\n",
      "Iteration 21  h1 new:  [137, 94]  probs:  [0.983108814250945, 0.9673996895772403]\n",
      "Iteration 21  h2 new:  [3, 95]  probs:  [0.9832231140386967, 0.9761017500888037]\n",
      "Iteration 22  h1 new:  [131, 142]  probs:  [0.982000249703885, 0.9691797401148753]\n",
      "Iteration 22  h2 new:  [140, 46]  probs:  [0.9687407605415462, 0.9719212847074753]\n",
      "Iteration 23  h1 new:  [31, 70]  probs:  [0.9849618849456908, 0.9735989454659616]\n",
      "Iteration 23  h2 new:  [7, 70]  probs:  [0.9844627558001405, 0.9699943658756629]\n",
      "Iteration 24  h1 new:  [149, 99]  probs:  [0.9895630855420425, 0.9732052711797713]\n",
      "Iteration 24  h2 new:  [149, 30]  probs:  [0.9905794797270394, 0.9830404290531776]\n",
      "Iteration 25  h1 new:  [22, 51]  probs:  [0.9837838335957679, 0.9717287928378356]\n",
      "Iteration 25  h2 new:  [52, 92]  probs:  [0.9857182958359464, 0.9810225346841421]\n",
      "Iteration 26  h1 new:  [122, 175]  probs:  [0.9866616686093437, 0.9715863838062666]\n",
      "Iteration 26  h2 new:  [170, 175]  probs:  [0.9637159620549738, 0.9543065226759248]\n",
      "Iteration 27  h1 new:  [24, 147]  probs:  [0.989854938780462, 0.9668730005171235]\n",
      "Iteration 27  h2 new:  [24, 148]  probs:  [0.9681165744562489, 0.9696041411161231]\n",
      "Iteration 28  h1 new:  [80, 156]  probs:  [0.9881251191071484, 0.991204573252949]\n",
      "Iteration 28  h2 new:  [25, 42]  probs:  [0.9635777992064796, 0.9865361722857079]\n",
      "Iteration 29  h1 new:  [109, 144]  probs:  [0.9933757860127653, 0.9882192645649007]\n",
      "Iteration 29  h2 new:  [83, 153]  probs:  [0.9717833347745594, 0.9654490773047685]\n",
      "Iteration 30  h1 new:  [15, 73]  probs:  [0.9850422408097103, 0.9811390802398305]\n",
      "Iteration 30  h2 new:  [15, 105]  probs:  [0.9731999256983264, 0.9687821552079988]\n",
      "Total Labeled number:  124  Still unlabeled number:  54\n",
      "co_SVM f1:                precision    recall  f1-score   support\n",
      "\n",
      "p_robinson_0       0.96      1.00      0.98        23\n",
      "p_robinson_1       1.00      0.96      0.98        26\n",
      "\n",
      " avg / total       0.98      0.98      0.98        49\n",
      "\n",
      "[23  0  1 25]\n",
      "Initial L size:  10\n",
      "Initial U size:  168\n",
      "U size after drawing sample to U prime: 0\n",
      "Initial U prime size:  168\n",
      "P value:  1  N value:  1\n",
      "Iteration 1  h1 new:  [100, 66]  probs:  [0.9254900341089003, 0.9119920562556033]\n",
      "Iteration 1  h2 new:  [157, 49]  probs:  [0.8866747226554398, 0.9595890279818505]\n",
      "Iteration 2  h1 new:  [32, 28]  probs:  [0.9456028937731009, 0.9343553539280568]\n",
      "Iteration 2  h2 new:  [84, 139]  probs:  [0.9501507409340184, 0.9517600908027831]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3  h1 new:  [141, 23]  probs:  [0.9595761224486052, 0.9352610799279214]\n",
      "Iteration 3  h2 new:  [141, 5]  probs:  [0.9542041452918886, 0.9647712162683929]\n",
      "Iteration 4  h1 new:  [150, 6]  probs:  [0.946673526026242, 0.9508504262582116]\n",
      "Iteration 4  h2 new:  [34, 172]  probs:  [0.953073572411791, 0.9593495521538983]\n",
      "Iteration 5  h1 new:  [125, 111]  probs:  [0.9621512557707922, 0.9601425953750394]\n",
      "Iteration 5  h2 new:  [64, 113]  probs:  [0.9495060871395462, 0.9648924980107436]\n",
      "Iteration 6  h1 new:  [71, 103]  probs:  [0.9640830949330159, 0.9681399840738815]\n",
      "Iteration 6  h2 new:  [135, 12]  probs:  [0.9583632056084243, 0.971532690312438]\n",
      "Iteration 7  h1 new:  [102, 127]  probs:  [0.9659418618773146, 0.9705468385782153]\n",
      "Iteration 7  h2 new:  [16, 165]  probs:  [0.9613749159268595, 0.9717116763298339]\n",
      "Iteration 8  h1 new:  [167, 130]  probs:  [0.9739099485718826, 0.9682443529745125]\n",
      "Iteration 8  h2 new:  [69, 27]  probs:  [0.9742729909762661, 0.9835461470366673]\n",
      "Iteration 9  h1 new:  [60, 117]  probs:  [0.9718545666240918, 0.967204657602746]\n",
      "Iteration 9  h2 new:  [60, 61]  probs:  [0.9599970492387045, 0.9772024680487792]\n",
      "Iteration 10  h1 new:  [38, 123]  probs:  [0.9735683823748525, 0.9636964775845552]\n",
      "Iteration 10  h2 new:  [114, 108]  probs:  [0.9616004057248614, 0.974050954792951]\n",
      "Iteration 11  h1 new:  [160, 121]  probs:  [0.9739731685275006, 0.9618241034422989]\n",
      "Iteration 11  h2 new:  [56, 97]  probs:  [0.9578740760894747, 0.9767499103248276]\n",
      "Iteration 12  h1 new:  [159, 101]  probs:  [0.9729872092965188, 0.9649801649734364]\n",
      "Iteration 12  h2 new:  [50, 18]  probs:  [0.9576100659314872, 0.9843633239428332]\n",
      "Iteration 13  h1 new:  [119, 11]  probs:  [0.9583066463953825, 0.9681584100405645]\n",
      "Iteration 13  h2 new:  [110, 129]  probs:  [0.9780501118898698, 0.9943872983051263]\n",
      "Iteration 14  h1 new:  [10, 175]  probs:  [0.9483301938984161, 0.9684669027154559]\n",
      "Iteration 14  h2 new:  [21, 171]  probs:  [0.9658552815345397, 0.9813070804807725]\n",
      "Iteration 15  h1 new:  [68, 48]  probs:  [0.9507073044094349, 0.9667309270960335]\n",
      "Iteration 15  h2 new:  [134, 126]  probs:  [0.9698421551636114, 0.9822050272762323]\n",
      "Iteration 16  h1 new:  [3, 53]  probs:  [0.951285302442428, 0.9684720097999013]\n",
      "Iteration 16  h2 new:  [133, 164]  probs:  [0.974715796389554, 0.9846529035437742]\n",
      "Iteration 17  h1 new:  [13, 29]  probs:  [0.9526418006102703, 0.9708009144578746]\n",
      "Iteration 17  h2 new:  [104, 168]  probs:  [0.9626177948963646, 0.9831439849866017]\n",
      "Iteration 18  h1 new:  [88, 45]  probs:  [0.9616506512437116, 0.9720034741045752]\n",
      "Iteration 18  h2 new:  [90, 78]  probs:  [0.9679141074719014, 0.987696745739893]\n",
      "Iteration 19  h1 new:  [80, 161]  probs:  [0.9639084400263545, 0.9712878561476975]\n",
      "Iteration 19  h2 new:  [116, 118]  probs:  [0.9710400622586428, 0.9878804610610776]\n",
      "Iteration 20  h1 new:  [137, 99]  probs:  [0.9672068685488161, 0.9678777500704926]\n",
      "Iteration 20  h2 new:  [137, 138]  probs:  [0.9856219840010034, 0.9790189425662404]\n",
      "Iteration 21  h1 new:  [4, 147]  probs:  [0.9643614724604669, 0.966300425679632]\n",
      "Iteration 21  h2 new:  [24, 92]  probs:  [0.9836410267809811, 0.9689142818928613]\n",
      "Iteration 22  h1 new:  [63, 46]  probs:  [0.9694559179510683, 0.9609680790770075]\n",
      "Iteration 22  h2 new:  [63, 58]  probs:  [0.9900929943408378, 0.9587892574793807]\n",
      "Iteration 23  h1 new:  [15, 51]  probs:  [0.9621728291619925, 0.9586481432902155]\n",
      "Iteration 23  h2 new:  [8, 95]  probs:  [0.9826052190207032, 0.9652297301793027]\n",
      "Iteration 24  h1 new:  [81, 153]  probs:  [0.9644171107638755, 0.9563479704750097]\n",
      "Iteration 24  h2 new:  [170, 70]  probs:  [0.9817991198708641, 0.9579598560378609]\n",
      "Iteration 25  h1 new:  [52, 57]  probs:  [0.9624499815614669, 0.958093058810358]\n",
      "Iteration 25  h2 new:  [128, 30]  probs:  [0.9691911819294325, 0.9611235534954881]\n",
      "Iteration 26  h1 new:  [98, 142]  probs:  [0.960203286251267, 0.9590598913786886]\n",
      "Iteration 26  h2 new:  [120, 42]  probs:  [0.9717850743680876, 0.9628726304375891]\n",
      "Iteration 27  h1 new:  [83, 156]  probs:  [0.9610143095628783, 0.9566827498216044]\n",
      "Iteration 27  h2 new:  [1, 94]  probs:  [0.9962316265557768, 0.941624277342105]\n",
      "Iteration 28  h1 new:  [149, 20]  probs:  [0.9554599910476704, 0.9531546182410034]\n",
      "Iteration 28  h2 new:  [44, 112]  probs:  [0.9823156353347037, 0.9119324025175045]\n",
      "Iteration 29  h1 new:  [74, 19]  probs:  [0.9534707368088909, 0.9530713247848657]\n",
      "Iteration 29  h2 new:  [177, 148]  probs:  [0.9858654885041632, 0.9597362958332467]\n",
      "Iteration 30  h1 new:  [109, 115]  probs:  [0.9516888818333957, 0.9665382915502906]\n",
      "Iteration 30  h2 new:  [25, 14]  probs:  [0.9898535940353402, 0.9489299760842267]\n",
      "Total Labeled number:  126  Still unlabeled number:  52\n",
      "co_LR_SVM f1:                precision    recall  f1-score   support\n",
      "\n",
      "p_robinson_0       0.96      1.00      0.98        23\n",
      "p_robinson_1       1.00      0.96      0.98        26\n",
      "\n",
      " avg / total       0.98      0.98      0.98        49\n",
      "\n",
      "[23  0  1 25]\n",
      "Initial L size:  10\n",
      "Initial U size:  168\n",
      "U size after drawing sample to U prime: 0\n",
      "Initial U prime size:  168\n",
      "[('p_robinson_1', 1.0), ('p_robinson_0', 1.0)]\n",
      "Iteration 1  h1 new:  [157, 66]  score:  [1.616308793170058, 1.6081897805837495]\n",
      "Iteration 1  h2 new:  [84, 172]  score:  [1.2318427110863106, 1.8332606097829038]\n",
      "Iteration 2  h1 new:  [100, 28]  score:  [2.629689160161772, 2.5972580770793154]\n",
      "Iteration 2  h2 new:  [141, 49]  score:  [2.0688482786165947, 2.8094787506909733]\n",
      "Iteration 3  h1 new:  [32, 103]  score:  [3.558811971135245, 3.535071331358906]\n",
      "Iteration 3  h2 new:  [16, 139]  score:  [3.65376548444191, 3.763226037897252]\n",
      "Iteration 4  h1 new:  [125, 111]  score:  [4.474900439247935, 4.489161748525196]\n",
      "Iteration 4  h2 new:  [135, 23]  score:  [4.652225081266336, 4.661265611230039]\n",
      "Iteration 5  h1 new:  [150, 127]  score:  [5.484007976190947, 5.396417618866314]\n",
      "Iteration 5  h2 new:  [34, 27]  score:  [5.6546399826859295, 5.6526136247088505]\n",
      "Iteration 6  h1 new:  [71, 6]  score:  [6.435246522288018, 6.3743207823295585]\n",
      "Iteration 6  h2 new:  [69, 165]  score:  [6.5815456357259, 6.606975027889569]\n",
      "Iteration 7  h1 new:  [167, 117]  score:  [7.367258468840128, 7.15715641679275]\n",
      "Iteration 7  h2 new:  [64, 118]  score:  [7.520305962454561, 7.553912615664921]\n",
      "Iteration 8  h1 new:  [102, 101]  score:  [8.279729344788363, 8.058413619408332]\n",
      "Iteration 8  h2 new:  [60, 5]  score:  [8.494403791080906, 8.46360852927218]\n",
      "Iteration 9  h1 new:  [160, 130]  score:  [9.236304643106914, 9.025434888154573]\n",
      "Iteration 9  h2 new:  [114, 113]  score:  [9.400902201881694, 9.313389416412369]\n",
      "Iteration 10  h1 new:  [159, 11]  score:  [10.191818818359842, 9.971831794306162]\n",
      "Iteration 10  h2 new:  [133, 61]  score:  [10.37421807267854, 10.221831091406282]\n",
      "Iteration 11  h1 new:  [38, 123]  score:  [11.150013123819216, 10.877513385196298]\n",
      "Iteration 11  h2 new:  [128, 129]  score:  [11.345091174539435, 11.120580896347615]\n",
      "Iteration 12  h1 new:  [119, 53]  score:  [12.007162391631532, 11.694484812651474]\n",
      "Iteration 12  h2 new:  [110, 97]  score:  [12.302849587505024, 12.068884050493065]\n",
      "Iteration 13  h1 new:  [10, 121]  score:  [12.893608985520848, 12.65217541200037]\n",
      "Iteration 13  h2 new:  [56, 12]  score:  [13.244224361687419, 13.012660387766374]\n",
      "Iteration 14  h1 new:  [13, 175]  score:  [13.522002470972222, 13.585192187325935]\n",
      "Iteration 14  h2 new:  [134, 46]  score:  [14.207121422331912, 13.880831094539387]\n",
      "Iteration 15  h1 new:  [15, 147]  score:  [14.434448737860915, 14.525675401612363]\n",
      "Iteration 15  h2 new:  [170, 164]  score:  [15.138354528798155, 14.800050747130067]\n",
      "Iteration 16  h1 new:  [4, 78]  score:  [15.353331092808729, 15.218281734430182]\n",
      "Iteration 16  h2 new:  [120, 108]  score:  [16.095245607379013, 15.770699941817186]\n",
      "Iteration 17  h1 new:  [68, 48]  score:  [16.284945244722188, 16.153106116099774]\n",
      "Iteration 17  h2 new:  [50, 171]  score:  [17.01869888070875, 16.708177854977837]\n",
      "Iteration 18  h1 new:  [80, 161]  score:  [17.116737547609542, 17.069220633666614]\n",
      "Iteration 18  h2 new:  [21, 18]  score:  [17.960043363119645, 17.643460203270255]\n",
      "Iteration 19  h1 new:  [88, 29]  score:  [17.986199871301853, 17.99771489607339]\n",
      "Iteration 19  h2 new:  [3, 126]  score:  [18.80722226990906, 18.596175294365754]\n",
      "Iteration 20  h1 new:  [137, 45]  score:  [18.89628850506926, 18.841140570101075]\n",
      "Iteration 20  h2 new:  [63, 168]  score:  [19.78108299642274, 19.44996767035958]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21  h1 new:  [24, 99]  score:  [19.84361982062656, 19.75253304339879]\n",
      "Iteration 21  h2 new:  [104, 138]  score:  [20.68698336101855, 19.99274412650509]\n",
      "Iteration 22  h1 new:  [83, 51]  score:  [20.704485559344732, 20.650514106847368]\n",
      "Iteration 22  h2 new:  [1, 58]  score:  [21.611627891097182, 20.53941662917631]\n",
      "Iteration 23  h1 new:  [109, 153]  score:  [21.593438700103214, 21.27186503502432]\n",
      "Iteration 23  h2 new:  [8, 42]  score:  [22.52436973169022, 21.120484391491225]\n",
      "Iteration 24  h1 new:  [81, 20]  score:  [22.45598519573857, 22.08998627838296]\n",
      "Iteration 24  h2 new:  [81, 94]  score:  [23.334819971908303, 21.94377115281291]\n",
      "Iteration 25  h1 new:  [177, 57]  score:  [23.225952427595626, 22.812538125953445]\n",
      "Iteration 25  h2 new:  [90, 70]  score:  [24.041718069143393, 22.858711396699437]\n",
      "Iteration 26  h1 new:  [98, 142]  score:  [24.066843266451624, 23.756466837721547]\n",
      "Iteration 26  h2 new:  [44, 95]  score:  [24.894619876519375, 23.711350896255652]\n",
      "Iteration 27  h1 new:  [166, 19]  score:  [24.837275283615682, 24.657715770044117]\n",
      "Iteration 27  h2 new:  [7, 30]  score:  [25.591238364119114, 24.18373769593549]\n",
      "Iteration 28  h1 new:  [54, 156]  score:  [25.372004083198526, 25.51642979945325]\n",
      "Iteration 28  h2 new:  [116, 92]  score:  [25.840179889498415, 23.827446539048022]\n",
      "Iteration 29  h1 new:  [52, 144]  score:  [26.277871752898132, 26.217728911182864]\n",
      "Iteration 29  h2 new:  [140, 112]  score:  [27.060257234562773, 24.223977163116704]\n",
      "Iteration 30  h1 new:  [36, 155]  score:  [27.180558720532595, 26.83842961952771]\n",
      "Iteration 30  h2 new:  [149, 14]  score:  [27.813065562258963, 24.47724882061328]\n",
      "Total Labeled number:  129  Still unlabeled number:  49\n",
      "Improved_co_LR f1:                precision    recall  f1-score   support\n",
      "\n",
      "p_robinson_0       0.96      1.00      0.98        23\n",
      "p_robinson_1       1.00      0.96      0.98        26\n",
      "\n",
      " avg / total       0.98      0.98      0.98        49\n",
      "\n",
      "[23  0  1 25]\n",
      "LR f1:                precision    recall  f1-score   support\n",
      "\n",
      "p_robinson_0       0.92      1.00      0.96        23\n",
      "p_robinson_1       1.00      0.92      0.96        26\n",
      "\n",
      " avg / total       0.96      0.96      0.96        49\n",
      "\n",
      "[23  0  2 24]\n",
      "SVM f1:                precision    recall  f1-score   support\n",
      "\n",
      "p_robinson_0       0.88      1.00      0.94        23\n",
      "p_robinson_1       1.00      0.88      0.94        26\n",
      "\n",
      " avg / total       0.95      0.94      0.94        49\n",
      "\n",
      "[23  0  3 23]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnUAAAH2CAYAAAAWO3q/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "co_LR  per fold iteration:  [30, 30, 30, 30, 30]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnUAAAI3CAYAAAD9Z60zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n06wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "co_SVM  per fold iteration:  [30, 30, 30, 30, 30]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnUAAAI3CAYAAAD9Z60zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzs3WdgVFXeBvBnasqkt0lPQHpHaoDQ\ni7go6K7CgigWdF1XXNcVEVdQFBR1ZcWCbV0R2/ruriJI6C10iUgVCCST3ttkZpKp9/0QMooESJmZ\nO+X5fXIyd8795zJOnjnnnnMkgiAIICIiIiKPJhW7ACIiIiLqOIY6IiIiIi/AUEdERETkBRjqiIiI\niLwAQx0RERGRF2CoIyIiIvICcrELIBKD2WxGQUEBGhoaxS6FiAgymQwREeGIioqCVMr+FmofCdep\nI1+Uk5MDudwPQUGhkEgkYpdDRD5MEARYrRZotTWQy6VISUkRuyTyUPw6QD6poaGRgY6I3IJEIoFc\nrkB4eBT0er3Y5ZAHY6gjn8VAR0TuRCKRgmNn1BEMdURERERegBMliER23313w2w2XZq8kY/OnW8A\nAHTr1h3PPvt8u9r8/PNPcfPNv0FYWPgVz1ksFowaNRR79hyEn59fh2pvq6VLn0H//gNx++2/u+Zx\nBQX5eOih+7Fp07brtvnOO29iz55diIyMwjvvvN/huo4ePQKbTcDQocPa1Za7qjNq8cGJTzG/310I\n9QtxSJt87xK5F4Y6IpF99NEnAIDi4mLce+9dWLfuyw63+cUXn2LEiJEt/mH0JoIg4PPP12HTpu0I\nCXFMUDl69HtYrVavC3UbL27DhZpcbLy4DXN6/dYhbfK9S+ReGOqIAKz45OgVPxvaS42Jg5NgNFvx\n9y+OXfF8ev94pPePR73BhDf/c+KK58cPSsTw3rEdrm3Dhm/w9df/hcViRUhIMBYufAbJycn44IM1\n0Gg0WL58JRoaGnDvvXfh8cf/ilOnTqKmphpPPfUElEolXnzxZaSkpF7R7rp1H+Pw4YPQarX44x8f\nxZgx4wAAf/vbIhQWFsBsNiMpKRnPPLMUwcHByM3NwYsvPgej0QibzYZbb52BWbPmwGQyYc2at3D8\n+DGYTCZ069YdCxc+DX//AJSVlWLZsiWoq6tDQkIiTCbTVX/Pf//7C/zf/32ByMgoDBw46LLn9u3b\ni7VrP4LJZIJSqcTjjz+JXr16Y/78ebBYLPjjH+cjLW0k7rhjFpYufQZ6vR4mkxFjxozFww8/CuDK\nnpaWel7Onz+Hb7/9BoIg4NChA5gyZSruuuuejv0DOtHB4qM4UHTkmsdk1+RCwM83au0tPIi9hQch\ngQRdwztd9XUjEoYiLX5wh+rzlfcukbtgqCNyY1lZR7Fnz268995HUCgUyMzcg5deWoY1az7E/fc/\nhEcf/QP++9//w+nTJzFmzFgMG5aGYcPS8M03/8PKlX9HaurV/2jL5XJ88MHHyM3NwUMP3Y/+/Qcg\nLCwcf/3rU/ZekrffXo3PPvsEf/jDI/jPf/6N0aPH4p577gMAaLVaAMDatR8hPDwcH320DgDwxhuv\nY926jzF//sN47bWVGDx4KO699wHk5+fj7rtnIT19zBW1nDt3Fp9+uhZr136OiIgIvPTSC/bn8vPz\nsHbtR3jjjXcQGBiI7OzzePLJx/HNN9/h3Xf/iVGjhuKf//wEfn5+aGxsxOuvr0ZAQADMZjMeffQP\nOHLkcKt73bp1645bb50Bq9WKRx5Z0Lp/JDfXKTQJFYYq6MwGCBAggQRBikBEB0Y69by+8t4lcicM\ndUQAFt999R4JP4Xsms8HByqv+XxHZGbuwblzZ3HffXMBNA03GgwGAIBUKsWyZSswd+7vkZCQiMWL\nl7Sp7VtumQEA6NSpM7p06YozZ05jxIhR2LjxW2zdugVWqwUGgwGdO3cGAAwYcCPWrHkLRqMRgwYN\nxo03Dr5U414YjY3Ytm0LAMBkMqFHj54Amv6wL1r0NwBAcnIyBg1q+TplZR3FqFGjERERAQCYMeO3\nyMzcCwA4ePAACgsL8NBD99mPt1jMqKurhUoVdFk7NpsNq1e/jpMnm3pOq6oqkZ19zuuGUpulxQ9u\nVW/aZ2f+i8zCQ1BI5bDYrBio7uewIdir8ZX3LpE7YagjcmsCZsy4Hfff/2CLzxYVFUEmk0GrrYPJ\nZIJc3r7/pZvWIJcgK+t7bNiwHu+990+EhYVj06aN2LRpIwBg0qQp6N9/AI4cOYR//euf+O67jViy\n5HkAAhYteuaKIdO2n/+qz2LkyHT87W/PXfGMxWK57PFnn62FwdCAf/3r00vDd8/BaGwaNpPJ5LDZ\nbPZjfWk4TWuqx+jENKQnDUdmwSHUmbQuOKtvvHeJ3AmXNCFyY+npY7Bp00ZUVFQAAKxWK86ePQMA\nqKurw/PPP4sVK1Zi7NjxWLlyuf11KpUKOp3umm1/9923AACNJhc5ORfRq1cv1NfXQ6UKQkhIKIxG\nIzZsWG8/Pj8/H1FR0Zg2bTruu28+zpw5danG0fj8809hNBoBAHq9DhpNLgBg8OAh9vMUFhbghx+y\nWqxl8OAh2L8/E7W1NQCAb7/9xv7c8OEjsH//PuTm5gBo+iN+5szpFtupr9chOjoKSqUSZWWl2Ldv\nr/25xMRE/PRT0+sqKipw7FjLtbTm2nmahwfMw+xetyMpOB6ze92OhwfMc/o5feW9S+RO2FNH5MYG\nDRqC++9/EE88sQA2mw0WiwUTJ05G9+49sWzZEsyYcTv69u2PXr364I9/nI/167/G9Om34c47f4/n\nn38W/v7+V73ZXCaTYf78edBqtVi8+FmEhYVj1Kh0bNmSgVmzfovo6Gj07NkL58+fAwBs374F27Zt\ngUKhACDB44//FQAwb94DeP/9NfZhNqlUigceeAipqZ3wxBML8fzzS7B162akpnbC4MFDW/w9u3fv\ngTlz7sYDD8xDZGQkRowYZX8uJSUVzz77PF54YSlMpqblMwYOvBG9evW+op1Zs2Zj8eKFuPvu30Ot\njsWgQUPsz91+++/w9NNP4q67ZiIlJRW9e/dpsZZx4ybg6aefxNy5s9x+ooQ785X3LpE74d6v5JNO\nnz6D+Hjur0hE7qW4OA+9e/cSuwzyUBx+JSIiIvICDHVEREREXoChjoiIiMgLMNQREREReQGGOiIi\nIiIvwFBHRERE5AUY6ojcgMVixvvvr8Edd8zA73//O8yceTveeON1WCxm7N27B2++uQoAUFxcjG++\n+W+r2qyvr8e6dR9f9rPly5fhxx9/cFjdGzd+i6efftJh7bXF8OE32redupYPPngXq1evuu5xdXW1\nmD9/HubOnYVPP13rkLq+/PIzVFdXt7stT8D3btu19r1L1FZcfJjIDbzwwnMwGo34+OPPoFKpYLGY\nsXHjtzCZzBg9egxGj27aSLykpBjffPM/zJhx/X076+vr8emnn2Du3Hn2nz3zTNv22PQlR44cRnBw\nCD744GOHtfnll59jyJBh9j1txVb+9f8Qc9vtDm2T710i98FQRySy/Px87NmzC99+uxkqlQoAIJcr\n7H/8Nm78Fvv3Z+Kll17Fa6+9jOLiYsydOwuJiUl46aVXsXr1Khw7lgWz2YywsDA888xSxMXF47XX\nXoZOV4+5c2fB398fH3zwMR5+eD7mzJmLUaNGo6qqCq+8sgKFhQUAgDlz7sbNN08DAMyY8RvcfPM0\nHDlyCJWVlZgzZy7uuGNWi/XrdDosWvRXFBYWIDQ0FEuXvoiYmBhcuJCNV199CQ0NjTCZjJgx43bM\nmjUHAPDNN//FF198BqVSCZvNhuXLVyI1tRPy8jRYteo11NXVwmw2Y9as2Zg2bToAYNeuHXj33bcR\nEhKCtLRRLdbSVE89li9fhtzcHMTGxiIsLBwREZEAALPZjHfffQvHjv0As9mMG27ogoULF+Onn07j\nrbfegF6vw9y5s/DEEwtRVlaGf//7C1gsZgDAo4/+GUOGDAPQ1NOyc+c+BAYGtvgYAP71rw9RWVmB\nxYsXQqlUYtmyFejUqXM73iGOU7n+G4eGOr53HfveJeoohjoiAPXfrLjuMYqUAfAfeLP9eGWPdPj1\nSIetoR76LW+2+JrgGYuv2+7582eRlJSMkJCQ6x77178uwptvrsLHH39m/9ndd8/DggWPAwDWr/8a\nb7+9Gi+++DL++tdFuPfeu7Bu3ZcttvX666+gc+cbsHLl31FZWYF77pmD7t174IYbugAAGhsb8eGH\na1FcXIw5c+7Ab35z62WhpdmJEz/ik0++QEpKKj788D2sWvUqXnrpVcTFxePNN9+FUqmEwWDAfffN\nxbBhaejUqTPefPMNfP75V1CrY2EymWCzWWGxWLBkyWI8//xypKZ2gl6vx7333oU+ffohJCQUL730\nIj744F9ISUm9Ymjul/75zw+gUqnw5Zf/RW1tDe65Zw4mTJgEAFi3bi1UqmB89NE6AMBbb72BtWs/\nwsMP/wnz5//BHkCApuHYyZNvgkQiQV6eBn/60x+wYcPm6/4bNbv33gewfv3XWLHiFfs1dQbNSysQ\nNiodYenpECwW5L36CsLGjEHYiJGwGY3If/3vCB8/HqHDhtuPj5g0CSGDh8BSX4/Ct95E5E1TETxw\nICy1tShc8w6ifjMNQf36XffcfO869r1L1FEMdUQi6+hOfQcP7sd//vMVGhoaYLVaW/26778/gsce\n+wsAICoqGiNGjEJW1lH7H8ZJk6YAAOLj4xEcHILy8jKkpna6op1+/QbY9+e89dbbcNdddwJo+sP6\nyisrcOFCNiQSCSorK3DhQjY6deqMwYOH4IUXnsPo0WMxcuQoJCQkIjc3BxqNBs8++7S9bZPJBI0m\nF1KpDN2797CfZ8aM3+Ltt1e3+HtlZR3FE08sBACEhYVj7Nhx9uf27dsDvV6PXbu229vv2rVbi+0U\nFhbivfcWo6KiHHK5HNXVVaiqqkRkZFSrrq+7MFVWoGjNOyha8w4AwHDuLAznziJq+gxETJzUobb5\n3nXse5eooxjqiNC6HrWrHS8NCG7z63+pe/eeKCjIh1arbVWPxy+VlBTjH/94Hf/61zrExyfgxInj\nWLKkLbVILn/0i4dKpdL+31KptJV/dAV7m++++xYiI6Pw7LPPQy6XY8GCP8JoNAIAXn75NZw5cxpZ\nWd/jkUcexMKFz1waKg1rsXdm797dbfidrh40BEHAk08uatXm7EuWLMaCBY9jzJhxsNlsGDt2BIxG\nE4CmDeUFwQYA9t9JLKlP//zvLZHLL3ss9fNDt7//PEnkzLy70evjT676enlY2GWPr4fvXUe/d4k6\nhrNfiUSWnJyM9PQxWLlyOfR6PQDAarXi3//+/IoZciqVCjqdzv5Yr9dDoZAjIiISNpsNX3/9n8uO\nbWxshMViafG8Q4YMxfr1TbMRq6oqcfDgPgwaNKTN9Z84cRz5+fkAgI0bN2DQoMEAmm52V6vVkMvl\nuHjxAo4fPwYAsFgsKCoqRO/efXD33fdi6NA0nD9/FsnJKfD390dGxkZ72xpNLvR6Hfr06Yfz58/Z\nz/Ptt19ftZ7Bg4di48ZvATQNoe7Zs8v+XHr6GHzxxadobGwE0HT9cnNzWmynvr4e8fEJl873DUwm\nk/25hIREnDlzGgCwdWvGVWv59b+Xt+F717HvXaKOYk8dkRtYsmQZPvzwPcybNwcKhQI2mw0jRoyC\nUqm47LguXboiJSUFs2ffgZSUVLz00qsYP34SZs++A2p1LG688UYcO9a07ENoaCimTJmKOXPuREjI\nlbM6//KXhVi5cjnmzGkacvrjHxegc+cb2lz7wIE34sMP30VOzkX7zeZA0z1lzz//LDZv3oSEhEQM\nGDAQAGCz2fDCC0uh0+kgkUigVqvxyCOPQi6X49VX/4F//OM1fPrpJ7DZbIiIiMDy5SsRERGBRYue\nwZNP/hkhISH2e+Ract99D+DFF5/HrFm/RVxcPIYOTbM/d/fd8/DBB+/hvvvmQiKRQCKR4P77H2xx\nAsPjjz+BhQv/gujoGAwceCNCQ8Psz/35z09g5crliIyMwsiRo69ay513/h4vvvgc/P393WKiRNT0\nGQ5vk+9dx713iTpKInT0pggiD3T69BnEx6eIXQYR0WWKi/PQu3cvscsgD8XhVyIiIiIvwFBHRERE\n5AUY6oiIiIi8AEMd+SzeTkpE7kQQbJctzULUVgx15JMCAvxRX1/HYEdEohMEARaLGdXVlfbt1oja\ng7NfySeZzWYUFBSgoaFR7FKIiCCXyxAeHo6oqChIpexvofZhqCMiIiLyAvw6QEREROQFGOqIiIiI\nvABDHREREZEXYKgjIiIi8gIMdURERERegKGOiIiIyAsw1BERERF5AYY6IiIiIi/AUEdERETkBRjq\niIiIiLwAQx0RERGRF2CoIyIiIvICDHVEREREXoChjoiIiMgLMNQREREReQGGOiIiIiIvwFBHRERE\n5AUY6oiIiIi8AEMdERERkRdgqCMiIiLyAgx1RERERF6AoY6IiIjICzDUEREREXkBhjoiIiIiL8BQ\nR0REROQFGOqIiIiIvABDHREREZEXYKgjIiIi8gIMdURERERegKGOiIiIyAsw1BERERF5AYY6IiIi\nIi/AUEdERETkBRjqiIiIiLwAQx0RERGRF2CoIyIiIvICDHVEREREXoChjoiIiMgLMNQREREReQGG\nOiIiIiIvwFBHRERE5AUY6oiIiIi8AEMdERERkRdgqCMiIiLyAgx1RERERF6AoY6IiIjIC8jFLsCV\namr0sNkEp7UfGRmEqiqd09r3dby+zsNr61y8vs7Da+tcvL7Oc61rK5VKEB6uanObPhXqbDbBqaGu\n+RzkPLy+zsNr61y8vs7Da+tcvL7O4+hry+FXIiIiIi/AUEdERETkBRjqiIiIiLwAQx0RERGRF2Co\nIyIiIvICDHVEREREXoChjoiIiMgLMNQREREReQGGOiIiIiIvwFBHRERE5AUY6oiIiIi8AEMdERER\nkRdgqCMiIiLyAgx1RERERF6AoY6IiIioleqMWqzKWoM6Y73YpVyBoY6IiIiolTJyt+NinQYZudvE\nLuUKcrELICIiInJ3j+1eDIvNYn+cWXwImcWHIJfK8cbYFSJW9jP21BERERFdx7K0Rega1tn+WCFV\nYIh6IJalPS1iVZdjqCMiIiK6jhBlMEoN5QAAuVQOi80Cf5kfQv2CRa7sZxx+JSIiIrqO8zUXUW/S\n4YbQVNzZbQb2FR+G1qgVu6zLMNQRERERXUeGZjtClcF4dMB8KGQKzOp+m9glXYHDr0RERETXcKE2\nF9m1OZiYMhYKmULscq6KoY6IiIjoGjJytyNYEYRR8cPELuWaGOqIiIiIriK3Lh9na7IxIXk0lDKl\n2OVcE0MdERER0VVs1myHShGI9IQ0sUu5LoY6IiIiohbkawtxquosxieNhr/cT+xyrouhjoiIiKgF\nmzU7ECAPwJjEEWKX0ioMdURERES/UqQrwfHK0xiXOBIBcn+xy2kVhjoiIiKiX9ms2QF/mR/GJY0S\nu5RWY6gjIiIi+oVSfRmOlZ/EmMSRCFQEil1OqzHUEREREf3CZs1OKGQKjE9KF7uUNmGoIyIiIrqk\n3FCBo2U/Ij1hOIKUKrHLaROGOiIiIqJLtmh2QS6VYWLyGLFLaTOGOiIiIiIAlQ3VOFL2A0bFD0eI\nMljsctqMoY6IiIgIwNa8XZBCgokpntdLBzDUEREREaG6sQaHSo4iLX4owvxCxS6nXRjqiIiIyOdt\ny9sDAJicMlbcQjqAoY6IiIh8Wp1RiwMlRzAsdhAi/MPFLqfdGOqIiIjIp23P3wObYMOU1HFil9Ih\nDHVERETks+pNOmQWHcIQ9UBEBUSKXU6HMNQRERGRz9qRvxcWmwVTUseLXUqHMdQRERGRT9KZ9NhT\ndACD1P2hDowWu5wOY6gjIiIin7SrIBNmqxlTUjy/lw5gqCMiIiIfZDA3YHfhAQyI7oP4oFixy3EI\nhjoiIiLyObsL96HR2oibUieIXYrDMNQRERGRT2mwNGJnwT70jeqFxOB4sctxGIY6IiIi8il7Cw+g\nwdKAqV7USwcw1BEREZEPMVpN2FmQiV6R3ZESkiR2OQ7FUEdEREQ+I7PoIHRmPaamThS7FIdjqCMi\nIiKfYLKasT1/D3qEd0Xn0BSxy3E4udgFEBERETlbnVGL17PWoN6kw029veteumbsqSMiIiKv913O\nVlQ2ViFEGYyu4Z3FLscp2FNHREREXuux3YthsVnsj7WmejyycyHkUjneGLtCxMocjz11RERE5LWW\npS3CYPUASCABACikCgxRD8SytKdFrszxGOqIiIjIa4X6hcBP5gcBAiSQwGKzwF/mh1C/YLFLczgO\nvxIREZFXqzBUAgCmdZ6MWqMWWqNW5Iqcg6GOiIiIvFq/6N44X3sRw2IHIdw/TOxynIbDr0REROTV\nLtTmINI/3KsDHcBQR0RERF5MEARcqM1FlzDvXMbklxjqiIiIyGuV6MugM+vRlaGOiIiIyHNdqM0B\nAK9dcPiXGOqIiIjIa2XX5iDMLxSR/hFil+J0DHVERETklQRBQHZtDrqGdYZEIhG7HKdjqCMiIiKv\nVG6oQL1Jhy5hncQuxSUY6oiIiMgrXajNBQCfmCQBMNQRERGRl8quzUGwMggxgdFil+ISLgt1ubm5\nmDlzJqZMmYKZM2dCo9FccUxFRQUefvhh3HLLLZg6dSrWr19/xTE5OTno378/Vq5c6YKqiYiIyBP5\n2v10gAtD3dKlSzF79mxs2bIFs2fPxpIlS6445uWXX0afPn2wYcMGfPbZZ1i1ahVKSkrsz1utVixd\nuhQTJ050VdlERETkgaoaq1FrrPOZoVfARaGuqqoKZ86cwbRp0wAA06ZNw5kzZ1BdXX3ZcWfPnkV6\nejoAICIiAj169EBGRob9+ffffx9jx45FamqqK8omIiIiD5Vd07Q+nS/sJNHMJaGupKQEarUaMpkM\nACCTyRATE3NZLxwA9O7dG5s2bYIgCCgoKMCxY8dQXFwMoCnw7du3D/PmzXNFyUREROTBsmtzEKRQ\nIU6lFrsUl5GLXcAvLVq0CCtWrMD06dMRHx+P4cOHQy6Xw2w249lnn8VLL71kD4btERkZ5MBqWxYd\nHez0c/gyXl/n4bV1Ll5f5+G1dS5Pvb659Rr0iumKmJgQsUu5KkdfW5eEuri4OJSVlcFqtUImk8Fq\ntaK8vBxxcXGXHRcREYHXXnvN/nj+/Pm44YYbUFFRgfz8fDz44IMAAK1WC0EQoNPp8MILL7S6jqoq\nHWw2wTG/VAuio4NRUVHvtPZ9Ha+v8/DaOhevr/Pw2jqXp17fmsZalOurMDp+pNvWf61rK5VK2tUR\n5ZJQFxkZiZ49e2Ljxo2YPn06Nm7ciJ49eyIi4vItO2pqahAcHAy5XI6DBw/i/PnzWL16NQICAnD4\n8GH7cW+++SYMBgOeeuopV5RPREREHiS71vfupwNcOPz63HPPYdGiRXjnnXcQEhJiX5Jk/vz5WLBg\nAfr27YsTJ05g+fLlkEqlCA8Px7vvvouAgABXlUhEREReILsmBwHyACQExYpdiktJBEFw3nikm+Hw\nq2fj9XUeXlvn4vV1Hl5b5/LU6/v8wVegVkXjD/3uFbuUq3LG8Ct3lCAiIiKvUWfUoryh0ueGXgGG\nOiIiIvIizffT+dKiw80Y6oiIiMhrXKjNhb/MD4lB8WKX4nIMdUREROQ1smtz0Dk0FTJp+9e19VQM\ndUREROQV6k06lOrLfHLoFWCoIyIiIi9xoTYXANAlnKGOiIiIyGNl1+ZAKVUgJThR7FJEwVBHRERE\nXuGCD99PBzDUERERkRfQmw0o1pWiS1gnsUsRDUMdERERebyLtbkQIPjkosPNGOqIiIjI42XX5kAu\nlSM1JEnsUkTDUEdEREQe70JtDjqFJEMhU4hdimgY6ojaqM6oxaqsNagzet4m10RE3qjB0oCC+mKf\nHnoFGOqI2iwjdzsu1mmQkbtN7FKIiAjAxVoNBAg+u+hwM7nYBRB5isd2L4bFZrE/ziw+hMziQ5BL\n5Xhj7AoRKyMi8m0XanMhk8jQKTRZ7FJExZ46olZalrYIvSO62x8rpAoMUQ/EsrSnRayKiIgu1OYg\nJSQRSplS7FJExVBH1EqhfiEoM1TYH1tsFvjL/BDqFyxiVUREvs1oNSGvvtDn76cDGOqIWq1UX4bK\nxmrIJU0rlafFDYHWxMkSRERiyqnTwCbYfP5+OoChjqjVNmt2QilT4s5uMwAAoxKG4cF+94hcFRGR\nb7tQkwOpRIrOoSlilyI6hjqiVigzVOBo2Y8YnZBm34KmVF8uclVERJRdm4Ok4AT4y/3FLkV0DHVE\nrbBFsxNyqRwTkkcjKiASMokMJfoyscsiIvJpJqsZedoCDr1ewiVNiK6jsqEK35cdw5jEEQhRNk2K\nUAdGo9TAUEdEJCaNNh8WwWofQfF17Kkjuo4tml2QSqSYmDzG/jO1KgYlHH4lIhJVdm0OJJDghlCG\nOoChjuiaqhpqcKj0KEbEDUWYX6j953GBMahqqIbJahaxOiIi33ahJgeJQXEIVASIXYpbYKgjuoat\n+bsggQSTU8Ze9vNYlRoCBJT/Yt06IiJyHbPNglxtHrqE8366Zgx1RFdR01iLQ8XfIy1uMML9wy57\nLk6lBtC0dh0REblenrYAZpuFkyR+gaGO6Cq25e+BDQImp4y74rnowChIJVKUGHhfHRGRGC7U5gAA\nbuAkCTuGOqIW1Bm12F98GMNiByEyIOKK5xVSOaICIthTR0QkkuyaHMSrYhGkUIldittgqCNqwfb8\nPbAJNkxJGX/VY+IC1VyAmIhIBFabFTnaPO73+isMdUS/Um/SIbPoEIaoByI6MPKqx8Wq1ChvqITF\nZnFhdURElF9fBJPVhK6cJHEZhjqiX9mRvxcWmwVTWriX7pdiVTGwCTZUNFS5qDIiIgJ+vp+Oiw5f\njqGO6Bd0Jj32FB3AIHV/qFUx1zy2eQYstwsjInKt7NocqANj7Lv8UBOGOqJf2FWQCbPVfM176Zqp\nA6MhgYSTJYiIXMgm2HCxVoOu7KW7AkMd0SUGswG7C/djQHQfxAfFXvd4pUyJCP9wTpYgInKhwvpi\nNFobuT5dCxjqiC7ZVbgfjVbeumIyAAAgAElEQVQjbkqd0OrXxKliUMq16oiIXMZ+Px0nSVyBoY4I\nQIOlAbsK9qF/VG8kBse3+nWxKjXKDBWw2qxOrI6IiJpl1+YiKiDysv24qQlDHRGAPYUH0GBpaFMv\nHdAU6iw2C6oaq51UGRERNWu6ny6XQ69XwVBHPq/R0oid+ZnoE9kDySGJbXpt3KUZsiW8r46IyOlK\n9GXQWwwMdVfBUEc+L7PoEPQWA25Kndjm16oDm0IdZ8ASETlfdk3z+nQMdS1hqCOfZrSasD1/D3pG\ndEOn0OQ2vz5A7o8wv1BOliAicoHs2hxE+IcjMiBc7FLcEkMd+bR9RYegM+sxtR29dM3iVGr21BER\nOZkgCLhQm8Oh12tgqCOfZbKasT1/D7qFd8ENYantbidWFYNSfTlsgs1xxRER0WXKDOXQmfXcGuwa\nGOrIZx0oPgKtqR43t3HG66/FBaphsplR01jroMqIiOjXsmt5P931MNSRTzLbLNiWvxs3hHZC1/Ab\nOtSW2j4DlkOwRETOkl2Tg1BlCKIDIsUuxW0x1JFPOlTyPWqNdbi5U/vvpWsWeynUcbIEEZFz2O+n\nC+8MiUQidjlui6GOfI7FZsEWzS50CklB9/AuHW4vSKFCsDKIe8ASETlJRUMl6kz1HHq9DoY68jmH\nS7NQY6zF1E4THPaNLy6QM2CJiH6tzqjFqqw1qDPWd6idE5WnATR91tLVMdSRT7HarNii2YXk4ET0\niujusHZjVWqU6MshCILD2iQi8nQZudtxsU6DjNxtHWpnf9ERAMD3ZcccUZbXkotdAFFr1Bm1eGvn\nB5jbbRZC/YLb3cY/fngPVY3VuKPbrQ69LyNWFYNGayPqTFpuMk1EPu+x3YthsVnsjzOLDyGz+BCk\nkCA9cUSr28ksPAAbfv6yvK/4EPYVH4JcKscbY1c4tGZvwFBHHuG7nG04W3EBG+QZuL3LLe1q49uc\nzShvqIBKHog+kT0dWl/zHrCl+nKGOiLyecvSFuF/2RuRVX4cwqVQJpPIoJQq8H3pD61ux0/mB5PN\nDKtgBQAopAoMiO6D27pMc0rdno6hjtzar7/tHSw5ioMlRzvUpt5iwJ92PeXQb3qxqqb7PEr0ZegR\n0dUhbRIReapQvxAYrUYIECCVSCEIAkbEDcGsHre3ua0vzv4P+4sPQyaVwWKzwF/m1+4RG2/HUEdu\nbVnaInx46lPk1GkANH3Ti1fFol90L/jL/VvVRoOlEScrzqBYXwqrYHXKN71gRRBU8kBOliAiuiSn\nLg9KqQKPDXwIh0qzoDVq29VOvakeoxKGY1T8MOwrPtzudnwBQx25taalQpqCkkIqh8VmRWpIEm7u\nNKlN7WiN9SjUFUMulTvlm55EIkGsKgYlXNaEiAjZNTnQWwz4XddbkRqajNTQ5Ha39WC/e+z/Pav7\nbY4oz2sx1JFbO1l5BgZLA7qHd8H9Q2Ziw+md7fqW5opverGqGPxYccrh7RIReZrNmh0IVgZhZPxQ\nsUvxKQx15LYEQUBG7nZEB0Tikf73IzY8rN3f0lzxTS9WpYa++AjqTToEK4Occg4iIneXU6fB2Zps\n3NblN1DKlGKX41O4Th25rdNVZ1GgK8aUlPGQSWVil3NdzYti8r46IvJlGbk7EKRQIT0hTexSfA5D\nHbklQRCQodmBSP9wDI29UexyWqV5D1jeV0dEvkqjzceZ6nOYkDwafuylczmGOnJLZ6uzodHmY3LK\nOI/opQOAML9Q+Mv8UGpgTx0R+aaM3O1QyQMxmr10omCoI7fT1Eu3HWF+oRgWN1jsclqtaQasmj11\nROST8rWFOFV1FuOT01u95BQ5FkMduZ3s2hxcrNNgUspYKKSeNZcnNjAGZbynjoh8UIZmBwLkARjT\nhm3AyLEY6sjtZGh2IFQZjJFxnjcVPlYVgzpTPQxmg9ilEBG5TEF9MU5Unsb4pFEIkAeIXY7PYqgj\nt3KxVoPzNRcwMXkMFDKF2OW0Wdyl7cJKDRyCJSLfsVmzA/4yf4xNHCV2KT6NoY7cSoZmO4IUKoxK\nGC52Ke3yyz1giYh8QbGuFD9WnMS4pJEIVLCXTkwMdeQ2NNp8/FR9HhOTx3jsgpUR/mFQSBUo5WQJ\nIvIRmzU74CdTYlxSutil+DyGOnIbGbk7oJIHIt1De+kAQCqRIjYwmj11ROQTSvVl+KH8BMYkjoRK\nESh2OT6PoY7cQkF9EU5V/YRxSZ4/FT5WpWZPHRH5hM2anVDIFJiQNFrsUggMdeQmNmt2IEDuj7FJ\nnj8VPlalRo2xFo2WRrFLISJymjJDBY6W/YjRCWkIUqrELofAUEduoEhXgh8rTmFsondMhY+7tF1Y\nmaFC5EqIiJxni2Yn5FI5JiSzl85dMNSR6LZodl66ydY7psJzBiwRebsKQxW+LzuG9IThCFEGi10O\nXcJQR6Iq1Zd73U22Uf4RkEtkvK+OiLzWlrydkEmkmJg8RuxS6BcY6khUW/J2QiGVY7wXTYWXSWWI\nCYxGqYE9dUTkfSobqnG4NAsj44ch1C9E7HLoFxjqSDTlhkp8X3oM6QlpCFYGiV2OQ8WqYlDCnjoi\n8kJb83ZBCgkmpYwVuxT6FYY6Es3WvF2QS2WY4IXd97EqNaoaqmGymsUuhYjIYaoba3Co5ChGxA9F\nmF+o2OXQrzDUkSiqLuu+976bbONUaggQOAOWiLzKtrzdAMBeOjfFUEei8Pbu+9jApmVNSjkDloi8\nRK2xDgeKj2B43GBE+IeLXQ61gKGOXK6msRaHSo5iePwQr+2+jw6MglQiRamB99URkXfYlrcbNgiY\nkjJO7FLoKhjqyOW25e+BDQImJ3vvB4NCKkd0QCR76ojIK9QZtdhffBjDYgchMiBC7HLoKhjqyKWa\nPxiGxw5CZIB3d9/HqtScAUtEXmF7/h5YBRumpIwXuxS6BoY6cqnt+XtgE2yY7AMfDHGBMahoqITF\nZhG7FCKidqs36ZBZdAhD1AMRHRgpdjl0DQx15DL1Jh32FR3CYPUAn/hgUKtiYBNsKDdUil0KEVG7\nbc/fA4vNgimp3v9l3NO5LNTl5uZi5syZmDJlCmbOnAmNRnPFMRUVFXj44Ydxyy23YOrUqVi/fr39\nubfffhu/+c1vcOutt+L2229HZmamq0onB9lZkAmzzYKbfKCXDmha1gQAJ0sQkcfSNtZjb+EBDFYP\ngDowWuxy6DpcFuqWLl2K2bNnY8uWLZg9ezaWLFlyxTEvv/wy+vTpgw0bNuCzzz7DqlWrUFJSAgDo\n168f/vOf/+Dbb7/FihUr8Pjjj6OxsdFV5VMH6cx67Cncjxtj+kGtihG7HJdQB0ZDAgknSxCRx9p4\nfkfTl3Ev6aWrXP+12CU4lUtCXVVVFc6cOYNp06YBAKZNm4YzZ86gurr6suPOnj2L9PSmPUAjIiLQ\no0cPZGRkAADS09MREBAAAOjevTsEQUBtba0ryicH2F2wD0arCTelThC7FJdRypSI9A9HKSdLtFqd\nUYtVWWtQZ6wXuxS34qjr4oh23K2WpTtfd5ta3Om6OKKWEn0Z1v+0FX0jeyL20siDp6vesP76B3kw\nuStOUlJSArVaDZlMBgCQyWSIiYlBSUkJIiJ+nhrdu3dvbNq0CX379kVhYSGOHTuGxMTEK9r75ptv\nkJycjNjYWFeUTx1QZ9Tig5PrUKIvxYDovogP8q1/s6YZsOypa62M3O24WKdBRu42zOpxu9jluI3m\n67IhJwO3d7ml3e1syNlsb+e3XdvXjiPacGQtZysuYIPcPWpxp+viiFo+Pv0FBAiQy1wSFcgBJIIg\nCM4+yalTp/DUU0/hu+++s//s5ptvxquvvorevXvbf1ZdXY0VK1bg/PnziI+Ph5+fH+Li4rBo0SL7\nMUeOHMHChQvx0UcfoXPnzs4unTrow6NfYOvFvQCAVyYvRmp4ksgVudanx7/GpvM7se63/4BMKnN4\n+zUNdfjHwX/i8bT7ERbguQs5z/m/R2FuYZawQirHZ3e8KUJF7uFq14VIDJ76/2P+F/9GwZdfXfHz\npFl3Ivn3M0WoyHlcEuqqqqowZcoUHD58GDKZDFarFcOGDcPWrVsv66n7tfnz52Py5Mm44447AADH\njh3Dn//8Z7zzzjuXhcHW16GDzea8Xzc6OhgVFRw2AoDHdi9ucSkPuVSON8auaFebnnh9D5Ycxac/\nfYWlw59EjBNuMl535iscLs3CqPhhHerZEvva1hm1+N+FjcgqOw4BAmQSGW6M6Yfbukzzir2B23t9\n64xa/Cd7A34oPw4AkElkiFfFol90L/jL/VvdToOlEScrzqBYXwKrYINMIkW8Kg79onq2up0GSyNO\nVp5Bsb603W04qh3W4qpaymAVrFBIFRgQ3cej/38ULBbkv7wcRk0uun34sdjlALj254JUKkFkZFCb\n23RJn2pkZCR69uyJjRs3Yvr06di4cSN69ux5RaCrqalBcHAw5HI5Dh48iPPnz2P16tUAgBMnTuDx\nxx/H6tWr2xXoyLWWpS3C/y5sxLHyk7AKVsglcgyM6YvbukwTuzSXirs0KaREX+7QUPfr0JxZfAiZ\nxYc6FJrFFOoXAkEABDR96bIKVvjJlB77B8RRQv1CoDVqATQFOptgQ2pIEm7uNKnNbWmN9SjUFUMu\nlcNqsza103ly29ow6VCoK+lQG45qp7kNhVQBi83iFrW403VxVC3N19df5ufR/z9K5HIkP7ME2fPv\nhbG4GH7x8WKX5BQuGyh/7rnnsGjRIrzzzjsICQnBypUrATT1xi1YsAB9+/bFiRMnsHz5ckilUoSH\nh+Pdd9+1T454/vnn0djYeNms2VdeeQXdu3d31a9AbRDqFwJ/mT+sghVA0x9pT/9QaI/YwKZQV6ov\nQ/9ox30ZWZa2CGuOf4QCXTGAph7QgdGeHZpztXmQQILxyenYkb8XBfXFYpckOqvNirz6QgQpVPjT\ngAewv/iIPeS1Vb2pHqMShmNU/DDsKz7crnYc0Yaja7ml93hsOL3TLWpxp+viqFo6en3dgdWgh0Sh\ngFShhDQ4BBVffobEvzwpdllO4ZLhV3fB4VfXev/EWhTqSiBAQO/IHtAatXiw3z3tbs9Tr+/f9q9A\nl7DOmNd7lsPaNFnNWJj5HMw2s/1n6fHD2z0EK/a1rTXWYemBlzE8fghmdpuBFw//HXKpHIuGPAap\nxPPXSG/v9T1ckoVPfvo3Hup7D/o58EuBNxH7vevtvOH6ln/xGXQ//oDUF19C44ULkAYEwj81Veyy\nnDL86vmfluS2Hux3DxRSOZKCEzCr+20dCnSeLFYVg1KDY2fAHig5ArPNjH6RvZAWNwQAUN7guTtX\n7CzIhFWwYVLyGEglUkxJGY8iXQlOVv4kdmmisQk2bMnbhYSgOPSJ6il2OUQeK2jgjQifMAlShRKB\nPXu5RaBzFoY6chqrzYqKhiqfX4U8VhWDUn05bILNIe2ZbRZsy9uNG0I74cF+92DGDTdDKVMiWNn2\nb3XuwGA2YF/RIQxS90dUQNP2cYPVAxAVEInNmu3wocGEy/xYcQplhnJMSRnvFb2VRGIJ7NET4ZNv\nsj82nD+HhosXRKzIefhJQU5T2VgNq2C131fmq+IC1TDbzKhpdMxi2YdKjqLWWIepnSZAIpEgSKnC\n6IQ0ZJUdR5kHLnS8p/AAjFYTJqeMs/9MJpVhSso45NcX4XTVWRGrE4cgCNis2QF1YDQGxvQVuxwi\nj2TV6VC9aSOsBsNlPy/75F+o/m6DSFU5F0MdOU1zwFCrfL2nrmkldkcsQmyxWbBFsxOdQpLRI7yr\n/ecTkkdDLpVjS96uDp/DlYxWE3YV7kOfyJ5ICIq77LmhsTciwj8cmzU7fK637lTVTyjSlWByyjj2\n0hG1k/7kCVR+/V9Yqqsu+3n8Q48g9r75IlXlXPy0IKcpM1QAAIdfLy1rUmroeC/akdIfUGOsxdRO\nEyGRSOw/D1EGIz1hOL4vO4YKQ9U1WnAvB4qPQG82YErquCuek0vlmJwyFrnafJyr8c6hkpY09dLt\nRKR/OIaoB4pdDpHHCkkbgdQVK+GXePmi935JSZAFeebtKtfDUEdOU6ovR6gyBAHyALFLEZVKEYgQ\nZXCHe+qsNiu2aHYiOTgRvSKuXMpn4qVJBlvzdnboPK5isVmwPX8PuoR1QufQ1BaPGR43BGF+odiU\nu921xYnoXM0FaLT5mJQy1im7kBD5AsHatJyWMvrK238Emw21e3dDf+qkq8tyOoY6cpoyQznUKt++\nn65ZbGDTZImOOFr2IyobqzE1dcJlvXTNQv1CMDJ+KA6VZqGqobpD53KF70uPodZYh8kp4696jEIq\nx8TkMbhYl4vsmosurE48mzU7EKoMwfDYwWKXQuSRbEYjNM8sQt2+vS0+L5FKUb1pI+qPHHZxZc7H\nUEdOIQgCSg0ViPXxoddmsSo1SvXl7b43zCbYsFmzAwlBcegb1euqx01KHgspJNjq5vfW2QQbtuXv\nRlJQPHpFdLvmsSPjhyFYGYQMzQ4XVSeeC7W5yK7NwcSUMVDIFGKXQ+SRbEYj/Lt0gVIde9Vjkhcv\ngfre+11YlWsw1JFT1Jt1aLA0QO3jM1+bxali0GhtRJ2pfauyZ5UdR3lDJaamTmyxl65ZuH8YhscP\nwcGSow6bbesMxytOo8xQgUkp4675+wCAUqbAxOQxOFdzATl1GtcUKJItmp0IUqgwMn6Y2KUQeSx5\nSAjiHngIAV2v/oVRHhJy3c8eT8RQR07Bma+X68gM2OZeujiVulVbjU1OHgcBArbl727zuVxBEARs\nzduJmICoVi/XkZ6QhiCFChm53ttbl68txJnqcxiflA4/mVLscog8UkP2eZirrr8Qu81sQsVXX0J3\nLMsFVbkOQx05RfNMT19fo65Z3KVQ15776n6sOIVSQzmmpk5o1fIWkQHhGB47CPuLj6DWWNfm8znb\n2Zps5NcXYWLKmFYv1+EnU2JC0micqT6HPG2BkysUx+a8nQiQB2B04gixSyHySIIgoHTtRyj54L3r\nHiuRK6D7IQvGwkIXVOY6DHXkFGX6CihlSoT5hYpdilsIUqigkge2uafOJtiQkbv90iK0/Vr9uskp\n42ETbNiev6etpTrdVs0uhCpDMDR2UJteNzoxDYHyAK+8t65YV4rjFacwNnEkAuT+YpdD5JEkEgkS\n//Ik1HPmturY1BUrEXnLdBdU5joMdeQUpYZyxAZGe+U9C+0hkUjs24W1xcnKMyjWl+KmVvbSNYsO\njMQQ9UDsKzoMrcl9NuPOrcvH+dqLGJ+cDoVU3qbX+sv9MS5pFE5WnkFBfbGTKhTHlrydUMqUGJs0\nUuxSiDyaIiISfknJrTpWIvW+COR9vxG5hTJDBSdJ/ErTDNiyVs+AFQQBGbnbER0QiUEx/dt8vikp\n42CxWbAjv+Vp/WLYmrcLgfIAjGrnRICxiaPgL/PHZi/qrSs3VCCr7DhGX7pvkIjarj7rKEreXwOr\nQd/q11j1ehStXoX67484sTLXYqgjhzNZTahurGGo+5U4lRp6iwE6c+s+dE5V/YQCXTGmpIxv1yK0\nalUMBqn7Y2/RQehMrf+gc5ZiXSlOVJ7GmMSR8G/nEGOgIgBjk0bix4qTKNaVOrhCcWzN2w25VIbx\nSaPFLoXIY1m1dTCVl0Pq3/rF7qUBAbDqdLCZjE6szLUY6sjh7NuDcebrZZq3C2vNfXWCICBDswOR\n/uEYGntju895U+oEmK1m7CzIbHcbjrItfzeUUkWHhxjHJY2Cn0yJLR6yc8a1VDfW4HBpFkbED0Wo\nX7DY5RB5rLBxE5C8+Nk2DalKpFIkL34WoSPTnViZazHUkcM1L2fCma+Xa74epa0IdT9Vn0eetqDd\nvXTN4lRqDIjpiz2F+6E3G9rdTkdVNVTjaNmPGJkwrMNDjEEKFUYnjEBW2XH7e81TbcvbAwkkmJg8\nRuxSiDySIAgwFjXNYO3IPXLtXRje3TDUkcOVGioggQTRgVFil+JWwvxC4S/zsy/3cjVNvXTbEe4X\nhmFxbZsh2pKpqRPQaDViV8G+DrfVXjsK9kICCSY4aIhxQvJoyKVybHHznTOupc6oxYGSIxgWOwgR\n/uFil0PkkQw/nUHe0r9B9+Oxdr3eXF2F3Gee8potwxjqyOHKDOWICoho8+xGb9c0A1aNkuv0Lp2v\nuYicujxMThkLuQOuYUJQHPpH98Huwn1osDR0uL22qjfpcKD4CIbG3ohw/zCHtBmsDEJ6wnB8X3YM\nFYYqh7TpajsK9sJqs2JSylixSyHyWP6dOiP6zt8jsHefdr1eHhYOv6RkyIKCHFyZOBjqyOE48/Xq\nmpY1ufbwa4ZmO0KVIUiLG+Kw896UOh4NlkbsLjjgsDZba1fBPlhsVkxy8BDjxOSmxYu3euC9dTqT\nHplFhzBYPQAx7NEmajdZQADCJ0+BVNG+vZIlUini//AIVO0Mhe6GoY4cyibYUG6o4CSJq4hTqaE1\n1V/1/rbsmhxk1+ZgUspYh27onhyciD6RPbGrIBONlkaHtXs9DZZG7C06gAHRfaBWOTboh/qFYGT8\nUBwqzUJVQ41D23a2XYX7YLKaMCV1vNilEHmsiv/+Hwxnf3JIWzaTCTaz2SFtiYmhjhyqurEGZpuF\nkySu4ufJEi0PwW7W7ECwMsgpG7pP7TQBeosBe4sOOrztq8ksOogGSyMmp45zSvuTksdCAonb7nPb\nkgZLA/YU7seA6L727eOIqG2sej3qDx1EY87FDrdlLCzAxQV/hP7EcQdUJi6GOnKo5rAS6+BeGW8R\n27wHrOHKIdicOg3O1mRjYvIYKB3YS9csNSQZvSK6Y0f+XhitJoe3/2umS0up9IzohuTgRKecI9w/\nDMPjBuOgm+5z25I9hU1B9yb20hG1m0ylQuqKlQibOLnDbSlj4xA2cTKUas//ksVQRw7VvEZdTCCH\nX1sS4R8GhVTRYk9dRu4OBClUSE9Ic9r5p3aaAJ1Zj0wX9NYdKjmKepMOk1Oc00vXbErKONggYFve\nbqeexxGMVhN2FuxF78geSApOELscIo9kbWiAIAiQKhSQKpUdbk8ilyP6d3fCLzHJAdWJi6GOHKrM\nUI4ghYrbHV2FVCJFrCrmigWINdp8nKk+hwlJo+En6/iH1NV0Dk1F9/Au2J6/Byar8+4fsdqs2J6/\nB51CktE1rLPTzgMAkQERGBp7I/YXH0ad0X32uW3JvqJD0JsNuCl1gtilEHms8nVrUbByhUPXlhME\nAaaSYtiMnr27BEMdOVSpnjNfryc2UH1FT91mzQ6o5IEYnei8XrpmU1Mnot6kw/5i563LlFV+HFWN\n1ZicMg4SicRp52k2JWU8LDYrdhTscfq52stsNWNH/h50C7sBnUNTxC6HyGOp+vdH8OChDv1saTh/\nDppnF8NwzjETL8TSplBns9lQXu7ZK7iTc5UZyhHLma/XFKeKQY2xFg2XZqEW1BfhZOVPGJeU3u49\nUduia3hndAnrhG15u2F2Qm+dTbBhW95uxKnU6BPV0+HttyQmMAqD1QORWXgQ9SadS87ZVgdLjqLO\nVM9eOqIOChmWhvCJkxzapn+nzlDfcy/8U1Id2q6rtSrUabVaPPHEE+jXrx8mT266KXHHjh1YtWqV\nU4sjz6Iz6aEz69lTdx3Nk0jKLu0skaHZgQC5P8YmjXBZDVNTJ6LOpMXBku8d3vbpqrMo1pdicso4\nSCWuGwy4KXU8zDaLW+xz+2sWmxXb8nejU0gKuoXfIHY5RB7JXFWFuv2ZECwWh7ctVSoRmj4G8lDH\nLJAullZ94i5duhRBQUHYuXMnFJcW+Bs4cCAyMjKcWhx5luZJEpz5em32GbD6chTpSnC84hTGJo5C\ngDzAZTV0D++CzqEp2Jq3Gxab4z4gBUHAFs0uRPqHY1BMf4e12xqxqhgMjOmLvYUHRN3ntiX78o6g\nurEGN6WOd8lwNJE30h7cj7JPPoZF65yZ7laDAbpjP3j0fXWtCnUHDx7E3/72N8TExNg/kCIiIlBV\n5Znb85BzNPc8safu2qL8IyCXyFCqL8dmzQ74y/wwLmmUS2uQSCSYmjoRNcZaHC7Jcli7F2pzkKvN\nw8TkMZBJZQ5rt7VuurTP7ebcHViVtcYtJk7UNNbig6NfIC5Qjd6RPcQuh8jlKtd/7ZB2BKsVKc8+\nB0VEpEPa+7XGnIsofns1Gi5ecEr7rtCqUBccHIyamstXbC8uLkZ0NO+dop+VGsqhkMoR4aD9Pb2V\nTCpDpH8Edhfuxw/lJzA6cQRUikCX19EzohtSgpOQodmOJTv+3uEAVGfU4r2Tn0AlD8RwB25x1hbN\n+9zuKTqAi3UaZORuE6WOX1r301cw28wI9Q9lLx35pOoN69t0vGCzwWY0QrDZAAA2swlWgwHVG9Y7\nddmRgK7dkLjwaQR07ea0czhbq0LdHXfcgQULFuDQoUOw2Ww4duwYnnrqKcyaNcvZ9ZEHKdOXIyYw\n2qX3UXkqq2CD2WaGFFJMSBotSg0SiQRTO01AjbEOZysvdDgAfXV+PRosDYgOiHTK4smt8djuxThe\ncQpWwQoBAjKLD+GRnQvx2O7FotTyyM6FOFfT9K3/bPV50WohElvOwifsPWCGsz/hwoJH0HBpNwjd\niR9xfv69aNTkNj0+loULjzwEU1FR0+MfspDz1z87vUapnx8Cu3Vv9z6y7kAitGKhF0EQsHbtWnz1\n1VcoLi5GXFwcZs6ciXvuucejvnlWVelgszluXZtfi44ORkWF+MM9Yll6cCWSgxNwf5+7nNK+N1zf\nx3YvbvEeNrlUjjfGrnCLWqSQYFTC8Fa3s6/oEGy48v8rMX6nOqMW/7uwEVllxyFAgEKqwIDoPrit\nyzSE+gW7vJZ/nvoMF+ua/lCJWYs384bPBXfW3utbuf7rFnvoIm6ZjpBhw1G7c0fTLg4xMTCVlkB7\n8ABCx4yDIiICptJS6I5lIWRkOmp37bhqO1HTb2vX73Qtpopy6I4eRdjEiZAqnLdmKHDtayuVShAZ\nGdTmNuXXO8BqteLrryACF+0AACAASURBVL/G7NmzMW/evDafgHyD2WpGVUM1hqoHil2KW1uWtgj/\nu7ARP5afgkWwQCGVY0B0X9zWZZqItZyERbACAOQSGZQyJX4oP9Hqdvzl/jBZTfY2fhleXC3ULwT+\nMn8Il0Km2WaGv8xPlBAVogy232eqkMphsVlEq4XI1aKm34ao6bfBWFSEvKXPoNuHH1/2fMzsn7/8\nK2PjEHXbb3/xOBYRU39zWTsAcP6BeVe042imwkJU/vcrBHTvjoDOnjdT/bqhTiaT4eWXX8bvfvc7\nV9RDHqqioQoCBKg58/WamkOHVbBCLpXDYrOK9of+51psUEgVsNgsSIsbglk9bm9zW1+c/R/2Fx+G\nTCoTPbzUm+qRnpCGfG0hivWlqDVpRanjbHU2dGY9uoZ1xvyhv8eG0zuhNYpTC5FYSta8JXYJbRLY\nuw86//0NyENDxS6lXa4b6gBg3Lhx2LlzJ8aP5wbU1LJSznxttXpTPUYlDMeo+GHYV3xY1D/0zbXc\n0nt8h0KHO/1OD/a7BwBwvuYC3jj2PnpFiHPTc4ZmB8L8QvGnAQ8gLjwcs7o7fqiIyN3FzJkL7WHH\n7DUdcct0h7RzLVKl0iH7yYqlVaHOaDRiwYIFGDhwIGJjYy+7j+6VV15xWnHkOcr0TWvUqQOjRK7E\n/TWHDgCi/6FvriU6PLhDtbjT79Ss66XtuLbm7cbI+GEuXWIluyYHF+tycUfX6ZBLW/UxS+SVAnv2\nQmDPXg5pyxn30LWkIecitPsyETP7LkjknvX/b6uq7datG7p189wpvuR8pYYyRPiHQ+nEzeiJ2kIi\nkeCm1Al45/hHOFL6A9LiXbfMymbNDgQrgjAifqjLzknkbqw6HUwlxfBLToHUz0/sclrNUl2F+qzv\nET7lJijVsWKX0yatCnV/+tOfnF0Hebj/Z+/OA6Mo7zeAPzN7J7u5T3JCIBDuUwGxCoIggoGqaLFa\nasF6t/pDC9QLWqlYa1vrLVWseFeBBpFDDhHkklvCIZAQjpybc7NJ9pj5/RGIhkDOnZ3dzfP5R4fd\nfffJy7D57jvvvG+hvRixQVy3kHxL74ieSLIkYO2pjbgyfohXltvJrczDkbIfMCVtompLuxD5AvvR\nw8h/7RUkPzUfxuQUteO0mnnQEJgHD4Ug+t/yXK0eV9y+fTtWrFiBoqIixMTE4KabbsKIESOUzEZ+\nQpIlFFYXoXvClWpHIWpEEARMSBmDt75/D3uKDmBo7EDF33N17gYEaU24ug3LwhAFoqD0Xkj4/aN+\nN9olaLy/G46ntKoM/fTTT/HII48gOjoa48aNQ0xMDGbPno1PPvlE6XzkByrqKuGQnLxJgnxS/+g+\niAuOxZrcDZBkSdH3OmvLx8GSbIxOGgWj1qjoexH5Oo3FguC+/f3q0usFVXt248zfX2jY1cJftGqk\nbvHixXjnnXfQq9eP+xbecMMNePjhhzFt2jTFwpF/uHDnaxwvv5IPEgUR41NG493sj3CwJBsDovsq\n9l5rcjfAqDHg2sSrFHsPIn9hP3YUotHoV5deL5BdTkg1NXBXVfnV8iatGqkrLy9HWlrjRfi6deuG\niooKRUKRfymoPr+cCdeoIx81JGYAokyRWJ27Hq3YRKddCquLGvbyDVJhL18iX1P84fuwLvtM7Rjt\nEnLFcCTPe9KvCjqglUXd4MGD8dxzz6GmpgYAYLfb8fzzz2PQIO4eQPU3SZi0Jlh0bd/ShMgbNKIG\n41NGI6/qLA6XHlPkPdac2gitqMWYpKsVaZ/I38T/9j5E3XKb2jE6lVYVdfPnz8fRo0cxdOhQjBw5\nEsOGDcORI0cwf/58pfORHyisLkJcULRf7QNMnc8VcYMRbgjDlwqM1llrSrGrcC9GJVwJi55fboiA\n+u2/DAkJasdot7L165D71B8VG91XQqvm1MXExGDp0qUoKChouPs1Ls6/7mYh5RTai5AR2VPtGETN\n0opajE25Bp8eW4Hj5SfRI9xz+zquzdsEEQLGJl/jsTaJ/JmrshL2I9kIzugDjcU/9zvWhobBmJoK\nua4OgtE/bnxq1Ujdli1bkJOTg7i4OPTv3x9xcXE4efIktm7dqnQ+8nE1rhpUOKoQxztfyQ+MjL8C\nFr0Zq3M3eKzN8roKbD+3C8PjhyLM4F/zb4iUUpubg4I3X4ejqFDtKO1mGToMcXfPgugnBR3QyqJu\nwYIFCA4ObvRnwcHBWLBggSKhyH8UNGwPxjtfyffpNTpcl/QzHCn7ATkVeR5p86u8ryFBxriU0R5p\njygQBGVkIGX+szAkJasdpcMkh0PtCK3WqqLOarUiJqbxSExMTAyKi4sVCUX+o9DOO1/Jv1ydMBzB\n2iCsObW+w21VOWzYcnYHhsUOQpQpwgPpiAKDqNPDkJAAUe/fW0cWf/whcp+Y4zfz6lpV1CUlJWHb\ntm2N/mzHjh1ITExUJBT5j0J7MTSCBlFG/kIj/2DUGjE6aRQOlhzGmapzHWprw+lv4JJcuJ6jdESN\n2PbtRfXBA2rH6DBTrwyEXjMacLvVjtIqrd779aGHHsItt9yCpKQknD59Gp9//jkWLlyodD7ycYXV\nRYgOioJG9N9tVajzuSZxJL7K+xprTm3Ab/r+sl1t2J12bD7zLQbF9EMcR6qJGildtRKCXo/gfv3V\njtIh5gEDYR6g/PaCntKqkbqxY8fi7bffht1ux9dffw273Y7Fixdj7NixSucjH1dgL+ZOEuR3gnRB\n+FniSOwtOtiweHZbfX3mW9S66zAh9ToPpyPyf4mPzkb8zHvUjuERsssFR3H7Pie8rVUjdQDQv39/\n9O/v3xU3eZZbcqO4pgQDFdx2iUgpY5KuxsbTW7D21Ebc1bttC6TWumqx8fQW9IvKQII5XqGERP5L\nNJogGk1qx/CI/LdeR11eHrr+5Xm1o7SoVSN177zzDg4fPgwA2L9/P6699lpcd9112Lt3r6LhyLcV\n11ghyRLvfCW/ZNGbMSrhSuwq3IuSmtI2vfabs9tR7bJjfApH6Ygu5qqoQOnqVXAGyM2UYaOvQ9Qt\nt/rFzRKtKuqWLFnScFPECy+8gBkzZuDee+/lnLpO7sKdr5xPRP5qbPI1ECFg3amNrX6Nw+3E+tOb\n0Su8B7qG+v9yDUSe5sg/h5L/fgKntUTtKB4R1CsDliHD/GLXpFYVdVVVVbBYLLDZbDh69CjuvPNO\n3HrrrcjJyVE6H/mwwvNr1MVwpI78VJghFMO7DMP2/O9QXlfRqtd8m78TVQ4bJqSOUTgdkX8K6pWB\ntJdehTGtu9pRPMZZXIyaE8fVjtGiVhV18fHx2LNnD1atWoWhQ4dCo9HAZrNBo+Edj51Zgb0IYYZQ\nmLT+s9o20cXGJV8LCTK+yvu6xee6JBfWndqEtNBUdA/r5oV0RP5JExQEUadTO4bHFL7/HxS++7ba\nMVrUqhslHn/8cTz88MPQ6/V46aWXAAAbN25Ev379FA1Hvq3QXsz5dOT3okwRGBY7CFvO7sD4lDGw\n6M2Xfe6Ogt0or6vAHb1u8YtLMURqqNz2LSSnA2E/u1btKB4TNfVmCFrfL1JbVdRdc8012LJlS6M/\nmzBhAiZMmKBIKPJ9siyjoLoIV8QNVjsKUYeNTxmNnQV7sOH0N8hMu+GSz3FLbqw9tQnJlgRkRKR7\nOSGR/6jauR1uuz2gijpjSqraEVql1UuaXEwXQMOq1HaVjirUumsRG8yROvJ/scExGBTTD5vPfItx\nydcgSBfU5Dm7i/ajpMaKe/rdxVE6omYk/O5RSE7/2S+1taqzD0F2OGAeOEjtKJfVqjl1RBdruPM1\niHe+UmCYkHodat112HRma5PHJFnCmtwN6BIch35RvVVIR+RfRJ1/7/l6KaWrVsK68n9qx2gWizpq\nl4Lzd75yTh0FigRzPPpF9cbG01tQ66pt9Nj+4kMosBdhfOoYiAI/Nokux1VViaIPlqLudJ7aUTwu\nbsbdSHpsjtoxmsVPJ2qXQnsRDBo9wgyhakch8pgJqWNgd9Xgm7PbG/5MlmWsyV2PGFMUBsdwVx2i\n5rjKylD57Ra4ysvVjuJxuqhoiAaD2jGa1e6iTpZl7Nq1y5NZyI8UVBchNiiGc4sooKSGJKNXeA+s\nz9sMh9sJADhkPYLTtnO4PmU0R+mIWmBMTkHav15DUJ/A3D6yfON6VG77Vu0Yl9XuTyin04m77rrL\nk1nIj9QvZ+J/8+lKVixTOwL5uAmp16HKacO353ZClmWszt2AcEMY7/QmaiVBECCIgfkFqHLHdtj2\n7vbZ3yXN3v26fPnyyz7mdDo9Hob8Q62rDmV15YjzwztfS7NWICpzqtoxyIf1CO+GtNBUrDm1AVvP\n7cC56gLclj4FGpGLrRO1pPzrjXCVlSJqys1qR1FE4iOzIRoMODZzhk/+Lmm2qJs7dy769OkDvb7p\nXSz+sLEtKaOo5sJNEv43UgcARR+9j+B+AxDcpy9kWYazIB/aiMg2zZUoWbGsw/+gPdGGJ9sJNB3p\nlwmp1+GV/f9GpaMKOlGHEfHDPJyOKDDV5eXBkX9O7RiK8fU5dc0WdSkpKZg9ezaGDx/e5LG6ujoM\nGDBAsWDkuy7s+RoX7B9FXcmKZSjNWtFwXP7VOpR/tQ4RkzMRPm48cp+ch6hbb0PE+BvgttuR/9rL\nCL9+AoL79YfkcMB+6CCMXdOgDQtraMMTI36eGjXk6OOltbdffrdpHlySq+HYKTnx+6//CK2oxT+v\nXejJiEQBJ/bOX6kdQTEX/y45NnMGACBicqbPfAY3W9RdccUVOHny5CWLOlEUMWwYv73+1NxXt8Dp\ncDf6s2EZMRgzOBF1Tjf+8cn+Jq+5ql88RvWPR5XdgVeXfd/k8dGDE3BFRixKK2vxVlZ2k8fHX5GM\ngT2ikG+txn9WH23y+KSrUtEnNQJ5hVX48Ksfmjx+8zVp6J4YiuNnKvDZ1yeaPP6LsT2QHGvBodxS\nrNyaCwAoNx8AzALeXZ6HX00wIz4yGPt+KMGanU1vYZ81uTciQozYebgQG/ecbfL4/VP7whKkx5YD\n+dh6ML/J47+fNgAGnQYb9pzBvhPWJv37hzvq5zmt3pGH/cdLGj2m04l4dNpARGVOxbdRA3Hm++MY\nu+t9LBv9MCDLMLv1uE+jQdys3+LrAgHZ7++BsbYKV54txfoNx1CXq8GvBofh3Cv/wokRmThgTIGl\n2oqR+1cgCMCSL4/g9t5GFLz1Bnb3ug7HhQiEV+RjyJGv8F3G9bD0SMOUZBlF/1mCb3uNw2khDFFl\npzHw2Cbs6DsRYwFUf38AxR99iI09b0CRaEZcyUn0PbEV3/bPREpGCkYbS2Bd9jm+SL8RNk0QEgqP\nISN3BzYPuhkZvZMxUjoDAFj0/p4mfdeWc+/FS5y7vnju/dRdE3pe9twLqyzE6PP/39ZzL1a8EWWW\nvXAEn4VLdkMDDQz2RIRXDWzUz6059wDgf1tzcPxcZaP+NZt0eODn9dss/nfTCZw4W9Ho9eEhBtwz\nuQ8A4IOvjuF0oa3R47ERQZhxQy8A9edhYam90eNJsWZMH1u/68WbWYdQVlnX6PG0hFDccm0aAOCV\nzw/CVtN4Ok1GajhuuqorAODFT/bB6ZQaPT6gexQmXJkMoOPnXkc/9178dH+Tc1fNcw/w7OfersNF\nTR5vy7l3OLes0eNtPfcKymoa9W/nPvdSMP4PL2BgjygcmzkD6YuXNHm92pot6hYsWHDZx3Q6Hd57\n7z2PByLf59RWQes2Q4AfzTGSJFy755Mfj8/ftSsaDAi5cgTqNp0AbBWoNVrw9ZBpAIBwALqYaCQ/\n8Qz2HipHr73fIiN3Z0MTIz97DnmfAbr4LnBr9YAbcGn0qDBHwa2p33FFNBqhT0yCW6MHJCCx8Bgs\n9jKM3fk+AODsP14EACQa96Go6yg4tYb615+fv6UJNkOfmATp/LFDZ0SFOQo98vai29bFKDifZerG\n+j2ZD6degSNdm34J6yx65Wxv9Hd0bOYMhAHo1YZ+0UgmiLIOblmCVtTCJbkgyjpoJJNCqYkCg85Z\ni7RvVsOeOBVBvTLUjtMpCXIzk+OKi4sRHe1/k+Evx2q1QZKUmwsYHW1BcXGVYu37imd3vIhIUzju\n7f9rr75vR/pXdrlQfXA/bAcPIu6uGR3O4olvaZ5ow2m1IucP/9fhdgLt3C1bvw7FH76P7q+92a6V\n7d888C5CDCEY1eVKbDm3A5V1lbinf/svKwVa//oS9q2y2tK/jqIinHv5n4i6+VaYBwxUOJm6PDGX\nubm+FUUBkZHmNrfZ7Ejd+PHjsWfPj8ObDz74IF5++eU2vwkFDkmWUGQvRu/InmpHaRNBq4V50BCY\nBw1RO4pH6SIj1Y7gU1yVlajc+g3CRo9B8Yfvt3urop8WcLf39I25MkS+Th8Tg9QFz6odwyt8ZQ7d\nxZpdSObiQbydO3de5pnUWVhryuCS3X5352t19iE4i4s91l7E5EyfaAMAgnr38enFML2pet9elHz+\nX7jKyxExORPOUivqzjWd00REFIiaLeq4WwBdrNBeP2nXX+58BQBZkpD/5mse3YjZE9/SPPVNT6qp\nQfX3Bz3Slr8L/dk1SF24CPq4eEROzsSZ559D0QdL1Y5F1CmUrV2D/H+/qXaMTq3Zy69utxvbt29v\nGLFzuVyNjgFgxIgRyiYkn1JwvqiLDfKjuZaCgKTH50EQA/NLSuLjc9p9mTGQyC4XBK0W+uj6LxyC\nKCJ2xt3QBdC8YCJfJjnqINntLT+RFNNsURcZGYl58+Y1HIeFhTU6FgQB69evVy4d+ZzC6iJYdGYE\n64LUjtJqgiDA0KWL2jEUw4IOcNurcerpJxB1yzSEXPnjF03egUfkPZGTblI7QqfXbFG3YcMGb+Ug\nP1FgL0asn20PVrljO7QhIQjK6K12FEXIkoTC95bAmJyKsNFj1I6jCtnhgKlHT+jjmxbv7qoqFH30\nPkJGjkJwgG4yTkQEtDCnjuhihfYiv7tJwrr8c5R/vUntGIoRRBGukhK4KitafnKA0oaFI/6ee2FM\nTmnymGA0ou7UKThLSi7xSiLyBKmuDqf+9Ayqdn+ndpROrdmROqKfsjmqUe20I86f5tMBSJn/54Cf\n55H4f4+rHUE11Ye+hz4+HrqISy/vIup0SFnwLASR32GJlCLV1kBjNkPQ6dSO0qnxU45areEmieBY\nlZO0jajXN9q3lQKH7Haj4O3FKHq/+d1tLhR0dWfPNFmqiYg6ThsahsRHZsPcn3vCq8lrRV1OTg5u\nu+02jB8/Hrfddhtyc3ObPKe4uBj33XcfJk+ejBtuuAErVvy4ca7b7cb8+fMxduxYjBs3Dp9++qm3\notN5hdXnlzPxo5G68o0bUP71RrVjKE5yOpH33LMoW7ta7SheJWg0SJ73BKJvvb3F51Z/fwCnnn4C\n1QcPeCEZEZH3ea2oe/rppzF9+nSsWbMG06dPx1NPPdXkOc899xz69u2LrKwsvP/++/j73/+O/Pz6\nzY6zsrKQl5eHtWvX4uOPP8a//vUvnDlzxlvxCfUjdTpRi3Cj/4x62fbtQfX+fWrHUJyo00EXEQkx\nOFjtKF6ni4yCPi6uxecF9eqN6Gm3w9Qj3QupiDqX0lUrkfeXP3MkXGVeKeqsViuys7MxadIkAMCk\nSZOQnZ2N0tLSRs87cuQIrr76agBAREQEevXqhS+//BIAsGrVKtx6660QRREREREYO3YsVq/uXKMS\naiu0FyMmKBqi4D9X7RMfmY34+x5UO4ZXxN9zL0KvulrtGF5TvnE98t96A5LT0arnC1otwq+fAI3J\npHAyos5HY7FAHxPLTQtU5pXfzvn5+YiNjYVGowEAaDQaxMTENIzCXdCnTx+sWrUKsizj9OnT2Lt3\nL86dO9fQRpefrDUWHx+PgoICb8Sn8wqrixDnZ3e+AvWjWJ2FLMuQXS61Y3iFVFsHd3V1m9fpqzlx\nHIXvLeGIApEHhV59DeJ+M0vtGJ2eT939OmfOHCxcuBCZmZno0qULhg8fDq3WcxEjI80ea+tyoqMt\nir+HGhwuB6y1ZRidNkLVn7Et733m8+VwlJai28y7FUzkO1y2auy5/0Ek3PxzJGRObvPr/e3cjb7r\nNsiy3OaRgcIDZag5eAAhqIUx2ntfUvytf/0J+1ZZ7F/leLpvvVLUxcfHo7CwEG63GxqNBm63G0VF\nRYiPj2/0vIiICLzwwgsNx7NmzUJaWlpDG+fOnUP//v0BNB25aw2r1QZJUu7beXS0BcXFVYq1r6az\ntnzIkGFBqGo/Y1v7t+JsIVxWa8D+nVyK+YoRcIXHtPln9qdzV3a5UHfu7CXXpGsNoc9gJD87AFWC\nHlVe+pn9qX/9DftWWa3pX9nlwsk/zEbkTZkIu2a0l5L5v+b6VhSFdg1EeeXya2RkJDIyMrBy5UoA\nwMqVK5GRkYGIiIhGzysrK4Pr/KWjbdu24dixYw3z8CZMmIBPP/0UkiShtLQUX331FcaPH++N+ASg\noPrCnq/+c/k15vbp6PLAQ2rH8Kro234RsDtnXFC5/VvkLXgatTkn2/V6QaOBqNNDlmW4Kso9nI6o\n85GcTpgHDIAuMkrtKJ2e1y6/PvPMM5gzZw5effVVhISEYNGiRQDqR+Mefvhh9OvXDwcOHMCzzz4L\nURQRHh6O119/Habzk5ozMzOxf/9+XH/99QCABx54AElJSd6K3+kV2osgQEBMEP/R+jp3VRWgEaEJ\nCsw7Yc2Dh0J2SzCkdu1QO/lvvAZHQT5SnprPhYmJOkBjMiH2rl+rHYMACHInmi3My6/t986hD5BT\ncQoLRs5VLUNb+teatQI1x44i4ZHZneoXtrO0FDmPP4qY6b9E2JixrX5dIJ+7l2Pbtxfu6mqEjBip\n+DnSGfvXW9i3ymrV5dd2zG0lP778Sv6voNq/9nzVmM3QRkR2qoIOALTh4Yi+7Rcw9Qq8S7CyJKHg\n7cWoOXHcI+2ZBw5C6FWjOt05QuRppSv/hxP/9/tOc+e9L/Opu1/JN0myhEJ7MdLD09SO0mpho69T\nO4IqBEFA+LjAnGvqLC5C9aHvEezBbYhkSULVjm3QWCwI7tvfY+0SdSaGpGRYhl0BwYOrVVD78G+A\nWlRWWwGn5ESsn2wPJrtcgEbTaS8HyC4Xak/lQhcdA21IiNpxPEYfG4euzz0PQePZj63SVV/AkJTE\noo6oncwDB8E8cJDaMQi8/EqtUGj3rztfS1evwsnZj0BytG6ngUDjLC7C6b/8GbZ9e9SO4jGuigrI\nsgxRp/fo5VJBFJHw6GOIm/lbj7VJ1NnwsqvvYFFHLSq0FwMA4oL9o6gzpqQiZPgIiPq27TQQKHRx\n8ejywEOwDB6qdhSPkGUZZ//xN+S//ooi7evCwyGIIiSnA7IkKfIeRIFKliQcf+g+WLNWqB2FwMuv\n1AoF9iIEaU0w6/xjiYzgfv0R3K/zXkoTBAHmQUPUjuE5soywsddDExSk2Fs4iotwetFCRN96O0Ku\nHK7Y+xAFGtntRsTESTD1SFc7CoFFHbVC4fk7X/1hjprbXg3IgCbYPwpQpbhtNtj27UVw337QhoWp\nHadDBFFE6FWjFH0PXWQUgvv0gy6K6zAStYWo0yFycqbaMeg8Xn6lFhXYi/zm0mvlli048fsH4aqo\nUDuKqlzlZShc8m/YDx9SO0qH2I8dReW2rZDdbkXfRxBFxP36NzCldVf0fYgCjVRXB8npVDsGncei\njppld9pR5bD5zZ2vQb17I3ra7dCGhqodRVX6LglImf9nWK4coXaUDqncugUlyz4HvLRGulRbg7L1\n6xQvIokCRfmG9Th+/z2QamvVjkLg5Vdqgb/dJGFITIIhkdvHCaIIQ0Ki2jE6LPZXv4arrNRr619V\nZ2ej+MP3oY/vguDefbzynkT+zJSejqipN0M0GtWOQuBIHbWg4HxR5w8jda7KStTmnOQoy3mO4iIU\n//cTuMr9c9N62eWCIIpe3STcPHAQkp+az4KOqJVMad0RMXGS2jHoPBZ11KzC6iJoBQ0ijRFqR2mR\nbe8e5D27AM7iYrWj+ASp2o6ydWtQd+a02lHazH7sKHLmPobavFNefV9BFGFMTgEALm9C1ArOUivX\nqfMhvPxKzSqwFyE6KAoaUaN2lBaZBw+GxhwMXWys2lF8giE5Gd1fehWiwaB2lDYTdToYUrtCHxun\nyvuXb9qAiq83InjAIERN+XmH28v78GOYxk70QDIi3yHLMk499UeEjByFmOm/VDsOgSN11ILob773\nm50ktJYQWIYM84ulV7xBEEWvF3QlK5Z5pA1j125IeOBh1QpSbXgE9F0SULryfx5p7/RHn3ikHSKf\nIsuIvv0OWK64Uu0kdB6LOrokWZbhklzot7cEcf4wn66iApXbtsJdXa12FJ9Sm3MS5175F1xVlV55\nv1IPrCpfmrUCUm2NB9K0n3nAQMTPuhcAUPTBeyj9clXDY4XvvYuytasbjgvefRtlX6378fjtt1C+\ncUPDcf7iN7yQmMj7BFFE6KirYereQ+0odB6LOoIsSXAUFkA+v2xE2fp1ODn79yiuPn+ThB/c+Wo/\nko2Cf78Fp7VE7Sg+RXa7UXcmDy5rqdfes/LbrSj8z5KG44pvvkbh++81HJd/vQlFH77/4/HG9Sj+\n+EMAgLuq6vyf/VgUeVvJimU4NnMGjs2cUZ9lw3qUfPZJwyikq7wMrvM5AcBVVga37cdjZ2kp3NW2\nhnaqtm8DgIY2PTGaSeQL3FVVcBQXcf6pDxFk2UsLQPkAq9UGSVLux42OtqC4uKrlJ3pByYpliMqc\nesnH3FVVsB/ORlC//tCYTCjfuAFF7/8HXRf9DbrISOS/9Qaqdmxr8rqIyZmXbdMbmutfWZJQd+Y0\nDIlJHt3w3d/Jstyqy9EdOXdLViy75AjdhfOlZNlnsB/ORvK8JwEAxf/9BLUnjiPpD/MAAEUffwjb\nnt1wXaIgV/ucOzZzBtIXL+lQG7Ik4Yd77u5wO3RpvvS5G4ia69+yr9ah+KP30e3Fl6ANCfFyMv/X\nXN+KooDISHObDSbBMAAAIABJREFU2+SNEgGqNGtFwy9DZ1kZytevQ8jIUTB06YLavFPIf/M1JP7f\n4wjK6I2gPn0RO+PuhnWG4mf9FvGzfovlP3yB3os+hbZXOrrNnqfmj9Oin961SD/yxvzCqMypcOSf\nQ82xo3BXVjYpXqKm3gxMvbnhOPqWaY0ej7ntF4i57RcNx54opHzJhS8ZdWfPQB/fhV86KGAE9+0H\nccbd0Fgsakeh8/jpEoAurEtWtWd3/R+4XShbtwaO80tbmLr3QPKTzzRswKyPiUHoqJ812S91f0n9\nFlMHRiR4KXn7uKuqULL8MziKi9SO4pNsB/Yh96k/1u+Lq5Con9+K2DtnKNa+WiI8tKdlzHWjcerp\nJ2D7bpdH2iPyBfq4OISO+hlvTvMhHKkLIBdfBst/9V/IR/0vpu4vvw5RpwMAiAYDjCmpl23nd5vm\nwSXVrzu0vW8QdtQdwroNj0MravHPaxcq+SO0S92Z0yhd9QWC+/QHon1//p+3aUzB0EZEwl1dDU1Q\ncMsvaAd9TAz0MTEeKYI8VUh5gqcu/XZ/8H7I0fEI6tPXI+0R+YLa3Fxow8KgDQtTOwqdxzl1HuRL\nczs6cgmroq4SSw59iGPlJwAAFoeIicd06D3hdkSlq7fSfnP9K9XWQNDpIWh8fz09X9Tec7fu3DmU\nrfkSUVNv5gd7M3zpsyHQsG+V1Vz/nnjkIZgHDUbsXb/2cqrAoMScOl5+DTCO4iI4ijp2GTJEb0F+\ndSEAQCtq4RDciDxRBO25Qk9EVIRoNLGga4ES26fV5eWiev8+9n0r1Zw8gcL3/4NO9F2aAljcrHsR\neu0YtWPQT7CoCzBlX36BU/OfRPiN7d+L75D1CKqcNnQP7YrHhjyIK1JHYtuvrkSYD/7jlWprcO71\nV1Bz4rjaUXxaxeavceJ3D0CqrfVouyHDR6Lr83/jROlWcuSfg+277+Aq4dI75P+Ce/fhDWo+hnPq\nAkzEpEwEDxwEc/+B7Xq9JEvIOrkGUcYIPDzoHmhEDW7v+eO8Ild5uU9dZnOWlKD25AmPFyuBRp+Q\ngJBRP4PkdDTc5dxRzrIy6MLDIer1HmmvMwi5cgQsQ6/wy63biH7KVV4GR1ERjKld+RngQzhSF2B0\nERHtLugAYF/x9zhjO4eJXcc12e+16rudOPn4oz61QbwhMQndnn8RQb3Vm+vnD0xp3RFz+3RoLZ5Z\nS8pRVIScObNRsWWzR9rrLAStFqLBAFmW4bbZ1I5D1G62/ftx5vm/NCwYTr6BI3UBpGzDVzAkJCKo\nZ692vV6SJaw8uRZxQTEYFjeoyeNBvXojYsJEaEN9Z6TuAt5S3zJZluGuKIc2LLzDbWnMwYiYOAnB\nfft7IFnnk//qy3BVVSLpD/N47pJfMg8aDF1UFLThHf88Ic/hSF2AkF0ulH6RBdvu79rdxq6CvSi0\nF+HGbtdDFJqeGhqzGVE/v8Vn5k9JTidO/Xk+qjrwM3cm1v8tR86cxyA5HR1uSxMUjKjMqT51Kd6f\nmIcOQ+jIUQBvmCA/pQ0JQXCfvlxM28dwpC5ACFotuj73V8h17fuF7Zbc+CJnHZLMXTAwuvm1tGpz\nc1Hzw1GEjxvfrvfyFLfNBk1QEOdztJJ5wCBoQ0OBDi7rU7bhKxiTUmDqwU282yvkyuFqRyDqkOrv\nD0IbGgpDUrLaUegnWGIHEFGnh8bc9nVtAODb/F2w1pZiUrfxlxyl+6mq73ai9IuVcNfUtOu9PEUX\nHo7ERx9DcD9eAmwNY2oqwq4d06FJ+pLTgbIvV6HyEnsDU9vIkoTKHdtRm3NS7ShEbVb4nyUoXbta\n7Rh0EY7UBQBnWRkK3nwN0dNuh7Frt7a/3u3E6tz16BqSgj6RLc/Hi5h4IyJunAyNydSeuB4jSxKH\n/tvIbbfDkX8OprTu7Xq9qNMj9c9/gex0ejhZ5yM7nSj+6AOYBw1u179bIjUlPvoYwM9fn8O/kQDg\nKiuDu7oaoimoXa//5tx2lNdV4Ka08a2atK0JCobGZIIsyx6Zn9UesiTh5OOPomzdGlXe319ZVyzD\nmb89D9nlavNrJacDsixDNBjaPSJMPxINBiTNmYeYX96ldhSiNtPHxUEfw20ZfQ1H6gKAqVs3pC54\ntl2vrXXVYU3uBqSHd0d6eOtHb2RZxtl//A3asHDE/fo37XrvjpDq6mAZPAT6uHivv7c/C73mWpgH\nDQbaccdlyX8/Re2pXCQ9Noc7SHiIPjYOAEedyb84rSWoOXYMwf0HQBOszH7S1D78FPFzUl0dZElq\n9+u/PrMVNmc1Jndr200PgiDA1CMdxtSu7X7vjtCYTIiZfifn07WRoUsCgnpltKsoMySntPu1dHm1\nubnImfsYak/lqh2FqFVqjh1Fwb/f5Bp1PogjdX6u9IssVG7/FqnPPgdR17a7QO3OGqzL+xp9I3uh\nW2jbt3qJnHRTm1/jKW6bjZcA26nu7Fk4CvJhGTK0Ta8LvWqUQok6N11MTP2Icwe+nBF5k3noMKSm\ndoUuKkrtKHQRjtT5OVN6OkJH/azNBR0ArD+9GTWuGkzqNqHd7y/LMmx7d8Nptba7jfa8Z+6T81D0\nwVKvvWcgKd/wFQrfWdzqEV5XZSWqdu7o0IgwXZ4mKAiJj8zmzRLkN0SdHvr4LhC0HBfyNSzq/Fxw\n3/6IvGlKm19X5bBh4+lvMCimP5IsXdr9/u6KcuS/8Roqvt7Y7jba/qZuREy+qX5uGLVZxMQbkfKn\nv7R6Dlfl1m+Q/9brcBYXKZysc5Nqa2Hbt1ftGEQtqtyxDdXfH1Q7Bl0Cy2w/VvPDMei7JLRrouq6\nU5vgcDsxqeu4DmXQhoUj8fG5MKakdqidthC0WoSPGeu19ws0usi2XTIJH38DTOk9Gyb1kzJK13yJ\n0pX/Q9fn/trmvyMib7L+bzkMiUkI7ttP7Sh0ERZ1fkp2uXD2lZcQ3Kcv4mfd26bXltdVYPPZb3FF\n3GDEBcd2OIupW1p9Jln2yj6WdWfPQBcZCdGo7jp5/sx2YB9cVivCRl/X7PNkWYYgiu1e145aL3zM\nWAT37ceCjnxeylMLIDnq1I5Bl8DLr/5Ko0HiI7MRMXFSm1+6JncD3LKEiV09N9pV88MPOPXMk3CV\nl3mszcs5+69/oOCdfyv+PoHMtmc3ytashtzM3qNumw2nnnkS1dmHvJis89JYLCyeyS+IBgO0lhC1\nY9AlcKTOTwmC0K5LntaaUmw9txMj44chyhTpsTyasFAIOh1clZXQhoV7rN2LybKM2DtnQDQaFXuP\nziB62u0QjaZmR1bdVZUQjcb6/WLJa6wr/we3rQoxt9+hdhSiJpzFxajatQOWEVdBF67cZz21D0fq\n/JBUW4Pizz6Fs6S4za9dlfsVBEHAhNTmL7u1lT46BilPPA1jctuXRmkLQRAQ3KcvRzQ6SBMU3OKN\nEvr4Lkie+wQMCYleSkVA/Qipu6qq2VFUIrXUns5Dyef/hWSzqR2FLoFFnR+qOXkSZWu+hKusvE2v\nK6wuwo783bg6YTjCjWGKZJOcDtQc/0GRtgHAfuQw6s6eUaz9zqTsq3WwZq245GM1J45DquOcGTVE\n3/YLxM+61yvzU4nayjJ4CLq//Dr0Xdq/agIph0WdHwru3Qfd/vYPGLu3bbTqi5x10IlajE8Zo1Ay\noPiTj3Dmxb/CrdC3uKIPlqL4k48UabuzqTuVi5oTJ5r8uVRXh7P/fBGF7y3xfihqKOZc5WVwVVSo\nnIaoKdFo5M4yPopz6vxUWyepnrXlY3fRflyfMhoWvXI7MYSPHQ/LkGEQFdoPMOF3j0CqrVWk7c4m\n9te/ueQlWNFgQMLDj0AMClIhFQGAu6YGOX+ci5CRVyH2jjvVjkPUoHzTBgg6HUKvulrtKHQJLOr8\nTPmmDbAfzkbczHvatItE1sk1MGmNGJd8jYLpAH1sLPSxHV8m5XK43IPnNDenztS9hxeT0MU0JhNi\nf3knjPx7IB9TtWM7xKAgFnU+ikWdn5FdLkh1dW0q6HIr83CwJBuTuo5HkE750RdZllH25ReARoOI\n8Td4rF3rth2oqqyBZfAQj7XZ2RV98B6g0SL6wXsA1G8h5qqsQORNU1u94wQpI2TEVWpHIGoi6Q/z\nILtcasegy2BR52fCx16P8LHXt+k1WSfWwKwLxugk7/ySEAQBtXmnPD7n4lzWSjgdLhZ1Cqo7exZO\nawkLOh/htFphzVqBqJ/fAm0I1wUj38A9X30X/2b8iLu6us1bgv1QdgJHyn7A1O43wqj13tpu8TN/\n6/F/+H0WPI3CnHyPttnZxUxvPF8r9s5f8Vu4D5GdDlTt2gHzwEEwDxykdhzq5BzFRShbsxrh142F\nPp53v/oifh33E7IsI+/P89t0R6Isy8g6uQahegt+ljBSuXCXcKGgc1dVofizTz3S5plPP4M2TJml\nWDo7d21tw24g/BbuO/Rx8Uj72z9gHjgIJSuWqR2HOjlXaSmqdu6Au6ZG7Sh0GSzq/IXbjbDrxiG4\n/8BWvyS79BhOVORiQup10Gt0Coa7NFdFBU7OmV0/v66DKrdtxemPPuGCrAo4/cIibL/tDuTMeQzO\n4rYvaE3KurDHcell1hQk8pagnr3Q/aVXYOzaTe0odBks6vyEoNUifOw4mAe0rqiTZRkrT65GhDEc\nI7tcoXC6S9OGhiIqc2rDccHbb6Fs7eqG4/y3XkfZ+nUNx+defxXlGzf8ePzqv1C+eRMAwH70KABw\nQVYFmHqkAwAiM38OXXS0ymnoUkpXrwIAuO3VKich4uewL+N1Fh9TUVeJt79/H3f3/SVCDRYAgOx2\no/rQQQT37tuqS2MVdZV4ae+bKLAX4Ze9boVW9P5fc8mKZY1GFo7NnAEAMKX3RPj1EwDUzxGUf7Jr\ngVRtg+T48dhts8G2+zsU/WdJk3YiJmc2Khip/aIyp6I0awUibpiodhS6yMX/jk48/AAAnv+kjtIv\nv4DsciFycqbaUegyBLkTXc+yWm2QJOV+3OhoC4qLqzrUxgeHP8O3+TsxLG4QMtPqlwOpO3QYFa++\njtB7ZsIwoF+Lbaw4/iV2Fu6BUWPA81c/A42o7srfx2bOQPriJT7TDtW7uGC4gAWD53nis4Hn/6V5\nom/p8n7av/lvvQHZ6UCX+x9SOVVgaO7cFUUBkZFt3yiAI3U+4neb5sEl/XjX4c6CPdhZsAcAIEoy\nUq4JRV7FCri3/q/Vbda66/DwprnQilr889qFHs9M/i0qc2pD8caCwT/Yjx2Fu7wcliuuVDsKdULx\ns36rdgRqAYs6H7FgxBx8fnwl9hV/D5fkgkbQIMncBYNi+sOkNQIZwIgW2qhx1WBP0UGcsZ2DW3ZD\nJ+owMLovpnaf5JWf4XIiPDRUn3T7NI+0Q+SPIiZnomz1KjhLimEedgXnNRFREyzqfESoIQRGjRFu\nyQ2tqIVbciPJkoARtkg4zp1F6LVjIOpavoO1uKYUeVVnoBW1cEkuGDWGhrl5avHUpbzkX9zGyywK\nYcHs+6Iyp8JZVgZNUBALOvI6Z0kxij58HxE33gRTN9796qtY1PmQKkcVRiUMx6guV2LLuR2orKtE\n9ff7YNu3B2HXjWt3G0QtYcHsH3Th4QDq726HJHl81xaiy3Hb7XBarYDkVjsKNYM3SniQUhN23TYb\nNOa2T5gMNJwQrRz2rbI82b9SbQ1Ov/A8LMOu8Ojeyv6K566y2L/KUeJGCa5T5wdY0BHRBaLRBENi\nEnThEWpHISIfw8uvPuzsy/+EqUc6v40TUSNxM+5WOwJ1MiXLP4e7sgKxd/1a7SjUDI7U+SjZ5YKg\n1UIQ+VdERE3JkoSqXTshOR1qR6HOQJIguzifztdxpM5HCVotutz7gNoxiMhH1Rz/AflvvIrYX89E\n6FWj1I5DAS7q57eoHYFagUWdD5JlGZLNBo1F3aVIiMh3mXqkI+HRxxDUK0PtKETkI3htzwfV5Z3C\nif/7HWwH9qsdhYh8lCAICO7dh1M0SHHO0lKcmv8Uqr8/qHYUagE/DXyQxmxB+PUTYErrrnYUIvJx\ntr17cPr5v0B2uVp+MlE7yG4XtOHhEI1GtaNQC3j51QdVbNmM6Fu4wj8RtUzQaiG73XBVVkAXEal2\nHApA+ugYJDz8iNoxqBVY1PmYunPnUJq1ApE3TeFWQETUoqC+/RDUtx8/L4iIO0p4knP1X+F0Nn8J\nRJs8EPoB9evO2bP+Al36KOh6Xg2ptgq1615G6d582PMqkTCpBwRN06vjFz9f338CtCmDIJXno/ab\nJS1mvPj5hmG3QBPXA+6CH1C3678tvv7i5xuvngExLB6uU3vhOLC6xddf/HzjuAchGi1wHv0GzmNb\nmn2tTqeF5tp7Gz0/aPJcAIBj/5dw5e1r8f1/+nx34XGYrn8IAFC381O4C483+1rBYG70fLnWBuPP\n6tdsqt38DqSKgmZfL4bGNXq+YDTDcMWtAICatf+CXGdr9vWa2O6Nnq+J7d7oXGpJc+eee9PrLZ67\nnfncA9Dk+W0593Q6LXQTHmt4vhLnnuSS4LY7oQsxNHm9L597tetebvH1zZ177u1LWzx3O/O5B3Ts\nc88QEobSonDU5Z1G9NWpAfW515FzzxOU2FGCI3U+wvrFKpR9ebTh+OzKHwAAlp6RCO0VpVYsIvIT\n1p1n4bY7EXtdV47akUdpI6O4Rp2f4EidB3lqj7xjM2cgffGSjgcKMNyDUDnsW2V5o39rfvgBEIVO\nd4MVz11lsX+Vw5E6IiK6JFOPHmpHICKVcUkTHxQxOVPtCETkhySnAyXLPkPV7u/UjkIBwlFegeO/\nfxCVO7apHYVagUWdD4rKnKp2BCLyQ4JGC9u+vajNOal2FAoUsgTLkKHQRUWrnYRagZdfiYgChCCK\nSP7jUxD1erWjUIDQh4cj9s4ZasegVuJIHRFRALlQ0LnKyyFLksppyN/xHPIvLOqIiAJMbc5J5MyZ\njer9e9WOQn4u59/vIGfuY2rHoFbi5VciogBjSE5B2LjxMCQlqx2F/FxIn95wiLyc7y9Y1BERBRhB\no0H0zbeqHYMCQNTIEZB79FU7BrUSL78SEQUoR2EhSlYsQydaY548zF1Xp3YEagMWdUREAarm2BGU\nrV4FZ0G+2lHID0m1Ndg+bTrKvlqrdhRqJV5+JSIKUCEjrkJw/4HQhoaqHYX8kCwDyb+cDqR0rq3n\n/JnXirqcnBzMmTMH5eXlCAsLw6JFi5CamtroOVarFXPnzkV+fj6cTieGDx+OJ554AlqtttnHiIio\nKUGrbSjoJKcDoo4T3qn1NCYT4m69mXu/+hGvXX59+umnMX36dKxZswbTp0/HU0891eQ5r7/+OtLS\n0pCVlYWsrCwcOnQIa9eubfExIiK6vMKl/8HZF1/g3DpqE3dNDefU+RmvFHVWqxXZ2dmYNGkSAGDS\npEnIzs5GaWlpo+cJgoDq6mpIkgSHwwGn04nY2NgWHyMiosszdu2KoN59AElCyYplHW7PE214sh1S\nRukXWdgx/S4uQOxHvHLtMj8/H7GxsdBoNAAAjUaDmJgY5OfnIyIiouF5999/Px566CGMGjUKNTU1\nuOOOOzBkyJAWH2utyEiz536oy4iOtij+Hp0Z+1c57Ftlqdm/0VMmNvz/D1krkDHzrg61d8wDbXiy\nHZ67yjBcMxJyUT5iYjknUymePnd9akLa6tWr0bNnT7z77ruorq7GrFmzsHr1akyYMKHZx1rLarVB\nkpS7/BAdbeHcAwWxf5XDvlWWr/Sv/chhAGjIUrF1CzQWC8z9B9Qfb9kMbXgEgvvUr0tWvnkTdFHR\nCO7dp/540wbo4+Ib2ijf8BX0iUkISu8JAChbvw7G5BSYeqTXH69bA2O3NJjSukOWJJSvXwdjWg+Y\nunVryNTRfvGVvg1I0Yko37OX/auQ5s5dURTaNRDllaIuPj4ehYWFcLvd0Gg0cLvdKCoqQnx8fKPn\nLV26FAsXLoQoirBYLBgzZgx27NiBCRMmNPsYERFdXsmKZSjNWtFwfGzmDACAGByM4D59G4o6a9YK\nBPXs1VDUWVcsh3nAAAT37nPZNozd0pA878n69/n0Y4SNG99Q1BV//CEiJmfClNYdkCQUf/whTL0y\nUHO+uPxpOxGTMxGVOVWRn5/ax1FUpHYEaiOvFHWRkZHIyMjAypUrkZmZiZUrVyIjI6PRpVcASExM\nxObNm9G/f384HA5s27YN48aNa/ExIiK6vKjMqYjKnArZ5cIP985E+uIlAOonwgvij1OrU575c6Pj\n1D8thHB+2kxU5lSEjxsPQaPB8Qd+i/TFS+C2V0PQ6hqe3+3FfzY6TnvpFQi688caTcOxqNPD+kUW\nrMs+Q/eXX4doNCr401NbXa6AZ+Ht+7x29+szzzyDpUuXYvz48Vi6dCnmz58PAJg1axYOHjwIAJg3\nbx52796NyZMnY8qUKUhNTcW0adNafIyIiFomXLQElMZkgmgwXP44KKiF42CIen3zx+eXUREEodFx\nxPgbAIAFnQ+KypyK7q+8gbiZ9wAA0hcvQfriJSzo/IAgd6J73Dmnzr+xf5XDvlWWL/VvyYplHf7l\n7Ik2LrQTMXES5Lo6aMztu5HNl/o2EB2bOaNhZJc8S4k5ddwmjIioE/FEMeapEZvISTfh1NNPoPjT\njz3SHnlO9aHv4aooR9LtvCLmT1jUERGRKgSNBuHjxiNk+Ai1o9BPSHV1OPvS31G2bi2Sf3Gb2nGo\nDXxqSRMiIupcwkaPUTsCXUTQ6ZA89wmIQcFqR6E24kgdERGpyl1Tg9JVK+GqrFQ7CgEQRBHG1K7Q\nx8SoHYXaiEUdERGpyl1RjpJln6F6/161oxDqF6Wuzc1ROwa1Ay+/EhGRqvRx8ei68HnooqPVjtLp\nyW43ij5YitBRV8OY2lXtONRGLOqIiEh1Fwo62e1uWPCYvE/QaNDtr3+D7HSqHYXagZdfiYjIJ1Rs\n/Qa5f5wDqa5O7SidmiYoGNrQMLVjUDuwqCMiIp+gj4mDMa07pNpataN0WqWrV6Hqu11qx6B24uVX\nIiLyCaYePWDq0UPtGJ2WLMuo3PINTD17wjJ0mNpxqB1Y1BERkU9xWq1wlZXC1J0FnjcJgoCUPy3k\nfDo/xsuvRETkU/LfeBWF/3kHnWhrcp8hCAJEvV7tGNROHKkjIiKfEvPLu6AJNkMQBLWjdCrF//0E\nmqAgREycpHYUaicWdURE5FOMySlqR+iUnMVFkMwWtWNQB7CoIyIin+O22VD04VJYrhwBc/8Basfp\nFLrc96DaEaiDOKeOiIh8jmg0ou7MGbjKStWOQuQ3OFJHREQ+R9BqkfL0Aggixx68ofC9dyG7XIj7\n9W/UjkIdwH8tRETkky4UdI6CApWTBD6NxQyNhfPp/B2LOiIi8lmVO7cj94k5qM05qXaUgBY15WZE\n3zJN7RjUQSzqiIjIZwX3G4Com6dBFxundpSAJbtcakcgD2FRR0REPktjMiHihonQBAWpHSVgFf5n\nCfIWLlA7BnkAizoiIvJ59iOHUbp6ldoxApKpZy+YBw1ROwZ5AO9+JSIin2fbvw+23d8hbMxYbmPl\nYaFXjVI7AnkIizoiIvJ5kTdNQdTPb4Go06kdJaC4qioh6g0QDQa1o5AH8PIrERH5PI3JBFGngyzL\nkJwOteMEDOvyZcj5w2zIkqR2FPIAjtQREZFfkF0u5C38E4IyeiP61tvUjhMQLFcOh7FrVy7yHCBY\n1BERkV8QtFoE9+0HQ2KS2lECRlB6TyC9p9oxyENY1BERkd+I+vktakcIGI7iIsgOJ/RdukAQBLXj\nkAdwvJWIiPyK7HKhcttWSLW1akfxa+Xr1yHv2fmA2612FPIQjtQREZFfqc3LQ8G/30Lsr1yITZqk\ndhy/FT72egT17gtBy1IgUHCkjoiI/IqpWzck/eGPCBn1M+R9+LFH2ixZscwj7fgTXVQ0zP0HqB2D\nPIhFHRER+R1Tjx4QBAGnP/rEI+2VZq3wSDv+ou7sWVTt/g6S06l2FPIgFnVEROSXqnbtBABItTUA\ngMrt3+L0ooUNhUrF1m9wetHChjXYyjdvwunn/9Lw+vKN63Hmb897ObVvqNz+LfLffA3g+nQBhRfS\niYjIr5SsWNZoZO34g/cBAIIHDwE0moY/F0SxybGg+fHXnu3gAdgPZ+PYzBkA0PDfiMmZiMqcquBP\noL6ozKkIuWI4d5IIMIIsy7LaIbzFarVBkpT7caOjLSgurlKs/c6O/asc9q2y2L/KOTZzBtIXL+lQ\nG5LDgeP334MuD/4O5oGDPBMsQPDcVU5zfSuKAiIjzW1uk5dfiYioU7tw92dtzkmVk3hHbd4plK5a\nCXd1tdpRyMNY1BERkd9Kun1ah9sQRBHhEychaurNHkjk+2qOHUXJ8s8Bbg0WcDinjoiI/FbyL27z\nyOXB6PM7VbiqKqG1hHS4PV8WPvZ6hIy8ChqTSe0o5GEs04mIiADYjxxGzmOPwn7sqNpRFKcJClY7\nAimARR0REREAY7c0hI6+DrrISLWjKKY2NwcFby+G02pVOwopgEUdERERAFGvR8xtv4AuMkrtKIpx\nlpSg+uB+iHq92lFIAZxTR0RE9BOOgnxUZx9C+JixakfxOMvQYTAPGQpBENSOQgrgSB0REdFPVG7f\nhpL/fgJXVaXaURTBgi5wsagjIiL6ifDrx6Prcy8E3F2wtadykfeXP6Pu9Gm1o5BCePmViIjoJ356\nZ6gsSfXbjQUA2eEAAGgsbd+pgPwDizoiIqKLyLKM/DdehSbYjNg7f6V2HI8w9UhH8twn1I5BCmJR\nR0REdBFBEKCLjgmYBXplWQZkOWBGHenS+LdLRER0CdE334qIiZPUjuERjrNncOLRh2E/nK12FFIQ\nizoiIqLLkGUZ9qNH4LbZ1I7SMaIG5gGDoIuJUTsJKYhFHRER0WU4S4px5q/PoXzTBrWjdIihSxfE\n/fo3Ab2wMnFOHRER0WXpo2PQ5eHfI6hXb7WjtJssy3DbqgJuiRZqiiN1REREzTD3H+jX22o5iwpx\n8pGHUbm/isnVAAATjElEQVRju9pRSGEs6oiIiFpQ88MxnPnH3yDV1akdpc1EoxFRN98KU1qa2lFI\nYSzqiIiIWiDLMpyFhXCWlKgdpc20oWGIuOFG6KKi1Y5CCmNRR0RE1AJTj3SkPvscDAkJakdps5qT\nJyA5nWrHIC9gUUdERNQCQRAgiCJkSYKzpFjtOK3mtJbg9MI/oWLzJrWjkBewqCMiImql/DdexZm/\nvwBZktSO0ioaswVdHngI5oGD1Y5CXsAlTYiIiFop7NoxcNvtasdoNdFggHnQELVjkJewqCMiImql\noAz/Wq+uaucOGLt3hy4iUu0o5AW8/EpERNQGssuF8o3rfX4fVVdFBfLffA1VO3eoHYW8hCN1RERE\nbVS6ehWC+w/w6ZE7TUgIUhYshCY4SO0o5CUs6oiIiNpA0GqRPO8paEJ8e9stQRBg6NJF7RjkRbz8\nSkRE1Eba0FAIggDJ6VA7ymWVrvkSNSdPqB2DvIhFHRERUTvUnDiOk489ipqTJ9WO0oRUWwvrss9g\nzz6kdhTyIl5+JSIiagdDQgKCe/eBaNCrHaUJ0WhE2j9fgex2qx2FvIhFHRERUTuIRhPi77lP7RiX\nJRoMakcgL+PlVyIiog5wVVWiatdOtWM0UvzJR6j6zrcykfJY1BEREXVA2ZrVyF/8Boo++cgj7ZWs\nWNah18tuNyq2bkHd6dMeyUP+g0UdERFRB4RfPwEpT/8J5WtXo+50HhwFBQ2P1eadgqOw8MfjU7lw\nFBX9eJybA2dx8Y/HOSdRmrWi4bjm5Ek4rSU/Hp84DmepFQAgy/L549L6Y0lCzYnjcFdVQqq2ITJz\nqud/WPJpLOqIiIg6QBsS0rAeXP6br6Nk+WcNj+W/9jKsWcsbjs+9/BJKV2U1HJ/954soXfNlw/GZ\nF//aqO0zLzyH8g3rG45PP/csKjZvqj+QZZz+y59RufWb+kOns/542zYAgCDyV3xnI8iyLKsdwlus\nVhskSbkfNzraguLiKsXa7+zYv8ph3yqL/asctfu2ZMWyRiNrF0RMzkRU5lTYjx6BJigYhqQkAID9\nyGFozGYYEs8fH86GJiQUVd/tvGQ7luEjEDnpJujj4gEA1d8fgC46BvrYOMiyDPuhg9BFx0IfG4uS\n5Z+jdOX/LpulPdTu30DWXN+KooDISHOb22RR50E8+ZXF/lUO+1ZZ7F/l+FLfHps5A+mLl/hEO57K\n4kv9G2iUKOo4NktEREQUAFjUEREReUDE5EyfacdTWci/sKgjIiLygPbOW1OiHU9lIf/Coo6IiIgo\nALCoIyIiIgoALOqIiIiIAgCLOiIiIqIAwKKOiIiIKABovfVGOTk5mDNnDsrLyxEWFoZFixYhNTW1\n0XOsVivmzp2L/Px8OJ1ODB8+HE888QS02vqYq1atwmuvvQZZliEIAt555x1ERUV560cgIiIi8lle\nG6l7+umnMX36dKxZswbTp0/HU0891eQ5r7/+OtLS0pCVlYWsrCwcOnQIa9euBQAcPHgQL7/8Mt5+\n+22sXLkSH3zwASwWi7fiExEREfk0rxR1VqsV2dnZmDRpEgBg0qRJyM7ORmlpaaPnCYKA6upqSJIE\nh8MBp9OJ2NhYAMCSJUtw9913Izo6GgBgsVhgMBi8EZ+IiIjI53nl8mt+fj5iY2Oh0WgAABqNBjEx\nMcjPz0dERETD8+6//3489NBDGDVqFGpqanDHHXdgyJAhAIATJ04gMTERd9xxB+x2O8aNG4f77rsP\ngiC0Okd79lFrq+hojh4qif2rHPatsti/ymHfKov9qxxP963X5tS1xurVq9GzZ0+8++67qK6uxqxZ\ns7B69WpMmDABbrcbR48exTvvvAOHw4GZM2eiS5cumDJlSqvbt1ptkCRZsfzc+FhZ7F/lsG+Vxf5V\nDvtWWexf5TTXt6IotGsgyiuXX+Pj41FYWAi32w0AcLvdKCoqQnx8fKPnLV26FDfddBNEUYTFYsGY\nMWOwY8cOAECXLl0wYcIE6PV6mM1mXHfddThw4IA34hMRERH5PK8UdZGRkcjIyMDKlSsBACtXrkRG\nRkajS68AkJiYiM2bNwMAHA4Htm3bhh49egCon4e3ZcsWyLIMp9OJ7du3o1evXt6IT0REROTzvHb3\n6zPPPIOlS5di/PjxWLp0KebPnw8AmDVrFg4ePAgAmDdvHnbv3o3JkydjypQpSE1NxbRp0wAAN954\nIyIjIzFx4kRMmTIF3bt3xy233OKt+EREREQ+TZBlWblJZj6mrKxa0Tl1kZFmWK02xdrv7Ni/ymHf\nKov9qxz2rbLYv8pprm9FUUB4eHCb2+xURR0RERFRoOI2YUREREQBgEUdERERUQBgUUdEREQUAFjU\nEREREQUAFnVEREREAYBFHREREVEAYFFHREREFABY1BEREREFABZ1RERERAFAq3aAQJGTk4M5c+ag\nvLwcYWFhWLRoEVJTU9WOFRDGjBkDvV4Pg8EAAJg9ezauvvpqlVP5r0WLFmHNmjU4e/YssrKykJ6e\nDoDnsCdcrm95DndcWVkZHn/8ceTl5UGv1yMlJQULFixAREQE9u3bh6eeegp1dXVISEjAX//6V0RG\nRqod2a801789e/ZEeno6RLF+HOj5559Hz549VU7sX+6//36cOXMGoigiKCgITz75JDIyMjz/uSuT\nR9x5553y8uXLZVmW5eXLl8t33nmnyokCx+jRo+WjR4+qHSNg7Nq1Sz537lyTfuU53HGX61uewx1X\nVlYmb9++veH4ueeek+fOnStLkiSPHTtW3rVrlyzLsvzKK6/Ic+bMUSum37pc/8qyLKenp8s2m02t\naAGhsrKy4f/XrVsnT5kyRZZlz3/u8vKrB1itVmRnZ2PSpEkAgEmTJiE7OxulpaUqJyNqaujQoYiP\nj2/0ZzyHPeNSfUueERYWhiuvvLLheODAgTh37hwOHjwIg8GAoUOH/n979x9TVfkHcPx97xUE+aGA\nXIRVa7ZMgrHdAGFBlkSGiClzIGltFawM0sIkDU2DuBhqZQXoTKNahMliiUhSaEAzB4GUlm2Jc+bo\nxo9Bify+l/v9o3m+4VcK8PK9cfu8/uLc8zzn+ZyHZ2efnc+95wCQkJDA0aNHrRXmpDXS/ArLcHFx\nUf6+cuUKKpVqQq67Un61AIPBgJeXFxqNBgCNRoNWq8VgMODu7m7l6GzD+vXrMZvNBAYGsm7dOlxd\nXa0dkk2RNTzxZA1bztDQEEVFRURERGAwGPDx8VH2ubu7MzQ0pJSzxNj9eX6vevTRRzGZTMyfP581\na9Zgb29vxQgnp02bNnHixAnMZjP79u2bkOuu3KkT/3iFhYWUlpbyySefYDabyczMtHZIQoyJrGHL\neuWVV5g2bRqPPPKItUOxSdfOb1VVFSUlJRQWFtLU1EReXp6VI5yc9Ho9VVVVpKamsn379gkZQ5I6\nC/D29qalpQWTyQSAyWSitbVVyjAWcnUe7e3tWblyJadOnbJyRLZH1vDEkjVsOTk5OVy8eJFdu3ah\nVqvx9vYeVibs6OhApVLJXbpxunZ+4b/r19nZmbi4OFm/N2jZsmXU1tYya9Ysi193JamzAA8PD3x9\nfSkrKwOgrKwMX19fKVtZQE9PD11dXQCYzWbKy8vx9fW1clS2R9bwxJE1bDlvvPEG33//PXl5eUr5\nz9/fn76+Purr6wE4cOAAixYtsmaYk9b15vf333+nr68PAKPRSEVFhazfMeru7sZgMCjbx48fZ/r0\n6RNy3VWZzWbzDUcsOH/+PBs3buTy5cu4urqSk5PD7NmzrR3WpHfp0iXWrFmDyWRiaGiI2267jc2b\nN6PVaq0d2qSVlZXF559/Tnt7O25ubsyYMYMjR47IGraA683tnj17ZA1bwLlz54iJieHWW2/FwcEB\ngJtuuom8vDxOnTrF1q1bhz3SZObMmVaOeHIZaX6TkpLYsmULKpUKo9GITqcjPT0dJycnK0c8ebS3\nt5OcnExvby9qtZrp06ezYcMG/Pz8LH7dlaROCCGEEMIGSPlVCCGEEMIGSFInhBBCCGEDJKkTQggh\nhLABktQJIYQQQtgASeqEEEIIIWyAJHVCCJs1MDCATqejpaXFom0no40bN/LOO+9YOwwhxASSR5oI\nIW7Y4cOHKSgo4MKFCzg5OTF37lxWr16tvGR9NEwm07D2vb292NvbK+9F1Ov1REdHWzz2/4edO3fS\n2dmJXq+nv7+fgIAAqqurmTVr1oSMV1RUREVFBe+9996EHF8I8c80xdoBCCEmt4KCAvbu3UtGRgbh\n4eHY2dnx1VdfcezYsTEldRqNhsbGRmV7/vz57Nixg5CQkBH7GI1Gpkz5d13G/o3nLIQYHSm/CiHG\nrauri7feeostW7awcOFCpk2bhp2dHREREWzYsAH4o6yp1+sJDw8nPDwcvV7PwMDAuMbbuXMnzz//\nPM899xw6nY4jR47Q0NBAXFwcgYGBhIeHk52djdFoBKC/v5877riDX3/9FYDU1FT0ej2JiYnodDoS\nEhJobm4ec1v44yXnCxcuJCgoCL1eT3x8PIcOHfrbc1i1ahUAUVFR6HQ6KisrAfjiiy9YsmQJQUFB\nrFy5kqamJqVPWFgY+/fvZ/HixQQGBgKQm5tLREQEOp2OmJgYqqqqADh79izZ2dnU1dWh0+kICwtT\nzic/P185ZmFhIZGRkYSEhPDMM8/Q3t4+bB4OHjxIZGQkwcHBZGdnK/3Onz/Pww8/TGBgIKGhobzw\nwguj/fcJISaYJHVCiHFrbGykv7+fBx54YMQ2u3fv5rvvvuPQoUOUlpZy5syZYcnFWFVUVBAbG0tD\nQwMPPvggdnZ2vPTSS9TV1fHRRx/x5ZdfUlxcPGL/srIy1q1bR11dHVqtlrfffnvMbdva2khNTeXF\nF1/k5MmTaLVafvjhh1HFX1hYCMDRo0dpbGwkMjKSb7/9loyMDLZt20ZtbS1Lly4lJSVFSU4BysvL\neffdd6mtrQVg9uzZHDhwgIaGBpKSkkhNTaWjo4M777yT9PR05s2bR2NjIydOnPifGKqrq8nPzyc3\nN5eamhrc3NxIS0sb1qampoZPP/2UkpISSkpKlHFff/11IiMjqa+vp6qqihUrVozqvIUQE0+SOiHE\nuP3222+4ubn9ZTnw8OHDpKSk4OHhgbu7OykpKZSWlo57zHnz5nHvvfeiVqtxcHAgICCAgIAANBoN\nt9xyC3FxcXzzzTcj9l+0aBF+fn7Y2dkRExPDjz/+OOa2x48fx9/fnwULFmBnZ0diYiKurq7jPqeP\nP/6YVatW4e/vj0ajYcWKFQwMDAxLFB977DG8vLyU93JGR0ej1WpRq9UsW7YMLy+vUSeWpaWlxMfH\nM3fuXKZOnUpaWhonT56kra1NafPUU0/h7OzMzTffTFBQkHLuU6ZMobm5mba2NhwcHJQ7h0II65Mv\nZgghxm3GjBl0dnb+5fe8Wltb8fHxUbZ9fHxobW0FICkpiYaGBgAyMjJ46KGH/nbMa39c0NTUxKuv\nvsrZs2fp6+vDZDJx1113jdj/zy96d3BwoKenZ8xtW1tb8fb2Vvap1Wq8vLz+NvaRNDc389lnn7F/\n/37ls8HBwWG/xP3zeADFxcV88MEHGAwGAHp6eujs7BzVeK2trYSGhirbrq6uODs709LSoiSnnp6e\nyn5HR0fl3NPT09m1axexsbG4u7uTlJTE0qVLx3jGQoiJIEmdEGLcdDodU6dOpbKykqioqOu20Wq1\n/PLLL9x+++0AGAwGtFotAPv27RvzmCqVatj25s2bCQkJ4c0338TJyYm9e/fy9ddfj/m4Y+Hp6Tns\nbuDQ0NCoH4VybfzwR8J233338cQTT4zqGBcuXCArK4v333+fgIAA1Go1UVFRXH2YwfXG+LOr/5Or\nurq6uHLlyqgSUy8vL7Zt24bZbKauro7ExESCg4OHJe5CCOuQ8qsQYtxcXFxYu3YtmZmZVFZW0tvb\ny+DgINXV1Wzfvh2AxYsXs3v3bjo6Oujo6CAvL48lS5ZYLIbu7m6cnZ1xcnLi3LlzHDx40GLHHklE\nRASnT5+muroao9FIQUEBly9fHlVfe3t7XFxcuHTpkvJZfHw8H374IWfOnMFsNtPd3c2xY8fo7e29\n7jF6enpQq9W4u7szNDREUVERP//8s7J/5syZGAwGBgcHr9s/JiaG4uJifvrpJ/r7+9mxYwehoaHD\n7s6NpLy8nJaWFlQqlXJX7+pjZ4QQ1iV36oQQN+Txxx/Hw8OD/Px81q9fj5OTE35+fqxevRqA5ORk\nuru7ldJqVFQUycnJFhs/PT2dl19+mfz8fPz9/YmOjub06dMWO/71aLVaXnvtNbKysujs7CQ2NpY5\nc+Zgb28/qv5r167l2WefZWBggJycHO6//342bdrE1q1buXjxIo6OjgQHByu/XL2Wn58fCQkJLF++\nHI1Gw/Lly/H391f233PPPRQVFXH33Xfj6OhITU3NsP4LFizgySef5Omnn6arq4ugoCAlCf87jY2N\nZGdn093djaenJ5mZmTdUehZCWI48fFgIIW6Q0WgkLCyMPXv2oNPprB2OEOJfSsqvQggxDtXV1XR1\nddHf309ubi6Ojo74+flZOywhxL+YlF+FEGIc6uvrSUtLw2g0MmfOHHJzc0ddfhVCiIkg5VchhBBC\nCBsg5VchhBBCCBsgSZ0QQgghhA2QpE4IIYQQwgZIUieEEEIIYQMkqRNCCCGEsAGS1AkhhBBC2ID/\nAOgT4UenrAkiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "co_LR_SVM  per fold iteration:  [30, 30, 30, 30, 30]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnUAAAI3CAYAAAD9Z60zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncHPVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved_co_LR  per fold iteration:  [30, 30, 30, 30, 30]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnwAAAI3CAYAAAABcob5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCseEj4iIiEjhmPARERERKdz/A795TIlaOvMmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For name:  c_zou\n",
      "(32, 2)\n",
      "c_zou  pass\n",
      "For name:  s_rana\n",
      "(42, 2)\n",
      "s_rana  pass\n",
      "For name:  a_nunes\n",
      "(61, 2)\n",
      "a_nunes  pass\n",
      "For name:  s_jeong\n",
      "(93, 2)\n",
      "s_jeong  pass\n",
      "For name:  b_olsen\n",
      "(213, 2)\n",
      "b_olsen  pass\n",
      "For name:  m_reilly\n",
      "(20, 2)\n",
      "m_reilly  pass\n",
      "For name:  d_nguyen\n",
      "(25, 2)\n",
      "d_nguyen  pass\n",
      "For name:  r_santos\n",
      "(184, 2)\n",
      "r_santos  pass\n",
      "For name:  f_ferreira\n",
      "(224, 2)\n",
      "f_ferreira  pass\n",
      "For name:  y_ng\n",
      "(19, 2)\n",
      "y_ng  pass\n",
      "For name:  j_madsen\n",
      "(69, 2)\n",
      "j_madsen  pass\n",
      "For name:  d_collins\n",
      "(31, 2)\n",
      "d_collins  pass\n",
      "For name:  l_davies\n",
      "(96, 2)\n",
      "l_davies  pass\n",
      "For name:  m_mora\n",
      "(131, 2)\n",
      "m_mora  pass\n",
      "For name:  a_fontana\n",
      "(203, 2)\n",
      "a_fontana  pass\n",
      "For name:  r_chen\n",
      "(367, 2)\n",
      "r_chen  pass\n",
      "For name:  s_krause\n",
      "(70, 2)\n",
      "s_krause  pass\n",
      "For name:  t_smith\n",
      "(603, 2)\n",
      "Total sample size before apply threshold:  603\n",
      "Counter({'0000-0002-3650-9381': 154, '0000-0003-1673-2954': 113, '0000-0002-2120-2766': 85, '0000-0002-6279-9685': 84, '0000-0003-3528-6793': 65, '0000-0003-4453-9713': 32, '0000-0002-5197-5030': 26, '0000-0002-3945-630X': 10, '0000-0001-7894-6814': 9, '0000-0002-5750-0706': 6, '0000-0002-5495-8906': 4, '0000-0003-3762-6253': 4, '0000-0002-0479-4261': 3, '0000-0003-2389-461X': 2, '0000-0001-6272-8871': 2, '0000-0001-7683-2653': 1, '0000-0002-2104-2264': 1, '0000-0001-9068-4642': 1, '0000-0002-1881-2766': 1})\n",
      "Total author before apply threshoid:  19\n",
      "Total author after apply threshoid:  2\n",
      "Total sample size after apply threshold:  267\n",
      "Total missing sample:  0\n",
      "(267, 101)\n",
      "Total missing sample:  12\n",
      "(267, 101)\n",
      "Labeled:  267  :  267\n",
      "(267, 103)\n",
      "(267, 103)\n",
      "t_smith is binary case\n",
      "[('t_smith_1', 0.4225352112676056), ('t_smith_0', 0.5774647887323944)]\n",
      "[('t_smith_1', 4, 11), ('t_smith_0', 6, 15)]\n",
      "Initial L size:  10\n",
      "Initial U size:  177\n",
      "U size after drawing sample to U prime: 0\n",
      "Initial U prime size:  177\n",
      "P value:  1  N value:  1\n",
      "Iteration 1  h1 new:  [171, 103]  probs:  [0.939977006930504, 0.9369057686666789]\n",
      "Iteration 1  h2 new:  [81, 66]  probs:  [0.9836836766408302, 0.9236225913042871]\n",
      "Iteration 2  h1 new:  [113, 116]  probs:  [0.9592198704829329, 0.960241552505439]\n",
      "Iteration 2  h2 new:  [182, 112]  probs:  [0.982529110236428, 0.9530318172404195]\n",
      "Iteration 3  h1 new:  [155, 180]  probs:  [0.9592446940700442, 0.9605314089326255]\n",
      "Iteration 3  h2 new:  [59, 41]  probs:  [0.983499161225349, 0.9598730581153992]\n",
      "Iteration 4  h1 new:  [34, 165]  probs:  [0.9636611947913332, 0.9666572893013258]\n",
      "Iteration 4  h2 new:  [88, 2]  probs:  [0.9809168087139383, 0.9671199008662635]\n",
      "Iteration 5  h1 new:  [58, 4]  probs:  [0.963904410766889, 0.974978792754576]\n",
      "Iteration 5  h2 new:  [26, 64]  probs:  [0.981462431301148, 0.9715510669366275]\n",
      "Iteration 6  h1 new:  [33, 68]  probs:  [0.9676507303556736, 0.9738394524034634]\n",
      "Iteration 6  h2 new:  [40, 13]  probs:  [0.9820133717253111, 0.9724994559250579]\n",
      "Iteration 7  h1 new:  [53, 71]  probs:  [0.972717532619974, 0.9747620803009034]\n",
      "Iteration 7  h2 new:  [42, 71]  probs:  [0.9865448645739631, 0.9760329240277501]\n",
      "Iteration 8  h1 new:  [37, 43]  probs:  [0.9767688821281242, 0.9768201660082277]\n",
      "Iteration 8  h2 new:  [18, 106]  probs:  [0.9871504165510343, 0.9745059978385915]\n",
      "Iteration 9  h1 new:  [93, 148]  probs:  [0.977596390300055, 0.9762672772193429]\n",
      "Iteration 9  h2 new:  [119, 17]  probs:  [0.986823979095605, 0.9758273638731733]\n",
      "Iteration 10  h1 new:  [153, 27]  probs:  [0.9806763240515185, 0.9791936161512748]\n",
      "Iteration 10  h2 new:  [46, 61]  probs:  [0.9862584134578117, 0.9775889648863395]\n",
      "Iteration 11  h1 new:  [174, 142]  probs:  [0.9815869459131413, 0.9802220073759951]\n",
      "Iteration 11  h2 new:  [45, 25]  probs:  [0.9850312272436905, 0.9785342068125412]\n",
      "Iteration 12  h1 new:  [137, 1]  probs:  [0.9794745595414507, 0.9814518010087642]\n",
      "Iteration 12  h2 new:  [170, 49]  probs:  [0.9847685302182919, 0.9792252373866771]\n",
      "Iteration 13  h1 new:  [35, 141]  probs:  [0.9809129627241651, 0.980278481552518]\n",
      "Iteration 13  h2 new:  [127, 184]  probs:  [0.9852627346883741, 0.9792605388922718]\n",
      "Iteration 14  h1 new:  [77, 36]  probs:  [0.9806400061030512, 0.9810498970608146]\n",
      "Iteration 14  h2 new:  [107, 16]  probs:  [0.9859156196275621, 0.9785397448590892]\n",
      "Iteration 15  h1 new:  [168, 156]  probs:  [0.9819635610225234, 0.9825759732973964]\n",
      "Iteration 15  h2 new:  [130, 29]  probs:  [0.986921423165332, 0.9792806775541932]\n",
      "Iteration 16  h1 new:  [140, 23]  probs:  [0.9826445202148245, 0.9830619301043401]\n",
      "Iteration 16  h2 new:  [74, 23]  probs:  [0.9859707224984617, 0.9783418281721339]\n",
      "Iteration 17  h1 new:  [15, 178]  probs:  [0.9815355018031262, 0.9823998965033446]\n",
      "Iteration 17  h2 new:  [15, 51]  probs:  [0.9863122950895377, 0.9787901833908441]\n",
      "Iteration 18  h1 new:  [104, 121]  probs:  [0.982443366447608, 0.9828162197397778]\n",
      "Iteration 18  h2 new:  [100, 60]  probs:  [0.9856438242805302, 0.978337399335723]\n",
      "Iteration 19  h1 new:  [95, 183]  probs:  [0.982630683536877, 0.9834818795141794]\n",
      "Iteration 19  h2 new:  [169, 73]  probs:  [0.9847405089525275, 0.9798904787820136]\n",
      "Iteration 20  h1 new:  [161, 87]  probs:  [0.9833109240315941, 0.983100694985402]\n",
      "Iteration 20  h2 new:  [97, 65]  probs:  [0.9851263187249861, 0.978616423093695]\n",
      "Iteration 21  h1 new:  [69, 47]  probs:  [0.9828705876906408, 0.9826872055137169]\n",
      "Iteration 21  h2 new:  [85, 92]  probs:  [0.9857251292162161, 0.9790242830371537]\n",
      "Iteration 22  h1 new:  [108, 44]  probs:  [0.9828008888761999, 0.9824498674857367]\n",
      "Iteration 22  h2 new:  [115, 31]  probs:  [0.9845715936379641, 0.9761904227335335]\n",
      "Iteration 23  h1 new:  [5, 120]  probs:  [0.9836846019196975, 0.9830529711176056]\n",
      "Iteration 23  h2 new:  [151, 67]  probs:  [0.9841247453840126, 0.9765294990117294]\n",
      "Iteration 24  h1 new:  [48, 50]  probs:  [0.9841885341416927, 0.9822836533089329]\n",
      "Iteration 24  h2 new:  [54, 160]  probs:  [0.9862230627186355, 0.9770560142599614]\n",
      "Iteration 25  h1 new:  [114, 30]  probs:  [0.983821286446741, 0.9834518749761221]\n",
      "Iteration 25  h2 new:  [139, 167]  probs:  [0.987678618343145, 0.9794197946823747]\n",
      "Iteration 26  h1 new:  [24, 158]  probs:  [0.9847331797347474, 0.9859759884955284]\n",
      "Iteration 26  h2 new:  [72, 186]  probs:  [0.9878452232235962, 0.977403646821197]\n",
      "Iteration 27  h1 new:  [83, 98]  probs:  [0.9848064294948805, 0.9826252601785299]\n",
      "Iteration 27  h2 new:  [8, 98]  probs:  [0.9874369672626067, 0.9759998644167942]\n",
      "Iteration 28  h1 new:  [56, 147]  probs:  [0.9847105217523704, 0.983246114659344]\n",
      "Iteration 28  h2 new:  [172, 147]  probs:  [0.9876655687548098, 0.9743904727232834]\n",
      "Iteration 29  h1 new:  [99, 52]  probs:  [0.9844587745917602, 0.9826165827196244]\n",
      "Iteration 29  h2 new:  [32, 6]  probs:  [0.9859366146533315, 0.972187826595319]\n",
      "Iteration 30  h1 new:  [152, 134]  probs:  [0.9846495066573203, 0.983183907660092]\n",
      "Iteration 30  h2 new:  [138, 111]  probs:  [0.9874911512103239, 0.9661242400163016]\n",
      "Total Labeled number:  125  Still unlabeled number:  62\n",
      "co_LR f1:               precision    recall  f1-score   support\n",
      "\n",
      "  t_smith_0       1.00      1.00      1.00        31\n",
      "  t_smith_1       1.00      1.00      1.00        23\n",
      "\n",
      "avg / total       1.00      1.00      1.00        54\n",
      "\n",
      "[31  0  0 23]\n",
      "Initial L size:  10\n",
      "Initial U size:  177\n",
      "U size after drawing sample to U prime: 0\n",
      "Initial U prime size:  177\n",
      "P value:  1  N value:  1\n",
      "Iteration 1  h1 new:  [174, 103]  probs:  [0.9402769523410153, 0.9675272671895966]\n",
      "Iteration 1  h2 new:  [81, 13]  probs:  [0.9833444700402704, 0.9121357835114056]\n",
      "Iteration 2  h1 new:  [113, 116]  probs:  [0.9433973745482718, 0.9610220208607885]\n",
      "Iteration 2  h2 new:  [182, 49]  probs:  [0.9781669474434866, 0.9291514985617737]\n",
      "Iteration 3  h1 new:  [171, 4]  probs:  [0.9606848022640462, 0.9636906181731695]\n",
      "Iteration 3  h2 new:  [59, 66]  probs:  [0.9800185267270515, 0.9382408471455567]\n",
      "Iteration 4  h1 new:  [33, 142]  probs:  [0.9618156986951899, 0.9682483980575939]\n",
      "Iteration 4  h2 new:  [88, 71]  probs:  [0.9796586291091821, 0.9413336067236014]\n",
      "Iteration 5  h1 new:  [34, 51]  probs:  [0.9628982982475739, 0.9597552179515977]\n",
      "Iteration 5  h2 new:  [18, 2]  probs:  [0.9860472215812962, 0.9587293573470408]\n",
      "Iteration 6  h1 new:  [127, 180]  probs:  [0.965292087844272, 0.9759236031085838]\n",
      "Iteration 6  h2 new:  [40, 64]  probs:  [0.9804265261568703, 0.9547569077814005]\n",
      "Iteration 7  h1 new:  [37, 120]  probs:  [0.9733028593475035, 0.9718446961522127]\n",
      "Iteration 7  h2 new:  [42, 41]  probs:  [0.9752091542764801, 0.951936192504973]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8  h1 new:  [153, 27]  probs:  [0.977931005239486, 0.9699399903255834]\n",
      "Iteration 8  h2 new:  [26, 27]  probs:  [0.9760964504438864, 0.9489689918894623]\n",
      "Iteration 9  h1 new:  [155, 165]  probs:  [0.978491447088967, 0.9688315756136531]\n",
      "Iteration 9  h2 new:  [155, 61]  probs:  [0.9737844782590296, 0.9465473187415395]\n",
      "Iteration 10  h1 new:  [5, 156]  probs:  [0.9779577402732634, 0.9724039254729812]\n",
      "Iteration 10  h2 new:  [45, 112]  probs:  [0.9722034970277506, 0.9488487118575948]\n",
      "Iteration 11  h1 new:  [93, 148]  probs:  [0.9802289938711065, 0.9735624546391705]\n",
      "Iteration 11  h2 new:  [119, 25]  probs:  [0.9952847287447353, 0.9601288253036389]\n",
      "Iteration 12  h1 new:  [130, 87]  probs:  [0.9797745237736399, 0.9766141954536857]\n",
      "Iteration 12  h2 new:  [107, 17]  probs:  [0.9951343292476899, 0.954193474521579]\n",
      "Iteration 13  h1 new:  [53, 36]  probs:  [0.9798206004745822, 0.9722624339956875]\n",
      "Iteration 13  h2 new:  [46, 106]  probs:  [0.9960258600999056, 0.9577721407340576]\n",
      "Iteration 14  h1 new:  [168, 141]  probs:  [0.9824701649832626, 0.9738553824913073]\n",
      "Iteration 14  h2 new:  [170, 1]  probs:  [0.9963793110329112, 0.9587585397336519]\n",
      "Iteration 15  h1 new:  [35, 23]  probs:  [0.9828929972054924, 0.9742442136169881]\n",
      "Iteration 15  h2 new:  [137, 65]  probs:  [0.9996198345798455, 0.9713111159796634]\n",
      "Iteration 16  h1 new:  [24, 68]  probs:  [0.9773557277994575, 0.9790348759215233]\n",
      "Iteration 16  h2 new:  [100, 16]  probs:  [0.9988177955979363, 0.9656702645833428]\n",
      "Iteration 17  h1 new:  [114, 63]  probs:  [0.9769673282643728, 0.9773962321754909]\n",
      "Iteration 17  h2 new:  [15, 29]  probs:  [0.9994310253963896, 0.9729515467753042]\n",
      "Iteration 18  h1 new:  [104, 43]  probs:  [0.9770544243030265, 0.9813096862124353]\n",
      "Iteration 18  h2 new:  [140, 184]  probs:  [0.9988455266466656, 0.9707393288820474]\n",
      "Iteration 19  h1 new:  [99, 76]  probs:  [0.9778169627736611, 0.9825579108570324]\n",
      "Iteration 19  h2 new:  [74, 158]  probs:  [0.9990597605566804, 0.970800814009868]\n",
      "Iteration 20  h1 new:  [85, 178]  probs:  [0.97913944803423, 0.9802788344944601]\n",
      "Iteration 20  h2 new:  [97, 98]  probs:  [0.9951443065272938, 0.9682640210564234]\n",
      "Iteration 21  h1 new:  [58, 121]  probs:  [0.9835376588658614, 0.9849473713306065]\n",
      "Iteration 21  h2 new:  [54, 60]  probs:  [0.9963940841384096, 0.9765271914357839]\n",
      "Iteration 22  h1 new:  [138, 183]  probs:  [0.9870267090398336, 0.9906210539084283]\n",
      "Iteration 22  h2 new:  [169, 73]  probs:  [0.9957606643392812, 0.9721750218120228]\n",
      "Iteration 23  h1 new:  [48, 52]  probs:  [0.9871723929659615, 0.9837046489108714]\n",
      "Iteration 23  h2 new:  [8, 67]  probs:  [0.9958596490106288, 0.9718660884285142]\n",
      "Iteration 24  h1 new:  [164, 47]  probs:  [0.9861808492167552, 0.9854315594769544]\n",
      "Iteration 24  h2 new:  [115, 90]  probs:  [0.9972400699100296, 0.9693524962730046]\n",
      "Iteration 25  h1 new:  [161, 30]  probs:  [0.9872515564427857, 0.9822269726795778]\n",
      "Iteration 25  h2 new:  [108, 92]  probs:  [0.9957784711224277, 0.9654133697547036]\n",
      "Iteration 26  h1 new:  [28, 78]  probs:  [0.9872634413101761, 0.9818537544285445]\n",
      "Iteration 26  h2 new:  [139, 44]  probs:  [0.9969206400064052, 0.9660793701575826]\n",
      "Iteration 27  h1 new:  [95, 147]  probs:  [0.9877488130234474, 0.9803751094793648]\n",
      "Iteration 27  h2 new:  [95, 147]  probs:  [0.996384597270437, 0.9666786845634542]\n",
      "Iteration 28  h1 new:  [83, 132]  probs:  [0.9864672733092111, 0.9794526001871701]\n",
      "Iteration 28  h2 new:  [151, 6]  probs:  [0.9967741937439412, 0.9640057878059691]\n",
      "Iteration 29  h1 new:  [11, 123]  probs:  [0.9868875315094366, 0.9823497162946411]\n",
      "Iteration 29  h2 new:  [172, 31]  probs:  [0.9963036027363545, 0.9659621557449958]\n",
      "Iteration 30  h1 new:  [84, 50]  probs:  [0.9895818684939263, 0.9803895679386755]\n",
      "Iteration 30  h2 new:  [56, 167]  probs:  [0.9972279806937353, 0.9625358605843933]\n",
      "Total Labeled number:  126  Still unlabeled number:  61\n",
      "co_SVM f1:               precision    recall  f1-score   support\n",
      "\n",
      "  t_smith_0       1.00      1.00      1.00        31\n",
      "  t_smith_1       1.00      1.00      1.00        23\n",
      "\n",
      "avg / total       1.00      1.00      1.00        54\n",
      "\n",
      "[31  0  0 23]\n",
      "Initial L size:  10\n",
      "Initial U size:  177\n",
      "U size after drawing sample to U prime: 0\n",
      "Initial U prime size:  177\n",
      "P value:  1  N value:  1\n",
      "Iteration 1  h1 new:  [171, 103]  probs:  [0.939977006930504, 0.9369057686666789]\n",
      "Iteration 1  h2 new:  [81, 13]  probs:  [0.984396429133512, 0.922618658103968]\n",
      "Iteration 2  h1 new:  [113, 116]  probs:  [0.960541128189135, 0.9637545866670181]\n",
      "Iteration 2  h2 new:  [182, 49]  probs:  [0.9710482242813181, 0.9410911962365371]\n",
      "Iteration 3  h1 new:  [34, 4]  probs:  [0.9602678541872397, 0.9657896644229147]\n",
      "Iteration 3  h2 new:  [59, 66]  probs:  [0.9793107350942941, 0.9378247303137341]\n",
      "Iteration 4  h1 new:  [33, 43]  probs:  [0.9597891609387683, 0.9686635778380653]\n",
      "Iteration 4  h2 new:  [88, 71]  probs:  [0.9801111298048039, 0.9392200042500656]\n",
      "Iteration 5  h1 new:  [53, 64]  probs:  [0.9623672126650044, 0.9715302792346665]\n",
      "Iteration 5  h2 new:  [18, 2]  probs:  [0.9762797776496462, 0.9468061999738968]\n",
      "Iteration 6  h1 new:  [26, 141]  probs:  [0.9681533601099142, 0.9734466855410241]\n",
      "Iteration 6  h2 new:  [40, 41]  probs:  [0.9777937647434878, 0.9461839321451945]\n",
      "Iteration 7  h1 new:  [58, 165]  probs:  [0.9741726597077687, 0.9733689621506484]\n",
      "Iteration 7  h2 new:  [42, 27]  probs:  [0.9741777937525017, 0.9566698123833515]\n",
      "Iteration 8  h1 new:  [37, 142]  probs:  [0.976301386110613, 0.9770571399178963]\n",
      "Iteration 8  h2 new:  [137, 61]  probs:  [0.9804441502454244, 0.9579996814781654]\n",
      "Iteration 9  h1 new:  [93, 68]  probs:  [0.9768881358762758, 0.9791766726994816]\n",
      "Iteration 9  h2 new:  [170, 36]  probs:  [0.9895985243377292, 0.9612771447985222]\n",
      "Iteration 10  h1 new:  [153, 1]  probs:  [0.9787284736335514, 0.9795040003590922]\n",
      "Iteration 10  h2 new:  [45, 112]  probs:  [0.9852575507873528, 0.9538460828709927]\n",
      "Iteration 11  h1 new:  [174, 180]  probs:  [0.9794271143049069, 0.9787839004998091]\n",
      "Iteration 11  h2 new:  [155, 65]  probs:  [0.9849786508448323, 0.9579112934857483]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "# loop through all files in directory add name to name list\n",
    "fileDir = \"../Data/\"+Dataset+\"/canopies_labeled/\"\n",
    "listfiles = os.listdir(fileDir)\n",
    "\n",
    "init_labeled_size = 10\n",
    "\n",
    "co_lr_diff_embedding_result = collections.defaultdict(list)\n",
    "\n",
    "#---------------- load different embeddings for view one ---------------#\n",
    "for select_emb in pp_text:\n",
    "    # read embeddings\n",
    "    print(\"Load text embedding: \", select_emb)\n",
    "    viewone_text_emb = com_func.read_text_embedding(emb_type=select_emb, training_size = \"140k\")\n",
    "    print(\"Load citation embedding: \", pp_citation)\n",
    "    viewtwo_citation_embedding = com_func.read_citation_embedding_sorted(emb_type = pp_citation, labeled_only = True)\n",
    "    # print(viewone_text_emb[0])\n",
    "    # print(viewtwo_citation_embedding[0])\n",
    "    \n",
    "    threshold_change_all_method_f1s = collections.defaultdict(list)\n",
    "    # -------------- different threshold (step by 10) -----------------------#\n",
    "    for step_threshold in range(threshold_lower, threshold_upper, 10):\n",
    "        plot_save_path = \"../../plot/co_train_detail_plots/threshold=\"+str(step_threshold)+\"/V1=\"+select_emb+\"_V2=\"+pp_citation+\"/\"\n",
    "        threshold_change_all_method_f1s[\"threshold\"].append(step_threshold)\n",
    "        # collect statistic to output\n",
    "        statistic_detail = collections.defaultdict(list)\n",
    "        total_selected_group = 0\n",
    "        selected_binary_case_group = 0\n",
    "\n",
    "        # ------- different name group in all name group --------------------#\n",
    "        for file in listfiles:\n",
    "            # group name\n",
    "            temp = file.split(\"_\")\n",
    "            name = str(temp[1]+\"_\"+temp[-1])\n",
    "            print(\"For name: \",name)\n",
    "            # read label (pid : author ORCID) from file\n",
    "            data = com_func.read_pid_aid(fileDir+file)\n",
    "            labeled_mask = data[\"authorID\"] != \"-1\"\n",
    "            labeled_data = data[labeled_mask]\n",
    "            print(labeled_data.shape)\n",
    "            # ---------------- collect all labeled sample -------------------- #\n",
    "            # ---------------- if use all samples as negative --------------- #\n",
    "            all_labeled_samples = labeled_data[\"paperID\"].tolist()\n",
    "            authorCounter = com_func.select_productive_groups(labeled_data=labeled_data, \n",
    "                                                              threshold=threshold_select_name_group)\n",
    "            ''' \n",
    "            Case 1: no author under this name have written more than threshold number of papers, dataset not used\n",
    "            Case 2: only one author under this name written more than threshold number of papers, dataset not used\n",
    "            Case 3: 2 or more author under this name written more than threshold number of papers, dataset used\n",
    "            '''\n",
    "            if(len(authorCounter)==0) or (len(authorCounter)==1):\n",
    "                print(name,\" pass\")\n",
    "            else:\n",
    "                total_selected_group+= 1\n",
    "                '''\n",
    "                Case 1: All papers under name group is used. We include authors written less than threshold of paper,\n",
    "                        and treat it as negative class. This will be OVR.(Not used)\n",
    "                Case 2: All papers under name group is used. We include authors written less than threshold of paper,\n",
    "                        and perform muti-class classification (Not used)\n",
    "                Case 3: Only Include author with more than threshold number of paper and perform muti-class classification (Not used)\n",
    "                Case 5: Only Include author with more than threshold number of paper and perform OVR(used)\n",
    "                Case 4: Only Include author with more than threshold number of paper and only select binary case(used)\n",
    "                '''\n",
    "                if apply_threshold_to_name_group_samples == True:\n",
    "                    # ---------- only use sample pass threshold ------- #\n",
    "                    #-------- only select authors in name group are very productive (more than threshold)---------#\n",
    "                    labeled_data, author_list, _= com_func.only_select_productive_authors(labeled_data, step_threshold)\n",
    "                    # ----------------- if use filtered samples as negative  --------- #\n",
    "                    filtered_all_labeled_samples = labeled_data[\"paperID\"].tolist()\n",
    "                else:\n",
    "                    # ----------- use all sample in name group --------- #\n",
    "                    author_list = com_func.productive_authors_list(labeled_data, step_threshold)\n",
    "                    print(name, \" name group sample size: \",labeled_data.shape)\n",
    "                # -------------- extract all samples for name group -------------- #\n",
    "                # for each name group\n",
    "                # read in labeled data\n",
    "                labeled_viewone_text = com_func.extract_sorted_embedding(viewone_text_emb, labeled_data[\"paperID\"])\n",
    "                print(labeled_viewone_text.shape)\n",
    "                labeled_viewtwo_citation = com_func.extract_sorted_embedding(viewtwo_citation_embedding, labeled_data[\"paperID\"])\n",
    "                print(labeled_viewtwo_citation.shape)\n",
    "                print(\"Labeled: \",len(labeled_viewone_text), \" : \", len(labeled_viewtwo_citation))\n",
    "                # ---------------- shuffle the data ----------------- #\n",
    "                labeled_data = labeled_data.sample(frac=1).reset_index(drop=True)\n",
    "                # ------------------ alignment and fill missing data with 0 ---------------------- #\n",
    "                labeled_viewone_text = pd.merge(labeled_data, labeled_viewone_text, left_on=\"paperID\", right_on = [0], how = \"left\")\n",
    "                labeled_viewtwo_citation = pd.merge(labeled_data, labeled_viewtwo_citation, left_on=\"paperID\", right_on = [0], how = \"left\")\n",
    "                labeled_viewtwo_citation.fillna(0, inplace=True)\n",
    "                unique_labels = labeled_viewone_text.authorID.unique()\n",
    "                map_dict = {}\n",
    "                for idx, unique_label in enumerate(unique_labels):\n",
    "                    map_dict[unique_label] = name+\"_\"+str(idx)\n",
    "                true_label = labeled_viewone_text[\"authorID\"].replace(map_dict)\n",
    "                \n",
    "                print(labeled_viewone_text.shape)\n",
    "                print(labeled_viewtwo_citation.shape)\n",
    "                '''\n",
    "                only work on binary case, ignored multi-class case\n",
    "                We need to check whether the name group only contain binary case or not\n",
    "                '''\n",
    "                if len(author_list) == 2:\n",
    "                    #if name in [\"p_robinson\",\"t_smith\",\"d_richardson\",\"y_wang\",\"w_lee\",\"k_becker\"]:\n",
    "                    #   print(name, \" Pass for error checking\")\n",
    "                    #    continue\n",
    "                    selected_binary_case_group +=1\n",
    "                    print(name + \" is binary case\")\n",
    "                    viewone_text_final = labeled_viewone_text.drop([\"paperID\", \"authorID\", 0], axis=1)\n",
    "                    viewtwo_citation_final = labeled_viewtwo_citation.drop([\"paperID\", \"authorID\", 0], axis=1)\n",
    "                    \n",
    "                    ''' Apply different algorithm:\n",
    "                    Part 1: Basic supervised algorithm \n",
    "                    Part 2: Basic co-training algorithm \n",
    "                    Part 3: 2 clf co-training \n",
    "                    Part 4: Improved co-training algorithm (Self-proposed with all different improvement)\n",
    "                    '''\n",
    "                    # -------------------- part 1 ------------------------- #\n",
    "                    LR_clf = LogisticRegression(solver= \"liblinear\")\n",
    "                    SVM_clf = SVC(gamma=\"auto\", kernel='linear')\n",
    "                    # -------------------- part 2 ------------------------- #\n",
    "                    initial_cotrain_parameters = {\"p\":1,\"n\":1,\"k\":30}\n",
    "                    co_LR_clf = Co_training_clf(clf1=LogisticRegression(solver= \"liblinear\"),**initial_cotrain_parameters)\n",
    "                    ''' For co-training with SVM\n",
    "                    Case 1: Using Scikit-learn where we set probability=True\n",
    "                    Case 1 details: Scikit-learn uses LibSVM internally, and this in turn uses Platt scaling\n",
    "                    Platt scaling requires first training the SVM as usual, then optimizing parameter \n",
    "                    vectors A and B such that: P(y|X) = 1 / (1 + exp(A * f(X) + B))\n",
    "                    where f(X) is the signed distance of a sample from the hyperplane\n",
    "                    (scikit-learn's decision_function method).\n",
    "                    Case 2: Using decision_function to get signed distance of sample from the hyperplane, calculate proba\n",
    "                    Case 2 details: use sigmoid (binary)/ softmax (muti-class) for calculate probability based on \n",
    "                    signed distance of sample from the hyperplane. Then follow same step as case 1. The result is relatively\n",
    "                    bad\n",
    "                    '''\n",
    "                    # ----------- Case 1 Using Scikit-learn where we set probability=True ------ #\n",
    "                    co_SVM_clf = Co_training_clf(clf1=SVC(gamma=\"auto\", kernel='linear',probability=True),**initial_cotrain_parameters)\n",
    "                    # ------- Case 2 Using decision_function get distance, calculate proba------ #\n",
    "                    #co_svm_clf = Co_training_clf(clf1=SVC(gamma=\"auto\", kernel='linear'),**initial_cotrain_parameters)\n",
    "                    #co_train_clfs = [(co_svm_clf,\"co_train_SVM\")]\n",
    "                    # -------------------- part 3 ------------------------- #\n",
    "                    # two different clf with basic co-training\n",
    "                    co_LR_SVM_clf = Co_training_clf(clf1 = LogisticRegression(solver= \"liblinear\"),\n",
    "                                                    clf2 = SVC(gamma=\"auto\", kernel='linear',probability=True),\n",
    "                                                    **initial_cotrain_parameters)\n",
    "                    # -------------------- part 4 ------------------------- #\n",
    "                    improved_cotrain_parameters = {\"sl_total\":2,\"k\":30}\n",
    "                    improved_co_LR = Improved_co_training_clf(clf1 = LogisticRegression(solver= \"liblinear\"), \n",
    "                                                              **improved_cotrain_parameters)\n",
    "                    # -------------------- train together ----------------- #\n",
    "                    baseline_clfs = [(LR_clf,\"LR\"),(SVM_clf,\"SVM\")]\n",
    "                    final_co_train_clfs = [(co_LR_clf,\"co_LR\"), (co_SVM_clf,\"co_SVM\"),\n",
    "                                           (co_LR_SVM_clf, \"co_LR_SVM\"), (improved_co_LR, \"Improved_co_LR\")]\n",
    "                    final_f1_score, cv_per_fold_status= k_fold_cv_all_algorithm(dv1=viewone_text_final,\n",
    "                                                                                dv2=viewtwo_citation_final,\n",
    "                                                                                label=true_label,\n",
    "                                                                                init_labeled_size=init_labeled_size,\n",
    "                                                                                muti_view_clf=final_co_train_clfs,\n",
    "                                                                                combined_clf=baseline_clfs,\n",
    "                                                                                num_fold=5,\n",
    "                                                                                dataset_name=name,\n",
    "                                                                                plot_save_path=plot_save_path)\n",
    "                    #print(final_f1_score)\n",
    "                    #print(cv_per_fold_status)\n",
    "                    statistic_detail['Name'].append(name)\n",
    "                    statistic_detail['Total_sample_size'].append(len(true_label))\n",
    "                    statistic_detail['Train_size'].append(cv_per_fold_status[0][\"train_size\"])\n",
    "                    statistic_detail['Unlabel_size'].append(cv_per_fold_status[0][\"unlabel_size\"])\n",
    "                    statistic_detail['Validation_size'].append(cv_per_fold_status[0][\"validation_size\"])\n",
    "                    statistic_detail['Test_size'].append(cv_per_fold_status[0][\"test_size\"])\n",
    "                    for clf, clf_name in final_co_train_clfs:\n",
    "                        statistic_detail[clf_name+'_total_self_labeled'].append(cv_per_fold_status[0][clf_name+\"_total_self_labeled\"])\n",
    "                    \n",
    "                    for clf_name, clf_f1 in final_f1_score:\n",
    "                        statistic_detail[clf_name+\"_f1\"].append(clf_f1)\n",
    "                else:\n",
    "                    print(name+ \" is multi-class case, ignored\")\n",
    "\n",
    "        # print(statistic_detail)\n",
    "        print(\"Total number of selected group:\",total_selected_group)\n",
    "        print(\"Total number of selected binary group:\",selected_binary_case_group)\n",
    "        print(\"Total number of selected muti-class group:\",(total_selected_group-selected_binary_case_group))\n",
    "        # write evaluation result to excel\n",
    "        output = pd.DataFrame(statistic_detail)\n",
    "        print(output)\n",
    "        savePath = \"../../result/\"+Dataset+\"/co_train_sample=140k/\"\n",
    "        filename = \"(init_labeled_size=\"+str(init_labeled_size)+\") V1=\"+select_emb+\"_V2=\"+pp_citation+\"_threshold=\"+str(step_threshold)+\".csv\"\n",
    "        com_func.write_csv_df(savePath, filename, output)\n",
    "        print(\"v1:\",select_emb,\" v2:\",pp_citation, \"threshold\",step_threshold,\" Done\")\n",
    "        \n",
    "        '''Save result with respect to threshold change'''\n",
    "        threshold_change_all_method_f1s['Name'].append(statistic_detail['Name'])\n",
    "        for col in output.columns: \n",
    "            if \"f1\" in col:\n",
    "                threshold_change_all_method_f1s[col].append(statistic_detail[col])\n",
    "\n",
    "    co_lr_diff_embedding_result[\"v1:\"+select_emb+\" v2:\"+pp_citation].append(threshold_change_all_method_f1s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#         # --------------- plot overall result f1 variance --------------- #\n",
    "#         all_per_fold_f1_score_variance_plot = pd.DataFrame(all_per_fold_f1_score_variance)\n",
    "#         ax = sns.boxplot(x=\"author\", y=\"f1\", data=all_per_fold_f1_score_variance_plot)\n",
    "#         ax = sns.swarmplot(x=\"author\", y=\"f1\", data=all_per_fold_f1_score_variance_plot, color=\".25\")\n",
    "#         plt.savefig(plot_save_path+\"all_result_variance.png\", dpi=300)\n",
    "#         # plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
