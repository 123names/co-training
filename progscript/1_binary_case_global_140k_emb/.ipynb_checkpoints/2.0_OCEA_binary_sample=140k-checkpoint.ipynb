{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T08:47:03.466120Z",
     "start_time": "2020-05-25T08:47:03.456947Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scikit-learn version is 0.23.1.\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One classifier each author (OCEA)\n",
    "1. This method throw away the authors write less than 100 papers\n",
    "2. This method will convert muti-class case to mutiple OVR binary classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T10:26:29.198373Z",
     "start_time": "2020-05-25T10:26:27.286180Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import com_func\n",
    "\n",
    "# parameters\n",
    "#----- filter for selecting set of name group -----------#\n",
    "filter_select_name_group = 100\n",
    "#----- filter for selecting productive authors ----#\n",
    "filter_lower = 100\n",
    "filter_upper = 110\n",
    "\n",
    "Dataset = \"pubmed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T08:47:06.028138Z",
     "start_time": "2020-05-25T08:47:06.022129Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# text embedding only\n",
    "pp_text_emb = [\"tf\", \"tf_idf\", \"lsa\", \"pv_dm\", \"pv_dbow\"]\n",
    "pp_citation_emb = [\"off\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T08:47:06.645827Z",
     "start_time": "2020-05-25T08:47:06.638321Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# citation embedding only\n",
    "pp_text_emb = [\"off\"]\n",
    "pp_citation_emb = [\"n2v\",\"node2vec\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T10:26:35.676672Z",
     "start_time": "2020-05-25T10:26:35.652307Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combined embedding\n",
    "pp_text_emb = [\"lsa\", \"pv_dm\", \"pv_dbow\"]\n",
    "pp_citation_emb = [\"n2v\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T10:26:36.333934Z",
     "start_time": "2020-05-25T10:26:36.325831Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lsa', 'pv_dm', 'pv_dbow']\n",
      "['n2v']\n"
     ]
    }
   ],
   "source": [
    "print(pp_text_emb)\n",
    "print(pp_citation_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T10:33:15.130647Z",
     "start_time": "2020-05-25T10:32:50.441907Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load text embedding:  lsa\n",
      "Total text vector records: 135796\n",
      "Vector dimension:  100\n",
      "Load citation embedding:  n2v\n",
      "Total citation vector records: 124922\n",
      "Vector dimension:  100\n",
      "For name:  j_read\n",
      "j_read  pass\n",
      "For name:  f_esteves\n",
      "f_esteves  pass\n",
      "For name:  c_miller\n",
      "c_miller  pass\n",
      "For name:  r_jha\n",
      "r_jha  pass\n",
      "For name:  a_lowe\n",
      "a_lowe  pass\n",
      "For name:  a_vega\n",
      "a_vega  pass\n",
      "For name:  k_smith\n",
      "k_smith  pass\n",
      "For name:  j_gordon\n",
      "j_gordon  pass\n",
      "For name:  s_liao\n",
      "s_liao  pass\n",
      "For name:  j_qian\n",
      "j_qian  pass\n",
      "For name:  s_bernardi\n",
      "s_bernardi  pass\n",
      "For name:  t_hill\n",
      "t_hill  pass\n",
      "For name:  s_schindler\n",
      "s_schindler  pass\n",
      "For name:  j_williams\n",
      "j_williams  pass\n",
      "For name:  s_jacobson\n",
      "s_jacobson  pass\n",
      "For name:  e_andrade\n",
      "e_andrade  pass\n",
      "For name:  t_santos\n",
      "t_santos  pass\n",
      "For name:  k_kim\n",
      "Muti-class case, convert to mutiple binary case.\n",
      "For name:  d_ricci\n",
      "d_ricci  pass\n",
      "For name:  s_cameron\n",
      "s_cameron  pass\n",
      "For name:  t_wright\n",
      "t_wright  pass\n",
      "For name:  r_cunha\n",
      "r_cunha  pass\n",
      "For name:  s_fuchs\n",
      "s_fuchs  pass\n",
      "For name:  m_nawaz\n",
      "m_nawaz  pass\n",
      "For name:  k_harris\n",
      "k_harris  pass\n",
      "For name:  r_daniel\n",
      "r_daniel  pass\n",
      "For name:  k_xu\n",
      "k_xu  pass\n",
      "For name:  s_antunes\n",
      "s_antunes  pass\n",
      "For name:  k_cho\n",
      "k_cho  pass\n",
      "For name:  j_sanderson\n",
      "j_sanderson  pass\n",
      "For name:  s_uddin\n",
      "s_uddin  pass\n",
      "For name:  a_batista\n",
      "a_batista  pass\n",
      "For name:  h_pereira\n",
      "h_pereira  pass\n",
      "For name:  a_patel\n",
      "a_patel  pass\n",
      "For name:  r_graham\n",
      "r_graham  pass\n",
      "For name:  a_nilsson\n",
      "a_nilsson  pass\n",
      "For name:  m_soto\n",
      "m_soto  pass\n",
      "For name:  g_guidi\n",
      "g_guidi  pass\n",
      "For name:  e_andersson\n",
      "e_andersson  pass\n",
      "For name:  s_reid\n",
      "s_reid  pass\n",
      "For name:  a_maleki\n",
      "a_maleki  pass\n",
      "For name:  j_moon\n",
      "j_moon  pass\n",
      "For name:  t_abe\n",
      "t_abe  pass\n",
      "For name:  x_fu\n",
      "x_fu  pass\n",
      "For name:  f_ortega\n",
      "f_ortega  pass\n",
      "For name:  r_morris\n",
      "r_morris  pass\n",
      "For name:  w_fang\n",
      "w_fang  pass\n",
      "For name:  m_amaral\n",
      "m_amaral  pass\n",
      "For name:  h_song\n",
      "h_song  pass\n",
      "For name:  h_dai\n",
      "h_dai  pass\n",
      "For name:  y_nakajima\n",
      "y_nakajima  pass\n",
      "For name:  t_warner\n",
      "t_warner  pass\n",
      "For name:  s_saha\n",
      "s_saha  pass\n",
      "For name:  j_fernandez\n",
      "j_fernandez  pass\n",
      "For name:  m_pan\n",
      "m_pan  pass\n",
      "For name:  a_simon\n",
      "a_simon  pass\n",
      "For name:  r_freitas\n",
      "r_freitas  pass\n",
      "For name:  c_yun\n",
      "c_yun  pass\n",
      "For name:  j_huang\n",
      "j_huang  pass\n",
      "For name:  p_santos\n",
      "p_santos  pass\n",
      "For name:  n_young\n",
      "n_young  pass\n",
      "For name:  d_ross\n",
      "d_ross  pass\n",
      "For name:  q_wang\n",
      "q_wang  pass\n",
      "For name:  c_cardoso\n",
      "c_cardoso  pass\n",
      "For name:  j_matthews\n",
      "j_matthews  pass\n",
      "For name:  g_lee\n",
      "g_lee  pass\n",
      "For name:  m_salem\n",
      "m_salem  pass\n",
      "For name:  h_lai\n",
      "h_lai  pass\n",
      "For name:  r_harris\n",
      "r_harris  pass\n",
      "For name:  c_vaughan\n",
      "c_vaughan  pass\n",
      "For name:  e_thompson\n",
      "e_thompson  pass\n",
      "For name:  r_gomes\n",
      "r_gomes  pass\n",
      "For name:  r_bennett\n",
      "r_bennett  pass\n",
      "For name:  m_collins\n",
      "m_collins  pass\n",
      "For name:  m_cowley\n",
      "m_cowley  pass\n",
      "For name:  p_teixeira\n",
      "p_teixeira  pass\n",
      "For name:  c_cox\n",
      "c_cox  pass\n",
      "For name:  s_hsu\n",
      "s_hsu  pass\n",
      "For name:  f_williams\n",
      "f_williams  pass\n",
      "For name:  d_parsons\n",
      "d_parsons  pass\n",
      "For name:  a_choudhury\n",
      "a_choudhury  pass\n",
      "For name:  c_richter\n",
      "c_richter  pass\n",
      "For name:  m_hossain\n",
      "m_hossain  pass\n",
      "For name:  v_alves\n",
      "v_alves  pass\n",
      "For name:  j_becker\n",
      "j_becker  pass\n",
      "For name:  m_soares\n",
      "m_soares  pass\n",
      "For name:  j_yi\n",
      "j_yi  pass\n",
      "For name:  s_khan\n",
      "s_khan  pass\n",
      "For name:  a_rao\n",
      "a_rao  pass\n",
      "For name:  d_cameron\n",
      "d_cameron  pass\n",
      "For name:  c_morgan\n",
      "c_morgan  pass\n",
      "For name:  h_cui\n",
      "h_cui  pass\n",
      "For name:  p_zhang\n",
      "p_zhang  pass\n",
      "For name:  j_fernandes\n",
      "j_fernandes  pass\n",
      "For name:  a_jain\n",
      "a_jain  pass\n",
      "For name:  d_zhang\n",
      "d_zhang  pass\n",
      "For name:  b_huang\n",
      "b_huang  pass\n",
      "For name:  m_chong\n",
      "m_chong  pass\n",
      "For name:  m_cerqueira\n",
      "m_cerqueira  pass\n",
      "For name:  p_yang\n",
      "p_yang  pass\n",
      "For name:  j_marques\n",
      "j_marques  pass\n",
      "For name:  n_ali\n",
      "n_ali  pass\n",
      "For name:  h_ng\n",
      "h_ng  pass\n",
      "For name:  m_viana\n",
      "m_viana  pass\n",
      "For name:  t_inoue\n",
      "t_inoue  pass\n",
      "For name:  b_meyer\n",
      "b_meyer  pass\n",
      "For name:  c_liao\n",
      "c_liao  pass\n",
      "For name:  k_wheeler\n",
      "k_wheeler  pass\n",
      "For name:  m_rizzo\n",
      "m_rizzo  pass\n",
      "For name:  y_shi\n",
      "y_shi  pass\n",
      "For name:  c_luo\n",
      "c_luo  pass\n",
      "For name:  j_arthur\n",
      "j_arthur  pass\n",
      "For name:  m_ansari\n",
      "m_ansari  pass\n",
      "For name:  g_anderson\n",
      "g_anderson  pass\n",
      "For name:  m_hidalgo\n",
      "m_hidalgo  pass\n",
      "For name:  k_jacobsen\n",
      "k_jacobsen  pass\n",
      "For name:  s_kelly\n",
      "s_kelly  pass\n",
      "For name:  s_james\n",
      "s_james  pass\n",
      "For name:  p_persson\n",
      "p_persson  pass\n",
      "For name:  y_tanaka\n",
      "y_tanaka  pass\n",
      "For name:  c_gao\n",
      "c_gao  pass\n",
      "For name:  w_jung\n",
      "w_jung  pass\n",
      "For name:  s_lewis\n",
      "s_lewis  pass\n",
      "For name:  w_han\n",
      "w_han  pass\n",
      "For name:  m_shah\n",
      "m_shah  pass\n",
      "For name:  c_arango\n",
      "c_arango  pass\n",
      "For name:  r_young\n",
      "r_young  pass\n",
      "For name:  r_coleman\n",
      "r_coleman  pass\n",
      "For name:  b_kang\n",
      "b_kang  pass\n",
      "For name:  s_carter\n",
      "s_carter  pass\n",
      "For name:  c_thomas\n",
      "c_thomas  pass\n",
      "For name:  m_gutierrez\n",
      "m_gutierrez  pass\n",
      "For name:  s_moon\n",
      "s_moon  pass\n",
      "For name:  r_pereira\n",
      "r_pereira  pass\n",
      "For name:  a_nielsen\n",
      "a_nielsen  pass\n",
      "For name:  j_conde\n",
      "j_conde  pass\n",
      "For name:  k_wright\n",
      "k_wright  pass\n",
      "For name:  m_parker\n",
      "m_parker  pass\n",
      "For name:  h_huang\n",
      "h_huang  pass\n",
      "For name:  j_terry\n",
      "j_terry  pass\n",
      "For name:  y_xu\n",
      "y_xu  pass\n",
      "For name:  a_melo\n",
      "a_melo  pass\n",
      "For name:  r_doyle\n",
      "r_doyle  pass\n",
      "For name:  m_bernardo\n",
      "m_bernardo  pass\n",
      "For name:  j_soares\n",
      "j_soares  pass\n",
      "For name:  j_richard\n",
      "j_richard  pass\n",
      "For name:  p_robinson\n",
      "Total sample size before apply filter:  275\n",
      "Counter({'0000-0002-7878-0313': 133, '0000-0002-0736-9199': 119, '0000-0002-3156-3418': 19, '0000-0002-0577-3147': 4})\n",
      "Total author before apply threshoid:  4\n",
      "['0000-0002-0736-9199', '0000-0002-7878-0313']\n",
      "Total author after apply threshoid:  2\n",
      "Total sample size after apply filter:  252\n",
      "Total missing sample:  0\n",
      "Text embedding shape:  (252, 100)\n",
      "Missing Sample:  10929617\n",
      "Missing Sample:  23920735\n",
      "Missing Sample:  18283030\n",
      "Missing Sample:  25355889\n",
      "Missing Sample:  21857152\n",
      "Missing Sample:  12192611\n",
      "Total missing sample:  6\n",
      "Citation embedding shape:  (252, 100)\n",
      "Final feature (combined embedding) shape:  (252, 200)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_percent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-23cbd12b2a8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m                     \u001b[0mstatistic_detail\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Orginal sample size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_orginal_sample_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                     \u001b[0mstatistic_detail\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Total selected sample size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabeled_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                     \u001b[0mstatistic_detail\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"used_train_percent\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_percent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m                     \u001b[0;31m# -------------- using converted feature vector to train classifier-------------------#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtext_emb\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tf\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_percent' is not defined"
     ]
    }
   ],
   "source": [
    "# load the file\n",
    "import io\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from statistics import mean \n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "fileDir = \"../Data/\"+Dataset+\"/canopies_labeled/\"\n",
    "listfiles = os.listdir(fileDir)\n",
    "\n",
    "diff_embedding_result = collections.defaultdict(list)\n",
    "\n",
    "# ----------------------- different text embedding ----------------------#\n",
    "for text_emb in pp_text_emb:\n",
    "    print(\"Load text embedding: \", text_emb)\n",
    "    # read pretrained embeddings\n",
    "    all_text_embedding = com_func.read_text_embedding(emb_type=text_emb, training_size=\"140k\")\n",
    "    all_text_emb_pid = [emb[0] for emb in all_text_embedding]\n",
    "    all_text_embedding = [emb[1:] for emb in all_text_embedding]\n",
    "    \n",
    "    for citation_emb in pp_citation_emb:\n",
    "        print(\"Load citation embedding: \", citation_emb)\n",
    "        all_citation_embedding = com_func.read_citation_embedding_sorted(emb_type = citation_emb)\n",
    "        all_citation_emb_pid = [emb[0] for emb in all_citation_embedding]\n",
    "        all_citation_embedding = [emb[1:] for emb in all_citation_embedding]\n",
    "        \n",
    "        diff_threshold_result = collections.defaultdict(list)\n",
    "\n",
    "        # -------------- different filter (step by 10) -----------------------#\n",
    "        for step_filter in range(filter_lower, filter_upper, 10):\n",
    "            # collect statistic to output\n",
    "            statistic_detail = collections.defaultdict(list)\n",
    "            \n",
    "            # ------- select useful name group in all name group --------------------#\n",
    "            for file in listfiles:\n",
    "                # group name\n",
    "                temp = file.split(\"_\")\n",
    "                name = temp[1]+\"_\"+temp[-1]\n",
    "                print(\"For name: \",name)\n",
    "                # read needed content in labeled file\n",
    "                labeled_data = com_func.read_pid_aid(fileDir+file)\n",
    "                #----------- select name group contain productive author------------------------------------#\n",
    "                #----------- (contain pair of author write more than 100 papers) ---------------------------#\n",
    "                # count number of paper each author write based on author ID\n",
    "                authorCounter = collections.Counter(labeled_data[\"authorID\"])\n",
    "                # remove name group that do not contain pair of author write more than 100 papers\n",
    "                for k in list(authorCounter):\n",
    "                    if authorCounter[k] < filter_select_name_group:\n",
    "                        del authorCounter[k]\n",
    "                # if only have one class or no class pass the filter, not applicable\n",
    "                if(len(authorCounter)==0) or (len(authorCounter)==1):\n",
    "                    print(name, \" pass\")\n",
    "                elif len(authorCounter)>2:\n",
    "                    print(\"Muti-class case, convert to mutiple binary case.\")\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                    temp_orginal_sample_size = len(labeled_data)\n",
    "                    #--------select authors in name group are very productive (more than filter)---------#\n",
    "                    print(\"Total sample size before apply filter: \",len(labeled_data))\n",
    "                    # count number of paper each author write based on author ID\n",
    "                    paperCounter = collections.Counter(labeled_data[\"authorID\"])\n",
    "                    print(paperCounter)\n",
    "                    print(\"Total author before apply threshoid: \", len(paperCounter))\n",
    "                    # collect per class statistic\n",
    "                    for k in list(paperCounter):\n",
    "                        if paperCounter[k] < step_filter:\n",
    "                            del paperCounter[k]\n",
    "                    temp =list(paperCounter.keys())\n",
    "                    print(temp)\n",
    "                    print(\"Total author after apply threshoid: \", len(temp))\n",
    "                    # remove samples that are smaller than filter\n",
    "                    labeled_data = labeled_data[labeled_data.authorID.isin(temp)]\n",
    "                    print(\"Total sample size after apply filter: \",len(labeled_data))\n",
    "                    #------------ extract paper representation -------------------------------------------#\n",
    "                    # shuffle the data\n",
    "                    labeled_data = labeled_data.sample(frac=1).reset_index(drop=True)\n",
    "                    # extract true label and pid\n",
    "                    label = labeled_data[\"authorID\"]\n",
    "                    pid = labeled_data[\"paperID\"]\n",
    "                    # list of different data field\n",
    "                    part_collection = []\n",
    "                    # select feature wanted to fit to clustering/classification algorithm\n",
    "                    # data part, text information\n",
    "                    data_part_text = com_func.extract_embedding(all_text_embedding, all_text_emb_pid, pid)\n",
    "                    print(\"Text embedding shape: \", data_part_text.shape)\n",
    "                    part_collection.append(data_part_text)\n",
    "                    # data part, citation information\n",
    "                    data_part_citation = com_func.extract_embedding(all_citation_embedding, all_citation_emb_pid, pid)\n",
    "                    data_part_citation.fillna(0, inplace=True)\n",
    "                    print(\"Citation embedding shape: \", data_part_citation.shape)\n",
    "                    part_collection.append(data_part_citation)\n",
    "                    # merge different part of data data together by concatenate it all together\n",
    "                    # remove empty emb (when emb set off)\n",
    "                    part_collection = [part for part in part_collection if len(part)!=0]\n",
    "                    if len(part_collection)>1:\n",
    "                        combinedata = np.concatenate(part_collection,axis=1)\n",
    "                    elif len(part_collection)==1:\n",
    "                        if isinstance(part_collection[0], pd.DataFrame):\n",
    "                            combinedata = part_collection[0].values\n",
    "                        else:\n",
    "                            combinedata = part_collection[0]\n",
    "                    else:\n",
    "                        print(\"No data available\")\n",
    "                        break\n",
    "                    print(\"Final feature (combined embedding) shape: \", combinedata.shape)\n",
    "                    statistic_detail[\"Name group\"].append(name)\n",
    "                    statistic_detail[\"Class number\"].append(len(paperCounter))\n",
    "                    statistic_detail[\"Per class size\"].append(paperCounter)\n",
    "                    statistic_detail[\"Orginal sample size\"].append(temp_orginal_sample_size)\n",
    "                    statistic_detail[\"Total selected sample size\"].append(len(labeled_data))\n",
    "                    # -------------- using converted feature vector to train classifier-------------------#\n",
    "                    if text_emb == \"tf\":\n",
    "                        # using multinomial naive bayes\n",
    "                        clf = MultinomialNB()    \n",
    "                        mnbaccuracy, mnbmarcof1= com_func.k_fold_cv(combinedata, label, clf, k=10, verbose=True)\n",
    "                        print(\"MNB F1: \", mnbmarcof1)\n",
    "                        statistic_detail['MNB Accuracy'].append(mnbaccuracy)\n",
    "                        statistic_detail['MNB macro F1'].append(mnbmarcof1)\n",
    "                    # using logistic regression\n",
    "                    clf = LogisticRegression(solver= \"liblinear\")\n",
    "                    LRaccuracy, LRmarcof1 = com_func.k_fold_cv(combinedata, label, clf, k=10, verbose=True)\n",
    "                    print(\"LR F1: \", LRmarcof1)\n",
    "                    statistic_detail[\"LR accuracy\"].append(LRaccuracy)\n",
    "                    statistic_detail[\"LR macro f1\"].append(LRmarcof1)\n",
    "                    # using SVM with linear kernal\n",
    "                    clf = SVC(gamma=\"auto\", kernel='linear')\n",
    "                    svcaccuracy, svcmarcof1 = com_func.k_fold_cv(combinedata, label, clf, k=10, verbose=True)\n",
    "                    print(\"SVM F1: \", svcmarcof1)\n",
    "                    statistic_detail[\"SVM(linear) accuracy\"].append(svcaccuracy)\n",
    "                    statistic_detail[\"SVM(linear) macro f1\"].append(svcmarcof1)\n",
    "\n",
    "            # write evaluation result to excel\n",
    "            output = pd.DataFrame(statistic_detail)\n",
    "            print(output)\n",
    "\n",
    "            #savePath = \"../../result/\"+Dataset+\"/3_OCEA_sample=140k/\"\n",
    "            #filename = \"citation=\"+citation_emb+\"_textual=\"+text_emb+\"_threshold=\"+str(step_filter)+\".csv\"\n",
    "            #com_func.write_csv_df(savePath, filename, output)\n",
    "            print(\"Done\")\n",
    "            \n",
    "            diff_threshold_result[step_filter].append(statistic_detail)\n",
    "        \n",
    "        diff_embedding_result[\"text=\"+text_emb+\"citation=\"+citation_emb]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T16:40:50.867327Z",
     "start_time": "2019-01-10T09:58:57.655751Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T16:40:50.910860Z",
     "start_time": "2019-01-10T16:40:50.871136Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%who"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
