{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chung-may yang1.txt', 'wei lu0.txt', 'yong wang1.txt', 'david g lloyd0.txt', 'wei lu1.txt', 'feng liu1.txt', 'david g lloyd1.txt', 'jeong hwan kim1.txt', 'chung-may yang0.txt', 'michael wagner0.txt', 'feng liu0.txt', 'hao song1.txt', 'hao song0.txt', 'kevin m. ryan0.txt', 'michael wagner1.txt', 'lei wang0.txt', 'jeong hwan kim0.txt', 'yong wang0.txt', 'lei wang1.txt', 'kevin m. ryan1.txt']\n",
      "['-0.29111460', '-0.57349956', '0.35524827', '0.12974840', '0.30130574', '-0.25134709', '0.21317476', '-0.74120957', '-0.05038653', '-0.44930771', '-0.02827097', '0.35546294', '0.11417462', '0.11215554', '0.00902790', '-0.20662700', '-0.40714705', '0.18991761', '0.18334278', '0.49447197', '0.21461654', '-0.34866148', '-0.12432522', '0.58297175', '0.19949827', '0.37721580', '-0.28323400', '-0.53867114', '0.16258357', '-0.05756472', '0.09064528', '0.18177433', '0.23114556', '-0.22056936', '-0.03383492', '0.22713418', '0.62852019', '0.14818428', '-0.09277134', '0.22546473', '-0.00073958', '0.28379443', '0.24716207', '0.04616041', '-0.29582870', '0.12199543', '0.23276801', '0.08426338', '0.18150330', '0.48511741', '0.03518584', '-0.18573113', '0.15923220', '0.18722166', '-0.08286999', '0.92173892', '-0.47948411', '-0.20138085', '-0.13308075', '-0.16949883', '-0.66930991', '-0.27881575', '0.15169108', '-0.08045414', '0.44854951', '-0.11193195', '0.32205865', '0.58506852', '-0.25695717', '0.06712881', '0.06763204', '0.01241753', '-0.09207717', '0.39172146', '-0.24621403', '0.31211156', '0.20003089', '-0.33453855', '-0.03267162', '0.48280972', '-0.11395531', '-0.18531565', '-0.35127044', '0.07696979', '-0.25818723', '-0.13042338', '-0.42942816', '-0.11625058', '0.19474849', '0.60119998', '-0.05024971', '0.54225159', '-0.09773158', '0.52069086', '0.52670187', '0.08883087', '0.83152694', '0.03487471', '0.06771380', '-0.43320087']\n",
      "['0.48811737', '-0.89885867', '0.63597471', '0.74343598', '0.39413932', '-0.19972996', '-0.37192163', '-0.54846859', '0.11815764', '0.27440688', '0.08777013', '1.11670542', '0.39519590', '0.06948131', '0.89608246', '0.28940451', '-0.24033083', '0.34529680', '-0.12072088', '0.77289879', '0.19952983', '-1.12572658', '0.10560617', '-0.20119810', '0.11866401', '0.60006714', '-0.16597860', '0.37303886', '0.58431685', '0.53690714', '0.07008725', '-0.03308011', '0.83598572', '-0.01733498', '0.07663001', '0.14173755', '0.90755218', '0.42818424', '0.50650179', '-0.08766368', '0.28023475', '0.46757829', '0.44325149', '-0.20686235', '-0.24372311', '0.63556534', '0.29122278', '-0.01377441', '0.40478453', '0.36952838', '-0.13007671', '-0.25054377', '0.54704148', '-0.00033016', '0.19884837', '0.76412088', '0.01011791', '-0.41842061', '-0.11263781', '0.60089415', '-0.33212543', '-0.94814461', '0.57095128', '-0.20849088', '0.16446288', '0.34934559', '0.59629029', '1.02091181', '-0.23383248', '-0.10511358', '-0.39743105', '0.13612102', '-0.74410897', '0.64863527', '-0.59933531', '-0.02761723', '-0.87829655', '-0.95933992', '0.41976884', '0.54103261', '1.11250508', '-0.22968370', '-0.06337447', '0.03990682', '0.10587190', '0.34913626', '0.14546487', '-0.13192964', '-0.31581873', '-0.38207194', '-0.16037741', '0.40482804', '0.09845087', '0.10937776', '-0.00518686', '0.03875283', '0.40064085', '0.39086735', '0.08817277', '-0.18701327']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# collect data\n",
    "# ../Data/DataForClassification/d2v/\n",
    "fileDir = \"../Data/DataForClassification/d2v/\"\n",
    "fileList = os.listdir(fileDir)\n",
    "print(fileList)\n",
    "\n",
    "# # auto method that go through all the file in directory\n",
    "# for file in fileList:\n",
    "#     if not file.startswith('.'):\n",
    "#         if file.endswith(\".txt\"):\n",
    "#             file = file[:-4]\n",
    "\n",
    "# hard code to read the file one by one\n",
    "author0 = []\n",
    "author1 = []\n",
    "with open(fileDir+\"chung-may yang0.txt\", 'r', encoding = 'utf8') as f:\n",
    "    for line in f:\n",
    "        read_data = line.strip().split(\"\\t\")\n",
    "        author0.append(read_data[1].split(\" \"))\n",
    "\n",
    "with open(fileDir+\"chung-may yang1.txt\", 'r', encoding = 'utf8') as f:\n",
    "    for line in f:\n",
    "        read_data = line.strip().split(\"\\t\")\n",
    "        author1.append(read_data[1].split(\" \"))\n",
    "print(author0[0])\n",
    "print(author1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "71\n"
     ]
    }
   ],
   "source": [
    "# size of each class\n",
    "print(len(author0))\n",
    "print(len(author1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# number of features (dimension)\n",
    "print(len(author0[0]))\n",
    "print(len(author1[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0            1            2            3            4  \\\n",
      "0  -0.29111460  -0.57349956   0.35524827   0.12974840   0.30130574   \n",
      "1   0.92845869  -0.93609655   0.31517544  -0.40926400   0.12442853   \n",
      "2   0.56281191  -0.17059737   0.21973124   0.56775701   0.17825009   \n",
      "3   0.02262557   0.13886765   0.08385008   0.44914705   0.25022486   \n",
      "4   0.22692077  -0.18510655  -0.13579145   0.54744142  -0.32509586   \n",
      "\n",
      "             5            6            7            8            9  ...   \\\n",
      "0  -0.25134709   0.21317476  -0.74120957  -0.05038653  -0.44930771  ...    \n",
      "1  -1.39242876  -1.08160102   0.23345214  -0.15230766   0.68835104  ...    \n",
      "2  -0.34945986  -0.13540348  -0.08924282   0.51040816   0.10645398  ...    \n",
      "3  -0.22274359  -0.12428337  -0.28856090   0.31007025   0.31714562  ...    \n",
      "4  -0.57200897   0.13991199  -0.08472289   0.28843415   0.14538938  ...    \n",
      "\n",
      "           91           92           93          94           95           96  \\\n",
      "0  0.54225159  -0.09773158   0.52069086  0.52670187   0.08883087   0.83152694   \n",
      "1  0.21228662  -0.29553482   0.13904431  1.22640836   0.30946401  -0.52207536   \n",
      "2  0.25717881  -0.44214025  -0.33171317  0.01272102  -0.18379262  -0.23683710   \n",
      "3  0.05496796  -0.55629671   0.36189812  0.11816001   0.06013902   0.23907846   \n",
      "4  0.52964157  -0.16966768   0.29633474  0.04132035   0.29781947   0.38301200   \n",
      "\n",
      "            97          98           99 label  \n",
      "0   0.03487471  0.06771380  -0.43320087     0  \n",
      "1  -0.95033252  1.21378624   0.30401042     0  \n",
      "2  -0.14294611  0.15258272  -0.15730695     0  \n",
      "3   0.29205322  0.08400392  -0.11653788     0  \n",
      "4  -0.16347094  0.19720869  -0.09567451     0  \n",
      "\n",
      "[5 rows x 101 columns]\n"
     ]
    }
   ],
   "source": [
    "# reconstract data so that we can feed it to svm\n",
    "import pandas as pd\n",
    "classOne = pd.DataFrame(author0)\n",
    "classOne[\"label\"] = 0\n",
    "#print(classOne[:2:])\n",
    "classTwo = pd.DataFrame(author1)\n",
    "classTwo[\"label\"] = 1\n",
    "#print(classTwo[:2:])\n",
    "# combine data from different class get all data\n",
    "combinedData = pd.concat([classOne, classTwo])\n",
    "print(combinedData[:5])\n",
    "combinedData = combinedData.sample(frac=1).reset_index(drop=True)\n",
    "# split data and label\n",
    "data = combinedData.drop('label', axis=1)\n",
    "label = combinedData['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation\n",
    "def k_fold_cv(data, label, classifier, clfname):\n",
    "    kf = KFold(n_splits=10, shuffle=True)\n",
    "    # create lists to collect statistic\n",
    "    tp = []\n",
    "    fp = []\n",
    "    tn = []\n",
    "    fn = []\n",
    "    roundf1 = []\n",
    "    for train_index, test_index in kf.split(data):\n",
    "        # print(\"TRAIN:\", train_index, \" \\n TEST:\", test_index)\n",
    "        # split train and test\n",
    "        data_train, data_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        label_train, test_true_label = label.iloc[train_index], label.iloc[test_index]\n",
    "        # fit data to svm\n",
    "        classifier.fit(data_train, label_train)\n",
    "        # get predicted label\n",
    "        label_pred = classifier.predict(data_test)\n",
    "        # find round confusion matrix\n",
    "        round_tn, round_fp, round_fn, round_tp = metrics.confusion_matrix(test_true_label, label_pred).ravel()\n",
    "        # add data data to array\n",
    "        tp.append(round_tp)\n",
    "        fp.append(round_fp)\n",
    "        fn.append(round_fn)\n",
    "        tn.append(round_tn)\n",
    "        roundf1.append(f1_score(test_true_label, label_pred,average='micro'))\n",
    "        # print(\"True positive: {tp}, False positive: {fp}, False negative: {fn}, True negative: {tn}\"\n",
    "        # .format(tp=round_tp, fp=round_fp, fn=round_fn, tn=round_tn))\n",
    "\n",
    "    print(\"Classifier: {name}\\nTrue positive: {tp}, False positive: {fp}, False negative: {fn}, True negative: {tn}\"\n",
    "          .format(name=clfname, tp=np.sum(tp), fp=np.sum(fp), fn=np.sum(fn), tn=np.sum(tn)))\n",
    "    f1 = np.average(roundf1)\n",
    "    ppv, npv, specificity, sensitivity, accuracy = calculate_important_value(np.sum(tp), np.sum(tn),\n",
    "                                                                             np.sum(fp), np.sum(fn), len(data),f1)\n",
    "    return ppv, npv, specificity, sensitivity, accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate ppv,npv,specificity,sensitivity, and accuracy\n",
    "def calculate_important_value(tp, tn, fp, fn, sample_length,f1):\n",
    "    # 1. Positive predicted value (PPV) or precision aka hit rate = True positive/ )True positive + False positive)\n",
    "    ppv = (tp / (tp + fp))\n",
    "    # 2. Negative predicted value (NPV) = True negative / (True negative + False negative)\n",
    "    npv = (tn / (tn + fn))\n",
    "    # 3. Specificity = (1 - False positive)\n",
    "    specificity = (tn / (tn + fp))\n",
    "    # 4. Sensitivity = True positive\n",
    "    sensitivity = (tp / (tp + fn))\n",
    "    # 5. Accuracy = (True positive + True negative) / Total number of sample\n",
    "    accuracy = (tp + tn) / sample_length\n",
    "    print('PPV: ', ppv)\n",
    "    print('NPV: ', npv)\n",
    "    print('Specificity: ', specificity)\n",
    "    print('Sensitivity: ', sensitivity)\n",
    "    print('Accuracy: ', accuracy)\n",
    "    print('F1: ', f1)\n",
    "    return ppv, npv, specificity, sensitivity, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "Classifier: logistic\n",
      "True positive: 44, False positive: 21, False negative: 27, True negative: 21\n",
      "PPV:  0.676923076923077\n",
      "NPV:  0.4375\n",
      "Specificity:  0.5\n",
      "Sensitivity:  0.6197183098591549\n",
      "Accuracy:  0.5752212389380531\n",
      "F1:  0.5742424242424242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.676923076923077,\n",
       " 0.4375,\n",
       " 0.5,\n",
       " 0.6197183098591549,\n",
       " 0.5752212389380531,\n",
       " 0.5742424242424242)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logistic regression\n",
    "logistic = linear_model.LogisticRegression(C=1e5)\n",
    "print(logistic)\n",
    "# fit model and do 10-fold cv\n",
    "k_fold_cv(data, label, logistic, \"logistic\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
