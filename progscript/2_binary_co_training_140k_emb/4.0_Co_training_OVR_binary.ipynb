{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OVR binary\n",
    "No filter, all samples in groups is selected and treated as negative group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T05:21:25.382571Z",
     "start_time": "2020-06-01T05:21:24.741642Z"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T05:21:25.841182Z",
     "start_time": "2020-06-01T05:21:25.838525Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import logging\n",
    "# logging.basicConfig(filename='./test.log', level=logging.DEBUG, \n",
    "#                     format='%(asctime)s %(levelname)s %(name)s %(message)s')\n",
    "# logger=logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T05:21:26.268428Z",
     "start_time": "2020-06-01T05:21:26.260229Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from IPython.core.magic import register_cell_magic\n",
    "\n",
    "# @register_cell_magic('handle')\n",
    "# def handle(line, cell):\n",
    "#     try:\n",
    "#         exec(cell)\n",
    "#     except Exception as e:\n",
    "#         logger.error(e)\n",
    "#         raise # if you want the full trace-back in the notebook\n",
    "\n",
    "\n",
    "# use %%handle when want to output error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co-training\n",
    "\n",
    "For visualization of co-training process, we apply PCA to feature before training. This will make co-training process clear, but the result will be not accuracy because apply PCA will loss lots of information.\n",
    "\n",
    "1. We assume only part of label exist\n",
    "\n",
    "2. We only select binary case (Only when one name indicate two and only two author)\n",
    "\n",
    "3. When we apply 10 fold with co-training, each fold of first iteration will be baseline compare to co-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T04:03:44.182708Z",
     "start_time": "2020-06-03T04:03:42.350334Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "#warnings.filterwarnings('error')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import com_func\n",
    "\n",
    "#----- threshold for selecting set of name group -----------#\n",
    "threshold_select_name_group = 100\n",
    "#----- threshold for selecting min sample in name group ----#\n",
    "threshold_lower = 100\n",
    "threshold_upper = 110\n",
    "\n",
    "apply_threshold_to_name_group_samples = False\n",
    "\n",
    "\n",
    "pp_text = [\"pv_dbow\"]\n",
    "pp_citation = [\"n2v\"]\n",
    "\n",
    "Dataset = \"pubmed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Co-training details\n",
    "1. Basic co-training algorithm required parameter p,n,k,u. Since we have 15 different dataset, we will assume p and n is 1, k is 30. (We are simulate real world situation where we do not know the distribution of unlabeled data amount 15 different dataset)\n",
    "2. We set the parameter u as size of input train data (labeled+unlabeled) since our ublabeled data is not that large.\n",
    "3. During co-training process, the confidence measure is using probability as confident score to evaluate whether unlabel sample should be label or not.\n",
    "4. Probability only have few issues: \n",
    "Case 1: View diagreement in early iteration. The view could disagree with each other in early stage of co-training which will introduce noise; Case 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T04:03:55.382959Z",
     "start_time": "2020-06-03T04:03:44.186751Z"
    },
    "code_folding": [
     12,
     29,
     33,
     36,
     52,
     64,
     65,
     91,
     134,
     142,
     145,
     191,
     392,
     395,
     398,
     444
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import warnings\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "from sklearn.metrics import precision_score, recall_score,f1_score,accuracy_score\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# create co training classifier\n",
    "class Co_training_clf(object):\n",
    "    \n",
    "    def __init__(self, clf1, clf2=None, p=1, n=1, k=30, u = 75):\n",
    "        \n",
    "        self.clf1 = clf1\n",
    "        # assume co_training on one classifier\n",
    "        if clf2 == None:\n",
    "            self.clf2 = copy.deepcopy(clf1)\n",
    "        else:\n",
    "            self.clf2 = clf2\n",
    "        # take p example from most confidently positive labels to example\n",
    "        self.p = p\n",
    "        # take n example from most confidently negative label to example\n",
    "        self.n = n\n",
    "        # number of iteration\n",
    "        self.k = k\n",
    "        # size of pool of unlabeled samples\n",
    "        self.u = u\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=1)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return np.around(1 / (1 + np.exp(-x)), decimals=4).item()\n",
    "\n",
    "    def init_L_U_U_prime(self, labels):\n",
    "        # index of the samples that are initially labeled\n",
    "        L = labels.index[labels != -1].tolist()\n",
    "        # index of unlabeled samples\n",
    "        U = labels.index[labels == -1].tolist()\n",
    "        print(\"Initial L size: \", len(L))\n",
    "        print(\"Initial U size: \", len(U))\n",
    "        # random drawing sample from U\n",
    "        random.shuffle(U)\n",
    "        U_prime = U[-min(len(U), self.u):]\n",
    "        # remove the samples in U_prime from U\n",
    "        U = U[:-len(U_prime)]\n",
    "        print(\"U size after drawing sample to U prime:\",len(U))\n",
    "        print(\"Initial U prime size: \", len(U_prime))\n",
    "        return L, U, U_prime\n",
    "    \n",
    "    def check_iter_label_mapping(self, iter_clf1, iter_clf2):\n",
    "        '''\n",
    "        In theory, it shouldn't occur that label not mapping since it trained on same dataset but different view\n",
    "        But add a check to make sure it won't occur and save the class mapping for later label unlabeled sample\n",
    "        '''\n",
    "        dv1_class_label = iter_clf1.classes_\n",
    "        dv2_class_label = iter_clf2.classes_\n",
    "        if all(dv1_class_label == dv2_class_label):\n",
    "            self.classes_ = dv1_class_label\n",
    "        else:\n",
    "            sys.exit(\"Two view classifier label not mapping\")\n",
    "\n",
    "    def get_confidence_score(self, clf_h1, clf_h2, dv1, dv2):\n",
    "        if hasattr(clf_h1, \"predict_proba\"):\n",
    "            dv1_proba = clf_h1.predict_proba(dv1)\n",
    "            dv2_proba = clf_h2.predict_proba(dv2)\n",
    "        elif hasattr(clf_h1, \"decision_function\"):    # use decision function\n",
    "            dv1_distance = np.array(clf_h1.decision_function(dv1))\n",
    "            dv2_distance = np.array(clf_h2.decision_function(dv2))\n",
    "            # if binary case, use sigmoid function to calculate probability\n",
    "            if iter_train_label.nunique()==2:\n",
    "                dv1_proba = []\n",
    "                dv2_proba = []\n",
    "                for distance in dv1_distance:\n",
    "                    dv1_proba.append([1-self.sigmoid(distance), self.sigmoid(distance)])\n",
    "                for distance in dv2_distance:\n",
    "                    dv2_proba.append([1-self.sigmoid(distance), self.sigmoid(distance)])\n",
    "            else:\n",
    "                dv1_proba = self.softmax(dv1_distance)\n",
    "                dv2_proba = self.softmax(dv2_distance)\n",
    "            dv1_proba = np.array(dv1_proba)\n",
    "            dv2_proba = np.array(dv2_proba)\n",
    "            #print(\"Distance to hyperplane (dv1): \",dv1_distance)\n",
    "            #print(\"Probability (dv1): \",dv1_proba)\n",
    "        else:\n",
    "            sys.exit(\"No confident score for process\")\n",
    "        \n",
    "        return dv1_proba, dv2_proba\n",
    "\n",
    "    def label_p_n_samples(self, proba, rank, proba_sample_idx_map):\n",
    "        U_prime_size = len(proba)\n",
    "        self_trained_sample_idx = []\n",
    "        self_trained_confident = []\n",
    "        for label, confident_rank in enumerate(rank):\n",
    "            # 0 positive sample\n",
    "            if label==0:\n",
    "                p = []\n",
    "                p_confident = []\n",
    "                index = 0\n",
    "                while(len(p) < self.p):\n",
    "                    max_conf_sample_index = confident_rank[index]\n",
    "                    # ---- if positive predict proba is more than 50% ------- #\n",
    "                    if (proba[max_conf_sample_index][label] > 0.5):\n",
    "                        #print('Sample idx: P: ', proba_sample_idx_map[max_conf_sample_index], \" : \", proba[max_conf_sample_index])\n",
    "                        p.append(proba_sample_idx_map[max_conf_sample_index])\n",
    "                        p_confident.append(proba[max_conf_sample_index][label])\n",
    "                    index +=1\n",
    "                    if (index>=U_prime_size):\n",
    "                        break\n",
    "                self_trained_sample_idx.append(p)\n",
    "                self_trained_confident.append(p_confident)\n",
    "            # 1 negative sample\n",
    "            elif label == 1:\n",
    "                n = []\n",
    "                n_confident = []\n",
    "                index = 0\n",
    "                while(len(n) < self.n):\n",
    "                    max_conf_sample_index = confident_rank[index]\n",
    "                    # ---- if negative predict proba is more than 50% ------- #\n",
    "                    if (proba[max_conf_sample_index][label] > 0.5):\n",
    "                        #print('Sample idx: N: ', proba_sample_idx_map[max_conf_sample_index], \" : \", proba[max_conf_sample_index])\n",
    "                        n.append(proba_sample_idx_map[max_conf_sample_index])\n",
    "                        n_confident.append(proba[max_conf_sample_index][label])\n",
    "                    index +=1\n",
    "                    if (index>=U_prime_size):\n",
    "                        break\n",
    "                self_trained_sample_idx.append(n)\n",
    "                self_trained_confident.append(n_confident)\n",
    "            else:\n",
    "                print(\"Class label error\")\n",
    "        return self_trained_sample_idx, self_trained_confident\n",
    "\n",
    "    def get_self_labeled_sample(self):\n",
    "        '''\n",
    "        return:\n",
    "            self-labeled new positive, self-labeled new negative (Index)\n",
    "        '''\n",
    "        \n",
    "        return self.new_labeled_idx\n",
    "\n",
    "    def set_PCA(self, pca=None):\n",
    "        self.pca = pca\n",
    "\n",
    "    def plot_co_training_process(self, iterCount, data, iter_train_label, unlabeled_idx, h1_new=[], h2_new=[],\n",
    "                                 h1_new_prob=[], h2_new_prob=[], clf1=None, clf2=None, plotSavingPath=None, name=None):\n",
    "        if not os.path.exists(plotSavingPath):\n",
    "            os.makedirs(plotSavingPath)\n",
    "        pca_one = data[:,0]\n",
    "        pca_two = data[:,1]\n",
    "        # Layer 1. plot unlabel samples in u_prime\n",
    "        fig, ax = plt.subplots(figsize=(9,7))\n",
    "        ax.scatter(pca_one[unlabeled_idx], pca_two[unlabeled_idx], color='grey', label = \"unlabeled\", s = 50, alpha = 0.8)\n",
    "        # Layer 2. plot the labeled samples\n",
    "        for author in np.unique(iter_train_label):\n",
    "            ix = iter_train_label.index[iter_train_label == author].tolist()\n",
    "            # print(ix)\n",
    "            ax.scatter(pca_one[ix], pca_two[ix], cmap='viridis', label = author, s = 50, alpha = 0.8)\n",
    "        # layer 3. mark self labeled samples\n",
    "        if iterCount != 0:\n",
    "            all_h1_new = list(itertools.chain(*h1_new))\n",
    "            all_h2_new = list(itertools.chain(*h2_new))\n",
    "            temp_h1 = ax.scatter(pca_one[all_h1_new], pca_two[all_h1_new], edgecolor='black', linewidth=1, s=50)\n",
    "            temp_h1.set_facecolor(\"none\")\n",
    "            temp_h1.set_label(\"h1 self-labeled\")\n",
    "            temp_h2 = ax.scatter(pca_one[all_h2_new], pca_two[all_h2_new], edgecolor='red', linewidth=1, s=50)\n",
    "            temp_h2.set_facecolor(\"none\")\n",
    "            temp_h2.set_label(\"h2 self-labeled\")\n",
    "            # layer 4. mark new samples confidence and which view produce it\n",
    "            last_iter_h1_new = h1_new[-1]\n",
    "            last_iter_h2_new = h2_new[-1]\n",
    "            last_iter_h1_new_prob = h1_new_prob[-1]\n",
    "            last_iter_h2_new_prob = h2_new_prob[-1]\n",
    "            text = []\n",
    "            for i, idx in enumerate(last_iter_h1_new):\n",
    "                text.append(plt.text(pca_one[idx], pca_two[idx], \"{:.2f}\".format(last_iter_h1_new_prob[i]), color='black'))\n",
    "            for i, idx in enumerate(last_iter_h2_new):\n",
    "                text.append(plt.text(pca_one[idx], pca_two[idx], \"{:.2f}\".format(last_iter_h2_new_prob[i]), color='red'))\n",
    "            #print(text)\n",
    "            adjust_text(text, x=pca_one, y=pca_two, force_points=0.3, force_text=0.3, expand_points=(2, 2), \n",
    "                        expand_text=(2, 2), arrowprops=dict(arrowstyle='Simple', color='red'))\n",
    "        legend = ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.25), ncol=3,prop={'size': 13})\n",
    "        plt.title('Co-training iteration: '+ str(iterCount), fontsize=14)\n",
    "        plt.xlabel(\"First principal component\",fontsize=14)\n",
    "        plt.ylabel(\"Second principal component\",fontsize=14)\n",
    "        plt.savefig(fname=plotSavingPath+name+\"_PCA_i-\"+str(iterCount)+\".png\", dpi=100, bbox_extra_artists=(legend,), bbox_inches='tight')\n",
    "        #plt.show()\n",
    "        plt.close(\"all\")\n",
    "        \n",
    "\n",
    "    def fit(self, train_data, validation_data, dataset_name=None, plot_save_path=None):\n",
    "        dv1 = train_data[0]\n",
    "        dv2 = train_data[1]\n",
    "        labels = train_data[2]\n",
    "        dv1_validation = validation_data[0]\n",
    "        dv2_validation = validation_data[1] \n",
    "        label_validation = validation_data[2]\n",
    "        # using all unlabeled sample instead of pool of unlabeled sample\n",
    "        self.u = len(labels)\n",
    "        # sync input datatype\n",
    "        if not all(isinstance(i, pd.DataFrame) for i in [dv1, dv2, labels]):\n",
    "            if not isinstance(dv1, pd.DataFrame):\n",
    "                dv1 = pd.DataFrame(dv1)\n",
    "            if not isinstance(dv2, pd.DataFrame):\n",
    "                dv2 = pd.DataFrame(dv2)\n",
    "            if not isinstance(labels, pd.DataFrame):\n",
    "                labels = pd.DataFrame(labels, index = dv1.index.values)\n",
    "        labels = pd.Series(labels[0].values, index=dv1.index.values) \n",
    "        \n",
    "        L, U, U_prime = self.init_L_U_U_prime(labels)\n",
    "        #print(\"All data index: \",dv1.index.values)\n",
    "        #print(\"L: \", L)\n",
    "        #print(\"U: \", U)\n",
    "        #print(\"U_prime: \", U_prime)\n",
    "        print(\"P value: \", self.p, \" N value: \", self.n)\n",
    "        \n",
    "        # index of self labeled samples\n",
    "        self.new_labeled_idx = defaultdict(list)\n",
    "        self.h1_new_idx = defaultdict(list)\n",
    "        self.h2_new_idx = defaultdict(list)\n",
    "        # when fit co-train, we collect f1 on validation samples wrt each iteration\n",
    "        self.f1_on_validation_dv1 = []\n",
    "        self.f1_on_validation_dv2 = []\n",
    "        \n",
    "        self.iterCounter = 0\n",
    "        # --------- plot PCA reduced plot for initial stage of train -------------- #\n",
    "        init_train_label = labels[L]\n",
    "        pca = self.pca\n",
    "        '''Notice pca will change input datatype dataframe to datatype list, keep it's order,\n",
    "           or re-assign dv1.index.values to the output'''\n",
    "        pca_dv1 = pca.fit_transform(X=dv1)\n",
    "        pca_dv2 = pca.fit_transform(X=dv2)\n",
    "        #loop until we have assigned labels to every sample in U and U_prime or we hit our iteration break condition\n",
    "        while self.iterCounter < self.k and U_prime:\n",
    "            '''\n",
    "            Extract labeled training sample from training sample and train classifier, then make prediction\n",
    "            Evaluate performance of current iteration classifier\n",
    "            '''\n",
    "            # print(\"Iteration:\",self.iterCounter, \" L: \",L)\n",
    "            # print(\"Iteration:\",self.iterCounter, \" U_prime: \",U_prime)\n",
    "            # ------------- get labeled samples for train ----------- # \n",
    "            iter_train_d1 = dv1.iloc[L]\n",
    "            iter_train_d2 = dv2.iloc[L]\n",
    "            iter_train_label = labels.iloc[L]\n",
    "            # ----------- get U_prime unlabeled samples  ------------ #\n",
    "            iter_unlabeled_d1 = dv1.iloc[U_prime]\n",
    "            iter_unlabeled_d2 = dv2.iloc[U_prime]\n",
    "            # ------------ train different view with classifier ----------- #\n",
    "            iter_clf1 = copy.deepcopy(self.clf1) \n",
    "            iter_clf2 = copy.deepcopy(self.clf2)\n",
    "            iter_clf1.fit(iter_train_d1, iter_train_label)\n",
    "            iter_clf2.fit(iter_train_d2, iter_train_label)\n",
    "            self.check_iter_label_mapping(iter_clf1, iter_clf2)\n",
    "            # --------- test error on validation data --------------------- #\n",
    "            # make prediction on validation data\n",
    "            y1 = iter_clf1.predict(dv1_validation)\n",
    "            y2 = iter_clf2.predict(dv2_validation)\n",
    "            # f1 score on each iteration\n",
    "            f1_dv1 = f1_dv2 = 0\n",
    "            f1_dv1 = f1_score(label_validation, y1, average='macro')\n",
    "            f1_dv2 = f1_score(label_validation, y2, average='macro')\n",
    "            # collect f1 for current iteration\n",
    "            self.f1_on_validation_dv1.append(f1_dv1)\n",
    "            self.f1_on_validation_dv2.append(f1_dv2)\n",
    "            ''' \n",
    "            # ----------- plot the co-training process -------------- #\n",
    "            \n",
    "            '''\n",
    "            if dataset_name != None:\n",
    "                new_train_label = labels[L]\n",
    "                if len(self.h1_new_idx[\"index\"])==0:\n",
    "                    print(\"Iteration 0, strat self label.\")\n",
    "                else:\n",
    "                    print(\"Iteration\",self.iterCounter,\" h1 new: \",self.h1_new_idx[\"index\"][-1],\" probs: \", self.h1_new_idx[\"confident\"][-1])\n",
    "                    print(\"Iteration\",self.iterCounter,\" h2 new: \", self.h2_new_idx[\"index\"][-1], \" probs: \", self.h2_new_idx[\"confident\"][-1])\n",
    "                # ----- save pca reduced plot ------ #\n",
    "                for pca_view,v_name,clf in [(pca_dv1,\"dv1\",copy.deepcopy(self.clf1)),(pca_dv2,\"dv2\",copy.deepcopy(self.clf2))]:\n",
    "                    self.plot_co_training_process(self.iterCounter, pca_view, new_train_label, U_prime,\n",
    "                                                  self.h1_new_idx[\"index\"], self.h2_new_idx[\"index\"],\n",
    "                                                  self.h1_new_idx[\"confident\"], self.h2_new_idx[\"confident\"],\n",
    "                                                  plotSavingPath = plot_save_path, name = dataset_name+\"_\"+v_name)\n",
    "            ''' \n",
    "            Notice here dv1_proba and dv2_proba's index is index for u' (Unlabeled data only)\n",
    "            We use index of u' to find index (position) of data in U where U and L is all data index\n",
    "            '''\n",
    "            # rank class probabilities for unlabeled sample for it's confidence measure\n",
    "            dv1_proba, dv2_proba = self.get_confidence_score(clf_h1=iter_clf1, clf_h2=iter_clf2, \n",
    "                                                             dv1=iter_unlabeled_d1, dv2=iter_unlabeled_d2)\n",
    "            proba_sample_idx_map = iter_unlabeled_d1.index\n",
    "            dv1_proba_rank = []\n",
    "            dv2_proba_rank = []\n",
    "            # proba1_rank[i] is label i's confidence measure\n",
    "            for class_proba in dv1_proba.T:\n",
    "                dv1_proba_rank.append((-class_proba).argsort())\n",
    "            for class_proba in dv2_proba.T:\n",
    "                dv2_proba_rank.append((-class_proba).argsort())\n",
    "            # print(dv1_proba)\n",
    "            # print(dv1_proba_rank)\n",
    "            # print(dv2_proba)\n",
    "            # print(dv2_proba_rank)\n",
    "            # h1 classifier self label data\n",
    "            h1_new_sample, h1_new_sample_probs = self.label_p_n_samples(dv1_proba, dv1_proba_rank, proba_sample_idx_map)\n",
    "            # h2 classifier\n",
    "            h2_new_sample, h2_new_sample_probs = self.label_p_n_samples(dv2_proba, dv2_proba_rank, proba_sample_idx_map)\n",
    "            \n",
    "            # collect statistic for plot only (before remove self-labeled sample from u')\n",
    "            iter_h1_for_plot = list(itertools.chain(*h1_new_sample))\n",
    "            iter_h2_for_plot = list(itertools.chain(*h2_new_sample))\n",
    "            iter_h1_prob = list(itertools.chain(*h1_new_sample_probs))\n",
    "            iter_h2_prob = list(itertools.chain(*h2_new_sample_probs))\n",
    "            self.h1_new_idx[\"index\"].append(iter_h1_for_plot)\n",
    "            self.h1_new_idx[\"confident\"].append(iter_h1_prob)\n",
    "            self.h2_new_idx[\"index\"].append(iter_h2_for_plot)\n",
    "            self.h2_new_idx[\"confident\"].append(iter_h2_prob)\n",
    "            '''\n",
    "            Add most confidence samples as new training samples, auto label the samples and remove it from U_prime\n",
    "            Special Case: if two view classifier give same most confident sample and p is 1, only 1 datapoint self-labeled\n",
    "            '''\n",
    "            roundNew = list(zip(h1_new_sample, h2_new_sample))\n",
    "            roundNew_flatten_unique = []\n",
    "            for label, round_new in enumerate(roundNew):\n",
    "                round_new = set([item for sublist in round_new for item in sublist])\n",
    "                round_new = [idx for idx in round_new]\n",
    "                self.new_labeled_idx[self.classes_[label]].append(round_new)\n",
    "                roundNew_flatten_unique.extend(round_new)\n",
    "                # add label to those new samples\n",
    "                #print(labels[round_new])\n",
    "                labels[round_new] = self.classes_[label]\n",
    "                #print(labels[round_new])\n",
    "                #print(self.classes_[label],\" (u' idx): \",round_new)\n",
    "            #print(roundNew_flatten_unique)\n",
    "            # extend the labeled sample\n",
    "            L.extend(roundNew_flatten_unique)\n",
    "            # remove the labeled sample from U_prime\n",
    "            U_prime = [x for x in U_prime if x not in roundNew_flatten_unique]\n",
    "            #print(U_prime)\n",
    "            # randomly choice 2p+2n examples from u to replenish u_prime\n",
    "            replenishItem = U[-(2*self.p+2*self.n):]\n",
    "            U_prime.extend(replenishItem)\n",
    "            U = U[:-len(replenishItem)]\n",
    "            self.iterCounter +=1\n",
    "            \n",
    "            ''' \n",
    "            # ----------- plot the last iteration of co-training process -------------- #\n",
    "            \n",
    "            '''\n",
    "            if (dataset_name != None) and (self.iterCounter == self.k):\n",
    "                new_train_label = labels[L]\n",
    "                print(\"Iteration\",self.iterCounter,\" h1 new: \",self.h1_new_idx[\"index\"][-1],\" probs: \", self.h1_new_idx[\"confident\"][-1])\n",
    "                print(\"Iteration\",self.iterCounter,\" h2 new: \", self.h2_new_idx[\"index\"][-1], \" probs: \", self.h2_new_idx[\"confident\"][-1])\n",
    "                # ----- save pca reduced plot ------ #\n",
    "                for pca_view,v_name,clf in [(pca_dv1,\"dv1\",copy.deepcopy(self.clf1)),(pca_dv2,\"dv2\",copy.deepcopy(self.clf2))]:\n",
    "                    self.plot_co_training_process(self.iterCounter, pca_view, new_train_label, U_prime,\n",
    "                                                  self.h1_new_idx[\"index\"], self.h2_new_idx[\"index\"],\n",
    "                                                  self.h1_new_idx[\"confident\"], self.h2_new_idx[\"confident\"],\n",
    "                                                  plotSavingPath = plot_save_path, name = dataset_name+\"_\"+v_name)\n",
    "            \n",
    "        print(\"Total Labeled number: \", len(L), \" Still unlabeled number: \", len(U_prime))\n",
    "        #print(self.f1_on_validation_dv1)\n",
    "        #print(self.f1_on_validation_dv2)\n",
    "        # final train\n",
    "        newtrain_d1 = dv1.iloc[L]\n",
    "        newtrain_d2 = dv2.iloc[L]\n",
    "        self.clf1.fit(newtrain_d1, labels.iloc[L])\n",
    "        self.clf2.fit(newtrain_d2, labels.iloc[L])\n",
    "        '''\n",
    "        Evalutation plot for co-training process, save f1 score vs number of iteration plot\n",
    "        '''\n",
    "        if dataset_name != None:\n",
    "            default_text_based = [self.f1_on_validation_dv1[0]] * self.iterCounter\n",
    "            default_citation_based = [self.f1_on_validation_dv2[0]] * self.iterCounter\n",
    "            default_step = np.arange(0,self.iterCounter)\n",
    "            co_train_text_based = self.f1_on_validation_dv1[1:]\n",
    "            co_train_citation_based = self.f1_on_validation_dv2[1:]\n",
    "            co_training_step = np.arange(1,self.iterCounter)\n",
    "\n",
    "            fig = plt.figure()\n",
    "            ax = plt.axes()\n",
    "            plt.plot(default_step, default_text_based, linestyle='dashed', label=\"Text based default\")\n",
    "            plt.plot(default_step, default_citation_based, linestyle='dashdot', label=\"Citation based default\")\n",
    "            plt.plot(co_training_step, co_train_text_based, linestyle='solid', marker = \"*\", label=\"Text based\")\n",
    "            plt.plot(co_training_step, co_train_citation_based, linestyle='dotted', marker = \"+\", label=\"Citation based\")\n",
    "            ax.autoscale_view()\n",
    "            legend = ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.2), ncol=2)\n",
    "            plt.xlabel('Co-Training Iterations')\n",
    "            plt.ylabel('F1 score')\n",
    "            plt.savefig((plot_save_path+dataset_name+\"_co_train_iteration_f1.png\"), dpi=150, bbox_extra_artists=(legend,), bbox_inches='tight')\n",
    "            # plt.show()\n",
    "            plt.close(\"all\")\n",
    "        return \"Training Done\"\n",
    "\n",
    "    def co_train_process_f1(self):\n",
    "        return self.f1_on_validation_dv1, self.f1_on_validation_dv2\n",
    "\n",
    "    def get_iter_count(self):\n",
    "        return self.iterCounter\n",
    "\n",
    "    def predict(self, data):\n",
    "        dv1=data[0]\n",
    "        dv2=data[1]\n",
    "        dv1_predict = self.clf1.predict(dv1)\n",
    "        dv2_predict = self.clf2.predict(dv2)\n",
    "        #fill pred with -1 so we can identify the samples in which error occur\n",
    "        y_pred = [\"-1\"] * dv1.shape[0]\n",
    "        for i, (dv1_y, dv2_y) in enumerate(zip(dv1_predict, dv2_predict)):\n",
    "            # if both agree on label\n",
    "            if dv1_y == dv2_y:\n",
    "                y_pred[i] = dv1_y\n",
    "            # If disagree on label and support proba method: We times probability together, choice class have higher probabilities\n",
    "            elif hasattr(self.clf1, \"predict_proba\") and hasattr(self.clf2, \"predict_proba\"):\n",
    "                h1_probas = self.clf1.predict_proba([dv1.iloc[i]])[0]\n",
    "                h2_probas = self.clf2.predict_proba([dv2.iloc[i]])[0]\n",
    "                final_y_probas = [proba_y1 * proba_y2 for (proba_y1, proba_y2) in zip(h1_probas, h2_probas)]\n",
    "                #print(\"h1 and h2 disagree on\",i, \" h1 Proba : \",h1_probas, \" h2 Proba: \", h2_probas)\n",
    "                #print(\"product probas:\",final_y_probas)\n",
    "                max_prob_idx = final_y_probas.index(max(final_y_probas))\n",
    "                y_pred[i] = self.classes_[max_prob_idx]\n",
    "                #print(\"result idx: \", max_prob_idx, \" result: \",y_pred[i])\n",
    "            # If disagree on label and not support proba method: We use decision_function to first calculate probability,\n",
    "            # then we times calculate probability together, choice class have higher probabilities\n",
    "            elif hasattr(self.clf1, \"decision_function\") and hasattr(self.clf2, \"decision_function\"):\n",
    "                dv1_distance = self.clf1.decision_function([dv1.iloc[i]])\n",
    "                dv2_distance = self.clf2.decision_function([dv2.iloc[i]])\n",
    "                if len(self.clf1.classes_)==2:\n",
    "                    h1_probas = np.array([1-self.sigmoid(dv1_distance), self.sigmoid(dv1_distance)])\n",
    "                    h2_probas = np.array([1-self.sigmoid(dv2_distance), self.sigmoid(dv2_distance)])\n",
    "                else:\n",
    "                    h1_probas = self.softmax(dv1_distance)\n",
    "                    h2_probas = self.softmax(dv2_distance)\n",
    "                final_y_probas = [proba_y1 * proba_y2 for (proba_y1, proba_y2) in zip(h1_probas, h2_probas)]\n",
    "                #print(\"h1 and h2 disagree on\",i, \" h1 Proba : \",h1_probas, \" h2 Proba: \", h2_probas)\n",
    "                #print(\"product probas:\",final_y_probas)\n",
    "                max_prob_idx = final_y_probas.index(max(final_y_probas))\n",
    "                y_pred[i] = self.classes_[max_prob_idx]\n",
    "                #print(\"result idx: \", max_prob_idx, \" result: \",y_pred[i])\n",
    "            # disagree and not support any confidence measure\n",
    "            else:\n",
    "                sys.exit(\"Error occur\")\n",
    "\n",
    "        # convert final result to np array\n",
    "        y_pred_np_array = np.asarray(y_pred)\n",
    "        return y_pred_np_array\n",
    "\n",
    "    def predict_proba(self, dv1, dv2):\n",
    "        # the predicted probabilities is simply a product (*) of probabilities given from each classifier trained\n",
    "        h1_probas = self.clf1.predict_proba(dv1)\n",
    "        h2_probas = self.clf2.predict_proba(dv2)\n",
    "        \n",
    "        proba = (h1_probas*h2_probas)\n",
    "        return proba\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved co-training\n",
    "\n",
    "Possible improvement:\n",
    "\n",
    "Part 1: Parameter input\n",
    "1. Using two different algorithm for view one and view two.\n",
    "2. Instead of using parameter k to control number of iteration, using a stop criterion where if unlabeled sample can't get more than 95% of confidence on their confidence score, then stop iteration and finish co-training.\n",
    "3. Parameter p, n are not needed, use input data class ratio replace it (unlabeled distribution may different from labeled, this is unlikely works in reality).\n",
    "4. u prime not needed, use all unlabeled data.\n",
    "\n",
    "Part 2: Change confident measure method\n",
    "1. Changes method used when determine which class unlabeled sample belones to, instead of using confident score, we could use classifiers saved during each iteration of training to perform an majority voting. (Need to careful about number of iteration in co-training, since adding bad unlabel data as label data is easy to occur) (Source: 2004_Co-training and Self-training for Word Sense Disambiguation)\n",
    "\n",
    "Part 3: Change details in confident score method\n",
    "1. Add an third classifier and only train with original labeled samples(no-self-labeled sample), then use third classifier to evaluate self-labeled samples and get it's confidence score. Combine three classifier's confidence score together to get final decision.\n",
    "2. COTRADE method: construct undirected neighborhood graph using all samples, after finding most confident sample with it's corresponding class. Then, calculate distance between all labeled samples in this class and most confident sample in this class. This distance from graph + probability from supervised algorithm will be used together as confidence score.\n",
    "\n",
    "Part 4: Add checking to reduce the chance of adding mislabeled data\n",
    "1. Add an check on validation after new label sample is added, if not improving h1/h2, remove new labeled sample.\n",
    "\n",
    "Part 5: Change algorithm and make it work for muti-class\n",
    "1. Using same concept of muti-class SVM, train many OVR binary classifier. (To expensive)\n",
    "2. Allow algorithm directly take muti-class case and using same concept as binary co-training.\n",
    "\n",
    "\n",
    "My idea:\n",
    "1. Use one classifier (check)\n",
    "2. Have parameter k as default, but can be stoped early (check)\n",
    "3. Parameter p, n are replace with sl_base which set as input data class ratio*sl_base. (Allow muti-class case and will maintaining the class distribution in L. However, unlabeled distribution may different from labeled, this is unlikely works in reality) (check)\n",
    "4. u prime not needed, use all unlabeled data. (check)\n",
    "5. Adopt filter view disagreed samples, instead directly remove disagreed sample, we will give a score to evaluate disagreement between views and add this score as part of confident score. (2008-Multi-view learning in the presence of view disagreement)\n",
    "6. We will accumulate the probability of first iteration and current iteration; add them together as part of confident score. (check)\n",
    "7. Adopt COTRADE method, their assumption where that a correctly labeled example should be very close to samples in L with corresponding label same to be very useful. Thus a k-mean clustering is train with labeled, then transform unlabel data into same instance space as labeled samples, then calculate distance to each cluster's centroid, then use distance perfrom an softmax to get reverse of probability of sample belone to class. After that 1- reversed probability get probability and add it as part of co-train confident score. (check)\n",
    "8. We can think confident score as quality of unlabeled data, after each co-training iteration, it should always increase in best case. However, when we see a decrease in confident score compare to pervious iteration, it means the data quality level have decresed, therefore we need to check whether heighest probability of samples is greater than 0.8 or not. \n",
    "\n",
    "\n",
    "9. Notice that our main evaluator for whether a new sample should be added to L is current iteration probability, not the confident score, confident score is only used for sample quality check. (check)\n",
    "10. When no sample been added for both view, we stop co-training iteration since data left may provide more noise than information. (check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T04:04:13.517088Z",
     "start_time": "2020-06-03T04:03:55.386916Z"
    },
    "code_folding": [
     16,
     32,
     36,
     39,
     55,
     70,
     90,
     123,
     183,
     194,
     197,
     241,
     496,
     499,
     502,
     548
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import warnings\n",
    "import copy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import precision_score, recall_score,f1_score,accuracy_score\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "\n",
    "# create co training classifier\n",
    "class Improved_co_training_clf(object):\n",
    "    \n",
    "    def __init__(self, clf=[], view_num=2, sl_class_max=1, k=30, **args):\n",
    "        if len(clf) == view_num:\n",
    "            self.clf = clf\n",
    "        elif len(clf) == 1:\n",
    "            self.clf = [copy.deepcopy(clf[0]) for i in range(view_num)]\n",
    "        else:\n",
    "            sys.exit(\"Classifier doesn't match with view number.\")\n",
    "        # self-label step for each iteration\n",
    "        if sl_class_max<1:\n",
    "            sl_class_max=1\n",
    "        self.sl_class_max=sl_class_max\n",
    "        self.view_num = view_num\n",
    "        self.iter_new_size =[]\n",
    "        # number of iteration\n",
    "        self.k = k\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return np.around(1 / (1 + np.exp(-x)), decimals=4).item()\n",
    "\n",
    "    def init_L_U_U_prime(self, labels):\n",
    "        # index of the samples that are initially labeled\n",
    "        L = labels.index[labels != -1].tolist()\n",
    "        # index of unlabeled samples\n",
    "        U = labels.index[labels == -1].tolist()\n",
    "        print(\"Initial L size: \", len(L))\n",
    "        print(\"Initial U size: \", len(U))\n",
    "        # random drawing sample from U\n",
    "        random.shuffle(U)\n",
    "        U_prime = U[-min(len(U), self.u):]\n",
    "        # remove the samples in U_prime from U\n",
    "        U = U[:-len(U_prime)]\n",
    "        print(\"U size after drawing sample to U prime:\",len(U))\n",
    "        print(\"Initial U prime size: \", len(U_prime))\n",
    "        return L, U, U_prime\n",
    "    \n",
    "    def check_iter_label_mapping(self, iter_clf):\n",
    "        '''\n",
    "        In theory, it shouldn't occur that label not mapping since it trained on same dataset but different view\n",
    "        But add a check to make sure it won't occur and save the class mapping for later label unlabeled sample\n",
    "        '''\n",
    "        class_label = defaultdict(list)\n",
    "        for idx,clf in enumerate(iter_clf):\n",
    "            if idx ==0:\n",
    "                self.classes_ = clf.classes_\n",
    "            \n",
    "            if not all(self.classes_ == clf.classes_):\n",
    "                sys.exit(\"Two view classifier label not mapping\")\n",
    "        #print(self.classes_)\n",
    "        return \"checked\"\n",
    "\n",
    "    def get_confidence_score(self, clf, data):\n",
    "        if hasattr(clf, \"predict_proba\"):\n",
    "            proba = clf.predict_proba(data)\n",
    "        elif hasattr(clf, \"decision_function\"):    # use decision function\n",
    "            all_distance = np.array(clf.decision_function(data))\n",
    "            # if binary case, use sigmoid function to calculate probability\n",
    "            if iter_train_label.nunique()==2:\n",
    "                proba = []\n",
    "                for distance in all_distance:\n",
    "                    proba.append([1-self.sigmoid(distance), self.sigmoid(distance)])\n",
    "            else:\n",
    "                proba = self.softmax(dv1_distance)\n",
    "            proba = np.array(proba)\n",
    "            #print(\"Distance to hyperplane: \",distance)\n",
    "            #print(\"Probability: \",proba)\n",
    "        else:\n",
    "            sys.exit(\"No confident score for process\")\n",
    "        \n",
    "        return proba\n",
    "\n",
    "    def get_cluster_distance_as_proba(self, label_train, iter_train_label, unlabeled):\n",
    "        kmeans = KMeans(n_clusters=len(self.classes_)).fit(label_train)\n",
    "        # map correct idx to label\n",
    "        kmeans_predict = kmeans.labels_\n",
    "        label_mapping = list(zip(iter_train_label, kmeans_predict))\n",
    "        lmc = Counter(label_mapping)\n",
    "        final_label_mapping = []\n",
    "        for label in np.unique(iter_train_label):\n",
    "            for (item,index),count in lmc.most_common():\n",
    "                if item == label:\n",
    "                    #print(\"label:\",label, \" idx:\",index, \" count:\",count)\n",
    "                    final_label_mapping.append((item,index))\n",
    "                    break\n",
    "        #print(final_label_mapping)\n",
    "        \n",
    "        # convert distance to centroid to probability of belone to class\n",
    "        dist_to_centroid = kmeans.transform(unlabeled)\n",
    "        cluster_proba = []\n",
    "        for distance in dist_to_centroid:\n",
    "            reverse_proba = self.softmax(distance)\n",
    "            proba = [1-x for x in reverse_proba]\n",
    "            cluster_proba.append(proba)\n",
    "        cluster_proba = np.array(cluster_proba)\n",
    "        \n",
    "        # map the proba same order as train in supervised learning\n",
    "        new_permutation = [idx for label, idx in final_label_mapping]\n",
    "        #print(new_permutation)\n",
    "        idx = np.empty_like(new_permutation)\n",
    "        idx[new_permutation] = np.arange(len(new_permutation))\n",
    "        cluster_proba[:] = cluster_proba[:, idx]\n",
    "        \n",
    "        return cluster_proba\n",
    "\n",
    "    def label_samples(self, score, rank, score_sample_idx_map,curr_iter_proba):\n",
    "        U_prime_size = len(score)\n",
    "        self_trained_sample_idx = []\n",
    "        self_trained_confident = []\n",
    "        self_trained_iter_proba = []\n",
    "        self.iter_new_size = defaultdict(list)\n",
    "        '''Automatic estimate the class ratio and try to get correct data class ratio'''\n",
    "        # if one class have lots of high confident samples while other don't, change class ratio\n",
    "        class_ratio = []\n",
    "        for class_idx in range(len(self.classes_)):\n",
    "            high_confident = [proba for proba in curr_iter_proba if proba[class_idx]>0.95]\n",
    "            high_confident_count = max(len(high_confident), 1)\n",
    "            class_ratio.append(high_confident_count)\n",
    "        for class_idx in range(len(self.classes_)):\n",
    "            class_new_sample_size = math.ceil(class_ratio[class_idx]/min(class_ratio))\n",
    "            if class_new_sample_size>self.sl_class_max:\n",
    "                class_new_sample_size=self.sl_class_max\n",
    "            self.iter_new_size[self.classes_[class_idx]].append(class_new_sample_size)\n",
    "        #print(self.iter_new_size)\n",
    "        \n",
    "        '''Search for samples that can be labeled'''\n",
    "        for label, confident_rank in enumerate(rank):\n",
    "            # ---------- find heightest current iteration probability for all class ----- #\n",
    "            class_curr_iter_proba = [row[label] for row in curr_iter_proba]\n",
    "            class_curr_iter_proba.sort()\n",
    "            class_heightest_proba = max(class_curr_iter_proba)\n",
    "            # ------------------- find last iteration max confidence score -------------- #\n",
    "            pervious_iter_confidence_score = self.new_labeled_idx[self.classes_[label]+\"_score\"]\n",
    "            if len(pervious_iter_confidence_score)==0:\n",
    "                last_iter_max_confidence_score=0\n",
    "            else:\n",
    "                last_iter_max_confidence_score = max(self.new_labeled_idx[self.classes_[label]+\"_score\"][-1], default=0)\n",
    "            \n",
    "            new_label_sample_size = [size for size in self.iter_new_size[self.classes_[label]]][0]\n",
    "            new_sample_idx = []\n",
    "            new_sample_confident_score = []\n",
    "            new_sample_proba = []\n",
    "            index = 0\n",
    "            while(len(new_sample_idx) < new_label_sample_size):\n",
    "                max_conf_sample_index = confident_rank[index]\n",
    "                #print('Select idx: ', max_conf_sample_index,\":\",score_sample_idx_map[max_conf_sample_index],\" have score: \",score[max_conf_sample_index], \" probability \",curr_iter_proba[max_conf_sample_index])\n",
    "                if (score_sample_idx_map[max_conf_sample_index]<last_iter_max_confidence_score) and (class_heightest_proba<0.8):\n",
    "                    print('Most confident idx: ', max_conf_sample_index,\":\",score_sample_idx_map[max_conf_sample_index],\" have score: \",score[max_conf_sample_index],\" smaller than last iteration max score: \",last_iter_max_confidence_score,\" probability \",curr_iter_proba[max_conf_sample_index], \" no bigger than 0.8, sample not used\")\n",
    "                    break\n",
    "                elif curr_iter_proba[max_conf_sample_index][label] <0.5:\n",
    "                    #print('Most confident idx: ', max_conf_sample_index,\":\",score_sample_idx_map[max_conf_sample_index],\" have score: \",score[max_conf_sample_index], \"have probability \",curr_iter_proba[max_conf_sample_index], \" not in top 3, thus sample not used\")\n",
    "                    index +=1\n",
    "                else:\n",
    "                    #print('Class:', self.classes_[label],' idx: ',max_conf_sample_index,\":\",score_sample_idx_map[max_conf_sample_index], \" score: \", score[max_conf_sample_index], \" iter proba: \", curr_iter_proba[max_conf_sample_index])\n",
    "                    new_sample_idx.append(score_sample_idx_map[max_conf_sample_index])\n",
    "                    new_sample_confident_score.append(score[max_conf_sample_index][label])\n",
    "                    new_sample_proba.append(curr_iter_proba[max_conf_sample_index][label])\n",
    "                    index +=1\n",
    "                if (index>=U_prime_size) :\n",
    "                    break\n",
    "            self_trained_sample_idx.append(new_sample_idx)\n",
    "            self_trained_confident.append(new_sample_confident_score)\n",
    "            self_trained_iter_proba.append(new_sample_proba)\n",
    "        return self_trained_sample_idx, self_trained_confident, self_trained_iter_proba\n",
    "\n",
    "    def get_self_labeled_sample(self):\n",
    "        '''\n",
    "        return:\n",
    "            self-labeled new positive, self-labeled new negative (Index)\n",
    "        '''\n",
    "        temp = defaultdict(list)\n",
    "        for label in self.classes_:\n",
    "            temp[label]=self.new_labeled_idx[label]\n",
    "        \n",
    "        return temp\n",
    "\n",
    "    def set_PCA(self, pca=None):\n",
    "        self.pca = pca\n",
    "\n",
    "    def plot_co_training_process(self, iterCount, data, iter_train_label, unlabeled_idx, h1_new = [], h2_new = [],\n",
    "                                 h1_new_prob = [], h2_new_prob = [], plotSavingPath=None, name=None):\n",
    "        if not os.path.exists(plotSavingPath):\n",
    "            os.makedirs(plotSavingPath)\n",
    "        pca_one = data[:,0]\n",
    "        pca_two = data[:,1]\n",
    "        # Layer 1. plot unlabel samples in u_prime\n",
    "        fig, ax = plt.subplots(figsize=(9,7))\n",
    "        ax.scatter(x=pca_one[unlabeled_idx], y=pca_two[unlabeled_idx], color='grey', label=\"unlabeled\", s=50, alpha=0.8)\n",
    "        # Layer 2. plot the labeled samples\n",
    "        for author in np.unique(iter_train_label):\n",
    "            ix = iter_train_label.index[iter_train_label == author].tolist()\n",
    "            ax.scatter(x=pca_one[ix], y=pca_two[ix], cmap='viridis', label=author, s=50, alpha=0.8)\n",
    "        # layer 3. mark self labeled samples\n",
    "        if iterCount != 0:\n",
    "            all_h1_new = list(itertools.chain(*h1_new))\n",
    "            all_h2_new = list(itertools.chain(*h2_new))\n",
    "            temp_h1 = ax.scatter(pca_one[all_h1_new], pca_two[all_h1_new], edgecolor='black', linewidth=1, s=50)\n",
    "            temp_h1.set_facecolor(\"none\")\n",
    "            temp_h1.set_label(\"h1 self-labeled\")\n",
    "            temp_h2 = ax.scatter(pca_one[all_h2_new], pca_two[all_h2_new], edgecolor='red', linewidth=1, s=50)\n",
    "            temp_h2.set_facecolor(\"none\")\n",
    "            temp_h2.set_label(\"h2 self-labeled\")\n",
    "            # layer 4. mark new samples confidence and which view produce it\n",
    "            last_iter_h1_new = h1_new[-1]\n",
    "            last_iter_h2_new = h2_new[-1]\n",
    "            last_iter_h1_new_prob = h1_new_prob[-1]\n",
    "            last_iter_h2_new_prob = h2_new_prob[-1]\n",
    "            text = []\n",
    "            for i, idx in enumerate(last_iter_h1_new):\n",
    "                text.append(plt.text(pca_one[idx], pca_two[idx], \"{:.2f}\".format(last_iter_h1_new_prob[i]), color='black'))\n",
    "            for i, idx in enumerate(last_iter_h2_new):\n",
    "                text.append(plt.text(pca_one[idx], pca_two[idx], \"{:.2f}\".format(last_iter_h2_new_prob[i]), color='red'))\n",
    "            adjust_text(text, x=pca_one, y=pca_two, force_points=0.3, force_text=0.3, expand_points=(2, 2), \n",
    "                        expand_text=(2, 2), arrowprops=dict(arrowstyle='Simple', color='red'))\n",
    "        legend = ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.25), ncol=3,prop={'size': 13})\n",
    "        plt.title('Co-training iteration: '+ str(iterCount), fontsize=14)\n",
    "        plt.xlabel(\"First principal component\",fontsize=14)\n",
    "        plt.ylabel(\"Second principal component\",fontsize=14)\n",
    "        plt.savefig(fname=plotSavingPath+name+\"_PCA_i-\"+str(iterCount)+\".png\", dpi=100, bbox_extra_artists=(legend,), bbox_inches='tight')\n",
    "        # plt.show()\n",
    "        plt.close(\"all\")\n",
    "        \n",
    "\n",
    "    def fit(self, train_data, validation_data, dataset_name=None, plot_save_path=None):\n",
    "        # using all unlabeled sample instead of pool of unlabeled sample\n",
    "        self.u = len(train_data[0])\n",
    "        # sync input datatype\n",
    "        for i in train_data:\n",
    "            if not isinstance(i, pd.DataFrame):\n",
    "                i = pd.DataFrame(i)\n",
    "        \n",
    "        labels = pd.Series(train_data[-1], index=train_data[0].index.values)\n",
    "        label_validation = validation_data[-1]\n",
    "        \n",
    "        L, U, U_prime = self.init_L_U_U_prime(labels)\n",
    "        #print(\"All data index: \",train_data[0].index.values)\n",
    "        #print(\"L: \", L)\n",
    "        #print(\"U: \", U)\n",
    "        #print(\"U_prime: \", U_prime)\n",
    "        \n",
    "        # index of self labeled samples\n",
    "        self.new_labeled_idx = defaultdict(list)\n",
    "        self.new_labeled_details = defaultdict(list)\n",
    "        self.f1_on_validation = defaultdict(list)\n",
    "        # save clf in each iteration\n",
    "        self.all_iter_clf = []\n",
    "        # when fit co-train, we collect f1 on validation samples wrt each iteration\n",
    "        for view_idx in range(self.view_num):\n",
    "            self.f1_on_validation[\"dv\"+str(view_idx)]=[]\n",
    "        self.iterCounter = 0\n",
    "        # --------- plot PCA reduced plot for initial stage of train -------------- #\n",
    "        init_train_label = labels[L]\n",
    "        pca = self.pca\n",
    "        '''Notice pca will change input datatype dataframe to datatype list, keep it's order,\n",
    "           or re-assign dv1.index.values to the output'''\n",
    "        pca_dv1 = pca.fit_transform(X=train_data[0])\n",
    "        pca_dv2 = pca.fit_transform(X=train_data[1])\n",
    "        #loop until we have assigned labels to every sample in U and U_prime or we hit our iteration break condition\n",
    "        while self.iterCounter < self.k and U_prime:\n",
    "            '''\n",
    "            Extract labeled training sample from training sample and train classifier, then make prediction\n",
    "            Evaluate performance of current iteration classifier\n",
    "            '''\n",
    "            # print(\"Iteration:\",self.iterCounter, \" L: \",L)\n",
    "            # print(\"Iteration:\",self.iterCounter, \" U_prime: \",U_prime)\n",
    "            iter_train = defaultdict(list)\n",
    "            iter_unlabeled = defaultdict(list)\n",
    "            iter_clf = copy.deepcopy(self.clf) \n",
    "            iter_train_label = labels.iloc[L]\n",
    "            for view_idx in range(self.view_num):\n",
    "                iter_train[\"dv\"+str(view_idx)]=train_data[view_idx].iloc[L]\n",
    "                iter_unlabeled[\"dv\"+str(view_idx)]=train_data[view_idx].iloc[U_prime]\n",
    "                \n",
    "                iter_clf[view_idx].fit(iter_train[\"dv\"+str(view_idx)], iter_train_label)\n",
    "\n",
    "            self.check_iter_label_mapping(iter_clf)\n",
    "            # --------- test error on validation data --------------------- #\n",
    "            y_predict = defaultdict(list)\n",
    "            f1 = defaultdict(list)\n",
    "            for view_idx in range(self.view_num):\n",
    "                y_predict[view_idx]=iter_clf[view_idx].predict(validation_data[view_idx])\n",
    "                f1[\"dv\"+str(view_idx)]=f1_score(label_validation,y_predict[view_idx],average='macro')\n",
    "                self.f1_on_validation[\"dv\"+str(view_idx)].append(f1[\"dv\"+str(view_idx)])\n",
    "            \n",
    "            self.all_iter_clf.append((self.iterCounter,iter_clf))\n",
    "            \n",
    "            '''\n",
    "            # ----------- plot the co-training process -------------- #\n",
    "            '''\n",
    "            if dataset_name != None:\n",
    "                new_train_label = labels[L]\n",
    "                if len(self.new_labeled_details[\"h0_new_idx\"])==0:\n",
    "                    print(\"Iteration 0, strat self label.\")\n",
    "                else:\n",
    "                    print(\"Iteration\",self.iterCounter,\" h1 new: \", self.new_labeled_details[\"h0_new_idx\"][-1], \" score: \", self.new_labeled_details[\"h0_new_proba\"][-1])\n",
    "                    print(\"Iteration\",self.iterCounter,\" h2 new: \", self.new_labeled_details[\"h1_new_idx\"][-1], \" score: \", self.new_labeled_details[\"h1_new_proba\"][-1])\n",
    "                # ----- save pca reduced plot ------ #\n",
    "                for pca_view,v_name in [(pca_dv1,\"dv1\"),(pca_dv2,\"dv2\")]:\n",
    "                    self.plot_co_training_process(self.iterCounter, pca_view, new_train_label, U_prime,\n",
    "                                                  self.new_labeled_details[\"h0_new_idx\"], self.new_labeled_details[\"h1_new_idx\"],\n",
    "                                                  self.new_labeled_details[\"h0_new_proba\"], self.new_labeled_details[\"h1_new_proba\"],\n",
    "                                                  plotSavingPath = plot_save_path, name = dataset_name+\"_\"+v_name)\n",
    "            '''\n",
    "            Confidence measure:\n",
    "            Part 1: View agreement score is probability of all view add together\n",
    "            Case 1: Two view agree with very large confidence score (check)\n",
    "            Case 2: Two view disagree, one view have large confidence, scoring towards large confident view's class (check)\n",
    "            Case 3: Two view disagree and both have large confidence socre\n",
    "            Case 4: Two view agree/disagree with small confidence score\n",
    "            '''\n",
    "            unlabeled_pred_proba = []\n",
    "            for view_idx in range(self.view_num):\n",
    "                view_pred_proba = self.get_confidence_score(clf=iter_clf[view_idx],data=iter_unlabeled[\"dv\"+str(view_idx)])\n",
    "                unlabeled_pred_proba.append(view_pred_proba)\n",
    "            view_agreement_score = np.array([sum(x) for x in zip(*unlabeled_pred_proba)])\n",
    "            #print(view_agreement_score)\n",
    "            ''' \n",
    "            # Part 2: Accumulate predicted probability on each iteration as score\n",
    "            '''\n",
    "            diff_iter_score = defaultdict(list)\n",
    "            curr_iter_proba = defaultdict(list)\n",
    "            final_score = defaultdict(list)\n",
    "            cluster_proba = defaultdict(list)\n",
    "            score_rank = defaultdict(list)\n",
    "            proba_sample_idx_map=iter_unlabeled[\"dv0\"].index\n",
    "            for view_idx in range(self.view_num):\n",
    "                # ------ Part 1: Accumulate predicted probability on first and current iteration as score ------ #\n",
    "                for iteration, clf in self.all_iter_clf:\n",
    "                    if (iteration ==0) or (iteration ==self.iterCounter):\n",
    "                        diff_iter_score[view_idx].append(self.get_confidence_score(clf=clf[view_idx], data=iter_unlabeled[\"dv\"+str(view_idx)]))\n",
    "                # add score in each iteration together\n",
    "                final_score[\"dv\"+str(view_idx)] = np.array([sum(x) for x in zip(*diff_iter_score[view_idx])])\n",
    "                # save last iteration probability\n",
    "                curr_iter_proba[\"dv\"+str(view_idx)] = diff_iter_score[view_idx][-1]\n",
    "                \n",
    "                # ----------- Part 2: Incorporate clustering distance as part of score ------------ #\n",
    "                cluster_proba[view_idx] = self.get_cluster_distance_as_proba(label_train=iter_train[\"dv\"+str(view_idx)], iter_train_label=iter_train_label,\n",
    "                                                                             unlabeled=iter_unlabeled[\"dv\"+str(view_idx)])\n",
    "                final_score[\"dv\"+str(view_idx)]=np.array([sum(x) for x in zip(final_score[\"dv\"+str(view_idx)],cluster_proba[view_idx])])\n",
    "                \n",
    "                # ----------------- Part 3: Incorporate view agreement score ---------------------- #\n",
    "                final_score[\"dv\"+str(view_idx)]=np.array([sum(x) for x in zip(final_score[\"dv\"+str(view_idx)],view_agreement_score)])\n",
    "                \n",
    "                # -------------- Part 4: Rank confidence and start self labeling ------------------ #\n",
    "                # proba1_rank[i] is label i's confidence measure\n",
    "                for class_proba in final_score[\"dv\"+str(view_idx)].T:\n",
    "                    score_rank[\"dv\"+str(view_idx)].append((-class_proba).argsort())\n",
    "                \n",
    "            #print(\"All: \", diff_iter_score)\n",
    "            #print(\"Current: \",curr_iter_proba)\n",
    "            #print(\"Cluster proba: \", cluster_proba)\n",
    "            #print(\"Final score: \",final_score)\n",
    "            #print(\"Score rank: \", score_rank)\n",
    "            \n",
    "            '''\n",
    "            Start labelling\n",
    "            '''\n",
    "            new_sample = defaultdict(list)\n",
    "            new_sample_score = defaultdict(list)\n",
    "            new_sample_proba = defaultdict(list)\n",
    "            flatten_new_idx = defaultdict(list)\n",
    "            flatten_score = defaultdict(list)\n",
    "            flatten_proba = defaultdict(list)\n",
    "            self.new_labeled_details[\"Iteration\"].append(self.iterCounter)\n",
    "            \n",
    "            for view_idx in range(self.view_num):\n",
    "                (new_sample[\"dv\"+str(view_idx)], \n",
    "                 new_sample_score[\"dv\"+str(view_idx)], \n",
    "                 new_sample_proba[\"dv\"+str(view_idx)]) = self.label_samples(final_score[\"dv\"+str(view_idx)], \n",
    "                                                                            score_rank[\"dv\"+str(view_idx)], \n",
    "                                                                            proba_sample_idx_map, \n",
    "                                                                            curr_iter_proba[\"dv\"+str(view_idx)])\n",
    "                flatten_new_idx[view_idx] = list(itertools.chain(*new_sample[\"dv\"+str(view_idx)]))\n",
    "                flatten_score[view_idx] = list(itertools.chain(*new_sample_score[\"dv\"+str(view_idx)]))\n",
    "                flatten_proba[view_idx] = list(itertools.chain(*new_sample_proba[\"dv\"+str(view_idx)]))\n",
    "            \n",
    "            # if no new label added to L, stop iteration\n",
    "            if not any(flatten_new_idx):\n",
    "                self.iterCounter += 1\n",
    "                print(\"Remaining unlabel sample is uncertain.\")\n",
    "                break\n",
    "                \n",
    "            for view_idx in range(self.view_num):\n",
    "                self.new_labeled_details[\"h\"+str(view_idx)+\"_new_idx\"].append(flatten_new_idx[view_idx])\n",
    "                self.new_labeled_details[\"h\"+str(view_idx)+\"_new_score\"].append(flatten_score[view_idx])\n",
    "                self.new_labeled_details[\"h\"+str(view_idx)+\"_new_proba\"].append(flatten_proba[view_idx])\n",
    "                \n",
    "            #print(self.new_labeled_details)\n",
    "            \n",
    "            '''\n",
    "            Add most confidence samples as new training samples, auto label the samples and remove it from U_prime\n",
    "            Special Case: if two view classifier give same most confident sample, only 1 datapoint self-labeled\n",
    "            '''\n",
    "            roundNewIdx = list(zip(*[value for key, value in new_sample.items()]))\n",
    "            roundNewScore = list(zip(*[value for key, value in new_sample_score.items()]))\n",
    "            roundNewProba = list(zip(*[value for key, value in new_sample_proba.items()]))\n",
    "            roundNew_flatten_unique = []\n",
    "            for label, round_new in enumerate(zip(roundNewIdx,roundNewScore,roundNewProba)):\n",
    "                #print(round_new)\n",
    "                round_new = list(zip(*round_new))\n",
    "                temp = []\n",
    "                for index, (idx,score,proba) in enumerate(round_new):\n",
    "                    for i in range(len(idx)):\n",
    "                        temp.append((idx[i],score[i],proba[i]))\n",
    "                #print(temp)\n",
    "                round_new = set(temp)\n",
    "                round_new_idx = [item[0] for item in round_new]\n",
    "                round_new_score = [item[1] for item in round_new]\n",
    "                round_new_proba = [item[2] for item in round_new]\n",
    "                self.new_labeled_idx[self.classes_[label]].append(round_new_idx)\n",
    "                self.new_labeled_idx[self.classes_[label]+\"_score\"].append(round_new_score)\n",
    "                self.new_labeled_idx[self.classes_[label]+\"_proba\"].append(round_new_proba)\n",
    "                roundNew_flatten_unique.extend(round_new_idx)\n",
    "                # add label to those new samples\n",
    "                #print(labels[round_new_idx])\n",
    "                labels[round_new_idx] = self.classes_[label]\n",
    "                #print(labels[round_new_idx])\n",
    "                #print(self.classes_[label],\" (u' idx): \",round_new_idx)\n",
    "            #print(roundNew_flatten_unique)\n",
    "            # extend the labeled sample\n",
    "            L.extend(roundNew_flatten_unique)\n",
    "            # remove the labeled sample from U_prime\n",
    "            U_prime = [x for x in U_prime if x not in roundNew_flatten_unique]\n",
    "            #print(U_prime)\n",
    "            # randomly choice view_num*self-label-class-max*class_num examples from u to replenish u_prime\n",
    "            replenishItem = U[-((self.view_num)*(self.sl_class_max)*len(self.classes_)):]\n",
    "            U_prime.extend(replenishItem)\n",
    "            U = U[:-len(replenishItem)]\n",
    "            self.iterCounter +=1\n",
    "            \n",
    "            '''\n",
    "            # ----------- plot the last iteration of co-training process -------------- #\n",
    "            '''\n",
    "            if (dataset_name != None) and (self.iterCounter == self.k):\n",
    "                new_train_label = labels[L]\n",
    "                print(\"Iteration\",self.iterCounter,\" h1 new: \", self.new_labeled_details[\"h0_new_idx\"][-1], \" score: \", self.new_labeled_details[\"h0_new_proba\"][-1])\n",
    "                print(\"Iteration\",self.iterCounter,\" h2 new: \", self.new_labeled_details[\"h1_new_idx\"][-1], \" score: \", self.new_labeled_details[\"h1_new_proba\"][-1])\n",
    "                # ----- save pca reduced plot ------ #\n",
    "                for pca_view,v_name in [(pca_dv1,\"dv1\"),(pca_dv2,\"dv2\")]:\n",
    "                    self.plot_co_training_process(self.iterCounter, pca_view, new_train_label, U_prime,\n",
    "                                                  self.new_labeled_details[\"h0_new_idx\"], self.new_labeled_details[\"h1_new_idx\"],\n",
    "                                                  self.new_labeled_details[\"h0_new_proba\"], self.new_labeled_details[\"h1_new_proba\"],\n",
    "                                                  plotSavingPath = plot_save_path, name = dataset_name+\"_\"+v_name)\n",
    "        \n",
    "        print(\"Total Labeled number: \", len(L), \" Still unlabeled number: \", len(U_prime))\n",
    "        #print(self.f1_on_validation[\"dv0\"])\n",
    "        #print(self.f1_on_validation[\"dv1\"])\n",
    "        # final train\n",
    "        final_train = defaultdict(list)\n",
    "        for view_idx in range(self.view_num):\n",
    "            final_train[view_idx] = train_data[view_idx].iloc[L]\n",
    "            self.clf[view_idx].fit(final_train[view_idx], labels.iloc[L])\n",
    "        '''\n",
    "        Evalutation plot for co-training process, save f1 score vs number of iteration plot\n",
    "        '''\n",
    "        if dataset_name != None:\n",
    "            default_text_based = [self.f1_on_validation[\"dv0\"][0]] * self.iterCounter\n",
    "            default_citation_based = [self.f1_on_validation[\"dv1\"][0]] * self.iterCounter\n",
    "            default_step = np.arange(0,self.iterCounter)\n",
    "            co_train_text_based = self.f1_on_validation[\"dv0\"][1:]\n",
    "            co_train_citation_based = self.f1_on_validation[\"dv1\"][1:]\n",
    "            co_training_step = np.arange(1,self.iterCounter)\n",
    "\n",
    "            fig = plt.figure()\n",
    "            ax = plt.axes()\n",
    "            plt.plot(default_step, default_text_based, linestyle='dashed', label=\"Text based default\")\n",
    "            plt.plot(default_step, default_citation_based, linestyle='dashdot', label=\"Citation based default\")\n",
    "            plt.plot(co_training_step, co_train_text_based, linestyle='solid', marker = \"*\", label=\"Text based\")\n",
    "            plt.plot(co_training_step, co_train_citation_based, linestyle='dotted', marker = \"+\", label=\"Citation based\")\n",
    "            ax.autoscale_view()\n",
    "            legend = ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.2), ncol=2)\n",
    "            plt.xlabel('Co-Training Iterations')\n",
    "            plt.ylabel('F1 score')\n",
    "            plt.savefig((plot_save_path+dataset_name+\"_co_train_iteration_f1.png\"), dpi=150, bbox_extra_artists=(legend,), bbox_inches='tight')\n",
    "            # plt.show()\n",
    "            plt.close(\"all\")\n",
    "        return \"Training Done\"\n",
    "\n",
    "    def co_train_process_f1(self):\n",
    "        return self.f1_on_validation[\"dv0\"], self.f1_on_validation[\"dv1\"]\n",
    "\n",
    "    def get_iter_count(self):\n",
    "        return self.iterCounter\n",
    "\n",
    "    def predict(self, data):\n",
    "        y_predict = defaultdict(list)\n",
    "        for view_idx in range(self.view_num):\n",
    "            y_predict[view_idx]=self.clf[view_idx].predict(data[view_idx])\n",
    "        y_predict = [value for key, value in y_predict.items()]\n",
    "        #fill pred with -1 so we can identify the samples in which error occur\n",
    "        y_pred = [\"-1\"] * data[0].shape[0]\n",
    "        for i, (dv1_y, dv2_y) in enumerate(zip(*y_predict)):\n",
    "            # if both agree on label\n",
    "            if dv1_y == dv2_y:\n",
    "                y_pred[i] = dv1_y\n",
    "            # If disagree on label and support proba method: We times probability together, choice class have higher probabilities\n",
    "            elif hasattr(self.clf[0], \"predict_proba\") and hasattr(self.clf[1], \"predict_proba\"):\n",
    "                h1_probas = self.clf[0].predict_proba([data[0].iloc[i]])[0]\n",
    "                h2_probas = self.clf[1].predict_proba([data[1].iloc[i]])[0]\n",
    "                final_y_probas = [proba_y1 * proba_y2 for (proba_y1, proba_y2) in zip(h1_probas, h2_probas)]\n",
    "                #print(\"h1 and h2 disagree on\",i, \" h1 Proba : \",h1_probas, \" h2 Proba: \", h2_probas)\n",
    "                #print(\"product probas:\",final_y_probas)\n",
    "                max_prob_idx = final_y_probas.index(max(final_y_probas))\n",
    "                y_pred[i] = self.classes_[max_prob_idx]\n",
    "                #print(\"result idx: \", max_prob_idx, \" result: \",y_pred[i])\n",
    "            # If disagree on label and not support proba method: We use decision_function to first calculate probability,\n",
    "            # then we times calculate probability together, choice class have higher probabilities\n",
    "            elif hasattr(self.clf[0], \"decision_function\") and hasattr(self.clf[1], \"decision_function\"):\n",
    "                dv1_distance = self.clf[0].decision_function([data[0].iloc[i]])\n",
    "                dv2_distance = self.clf[1].decision_function([data[1].iloc[i]])\n",
    "                if len(self.clf1.classes_)==2:\n",
    "                    h1_probas = np.array([1-self.sigmoid(dv1_distance), self.sigmoid(dv1_distance)])\n",
    "                    h2_probas = np.array([1-self.sigmoid(dv2_distance), self.sigmoid(dv2_distance)])\n",
    "                else:\n",
    "                    h1_probas = self.softmax(dv1_distance)\n",
    "                    h2_probas = self.softmax(dv2_distance)\n",
    "                final_y_probas = [proba_y1 * proba_y2 for (proba_y1, proba_y2) in zip(h1_probas, h2_probas)]\n",
    "                #print(\"h1 and h2 disagree on\",i, \" h1 Proba : \",h1_probas, \" h2 Proba: \", h2_probas)\n",
    "                #print(\"product probas:\",final_y_probas)\n",
    "                max_prob_idx = final_y_probas.index(max(final_y_probas))\n",
    "                y_pred[i] = self.classes_[max_prob_idx]\n",
    "                #print(\"result idx: \", max_prob_idx, \" result: \",y_pred[i])\n",
    "            # disagree and not support any confidence measure\n",
    "            else:\n",
    "                sys.exit(\"Error occur\")\n",
    "\n",
    "        # convert final result to np array\n",
    "        y_pred_np_array = np.asarray(y_pred)\n",
    "        return y_pred_np_array\n",
    "\n",
    "    def predict_proba(self, dv1, dv2):\n",
    "        # the predicted probabilities is simply a product (*) of probabilities given from each classifier trained\n",
    "        h1_probas = self.clf1.predict_proba(dv1)\n",
    "        h2_probas = self.clf2.predict_proba(dv2)\n",
    "        \n",
    "        proba = (h1_probas*h2_probas)\n",
    "        return proba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T04:24:37.016903Z",
     "start_time": "2020-06-03T04:24:36.994612Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def upsampling(data,label):\n",
    "    unique_num = label.value_counts()\n",
    "    #print(unique_num)\n",
    "    temp = pd.concat([data, label], axis=1)\n",
    "    majority_class = temp[temp.authorID==unique_num.index.tolist()[0]]\n",
    "    minority_class = temp[temp.authorID==unique_num.index.tolist()[1]]\n",
    "    # upsample minority\n",
    "    minority_upsampled = resample(minority_class, replace=True, n_samples=len(majority_class), random_state=27)\n",
    "    # combine majority and upsampled minority\n",
    "    temp = pd.concat([majority_class, minority_upsampled])\n",
    "    label = temp[['authorID']].squeeze()\n",
    "    data = temp.drop([\"authorID\"], axis=1)\n",
    "    #print(temp.authorID.value_counts())\n",
    "    return data, label\n",
    "\n",
    "'''\n",
    "1. Random oversampling minority class\n",
    "2. Generate synthetic samples using SMOTE(Synthetic Minority Oversampling Technique)\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=27, ratio=1.0)\n",
    "X_train, y_train = sm.fit_sample(X_train, y_train)\n",
    "Then use new X_train, y_train\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-03T04:24:36.415Z"
    },
    "code_folding": [
     13,
     75,
     105,
     181,
     194,
     216,
     251,
     258
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import collections\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import resample\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score,f1_score,accuracy_score\n",
    "\n",
    "# cross validation\n",
    "def k_fold_cv_all_algorithm(dv1, dv2, label, init_labeled_size, muti_view_clf=[], combined_clf=[],\n",
    "                            num_fold=10, dataset_name=None, plot_save_path=None, validation=False):\n",
    "    # set validation dataset as 10% of all data\n",
    "    if validation:\n",
    "        init_validation_size = len(label)*0.1\n",
    "    else:\n",
    "        init_validation_size = 0\n",
    "    kf = StratifiedKFold(n_splits=num_fold)\n",
    "    allTrueLabel = []\n",
    "    co_train_algorithm = [name for clf,name in muti_view_clf]\n",
    "    baseline_algorithm = [name for clf,name in combined_clf]\n",
    "    allPredLabel = collections.defaultdict(list)\n",
    "    all_fold_coTrain_diff_iteration = collections.defaultdict(list)\n",
    "    co_train_iteration = collections.defaultdict(list)\n",
    "    \n",
    "    all_fold_statistic = []\n",
    "    fold = 0\n",
    "    # convert different input type to dataframe for consistency\n",
    "    dv1 = pd.DataFrame(dv1)\n",
    "    dv2 = pd.DataFrame(dv2)\n",
    "    \n",
    "    for train_index, test_index in kf.split(dv1, label):\n",
    "        fold +=1\n",
    "        # print(\"TRAIN:\", train_index, \" \\n TEST:\", test_index)\n",
    "        # ---------------split train and test -------------------- #\n",
    "        dv1_train, dv1_test = dv1.iloc[train_index], dv1.iloc[test_index]\n",
    "        dv2_train, dv2_test = dv2.iloc[train_index], dv2.iloc[test_index]\n",
    "        all_label_train, label_test = label.iloc[train_index], label.iloc[test_index]\n",
    "        '''\n",
    "        Plot true labeled result for different view, use PCA to reduce views to 2d\n",
    "        Real training process will not using PCA to reduce it's dimension \n",
    "        '''\n",
    "        detailed_plot_path = plot_save_path+dataset_name+\"/fold\"+str(fold)+\"/\"\n",
    "        if not os.path.exists(detailed_plot_path):\n",
    "            os.makedirs(detailed_plot_path)\n",
    "        pca = PCA(n_components=2)\n",
    "        for view,name in [(dv1,\"dv1\"),(dv2,\"dv2\")]:\n",
    "            pca_view = pca.fit_transform(X=view)\n",
    "            first_principal_component = pca_view[:,0]\n",
    "            second_principal_component = pca_view[:,1]\n",
    "            fig, ax = plt.subplots(figsize=(9,7))\n",
    "            for author in np.unique(label):\n",
    "                ix = label.index[label == author].tolist()\n",
    "                ax.scatter(x=first_principal_component[ix], y=second_principal_component[ix],\n",
    "                           cmap='viridis', label = author, s = 50, alpha = 0.8)\n",
    "            legend = ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.2), ncol=2,prop={'size': 13})\n",
    "            plt.title('True label', fontsize=14)\n",
    "            plt.xlabel(\"First principal component\",fontsize=14)\n",
    "            plt.ylabel(\"Second principal component\",fontsize=14)\n",
    "            plt.savefig(fname=detailed_plot_path+dataset_name+\"_PCA_orginal_true_label_\"+name+\".png\",\n",
    "                        dpi=100, bbox_extra_artists=(legend,), bbox_inches='tight')\n",
    "            #plt.show()\n",
    "            plt.close(\"all\")\n",
    "        ''' Sampling to deal with imbalance data\n",
    "        Since our negative class contain lots of variance, downsampling may loss important information \n",
    "        Here we perform upsampling\n",
    "        '''\n",
    "        dv1_train, temp_label1 = upsampling(dv1_train, all_label_train)\n",
    "        dv2_train, temp_label2 = upsampling(dv2_train, all_label_train)\n",
    "        if temp_label1.equals(temp_label2):\n",
    "            all_label_train = temp_label1\n",
    "        else:\n",
    "            sys.exit(\"error\")\n",
    "        for view,name in [(dv1_train,\"dv1\"),(dv2_train,\"dv2\")]:\n",
    "            pca_view = pca.fit_transform(X=view)\n",
    "            first_principal_component = pca_view[:,0]\n",
    "            second_principal_component = pca_view[:,1]\n",
    "            fig, ax = plt.subplots(figsize=(9,7))\n",
    "            for author in np.unique(all_label_train):\n",
    "                ix = all_label_train.index[all_label_train==author].tolist()\n",
    "                ax.scatter(x=first_principal_component[ix], y=second_principal_component[ix],\n",
    "                           cmap='viridis', label = author, s = 50, alpha = 0.8)\n",
    "            legend = ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.2), ncol=2,prop={'size': 13})\n",
    "            plt.title('True label', fontsize=14)\n",
    "            plt.xlabel(\"First principal component\",fontsize=14)\n",
    "            plt.ylabel(\"Second principal component\",fontsize=14)\n",
    "            plt.savefig(fname=detailed_plot_path+dataset_name+\"_PCA_UpSampled_true_label_\"+name+\".png\",\n",
    "                        dpi=100, bbox_extra_artists=(legend,), bbox_inches='tight')\n",
    "            #plt.show()\n",
    "            plt.close(\"all\")\n",
    "        ''' \n",
    "        Comparison Part 1: Use two view concatenated features and train with supervise learning (up bound)\n",
    "        Notice we train supervise learning with all labeled data, no data has been mark as unlabeled\n",
    "        '''\n",
    "        # ---------- collect per fold statistic ---------------------- #\n",
    "        curr_fold_statistic = {'author': dataset_name, 'fold':fold, 'test_size': dv1_test.shape[0]} \n",
    "        ub_concatenated_train = pd.concat([dv1_train,dv2_train], axis=1, ignore_index=True)\n",
    "        concatenated_test = pd.concat([dv1_test,dv2_test], axis=1, ignore_index=True)\n",
    "        for in_clf, clf_name in combined_clf:\n",
    "            per_fold_clf = copy.deepcopy(in_clf) \n",
    "            per_fold_clf.fit(ub_concatenated_train, all_label_train)\n",
    "            ub_pred = per_fold_clf.predict(concatenated_test)\n",
    "            print(clf_name+\"-UB f1: \", metrics.classification_report(label_test, ub_pred))\n",
    "            print(metrics.confusion_matrix(label_test, ub_pred).ravel())\n",
    "            curr_fold_statistic[clf_name+\"-UB f1\"] = f1_score(label_test.values.tolist(), ub_pred,average='macro')\n",
    "            allPredLabel[clf_name+\"-UB predict label\"].append(ub_pred)\n",
    "\n",
    "        ''' \n",
    "        Comparison Part 2: Use two view and train with co-training (Check effective of co-training)\n",
    "        Notice here we are simulating situation for co-training by set most of data to unlabelled\n",
    "        Case 1: Using validation dataset for each fold of train\n",
    "        Case 2: Not Using validation dataset for each fold of train\n",
    "        '''\n",
    "        # ----------- set some labeled data as unlabeled ------------ #\n",
    "        # 1. obtain data ratio\n",
    "        c = Counter(all_label_train)\n",
    "        data_ratio = [(i, c[i] / len(all_label_train)) for i in c]\n",
    "        print(data_ratio)\n",
    "        # 2. calculate per class size \n",
    "        # co_train_per_class_size contain (label,initial train size for each class, initial validation data for each class)\n",
    "        co_train_per_class_size = [(label, round(ratio*init_labeled_size),round(ratio*init_validation_size)) for label, ratio in data_ratio]\n",
    "        print(co_train_per_class_size)\n",
    "        # 3.Initialize train and validation sample index list save it for later use\n",
    "        temp_train_label = all_label_train.tolist()\n",
    "        train_sample_idx = []\n",
    "        validation_sample_idx = []\n",
    "        # 4. random draw both train labeled samples and validation samples, mark other as unlabeled\n",
    "        # we could also use validation samples to improve performance\n",
    "        for unique_label, training_size, validation_size in co_train_per_class_size:\n",
    "            curr_label_idx = [i for i, x in enumerate(temp_train_label) if x == unique_label]\n",
    "            curr_label_size = len(curr_label_idx)\n",
    "            # 1. get train sample idx\n",
    "            temp_train_sample_idx = random.sample(curr_label_idx, training_size)\n",
    "            train_sample_idx += temp_train_sample_idx\n",
    "            curr_label_idx_no_train = [x for x in curr_label_idx if x not in temp_train_sample_idx]\n",
    "            # 2. get validation sample idx\n",
    "            temp_validation_sample_idx = random.sample(curr_label_idx_no_train, validation_size)\n",
    "            validation_sample_idx += temp_validation_sample_idx\n",
    "            # 3. set other samples to -1 as unlabeled\n",
    "            unlabel_item_idx = [x for x in curr_label_idx_no_train if x not in temp_validation_sample_idx]\n",
    "            for unlabel_idx in unlabel_item_idx:\n",
    "                temp_train_label[unlabel_idx]=-1\n",
    "\n",
    "        curr_fold_statistic[\"train_size\"]=len(train_sample_idx)\n",
    "        #print(temp_train_label)\n",
    "        # -------------------- train with co-training ------------------- #\n",
    "        for in_clf, clf_name in muti_view_clf: \n",
    "            per_clf_plot_save_path = detailed_plot_path+clf_name+\"/\"\n",
    "            if not os.path.exists(per_clf_plot_save_path):\n",
    "                os.makedirs(per_clf_plot_save_path)\n",
    "            if validation:\n",
    "                unlabeled_sample_size = len(temp_train_label)-len(train_sample_idx)-len(validation_sample_idx)\n",
    "                # extract validation data\n",
    "                dv1_validation = dv1_train.iloc[validation_sample_idx,:]\n",
    "                dv2_validation = dv2_train.iloc[validation_sample_idx,:]\n",
    "                validation_label = [temp_train_label[idx] for idx in validation_sample_idx]\n",
    "                # use other data as train\n",
    "                final_dv1_train = dv1_train[~dv1_train.isin(dv1_validation)].dropna(axis=0, how=\"all\").reset_index(drop=True)\n",
    "                final_dv2_train = dv2_train[~dv2_train.isin(dv2_validation)].dropna(axis=0, how=\"all\").reset_index(drop=True)\n",
    "                final_train_label = [x for i, x in enumerate(temp_train_label) if i not in validation_sample_idx]\n",
    "                # check data sync\n",
    "                # print(validation_label)\n",
    "                # print(all_label_train[dv1_validation.index])\n",
    "                per_fold_clf = copy.deepcopy(in_clf)\n",
    "                per_fold_clf.set_PCA(pca)\n",
    "                per_fold_clf.fit(train_data=[final_dv1_train, final_dv2_train, final_train_label],\n",
    "                                 validation_data=[dv1_validation,dv2_validation, validation_label],\n",
    "                                 dataset_name=dataset_name, plot_save_path=per_clf_plot_save_path)\n",
    "            else:\n",
    "                unlabeled_sample_size = len(temp_train_label)-len(train_sample_idx)\n",
    "                final_dv1_train = dv1_train.reset_index(drop=True)\n",
    "                final_dv2_train = dv2_train.reset_index(drop=True)\n",
    "                final_train_label=temp_train_label\n",
    "                per_fold_clf = copy.deepcopy(in_clf)\n",
    "                per_fold_clf.set_PCA(pca)\n",
    "                per_fold_clf.fit(train_data=[final_dv1_train, final_dv2_train, final_train_label], \n",
    "                                 validation_data=[dv1_test, dv2_test, label_test],\n",
    "                                 dataset_name=dataset_name, plot_save_path=per_clf_plot_save_path)\n",
    "            curr_fold_statistic[\"unlabel_size\"]=unlabeled_sample_size\n",
    "            curr_fold_statistic['validation_size']=len(validation_sample_idx)\n",
    "            curr_fold_statistic[\"total_iteration\"]= per_fold_clf.get_iter_count()\n",
    "            # -------------- get self-labeled sample index --------------- #\n",
    "            self_labeled_index = per_fold_clf.get_self_labeled_sample()\n",
    "            #print(\"Self labeled sample index: \", self_labeled_index)\n",
    "            self_labeled_idx_temp = [idx for idx in self_labeled_index.values()]\n",
    "            all_self_labeled_index = [val for sublist in self_labeled_idx_temp for subsublist in sublist for val in subsublist]\n",
    "            curr_fold_statistic[clf_name+'_total_self_labeled']= len(all_self_labeled_index)\n",
    "            # ------------- get predicted label for test set ------------- #\n",
    "            co_train_predict = per_fold_clf.predict([dv1_test, dv2_test])\n",
    "            print(clf_name+\" f1: \", metrics.classification_report(label_test, co_train_predict))\n",
    "            print(metrics.confusion_matrix(label_test, co_train_predict).ravel())\n",
    "            curr_fold_statistic[clf_name+\" f1\"] = f1_score(label_test.values.tolist(), co_train_predict,average='macro')\n",
    "            allPredLabel[clf_name+\" predict label\"].append(co_train_predict)\n",
    "            # ------------- get co-training iterations f1 score ---------- #\n",
    "            co_train_iteration[clf_name].append(per_fold_clf.get_iter_count())\n",
    "            coTrain_diff_iter_on_test_dv1, coTrain_diff_iter_on_test_dv2 = per_fold_clf.co_train_process_f1()\n",
    "            all_fold_coTrain_diff_iteration[clf_name+\"_dv1\"].append(coTrain_diff_iter_on_test_dv1)\n",
    "            all_fold_coTrain_diff_iteration[clf_name+\"_dv2\"].append(coTrain_diff_iter_on_test_dv2)\n",
    "        ''' \n",
    "        Comparison Part 3: Use two view concatenated features and train with supervise learning (lower bound)\n",
    "        Notice we only train supervise learning with labeled train, data mark as unlabeled is not used\n",
    "        '''\n",
    "        lb_concatenated_train = pd.concat([dv1_train.iloc[train_sample_idx],dv2_train.iloc[train_sample_idx]], axis=1, ignore_index=True)\n",
    "        lb_train_label = [temp_train_label[i] for i in train_sample_idx]\n",
    "        for in_clf, clf_name in combined_clf:\n",
    "            per_fold_clf = copy.deepcopy(in_clf)\n",
    "            per_fold_clf.fit(lb_concatenated_train, lb_train_label)\n",
    "            lb_pred = per_fold_clf.predict(concatenated_test)\n",
    "            print(clf_name+\"-LB f1: \", metrics.classification_report(label_test, lb_pred))\n",
    "            print(metrics.confusion_matrix(label_test, lb_pred).ravel())\n",
    "            curr_fold_statistic[clf_name+\"-LB f1\"] = f1_score(label_test.values.tolist(), lb_pred,average='macro')\n",
    "            allPredLabel[clf_name+\"-LB predict label\"].append(lb_pred)\n",
    "        \n",
    "        allTrueLabel.extend(label_test.values.tolist())\n",
    "        all_fold_statistic.append(curr_fold_statistic)\n",
    "\n",
    "    ''' # --------------- plot per fold result f1 variance --------------- # '''\n",
    "    if plot_save_path !=None:\n",
    "        all_per_fold_f1_score_variance_plot = pd.DataFrame(all_fold_statistic)\n",
    "        #print(all_per_fold_f1_score_variance_plot)\n",
    "        sns.set(rc={'figure.figsize':(10,8)})\n",
    "        plot_temp_f1 = pd.DataFrame()\n",
    "        for clf_name in co_train_algorithm:\n",
    "            clf_temp_f1 = all_per_fold_f1_score_variance_plot[[clf_name+' f1']].values\n",
    "            plot_temp_f1[clf_name]=clf_temp_f1.flatten()\n",
    "        for clf_name in baseline_algorithm:\n",
    "            for clf_type in [\"-UB\",\"-LB\"]:\n",
    "                clf_temp_f1 = all_per_fold_f1_score_variance_plot[[clf_name+clf_type+' f1']].values\n",
    "                plot_temp_f1[clf_name+clf_type]=clf_temp_f1.flatten()\n",
    "        plot_temp_f1 = pd.melt(plot_temp_f1, var_name='methods', value_name='f1')\n",
    "        #print(plot_temp_f1)\n",
    "        ax = sns.boxplot(x=\"methods\", y=\"f1\", data=plot_temp_f1)\n",
    "        ax = sns.swarmplot(x=\"methods\", y=\"f1\", data=plot_temp_f1, color=\".25\")\n",
    "        ax.set_title(dataset_name+\" result variance within \"+str(num_fold)+\" fold\")\n",
    "        plt.savefig(plot_save_path+dataset_name+\"/all_method_result_variance.png\", dpi=150)\n",
    "        #plt.show()\n",
    "        plt.close(\"all\")\n",
    "\n",
    "    '''  plot averaged f1 score wrt different fold in different iterations in co-training process '''\n",
    "    for clf_name in co_train_algorithm:\n",
    "        print(clf_name,\" per fold iteration: \", co_train_iteration[clf_name])\n",
    "        # each fold have different iteration, find minimum of iteration and \n",
    "        co_train_min_iteration_num = min(co_train_iteration[clf_name])\n",
    "        for sublist in all_fold_coTrain_diff_iteration[clf_name+\"_dv1\"]:\n",
    "            sublist[:] = sublist[:co_train_min_iteration_num]\n",
    "        for sublist in all_fold_coTrain_diff_iteration[clf_name+\"_dv2\"]:\n",
    "            sublist[:] = sublist[:co_train_min_iteration_num]\n",
    "        # -------- mean with respect to all fold --------- #\n",
    "        averaged_coTrain_diff_iter_dv1 = np.mean(all_fold_coTrain_diff_iteration[clf_name+\"_dv1\"], axis=0)\n",
    "        averaged_coTrain_diff_iter_dv2 = np.mean(all_fold_coTrain_diff_iteration[clf_name+\"_dv2\"], axis=0)\n",
    "        # -------- initial variables for plot ------------ #\n",
    "        default_text_based = [averaged_coTrain_diff_iter_dv1[0]] * co_train_min_iteration_num\n",
    "        default_citation_based = [averaged_coTrain_diff_iter_dv2[0]] * co_train_min_iteration_num\n",
    "        default_step = np.arange(0, co_train_min_iteration_num)\n",
    "        co_train_text_based = averaged_coTrain_diff_iter_dv1[1:]\n",
    "        co_train_citation_based = averaged_coTrain_diff_iter_dv2[1:]\n",
    "        co_training_step = np.arange(1, co_train_min_iteration_num)\n",
    "        # ----------- plot details ----------------------- #\n",
    "        fig = plt.figure()\n",
    "        ax = plt.axes()\n",
    "        plt.plot(default_step, default_text_based, linestyle='dashed', label=\"Text based default\")\n",
    "        plt.plot(default_step, default_citation_based, linestyle='dashdot', label=\"Citation based default\")\n",
    "        plt.plot(co_training_step, co_train_text_based, linestyle='solid', marker = \"*\", label=\"Text based\")\n",
    "        plt.plot(co_training_step, co_train_citation_based, linestyle='dotted', marker = \"+\", label=\"Citation based\")\n",
    "        ax.autoscale_view()\n",
    "        legend = ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.2), ncol=2)\n",
    "        plt.xlabel('Co-Training Iterations')\n",
    "        plt.ylabel('F1 score')\n",
    "        plt.savefig((plot_save_path+dataset_name+\"/\"+clf_name+\"_mean_diff_iter_f1.png\"), dpi=150, bbox_extra_artists=(legend,), bbox_inches='tight')\n",
    "        #plt.show()\n",
    "        plt.close(\"all\")\n",
    "    \n",
    "    ''' The results of a k-fold cross-validation run are often summarized with the mean of the model scores.'''\n",
    "    final_f1_score = []\n",
    "    for clf_name in co_train_algorithm:\n",
    "        clf_all_fold_f1 =[]\n",
    "        for per_fold_statistic in all_fold_statistic:\n",
    "            clf_per_fold_f1 = per_fold_statistic[clf_name+\" f1\"]\n",
    "            clf_all_fold_f1.append(clf_per_fold_f1)\n",
    "        clf_mean_f1 = np.mean(clf_all_fold_f1, axis=0)\n",
    "        final_f1_score.append((clf_name,clf_mean_f1))\n",
    "    for clf_name in baseline_algorithm:\n",
    "        for clf_type in [\"-UB\",\"-LB\"]:\n",
    "            clf_all_fold_f1 =[]\n",
    "            for per_fold_statistic in all_fold_statistic:\n",
    "                clf_per_fold_f1 = per_fold_statistic[clf_name+clf_type+\" f1\"]\n",
    "                clf_all_fold_f1.append(clf_per_fold_f1)\n",
    "            clf_mean_f1 = np.mean(clf_all_fold_f1, axis=0)\n",
    "            final_f1_score.append((clf_name+clf_type,clf_mean_f1))\n",
    "    #print(final_f1_score)\n",
    "    \n",
    "    return final_f1_score, all_fold_statistic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-03T04:24:36.834Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load text embedding:  pv_dbow\n",
      "Total text vector records: 135796\n",
      "Vector dimension:  100\n",
      "Load citation embedding:  n2v\n",
      "Total citation vector records: 124922\n",
      "Vector dimension:  100\n",
      "For name:  j_read\n",
      "(136, 2)\n",
      "j_read  pass\n",
      "For name:  f_esteves\n",
      "(34, 2)\n",
      "f_esteves  pass\n",
      "For name:  c_miller\n",
      "(252, 2)\n",
      "c_miller  pass\n",
      "For name:  r_jha\n",
      "(11, 2)\n",
      "r_jha  pass\n",
      "For name:  a_lowe\n",
      "(102, 2)\n",
      "a_lowe  pass\n",
      "For name:  a_vega\n",
      "(20, 2)\n",
      "a_vega  pass\n",
      "For name:  k_smith\n",
      "(338, 2)\n",
      "k_smith  pass\n",
      "For name:  j_gordon\n",
      "(19, 2)\n",
      "j_gordon  pass\n",
      "For name:  s_liao\n",
      "(104, 2)\n",
      "s_liao  pass\n",
      "For name:  j_qian\n",
      "(17, 2)\n",
      "j_qian  pass\n",
      "For name:  s_bernardi\n",
      "(91, 2)\n",
      "s_bernardi  pass\n",
      "For name:  t_hill\n",
      "(15, 2)\n",
      "t_hill  pass\n",
      "For name:  s_schindler\n",
      "(51, 2)\n",
      "s_schindler  pass\n",
      "For name:  j_williams\n",
      "(625, 2)\n",
      "j_williams  pass\n",
      "For name:  s_jacobson\n",
      "(28, 2)\n",
      "s_jacobson  pass\n",
      "For name:  e_andrade\n",
      "(17, 2)\n",
      "e_andrade  pass\n",
      "For name:  t_santos\n",
      "(45, 2)\n",
      "t_santos  pass\n",
      "For name:  k_kim\n",
      "(1111, 2)\n",
      "Counter({'0000-0002-6929-5359': 211, '0000-0001-9498-284X': 154, '0000-0002-5878-8895': 139, '0000-0002-1864-3392': 92, '0000-0002-7045-8004': 57, '0000-0001-7896-6751': 57, '0000-0002-7991-9428': 55, '0000-0002-4010-1063': 45, '0000-0002-2186-3484': 28, '0000-0002-4899-1929': 25, '0000-0003-0487-4242': 24, '0000-0002-3642-1486': 22, '0000-0001-9965-3535': 17, '0000-0002-4168-757X': 17, '0000-0001-6525-3744': 14, '0000-0002-3897-0278': 14, '0000-0002-1181-5112': 12, '0000-0003-1447-9385': 11, '0000-0002-7305-8786': 11, '0000-0002-2655-7806': 10, '0000-0003-3466-5353': 9, '0000-0002-7359-663X': 8, '0000-0003-4600-8668': 6, '0000-0002-1382-7088': 5, '0000-0002-9505-4882': 5, '0000-0003-3667-9900': 4, '0000-0001-9714-6038': 4, '0000-0002-4760-0228': 3, '0000-0003-4188-7915': 3, '0000-0001-9454-0427': 3, '0000-0002-0333-6808': 3, '0000-0003-2134-4964': 3, '0000-0002-6658-047X': 3, '0000-0003-1273-379X': 3, '0000-0002-7047-3183': 3, '0000-0002-1814-9546': 3, '0000-0003-4812-6297': 2, '0000-0001-6597-578X': 2, '0000-0002-5285-9138': 2, '0000-0002-6796-7844': 2, '0000-0002-1130-8698': 2, '0000-0001-8518-8150': 2, '0000-0002-7103-924X': 2, '0000-0002-5407-0202': 1, '0000-0001-6220-8411': 1, '0000-0002-7440-6703': 1, '0000-0002-1603-7559': 1, '0000-0003-0257-1707': 1, '0000-0001-8532-6517': 1, '0000-0001-6626-316X': 1, '0000-0002-3246-9861': 1, '0000-0002-7207-4389': 1, '0000-0001-9682-9654': 1, '0000-0002-0196-3832': 1, '0000-0001-8063-6081': 1, '0000-0003-2037-3333': 1, '0000-0001-8872-6751': 1})\n",
      "Total author before apply threshoid:  57\n",
      "Total author after apply threshoid:  3\n",
      "k_kim  name group sample size:  (1111, 2)\n",
      "Total missing sample:  0\n",
      "(1111, 101)\n",
      "Total missing sample:  103\n",
      "(1111, 101)\n",
      "Labeled:  1111  :  1111\n",
      "{'0000-0002-6929-5359', '0000-0001-9498-284X', '0000-0002-5878-8895'}\n",
      "k_kim_0 Check\n",
      "(1008, 100)\n",
      "(1008, 100)\n",
      "LR-UB f1:                precision    recall  f1-score   support\n",
      "\n",
      "       Other       1.00      1.00      1.00       165\n",
      "     k_kim_0       1.00      1.00      1.00        37\n",
      "\n",
      "    accuracy                           1.00       202\n",
      "   macro avg       1.00      1.00      1.00       202\n",
      "weighted avg       1.00      1.00      1.00       202\n",
      "\n",
      "[165   0   0  37]\n",
      "[('Other', 0.5), ('k_kim_0', 0.5)]\n",
      "[('Other', 5, 0), ('k_kim_0', 5, 0)]\n",
      "Initial L size:  10\n",
      "Initial U size:  1302\n",
      "U size after drawing sample to U prime: 0\n",
      "Initial U prime size:  1302\n",
      "P value:  1  N value:  1\n",
      "Iteration 0, strat self label.\n",
      "Iteration 1  h1 new:  [624, 666]  probs:  [0.937946288743398, 0.9494551744373916]\n",
      "Iteration 1  h2 new:  [652, 667]  probs:  [0.9366920356610978, 0.9708340829899064]\n",
      "Iteration 2  h1 new:  [450, 663]  probs:  [0.9527751494132615, 0.9629306547157249]\n",
      "Iteration 2  h2 new:  [172, 743]  probs:  [0.950555990876699, 0.980409669651805]\n",
      "Iteration 3  h1 new:  [646, 1308]  probs:  [0.962509744539253, 0.9759542830823423]\n",
      "Iteration 3  h2 new:  [456, 911]  probs:  [0.9632391052129233, 0.9854907403107203]\n",
      "Iteration 4  h1 new:  [566, 1002]  probs:  [0.9701716552076289, 0.9820815009718403]\n",
      "Iteration 4  h2 new:  [548, 1067]  probs:  [0.9537483152935508, 0.9883376515406407]\n",
      "Iteration 5  h1 new:  [345, 1248]  probs:  [0.9710289281315049, 0.9849880299686016]\n",
      "Iteration 5  h2 new:  [271, 1049]  probs:  [0.9576298818286817, 0.9905500936374355]\n",
      "Iteration 6  h1 new:  [241, 792]  probs:  [0.9774846123967277, 0.9887577607081992]\n",
      "Iteration 6  h2 new:  [567, 1138]  probs:  [0.9676736583842539, 0.9922888607933567]\n",
      "Iteration 7  h1 new:  [60, 736]  probs:  [0.9795838022258038, 0.9900830124884704]\n",
      "Iteration 7  h2 new:  [16, 1158]  probs:  [0.9620458597905792, 0.9936037406787364]\n",
      "Iteration 8  h1 new:  [572, 727]  probs:  [0.9818052397184551, 0.9908705044476698]\n",
      "Iteration 8  h2 new:  [368, 690]  probs:  [0.9669045983172101, 0.9941201469332293]\n",
      "Iteration 9  h1 new:  [259, 856]  probs:  [0.9815504892549545, 0.9918745640486011]\n",
      "Iteration 9  h2 new:  [287, 1151]  probs:  [0.968382605095257, 0.9950838384760008]\n",
      "Iteration 10  h1 new:  [299, 793]  probs:  [0.981187711415997, 0.9919555904430026]\n",
      "Iteration 10  h2 new:  [421, 694]  probs:  [0.9739493958178507, 0.9959580523055688]\n",
      "Iteration 11  h1 new:  [25, 1069]  probs:  [0.9828631751922883, 0.9924951468424453]\n",
      "Iteration 11  h2 new:  [442, 1232]  probs:  [0.9821168398095066, 0.996777791333369]\n",
      "Iteration 12  h1 new:  [229, 779]  probs:  [0.9825873704672758, 0.9865989172714709]\n",
      "Iteration 12  h2 new:  [513, 1145]  probs:  [0.9851345288979283, 0.9969130720124583]\n",
      "Iteration 13  h1 new:  [151, 809]  probs:  [0.9833040932893284, 0.9891193183514229]\n",
      "Iteration 13  h2 new:  [12, 757]  probs:  [0.9853445163930911, 0.9969524365858625]\n",
      "Iteration 14  h1 new:  [29, 691]  probs:  [0.9830337382585236, 0.9904589637061216]\n",
      "Iteration 14  h2 new:  [441, 1100]  probs:  [0.9836176152392697, 0.9968860451774607]\n",
      "Iteration 15  h1 new:  [478, 1215]  probs:  [0.9846198868910733, 0.991345506590171]\n",
      "Iteration 15  h2 new:  [477, 1191]  probs:  [0.9846611490905077, 0.9972377289107422]\n",
      "Iteration 16  h1 new:  [530, 868]  probs:  [0.9872382734599662, 0.9920250524760332]\n",
      "Iteration 16  h2 new:  [542, 1026]  probs:  [0.9862707587466287, 0.9974698224735747]\n",
      "Iteration 17  h1 new:  [26, 1149]  probs:  [0.9849774323773599, 0.9926115435010199]\n",
      "Iteration 17  h2 new:  [304, 973]  probs:  [0.9862085422470871, 0.997252692583289]\n",
      "Iteration 18  h1 new:  [340, 1043]  probs:  [0.9854856694135958, 0.9897406988903342]\n",
      "Iteration 18  h2 new:  [157, 996]  probs:  [0.9868202636426661, 0.9974246069602835]\n",
      "Iteration 19  h1 new:  [61, 937]  probs:  [0.9863989894042174, 0.9907776911935318]\n",
      "Iteration 19  h2 new:  [632, 1135]  probs:  [0.9877696927892041, 0.9975292222523451]\n",
      "Iteration 20  h1 new:  [190, 819]  probs:  [0.9866917094709307, 0.9913935992207215]\n",
      "Iteration 20  h2 new:  [512, 1292]  probs:  [0.988369474855705, 0.99763501001485]\n",
      "Iteration 21  h1 new:  [398, 1257]  probs:  [0.9871240277852451, 0.9920465574682819]\n",
      "Iteration 21  h2 new:  [143, 941]  probs:  [0.9896314486632345, 0.9977698604020624]\n",
      "Iteration 22  h1 new:  [120, 900]  probs:  [0.9874847290109288, 0.991773135317693]\n",
      "Iteration 22  h2 new:  [344, 741]  probs:  [0.988672547280445, 0.9978937650034685]\n",
      "Iteration 23  h1 new:  [426, 1277]  probs:  [0.9876762840771434, 0.9924935708060308]\n",
      "Iteration 23  h2 new:  [50, 964]  probs:  [0.9895434950199988, 0.9980197472908702]\n",
      "Iteration 24  h1 new:  [221, 905]  probs:  [0.988235936885778, 0.9931150008808637]\n",
      "Iteration 24  h2 new:  [473, 1136]  probs:  [0.9904961966051209, 0.9981584881834515]\n",
      "Iteration 25  h1 new:  [213, 1114]  probs:  [0.9884885260639499, 0.9908942827861065]\n",
      "Iteration 25  h2 new:  [490, 812]  probs:  [0.9907579650361718, 0.9976915302621873]\n",
      "Iteration 26  h1 new:  [176, 1218]  probs:  [0.9887226615610175, 0.9909902852826873]\n",
      "Iteration 26  h2 new:  [138, 816]  probs:  [0.9908578832557746, 0.9979681032374211]\n",
      "Iteration 27  h1 new:  [21, 773]  probs:  [0.9890618388901082, 0.9916503033807679]\n",
      "Iteration 27  h2 new:  [383, 820]  probs:  [0.9918770294164321, 0.9981196033732168]\n",
      "Iteration 28  h1 new:  [57, 914]  probs:  [0.9898760915335285, 0.9923320486982897]\n",
      "Iteration 28  h2 new:  [300, 970]  probs:  [0.9919900345859132, 0.9981726129767701]\n",
      "Iteration 29  h1 new:  [113, 1071]  probs:  [0.9893701589742249, 0.9925360031924597]\n",
      "Iteration 29  h2 new:  [604, 687]  probs:  [0.9922886934749007, 0.9980506189386057]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 30  h1 new:  [394, 1177]  probs:  [0.9919060987136334, 0.993120236667676]\n",
      "Iteration 30  h2 new:  [405, 814]  probs:  [0.9924256316608872, 0.9981134453526178]\n",
      "Total Labeled number:  130  Still unlabeled number:  1182\n",
      "co_LR f1:                precision    recall  f1-score   support\n",
      "\n",
      "       Other       1.00      0.78      0.88       165\n",
      "     k_kim_0       0.51      1.00      0.67        37\n",
      "\n",
      "    accuracy                           0.82       202\n",
      "   macro avg       0.75      0.89      0.78       202\n",
      "weighted avg       0.91      0.82      0.84       202\n",
      "\n",
      "[129  36   0  37]\n",
      "Initial L size:  10\n",
      "Initial U size:  1302\n",
      "U size after drawing sample to U prime: 0\n",
      "Initial U prime size:  1302\n",
      "Iteration 0, strat self label.\n",
      "Iteration 1  h1 new:  [241, 666]  score:  [0.9340247763061559, 0.9494551744373916]\n",
      "Iteration 1  h2 new:  [652, 666, 940]  score:  [0.9366920356610978, 0.9545987907862558, 0.9423802420176243]\n",
      "Iteration 2  h1 new:  [566, 969, 1305]  score:  [0.9495349645442582, 0.961086372518337, 0.961086372518337]\n",
      "Iteration 2  h2 new:  [172, 757, 1145]  score:  [0.952878583506781, 0.9745860955484154, 0.9745860955484154]\n",
      "Iteration 3  h1 new:  [548, 1189, 688]  score:  [0.9497115377872579, 0.9755784738860951, 0.9755784738860951]\n",
      "Iteration 3  h2 new:  [548, 690, 694]  score:  [0.945276522618161, 0.9842265452727363, 0.9842265452727363]\n",
      "Iteration 4  h1 new:  [624, 1018, 1309]  score:  [0.9672288878170443, 0.982649474995248, 0.982649474995248]\n",
      "Iteration 4  h2 new:  [646, 1151, 1232]  score:  [0.9377657252037724, 0.9877485196522303, 0.9877485196522303]\n",
      "Iteration 5  h1 new:  [572, 774, 812]  score:  [0.9653222458420904, 0.9856891794600993, 0.9802104415648893]\n",
      "Iteration 5  h2 new:  [368, 1100, 1026]  score:  [0.9558088832188574, 0.987289759221586, 0.987289759221586]\n",
      "Iteration 6  h1 new:  [290, 820, 970]  score:  [0.960461825117151, 0.9848792238027416, 0.9848792238027416]\n",
      "Iteration 6  h2 new:  [290, 1191, 1136]  score:  [0.9464818289771334, 0.9892276675905405, 0.987806862890096]\n",
      "Iteration 7  h1 new:  [345, 816, 779]  score:  [0.975082716080113, 0.9879790107656405, 0.9881410603099716]\n",
      "Iteration 7  h2 new:  [467, 1292, 964]  score:  [0.9702940072929357, 0.9894528543467873, 0.9894528543467873]\n",
      "Iteration 8  h1 new:  [259, 691, 868]  score:  [0.9709181246730011, 0.9898666313599825, 0.9898666313599825]\n",
      "Iteration 8  h2 new:  [553, 996, 741]  score:  [0.9723690649186785, 0.9912103910880763, 0.9912103910880763]\n",
      "Iteration 9  h1 new:  [60, 1149, 809]  score:  [0.9762385381000536, 0.9913750052362674, 0.9913750052362674]\n",
      "Iteration 9  h2 new:  [411, 941, 973]  score:  [0.9827583577059793, 0.9928769323193053, 0.9928769323193053]\n",
      "Iteration 10  h1 new:  [304, 1215, 1135]  score:  [0.970270495802785, 0.992343870472456, 0.9886249040206053]\n",
      "Iteration 10  h2 new:  [130, 1135, 1228]  score:  [0.9764145522861658, 0.9940079891909825, 0.9919918133253519]\n",
      "Iteration 11  h1 new:  [155, 914, 1177]  score:  [0.975782592206452, 0.9901725958423804, 0.9901725958423804]\n",
      "Iteration 11  h2 new:  [544, 814, 1106]  score:  [0.9776391424390771, 0.9928522680868037, 0.9928522680868037]\n",
      "Iteration 12  h1 new:  [450, 1071, 1218]  score:  [0.9763207122338168, 0.991603916035262, 0.991603916035262]\n",
      "Iteration 12  h2 new:  [157, 687, 1233]  score:  [0.9752858429849991, 0.9934304974804037, 0.9923949450244886]\n",
      "Iteration 13  h1 new:  [229, 773, 1062]  score:  [0.9831689830315953, 0.9925381354922304, 0.9883771122757681]\n",
      "Iteration 13  h2 new:  [421, 827, 990]  score:  [0.9809208484972847, 0.9930245675793652, 0.9930245675793652]\n",
      "Iteration 14  h1 new:  [26, 1034, 1116]  score:  [0.9735903527521415, 0.9894617264116451, 0.9894617264116451]\n",
      "Iteration 14  h2 new:  [12, 683, 1040]  score:  [0.9838460040917111, 0.9933559044351353, 0.9933559044351353]\n",
      "Iteration 15  h1 new:  [21, 1032, 770]  score:  [0.9831569851983776, 0.9910858245342109, 0.9915803475124181]\n",
      "Iteration 15  h2 new:  [7, 770, 713]  score:  [0.9786789161708787, 0.9938226561036017, 0.9938226561036017]\n",
      "Iteration 16  h1 new:  [439, 716, 795]  score:  [0.9794967285624222, 0.9926958848934099, 0.9912690363999416]\n",
      "Iteration 16  h2 new:  [473, 716, 1011]  score:  [0.9792927686093449, 0.9941094815719944, 0.9914111020821905]\n",
      "Iteration 17  h1 new:  [394, 907, 659]  score:  [0.9803993551886557, 0.9921964295697769, 0.9921964295697769]\n",
      "Iteration 17  h2 new:  [192, 1214, 1155]  score:  [0.9860148082397924, 0.9918575549686133, 0.9918575549686133]\n",
      "Iteration 18  h1 new:  [299, 903, 876]  score:  [0.9881693219526925, 0.9933294777996461, 0.9933294777996461]\n",
      "Iteration 18  h2 new:  [16, 1192, 1075]  score:  [0.9910877067029042, 0.9924660612162612, 0.9924660612162612]\n",
      "Iteration 19  h1 new:  [478, 953, 1039]  score:  [0.9808150962767972, 0.9939433639885992, 0.9907908654364]\n",
      "Iteration 19  h2 new:  [513, 1039, 1239]  score:  [0.9909259739421664, 0.9930478058480132, 0.9936010694117615]\n",
      "Iteration 20  h1 new:  [344, 902, 799]  score:  [0.9801085105910584, 0.994772875302558, 0.994772875302558]\n",
      "Iteration 20  h2 new:  [442, 1284, 711]  score:  [0.9940338179860053, 0.9939669371387427, 0.9939669371387427]\n",
      "Iteration 21  h1 new:  [530, 1055, 1052]  score:  [0.9842729603775143, 0.9951074746884196, 0.9951074746884196]\n",
      "Iteration 21  h2 new:  [50, 787, 708]  score:  [0.9888887175734964, 0.9942568532946207, 0.9942568532946207]\n",
      "Iteration 22  h1 new:  [643, 943, 1156]  score:  [0.9846897762442892, 0.9954708414209114, 0.9954708414209114]\n",
      "Iteration 22  h2 new:  [441, 1236, 978]  score:  [0.9907794032381333, 0.9917324004876042, 0.9917324004876042]\n",
      "Iteration 23  h1 new:  [61, 900, 905]  score:  [0.9840049999178584, 0.9945948592409702, 0.9945948592409702]\n",
      "Iteration 23  h2 new:  [542, 739, 759]  score:  [0.9912068618817425, 0.9923157856328183, 0.9921408709289878]\n",
      "Iteration 24  h1 new:  [66, 1277, 932]  score:  [0.9848237969683301, 0.9951174292462204, 0.9950306176259721]\n",
      "Iteration 24  h2 new:  [271, 692, 948]  score:  [0.996027546017263, 0.9926520719861134, 0.9926520719861134]\n",
      "Iteration 25  h1 new:  [117, 962, 1273]  score:  [0.9832320233813914, 0.995317806075893, 0.995317806075893]\n",
      "Iteration 25  h2 new:  [405, 845, 898]  score:  [0.9877381019505355, 0.9931702902678494, 0.9931702902678494]\n",
      "Iteration 26  h1 new:  [256, 1130, 714]  score:  [0.9863256567091162, 0.995633322175507, 0.9935779268012609]\n",
      "Iteration 26  h2 new:  [567, 1165, 1279]  score:  [0.9900555703965467, 0.9936537439908683, 0.9962764497870747]\n",
      "Iteration 27  h1 new:  [449, 1006, 777]  score:  [0.9891230207294308, 0.9941415489794884, 0.9941415489794884]\n",
      "Iteration 27  h2 new:  [151, 1264, 1252]  score:  [0.990382603536155, 0.9965427149409308, 0.9965427149409308]\n",
      "Iteration 28  h1 new:  [391, 837, 1166]  score:  [0.9869908764092797, 0.9944193642608289, 0.9944193642608289]\n",
      "Iteration 28  h2 new:  [490, 995, 1047]  score:  [0.9904896665111396, 0.9967973240258571, 0.9943318176079334]\n",
      "Iteration 29  h1 new:  [213, 1094, 1150]  score:  [0.9821676315106258, 0.9952271644085978, 0.9915886019518075]\n",
      "Iteration 29  h2 new:  [512, 850, 1176]  score:  [0.9945635639780892, 0.9946836605073257, 0.9948313184292112]\n",
      "Iteration 30  h1 new:  [419, 1227, 1242]  score:  [0.9852756208216982, 0.9920123986090077, 0.9920123986090077]\n",
      "Iteration 30  h2 new:  [25, 1211, 702]  score:  [0.9903923796103967, 0.995163564113558, 0.995163564113558]\n",
      "Total Labeled number:  189  Still unlabeled number:  1130\n",
      "Improved_co_LR f1:                precision    recall  f1-score   support\n",
      "\n",
      "       Other       1.00      0.78      0.87       165\n",
      "     k_kim_0       0.50      1.00      0.67        37\n",
      "\n",
      "    accuracy                           0.82       202\n",
      "   macro avg       0.75      0.89      0.77       202\n",
      "weighted avg       0.91      0.82      0.84       202\n",
      "\n",
      "[128  37   0  37]\n",
      "LR-LB f1:                precision    recall  f1-score   support\n",
      "\n",
      "       Other       1.00      0.73      0.84       165\n",
      "     k_kim_0       0.45      1.00      0.62        37\n",
      "\n",
      "    accuracy                           0.78       202\n",
      "   macro avg       0.73      0.86      0.73       202\n",
      "weighted avg       0.90      0.78      0.80       202\n",
      "\n",
      "[120  45   0  37]\n",
      "LR-UB f1:                precision    recall  f1-score   support\n",
      "\n",
      "       Other       1.00      0.98      0.99       164\n",
      "     k_kim_0       0.93      1.00      0.96        38\n",
      "\n",
      "    accuracy                           0.99       202\n",
      "   macro avg       0.96      0.99      0.98       202\n",
      "weighted avg       0.99      0.99      0.99       202\n",
      "\n",
      "[161   3   0  38]\n",
      "[('Other', 0.5), ('k_kim_0', 0.5)]\n",
      "[('Other', 5, 0), ('k_kim_0', 5, 0)]\n",
      "Initial L size:  10\n",
      "Initial U size:  1304\n",
      "U size after drawing sample to U prime: 0\n",
      "Initial U prime size:  1304\n",
      "P value:  1  N value:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, strat self label.\n",
      "Iteration 1  h1 new:  [549, 913]  probs:  [0.948542123854445, 0.9316278502996354]\n",
      "Iteration 1  h2 new:  [117, 1156]  probs:  [0.9261658274256153, 0.958486861707628]\n",
      "Iteration 2  h1 new:  [625, 785]  probs:  [0.9632359648281376, 0.9610180992247445]\n",
      "Iteration 2  h2 new:  [412, 1076]  probs:  [0.948557340181487, 0.972470209267364]\n",
      "Iteration 3  h1 new:  [260, 1193]  probs:  [0.9727898601258186, 0.9689683111434853]\n",
      "Iteration 3  h2 new:  [653, 1040]  probs:  [0.9646390891084691, 0.9776597068851597]\n",
      "Iteration 4  h1 new:  [647, 1297]  probs:  [0.9775241338913014, 0.9741449944782151]\n",
      "Iteration 4  h2 new:  [272, 1192]  probs:  [0.9752120421640051, 0.9809502133840518]\n",
      "Iteration 5  h1 new:  [137, 1030]  probs:  [0.9823600152442322, 0.9775057130085133]\n",
      "Iteration 5  h2 new:  [173, 1214]  probs:  [0.9768608739058253, 0.9834255495003512]\n",
      "Iteration 6  h1 new:  [395, 1120]  probs:  [0.9815625163480305, 0.971299359557014]\n",
      "Iteration 6  h2 new:  [94, 1095]  probs:  [0.980158171567857, 0.9783472872079363]\n",
      "Iteration 7  h1 new:  [451, 1187]  probs:  [0.9841840840981606, 0.9803373652305765]\n",
      "Iteration 7  h2 new:  [123, 838]  probs:  [0.9785147691764307, 0.9807744578046125]\n",
      "Iteration 8  h1 new:  [86, 1250]  probs:  [0.9826337672520083, 0.9847763794363077]\n",
      "Iteration 8  h2 new:  [514, 1166]  probs:  [0.9777323131691744, 0.983899067687476]\n",
      "Iteration 9  h1 new:  [300, 992]  probs:  [0.9839941904269582, 0.9797554755532041]\n",
      "Iteration 9  h2 new:  [36, 1296]  probs:  [0.9828337254967675, 0.980681047908085]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "# loop through all files in directory add name to name list\n",
    "fileDir = \"../Data/\"+Dataset+\"/canopies_labeled/\"\n",
    "listfiles = os.listdir(fileDir)\n",
    "\n",
    "init_labeled_size = 10\n",
    "\n",
    "co_lr_diff_embedding_final_result = collections.defaultdict(list)\n",
    "\n",
    "#---------------- load different embedding combination ---------------#\n",
    "for v1_emb, v2_emb in zip(pp_text, pp_citation):\n",
    "    # read embeddings\n",
    "    print(\"Load text embedding: \", v1_emb)\n",
    "    viewone_text_emb = com_func.read_text_embedding(emb_type=v1_emb, training_size = \"140k\")\n",
    "    print(\"Load citation embedding: \", v2_emb)\n",
    "    viewtwo_citation_embedding = com_func.read_citation_embedding_sorted(emb_type = v2_emb, labeled_only = True)\n",
    "    # print(viewone_text_emb[0])\n",
    "    # print(viewtwo_citation_embedding[0])\n",
    "    \n",
    "    threshold_change_all_method_f1s = collections.defaultdict(list)\n",
    "    # -------------- different threshold (step by 10) -----------------------#\n",
    "    for step_threshold in range(threshold_lower, threshold_upper, 10):\n",
    "        plot_save_path = \"../../plot/4_co_train_OVR_detail_plots/author_threshold=\"+str(step_threshold)+\"/V1=\"+v1_emb+\"_V2=\"+v2_emb+\"/\"\n",
    "        threshold_change_all_method_f1s[\"threshold\"].append(step_threshold)\n",
    "        # collect statistic to output\n",
    "        statistic_detail = collections.defaultdict(list)\n",
    "        total_selected_group = 0\n",
    "        selected_binary_case_group = 0\n",
    "\n",
    "        # ------- different name group in all name group --------------------#\n",
    "        for file in listfiles:\n",
    "            # group name\n",
    "            temp = file.split(\"_\")\n",
    "            name = str(temp[1]+\"_\"+temp[-1])\n",
    "            print(\"For name: \",name)\n",
    "            # read label (pid : author ORCID) from file\n",
    "            data = com_func.read_pid_aid(fileDir+file)\n",
    "            labeled_mask = data[\"authorID\"] != \"-1\"\n",
    "            labeled_data = data[labeled_mask]\n",
    "            print(labeled_data.shape)\n",
    "            # ---------------- collect all labeled sample -------------------- #\n",
    "            # ---------------- if use all samples as negative --------------- #\n",
    "            all_labeled_samples = labeled_data[\"paperID\"].tolist()\n",
    "            authorCounter = com_func.select_productive_groups(labeled_data=labeled_data, \n",
    "                                                              threshold=threshold_select_name_group)\n",
    "            ''' \n",
    "            Case 1: no author under this name have written more than threshold number of papers, dataset not used\n",
    "            Case 2: only one author under this name written more than threshold number of papers, dataset not used\n",
    "            Case 3: 2 or more author under this name written more than threshold number of papers, dataset used\n",
    "            '''\n",
    "            if(len(authorCounter)==0) or (len(authorCounter)==1):\n",
    "                print(name,\" pass\")\n",
    "            else:\n",
    "                total_selected_group+= 1\n",
    "                '''\n",
    "                Case 1: All papers under name group is used. We include authors written less than threshold of paper,\n",
    "                        and treat it as negative class. This will be OVR.(Not used)\n",
    "                Case 2: All papers under name group is used. We include authors written less than threshold of paper,\n",
    "                        and perform muti-class classification (Not used)\n",
    "                Case 3: Only Include author with more than threshold number of paper and perform muti-class classification (Not used)\n",
    "                Case 5: Only Include author with more than threshold number of paper and perform OVR(used)\n",
    "                Case 4: Only Include author with more than threshold number of paper and only select binary case(used)\n",
    "                '''\n",
    "                if apply_threshold_to_name_group_samples == True:\n",
    "                    # ---------- only use sample pass threshold ------- #\n",
    "                    #-------- only select authors in name group are very productive (more than threshold)---------#\n",
    "                    labeled_data, author_list, _= com_func.only_select_productive_authors(labeled_data, step_threshold)\n",
    "                    # ----------------- if use filtered samples as negative  --------- #\n",
    "                    filtered_all_labeled_samples = labeled_data[\"paperID\"].tolist()\n",
    "                else:\n",
    "                    # ----------- use all sample in name group --------- #\n",
    "                    author_list = com_func.productive_authors_list(labeled_data, step_threshold)\n",
    "                    print(name, \" name group sample size: \",labeled_data.shape)\n",
    "                # -------------- extract all samples for name group -------------- #\n",
    "                # for each name group\n",
    "                # read in labeled data\n",
    "                labeled_viewone_text = com_func.extract_sorted_embedding(viewone_text_emb, labeled_data[\"paperID\"])\n",
    "                print(labeled_viewone_text.shape)\n",
    "                labeled_viewtwo_citation = com_func.extract_sorted_embedding(viewtwo_citation_embedding, labeled_data[\"paperID\"])\n",
    "                print(labeled_viewtwo_citation.shape)\n",
    "                print(\"Labeled: \",len(labeled_viewone_text), \" : \", len(labeled_viewtwo_citation))\n",
    "                # ---------------- shuffle the data ----------------- #\n",
    "                labeled_data = labeled_data.sample(frac=1).reset_index(drop=True)\n",
    "                # ------------------ alignment and fill missing data with 0 ---------------------- #\n",
    "                labeled_viewone_text = pd.merge(labeled_data, labeled_viewone_text, left_on=\"paperID\", right_on = [0], how = \"left\")\n",
    "                labeled_viewtwo_citation = pd.merge(labeled_data, labeled_viewtwo_citation, left_on=\"paperID\", right_on = [0], how = \"left\")\n",
    "                rows_with_nan = [index for index, row in labeled_viewtwo_citation.iterrows() if row.isnull().any()]\n",
    "                labeled_viewone_text = labeled_viewone_text.drop(rows_with_nan).reset_index(drop=True)\n",
    "                labeled_viewtwo_citation = labeled_viewtwo_citation.drop(rows_with_nan).reset_index(drop=True)\n",
    "                print(author_list)\n",
    "                '''\n",
    "                Covert all case to OVR binary case\n",
    "                '''\n",
    "                for idx, unique_label in enumerate(author_list):\n",
    "                    true_label = labeled_viewone_text[\"authorID\"].copy()\n",
    "                    name = name+\"_\"+str(idx)\n",
    "                    mask = true_label.isin([unique_label])\n",
    "                    true_label[mask] = name\n",
    "                    true_label[~mask] = \"Other\"\n",
    "                    #print(true_label)\n",
    "                    print(name + \" Check\")\n",
    "                    viewone_text_final = labeled_viewone_text.drop([\"paperID\", \"authorID\", 0], axis=1)\n",
    "                    viewtwo_citation_final = labeled_viewtwo_citation.drop([\"paperID\", \"authorID\", 0], axis=1)\n",
    "                    print(viewone_text_final.shape)\n",
    "                    print(viewtwo_citation_final.shape)\n",
    "                    ''' Apply different algorithm:\n",
    "                    Part 1: Basic supervised algorithm \n",
    "                    Part 2: Basic co-training algorithm \n",
    "                    Part 3: 2 clf co-training \n",
    "                    Part 4: Improved co-training algorithm (Self-proposed with all different improvement)\n",
    "                    '''\n",
    "                    # -------------------- part 1 ------------------------- #\n",
    "                    LR_clf = LogisticRegression(solver= \"liblinear\")\n",
    "                    SVM_clf = SVC(gamma=\"auto\", kernel='linear')\n",
    "                    # -------------------- part 2 ------------------------- #\n",
    "                    initial_cotrain_parameters = {\"p\":1,\"n\":1,\"k\":30}\n",
    "                    co_LR_clf = Co_training_clf(clf1=LogisticRegression(solver= \"liblinear\"),**initial_cotrain_parameters)\n",
    "                    ''' For co-training with SVM\n",
    "                    Case 1: Using Scikit-learn where we set probability=True\n",
    "                    Case 1 details: Scikit-learn uses LibSVM internally, and this in turn uses Platt scaling\n",
    "                    Platt scaling requires first training the SVM as usual, then optimizing parameter \n",
    "                    vectors A and B such that: P(y|X) = 1 / (1 + exp(A * f(X) + B))\n",
    "                    where f(X) is the signed distance of a sample from the hyperplane\n",
    "                    (scikit-learn's decision_function method).\n",
    "                    Case 2: Using decision_function to get signed distance of sample from the hyperplane, calculate proba\n",
    "                    Case 2 details: use sigmoid (binary)/ softmax (muti-class) for calculate probability based on \n",
    "                    signed distance of sample from the hyperplane. Then follow same step as case 1. The result is relatively\n",
    "                    bad\n",
    "                    '''\n",
    "                    # ----------- Case 1 Using Scikit-learn where we set probability=True ------ #\n",
    "                    co_SVM_clf = Co_training_clf(clf1=SVC(gamma=\"auto\", kernel='linear',probability=True),**initial_cotrain_parameters)\n",
    "                    # ------- Case 2 Using decision_function get distance, calculate proba------ #\n",
    "                    #co_svm_clf = Co_training_clf(clf1=SVC(gamma=\"auto\", kernel='linear'),**initial_cotrain_parameters)\n",
    "                    #co_train_clfs = [(co_svm_clf,\"co_train_SVM\")]\n",
    "                    # -------------------- part 3 ------------------------- #\n",
    "                    # two different clf with basic co-training\n",
    "                    co_LR_SVM_clf = Co_training_clf(clf1 = LogisticRegression(solver= \"liblinear\"),\n",
    "                                                    clf2 = SVC(gamma=\"auto\", kernel='linear',probability=True),\n",
    "                                                    **initial_cotrain_parameters)\n",
    "                    # -------------------- part 4 ------------------------- #\n",
    "                    improved_cotrain_parameters = {\"view_num\":2,\"sl_class_max\":2,\"k\":30}\n",
    "                    improved_co_LR = Improved_co_training_clf(clf = [LogisticRegression(solver= \"liblinear\")], \n",
    "                                                              **improved_cotrain_parameters)\n",
    "                    # -------------------- train together ----------------- #\n",
    "                    #baseline_clfs = [(LR_clf,\"LR\"),(SVM_clf,\"SVM\")]\n",
    "                    #final_co_train_clfs = [(co_LR_clf,\"co_LR\"), (co_SVM_clf,\"co_SVM\"),\n",
    "                    #                       (co_LR_SVM_clf, \"co_LR_SVM\"), (improved_co_LR, \"Improved_co_LR\")]\n",
    "                    baseline_clfs = [(LR_clf,\"LR\")]\n",
    "                    final_co_train_clfs = [(co_LR_clf,\"co_LR\"),(improved_co_LR, \"Improved_co_LR\")]\n",
    "                    final_f1_score, cv_per_fold_status= k_fold_cv_all_algorithm(dv1=viewone_text_final,\n",
    "                                                                                dv2=viewtwo_citation_final,\n",
    "                                                                                label=true_label,\n",
    "                                                                                init_labeled_size=init_labeled_size,\n",
    "                                                                                muti_view_clf=final_co_train_clfs,\n",
    "                                                                                combined_clf=baseline_clfs,\n",
    "                                                                                num_fold=5,\n",
    "                                                                                dataset_name=name,\n",
    "                                                                                plot_save_path=plot_save_path)\n",
    "                    #print(final_f1_score)\n",
    "                    #print(cv_per_fold_status)\n",
    "                    statistic_detail['Name'].append(name)\n",
    "                    statistic_detail['Total_sample_size'].append(len(true_label))\n",
    "                    statistic_detail['Train_size'].append(cv_per_fold_status[0][\"train_size\"])\n",
    "                    statistic_detail['Test_size'].append(cv_per_fold_status[0][\"test_size\"])\n",
    "                    statistic_detail[\"All_fold_details\"].append(cv_per_fold_status)\n",
    "                    if len(final_co_train_clfs)!=0:\n",
    "                        statistic_detail['Unlabel_size'].append(cv_per_fold_status[0][\"unlabel_size\"])\n",
    "                        statistic_detail['Validation_size'].append(cv_per_fold_status[0][\"validation_size\"])\n",
    "                        for clf, clf_name in final_co_train_clfs:\n",
    "                            statistic_detail[clf_name+'_total_self_labeled'].append(cv_per_fold_status[0][clf_name+\"_total_self_labeled\"])\n",
    "\n",
    "                    for clf_name, clf_f1 in final_f1_score:\n",
    "                        statistic_detail[clf_name+\"_f1\"].append(clf_f1)\n",
    "\n",
    "        # print(statistic_detail)\n",
    "        print(\"Total number of selected group:\",total_selected_group)\n",
    "        print(\"Total number of selected binary group:\",selected_binary_case_group)\n",
    "        print(\"Total number of selected muti-class group:\",(total_selected_group-selected_binary_case_group))\n",
    "        # write evaluation result to excel\n",
    "        output = pd.DataFrame(statistic_detail)\n",
    "        print(output)\n",
    "        savePath = \"../../result/\"+Dataset+\"/4_co_train_OVR_sample=140k/\"\n",
    "        filename = \"(init_labeled_size=\"+str(init_labeled_size)+\") V1=\"+v1_emb+\"_V2=\"+v2_emb+\"_author_threshold=\"+str(step_threshold)+\".csv\"\n",
    "        com_func.write_csv_df(savePath, filename, output)\n",
    "        print(\"v1:\",v1_emb,\" v2:\",v2_emb, \"threshold\",step_threshold,\" Done\")\n",
    "        \n",
    "        '''Save result with respect to threshold change'''\n",
    "        threshold_change_all_method_f1s['Name'].append(statistic_detail['Name'])\n",
    "        threshold_change_all_method_f1s['All_details'].append(statistic_detail[\"All_fold_details\"])\n",
    "        for col in output.columns: \n",
    "            if \"f1\" in col:\n",
    "                threshold_change_all_method_f1s[col].append(statistic_detail[col])\n",
    "\n",
    "    co_lr_diff_embedding_final_result[\"v1:\"+v1_emb+\" v2:\"+v2_emb].append(threshold_change_all_method_f1s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots: 1. All method per fold variance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T22:56:58.072177Z",
     "start_time": "2020-06-02T22:56:58.024267Z"
    }
   },
   "outputs": [],
   "source": [
    "all_author_details = co_lr_diff_embedding_final_result['v1:pv_dbow v2:n2v'][0]['All_details'][0]\n",
    "\n",
    "cotrain_all_per_fold_result= {}\n",
    "\n",
    "for author in all_author_details:\n",
    "    per_fold_f1 = collections.defaultdict(list)\n",
    "    for per_fold_details in author:\n",
    "        #print(per_fold_details)\n",
    "        for key in per_fold_details:\n",
    "            if key in ['co_LR f1','co_SVM f1','co_LR_SVM f1','Improved_co_LR f1','LR-LB f1','SVM-LB f1','LR-UB f1','SVM-UB f1']:\n",
    "                per_fold_f1[key].append(per_fold_details[key])\n",
    "    cotrain_all_per_fold_result[author[0]['author']]=(per_fold_f1)\n",
    "#print(cotrain_all_per_fold_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T00:03:13.546815Z",
     "start_time": "2020-05-22T00:03:13.480794Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name_list = list(cotrain_all_per_fold_result.keys())\n",
    "method_list = []\n",
    "f1_mean = collections.defaultdict(list)\n",
    "f1_min = collections.defaultdict(list)\n",
    "f1_max = collections.defaultdict(list)\n",
    "for author, author_result in cotrain_all_per_fold_result.items():\n",
    "    #print(author)\n",
    "    for method, method_result in author_result.items():\n",
    "        if method.replace(' f1','') not in method_list:\n",
    "            method_list.append(method.replace(' f1',''))\n",
    "        f1_mean[method.replace(' f1','')].append(np.mean(method_result))\n",
    "        f1_min[method.replace(' f1','')].append(np.min(method_result))\n",
    "        f1_max[method.replace(' f1','')].append(np.max(method_result))\n",
    "        #print(method_result, \" mean: \", np.mean(method_result), \" min: \",np.min(method_result),\" max: \", np.max(method_result))\n",
    "print(name_list)\n",
    "print(method_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T00:03:17.829954Z",
     "start_time": "2020-05-22T00:03:17.150210Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = name_list\n",
    "random_color = np.random.rand(len(method_list),3)\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "for idx, method in enumerate(method_list):\n",
    "    y = f1_mean[method]\n",
    "    y_min = f1_min[method]\n",
    "    y_max = f1_max[method]\n",
    "    \n",
    "    plt.plot(x, y, 'k', color = random_color[idx], label=method)\n",
    "    plt.fill_between(x, y_min, y_max, alpha=0.5, edgecolor = random_color[idx], facecolor=random_color[idx])\n",
    "    plt.xticks(rotation=45, horizontalalignment='center')\n",
    "\n",
    "legend = plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=2)\n",
    "plt.xlabel('Datasets')\n",
    "plt.ylabel('F1 score')\n",
    "#plt.savefig(plot_save_path+\"/all_method_result_variance.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T18:17:26.057759Z",
     "start_time": "2020-05-18T18:17:25.094557Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = name_list\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "\n",
    "for idx, method in enumerate(method_list):\n",
    "    y = f1_mean[method]\n",
    "    y_min_error = [a - b for a, b in zip(f1_mean[method], f1_min[method])]\n",
    "    y_max_error = [a - b for a, b in zip(f1_max[method], f1_mean[method])]\n",
    "    y_error = [y_min_error,y_max_error]\n",
    "\n",
    "    (_, caps, _) = ax.errorbar(x, y,yerr=y_error, fmt='-o',capsize=10, label=method)\n",
    "    for cap in caps:\n",
    "        cap.set_markeredgewidth(1)\n",
    "    plt.xticks(rotation=45, horizontalalignment='center')\n",
    "\n",
    "legend = plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=2)\n",
    "plt.xlabel('Datasets')\n",
    "plt.ylabel('F1 score')\n",
    "#plt.savefig(plot_save_path+\"/all_method_result_variance.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Plot: Each method per fold variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T18:17:39.004740Z",
     "start_time": "2020-05-18T18:17:36.577384Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for idx, method in enumerate(method_list):\n",
    "    plt.figure(figsize=(15,10))\n",
    "    x = name_list\n",
    "    y = f1_mean[method]\n",
    "    y_min = f1_min[method]\n",
    "    y_max = f1_max[method]\n",
    "    \n",
    "    plt.plot(x, y, 'k', color = random_color[idx],marker='o', label=method)\n",
    "    plt.fill_between(x, y_min, y_max, alpha=0.5, edgecolor = random_color[idx], facecolor=random_color[idx])\n",
    "    plt.xticks(rotation=45, horizontalalignment='center')\n",
    "    legend = plt.legend()\n",
    "    plt.xlabel('Datasets')\n",
    "    plt.ylabel('F1 score')\n",
    "    #plt.savefig(plot_save_path+\"/\"+method+\"_result_variance.png\", dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T22:02:47.584191Z",
     "start_time": "2020-05-17T22:02:45.088585Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for idx, method in enumerate(method_list):\n",
    "    fig, ax = plt.subplots(figsize=(15,10))\n",
    "    x = name_list\n",
    "    y = f1_mean[method]\n",
    "    y_min_error = [a - b for a, b in zip(f1_mean[method], f1_min[method])]\n",
    "    y_max_error = [a - b for a, b in zip(f1_max[method], f1_mean[method])]\n",
    "    y_error = [y_min_error,y_max_error]\n",
    "\n",
    "    (_, caps, _) = ax.errorbar(x, y,yerr=y_error, fmt='-o',capsize=10, label=method)\n",
    "    for cap in caps:\n",
    "        cap.set_markeredgewidth(1)\n",
    "    plt.xticks(rotation=45, horizontalalignment='center')\n",
    "    legend = plt.legend()\n",
    "    plt.xlabel('Datasets')\n",
    "    plt.ylabel('F1 score')\n",
    "    #plt.savefig(plot_save_path+\"/\"+method+\"_result_variance.png\", dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#         # --------------- plot overall result f1 variance --------------- #\n",
    "#         all_per_fold_f1_score_variance_plot = pd.DataFrame(all_per_fold_f1_score_variance)\n",
    "#         ax = sns.boxplot(x=\"author\", y=\"f1\", data=all_per_fold_f1_score_variance_plot)\n",
    "#         ax = sns.swarmplot(x=\"author\", y=\"f1\", data=all_per_fold_f1_score_variance_plot, color=\".25\")\n",
    "#         plt.savefig(plot_save_path+\"all_result_variance.png\", dpi=300)\n",
    "#         # plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
