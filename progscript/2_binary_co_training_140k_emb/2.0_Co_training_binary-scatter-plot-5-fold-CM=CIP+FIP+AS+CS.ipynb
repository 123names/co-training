{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T01:52:33.317526Z",
     "start_time": "2020-07-08T01:52:32.743302Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T01:52:33.322283Z",
     "start_time": "2020-07-08T01:52:33.319368Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import logging\n",
    "# logging.basicConfig(filename='./test.log', level=logging.DEBUG, \n",
    "#                     format='%(asctime)s %(levelname)s %(name)s %(message)s')\n",
    "# logger=logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T01:52:33.327636Z",
     "start_time": "2020-07-08T01:52:33.323906Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from IPython.core.magic import register_cell_magic\n",
    "\n",
    "# @register_cell_magic('handle')\n",
    "# def handle(line, cell):\n",
    "#     try:\n",
    "#         exec(cell)\n",
    "#     except Exception as e:\n",
    "#         logger.error(e)\n",
    "#         raise # if you want the full trace-back in the notebook\n",
    "\n",
    "\n",
    "# use %%handle when want to output error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co-training\n",
    "\n",
    "For visualization of co-training process, we apply PCA to feature before training. This will make co-training process clear, but the result will be not accuracy because apply PCA will loss lots of information.\n",
    "\n",
    "1. We assume only part of label exist\n",
    "\n",
    "2. We only select binary case (Only when one name indicate two and only two author)\n",
    "\n",
    "3. When we apply 10 fold with co-training, each fold of first iteration will be baseline compare to co-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-07T23:17:47.351481Z",
     "start_time": "2020-08-07T23:17:45.711053Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "#warnings.filterwarnings('error')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import com_func\n",
    "\n",
    "#----- threshold for selecting set of name group -----------#\n",
    "threshold_select_name_group = 100\n",
    "#----- threshold for selecting min sample in name group ----#\n",
    "threshold_lower = 100\n",
    "threshold_upper = 110\n",
    "\n",
    "apply_threshold_to_name_group_samples = True\n",
    "\n",
    "pp_text = [\"pv_dbow\"]\n",
    "pp_citation = [\"n2v\"]\n",
    "\n",
    "Dataset = \"pubmed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Co-training details\n",
    "1. Basic co-training algorithm required parameter p,n,k,u. Since we have 15 different dataset, we will assume p and n is 1, k is 30. (We are simulate real world situation where we do not know the distribution of unlabeled data amount 15 different dataset)\n",
    "2. We set the parameter u as size of input train data (labeled+unlabeled) since our ublabeled data is not that large.\n",
    "3. During co-training process, the confidence measure is using probability as confident score to evaluate whether unlabel sample should be label or not.\n",
    "4. Probability only have few issues: \n",
    "Case 1: View diagreement in early iteration. The view could disagree with each other in early stage of co-training which will introduce noise; Case 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-07T23:17:57.270536Z",
     "start_time": "2020-08-07T23:17:47.353971Z"
    },
    "code_folding": [
     12,
     29,
     33,
     36,
     52,
     64,
     65,
     91,
     134,
     141,
     144,
     189,
     410,
     456
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import warnings\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "from sklearn.metrics import precision_score, recall_score,f1_score,accuracy_score\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# create co training classifier\n",
    "class Co_training_clf(object):\n",
    "    \n",
    "    def __init__(self, clf1, clf2=None, p=1, n=1, k=30, u = 75):\n",
    "        \n",
    "        self.clf1 = clf1\n",
    "        # assume co_training on one classifier\n",
    "        if clf2 == None:\n",
    "            self.clf2 = copy.deepcopy(clf1)\n",
    "        else:\n",
    "            self.clf2 = clf2\n",
    "        # take p example from most confidently positive labels to example\n",
    "        self.p = p\n",
    "        # take n example from most confidently negative label to example\n",
    "        self.n = n\n",
    "        # number of iteration\n",
    "        self.k = k\n",
    "        # size of pool of unlabeled samples\n",
    "        self.u = u\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=1)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return np.around(1 / (1 + np.exp(-x)), decimals=4).item()\n",
    "\n",
    "    def init_L_U_U_prime(self, labels):\n",
    "        # index of the samples that are initially labeled\n",
    "        L = labels.index[labels[\"authorID\"] != -1].tolist()\n",
    "        # index of unlabeled samples\n",
    "        U = labels.index[labels[\"authorID\"] == -1].tolist()\n",
    "        print(\"Initial L size: \", len(L))\n",
    "        print(\"Initial U size: \", len(U))\n",
    "        # random drawing sample from U\n",
    "        random.shuffle(U)\n",
    "        U_prime = U[-min(len(U), self.u):]\n",
    "        # remove the samples in U_prime from U\n",
    "        U = U[:-len(U_prime)]\n",
    "        print(\"U size after drawing sample to U prime:\",len(U))\n",
    "        print(\"Initial U prime size: \", len(U_prime))\n",
    "        return L, U, U_prime\n",
    "    \n",
    "    def check_iter_label_mapping(self, iter_clf1, iter_clf2):\n",
    "        '''\n",
    "        In theory, it shouldn't occur that label not mapping since it trained on same dataset but different view\n",
    "        But add a check to make sure it won't occur and save the class mapping for later label unlabeled sample\n",
    "        '''\n",
    "        dv1_class_label = iter_clf1.classes_\n",
    "        dv2_class_label = iter_clf2.classes_\n",
    "        if all(dv1_class_label == dv2_class_label):\n",
    "            self.classes_ = dv1_class_label\n",
    "        else:\n",
    "            sys.exit(\"Two view classifier label not mapping\")\n",
    "\n",
    "    def get_confidence_score(self, clf_h1, clf_h2, dv1, dv2):\n",
    "        if hasattr(clf_h1, \"predict_proba\"):\n",
    "            dv1_proba = clf_h1.predict_proba(dv1)\n",
    "            dv2_proba = clf_h2.predict_proba(dv2)\n",
    "        elif hasattr(clf_h1, \"decision_function\"):    # use decision function\n",
    "            dv1_distance = np.array(clf_h1.decision_function(dv1))\n",
    "            dv2_distance = np.array(clf_h2.decision_function(dv2))\n",
    "            # if binary case, use sigmoid function to calculate probability\n",
    "            if iter_train_label.nunique()==2:\n",
    "                dv1_proba = []\n",
    "                dv2_proba = []\n",
    "                for distance in dv1_distance:\n",
    "                    dv1_proba.append([1-self.sigmoid(distance), self.sigmoid(distance)])\n",
    "                for distance in dv2_distance:\n",
    "                    dv2_proba.append([1-self.sigmoid(distance), self.sigmoid(distance)])\n",
    "            else:\n",
    "                dv1_proba = self.softmax(dv1_distance)\n",
    "                dv2_proba = self.softmax(dv2_distance)\n",
    "            dv1_proba = np.array(dv1_proba)\n",
    "            dv2_proba = np.array(dv2_proba)\n",
    "            #print(\"Distance to hyperplane (dv1): \",dv1_distance)\n",
    "            #print(\"Probability (dv1): \",dv1_proba)\n",
    "        else:\n",
    "            sys.exit(\"No confident score for process\")\n",
    "        \n",
    "        return dv1_proba, dv2_proba\n",
    "\n",
    "    def label_p_n_samples(self, proba, rank, proba_sample_idx_map):\n",
    "        U_prime_size = len(proba)\n",
    "        self_trained_sample_idx = []\n",
    "        self_trained_confident = []\n",
    "        for label, confident_rank in enumerate(rank):\n",
    "            # 0 positive sample\n",
    "            if label==0:\n",
    "                p = []\n",
    "                p_confident = []\n",
    "                index = 0\n",
    "                while(len(p) < self.p):\n",
    "                    max_conf_sample_index = confident_rank[index]\n",
    "                    # ---- if positive predict proba is more than 50% ------- #\n",
    "                    if (proba[max_conf_sample_index][label] > 0.5):\n",
    "                        #print('Sample idx: P: ', proba_sample_idx_map[max_conf_sample_index], \" : \", proba[max_conf_sample_index])\n",
    "                        p.append(proba_sample_idx_map[max_conf_sample_index])\n",
    "                        p_confident.append(proba[max_conf_sample_index][label])\n",
    "                    index +=1\n",
    "                    if (index>=U_prime_size):\n",
    "                        break\n",
    "                self_trained_sample_idx.append(p)\n",
    "                self_trained_confident.append(p_confident)\n",
    "            # 1 negative sample\n",
    "            elif label == 1:\n",
    "                n = []\n",
    "                n_confident = []\n",
    "                index = 0\n",
    "                while(len(n) < self.n):\n",
    "                    max_conf_sample_index = confident_rank[index]\n",
    "                    # ---- if negative predict proba is more than 50% ------- #\n",
    "                    if (proba[max_conf_sample_index][label] > 0.5):\n",
    "                        #print('Sample idx: N: ', proba_sample_idx_map[max_conf_sample_index], \" : \", proba[max_conf_sample_index])\n",
    "                        n.append(proba_sample_idx_map[max_conf_sample_index])\n",
    "                        n_confident.append(proba[max_conf_sample_index][label])\n",
    "                    index +=1\n",
    "                    if (index>=U_prime_size):\n",
    "                        break\n",
    "                self_trained_sample_idx.append(n)\n",
    "                self_trained_confident.append(n_confident)\n",
    "            else:\n",
    "                print(\"Class label error\")\n",
    "        return self_trained_sample_idx, self_trained_confident\n",
    "\n",
    "    def get_self_labeled_sample(self):\n",
    "        '''\n",
    "        return:\n",
    "            self-labeled new positive, self-labeled new negative (Index)\n",
    "        '''\n",
    "        return self.new_labeled_idx\n",
    "\n",
    "    def set_PCA(self, pca=None):\n",
    "        self.pca = pca\n",
    "\n",
    "    def plot_co_training_process(self, iterCount, data, iter_train_label, unlabeled_idx, h1_new=[], h2_new=[],\n",
    "                                 h1_new_prob=[], h2_new_prob=[], clf1=None, clf2=None, plotSavingPath=None, name=None):\n",
    "        if not os.path.exists(plotSavingPath):\n",
    "            os.makedirs(plotSavingPath)\n",
    "        pca_one = data[:,0]\n",
    "        pca_two = data[:,1]\n",
    "        # Layer 1. plot unlabel samples in u_prime\n",
    "        fig, ax = plt.subplots(figsize=(9,7))\n",
    "        unlabelled = ax.scatter(pca_one[unlabeled_idx], pca_two[unlabeled_idx], color='grey', label = \"unlabeled\", s = 50, alpha = 0.5)\n",
    "        # Layer 2. plot the labeled samples\n",
    "        for author in np.unique(iter_train_label[\"authorID\"]):\n",
    "            ix = iter_train_label.index[iter_train_label[\"authorID\"] == author].tolist()\n",
    "            # print(ix)\n",
    "            labelled = ax.scatter(pca_one[ix], pca_two[ix], cmap='viridis', label = author, s = 50, alpha = 0.9)\n",
    "        # layer 3. mark self labeled samples\n",
    "        if iterCount != 0:\n",
    "            #all_h1_new = list(itertools.chain(*h1_new))\n",
    "            #all_h2_new = list(itertools.chain(*h2_new))\n",
    "            last_iter_h1_new = h1_new[-1]\n",
    "            last_iter_h2_new = h2_new[-1]\n",
    "            temp_h1 = ax.scatter(pca_one[last_iter_h1_new], pca_two[last_iter_h1_new], edgecolor='black', linewidth=1, s=50)\n",
    "            temp_h1.set_facecolor(\"none\")\n",
    "            temp_h1.set_label(\"h1 self-labeled\")\n",
    "            temp_h2 = ax.scatter(pca_one[last_iter_h2_new], pca_two[last_iter_h2_new], edgecolor='red', linewidth=1, s=50)\n",
    "            temp_h2.set_facecolor(\"none\")\n",
    "            temp_h2.set_label(\"h2 self-labeled\")\n",
    "            # layer 4. label idx and probability\n",
    "            last_iter_h1_new_prob = h1_new_prob[-1]\n",
    "            last_iter_h2_new_prob = h2_new_prob[-1]\n",
    "            text = []\n",
    "            for i, idx in enumerate(last_iter_h1_new):\n",
    "                text.append(plt.text(pca_one[idx], pca_two[idx], '{0:d}({1:.2f})'.format(idx, last_iter_h1_new_prob[i]), color='black'))\n",
    "            for i, idx in enumerate(last_iter_h2_new):\n",
    "                text.append(plt.text(pca_one[idx], pca_two[idx], '{0:d}({1:.2f})'.format(idx, last_iter_h2_new_prob[i]), color='red'))\n",
    "            #print(text)\n",
    "            adjust_text(text, x=pca_one, y=pca_two, force_points=1, force_text=1, arrowprops=dict(arrowstyle='->', color='red', lw=1))\n",
    "        legend = ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.25), ncol=3,prop={'size': 13})\n",
    "        plt.title('Co-training iteration: '+ str(iterCount), fontsize=14)\n",
    "        plt.xlabel(\"First principal component\",fontsize=14)\n",
    "        plt.ylabel(\"Second principal component\",fontsize=14)\n",
    "        plt.savefig(fname=plotSavingPath+name+\"_PCA_i-\"+str(iterCount)+\".png\", dpi=100, bbox_extra_artists=(legend,), bbox_inches='tight')\n",
    "        #plt.show()\n",
    "        plt.close(\"all\")\n",
    "        \n",
    "\n",
    "    def fit(self, train_data, validation_data, dataset_name=None, plot_save_path=None):\n",
    "        # sync input datatype\n",
    "        for idx, item in enumerate(train_data):\n",
    "            if not isinstance(item, pd.DataFrame):\n",
    "                item = pd.DataFrame(item)\n",
    "            item.reset_index(drop=True,inplace=True)\n",
    "            train_data[idx] = item\n",
    "        dv1 = train_data[0]\n",
    "        dv2 = train_data[1]\n",
    "        labels = train_data[-1]\n",
    "        dv1_validation = validation_data[0]\n",
    "        dv2_validation = validation_data[1] \n",
    "        label_validation = validation_data[-1]\n",
    "        # using all unlabeled sample instead of pool of unlabeled sample\n",
    "        self.u = len(labels)\n",
    "        L, U, U_prime = self.init_L_U_U_prime(labels)\n",
    "        #print(\"All data index: \",dv1.index.values)\n",
    "        #print(\"L: \", L)\n",
    "        #print(\"U: \", U)\n",
    "        #print(\"U_prime: \", U_prime)\n",
    "        print(\"P value: \", self.p, \" N value: \", self.n)\n",
    "        \n",
    "        # index of self labeled samples\n",
    "        self.new_labeled_idx = defaultdict(list)\n",
    "        self.h1_new_idx = defaultdict(list)\n",
    "        self.h2_new_idx = defaultdict(list)\n",
    "        # when fit co-train, we collect f1 on validation samples wrt each iteration\n",
    "        self.f1_on_validation_dv1 = []\n",
    "        self.f1_on_validation_dv2 = []\n",
    "        \n",
    "        self.iterCounter = 0\n",
    "        # --------- plot PCA reduced plot for initial stage of train -------------- #\n",
    "        pca = self.pca\n",
    "        '''Notice pca will change input datatype dataframe to datatype list, keep it's order,\n",
    "           or re-assign dv1.index.values to the output'''\n",
    "        pca_dv1 = pca.fit_transform(X=dv1)\n",
    "        pca_dv2 = pca.fit_transform(X=dv2)\n",
    "        #loop until we have assigned labels to every sample in U and U_prime or we hit our iteration break condition\n",
    "        while self.iterCounter < self.k and U_prime:\n",
    "            '''\n",
    "            Extract labeled training sample from training sample and train classifier, then make prediction\n",
    "            Evaluate performance of current iteration classifier\n",
    "            '''\n",
    "            # print(\"Iteration:\",self.iterCounter, \" L: \",L)\n",
    "            # print(\"Iteration:\",self.iterCounter, \" U_prime: \",U_prime)\n",
    "            # ------------- get labeled samples for train ----------- # \n",
    "            iter_train_d1 = dv1.iloc[L]\n",
    "            iter_train_d2 = dv2.iloc[L]\n",
    "            iter_train_label = labels.iloc[L]\n",
    "            # ----------- get U_prime unlabeled samples  ------------ #\n",
    "            iter_unlabeled_d1 = dv1.iloc[U_prime]\n",
    "            iter_unlabeled_d2 = dv2.iloc[U_prime]\n",
    "            # ------------ train different view with classifier ----------- #\n",
    "            iter_clf1 = copy.deepcopy(self.clf1) \n",
    "            iter_clf2 = copy.deepcopy(self.clf2)\n",
    "            iter_clf1.fit(iter_train_d1, iter_train_label.values.ravel())\n",
    "            iter_clf2.fit(iter_train_d2, iter_train_label.values.ravel())\n",
    "            self.check_iter_label_mapping(iter_clf1, iter_clf2)\n",
    "            # --------- test error on validation data --------------------- #\n",
    "            # make prediction on validation data\n",
    "            y1 = iter_clf1.predict(dv1_validation)\n",
    "            y2 = iter_clf2.predict(dv2_validation)\n",
    "            # f1 score on each iteration\n",
    "            f1_dv1 = f1_dv2 = 0\n",
    "            f1_dv1 = f1_score(label_validation, y1, average='macro')\n",
    "            f1_dv2 = f1_score(label_validation, y2, average='macro')\n",
    "            # collect f1 for current iteration\n",
    "            self.f1_on_validation_dv1.append(f1_dv1)\n",
    "            self.f1_on_validation_dv2.append(f1_dv2)\n",
    "            \n",
    "            new_train_label = labels.iloc[L]\n",
    "            if len(self.h1_new_idx[\"index\"])==0:\n",
    "                print(\"Iteration 0, strat self label.\")\n",
    "            else:\n",
    "                print(\"Iteration\",self.iterCounter,\" h1 new: \",self.h1_new_idx[\"index\"][-1],\" probs: \", self.h1_new_idx[\"confident\"][-1])\n",
    "                print(\"Iteration\",self.iterCounter,\" h2 new: \", self.h2_new_idx[\"index\"][-1], \" probs: \", self.h2_new_idx[\"confident\"][-1])\n",
    "            '''  # ----------- plot the co-training process -------------- # '''\n",
    "            if (plot_save_path != None) and (dataset_name != None):\n",
    "                for pca_view,v_name,clf in [(pca_dv1,\"dv1\",copy.deepcopy(self.clf1)),(pca_dv2,\"dv2\",copy.deepcopy(self.clf2))]:\n",
    "                    self.plot_co_training_process(self.iterCounter, pca_view, new_train_label, U_prime,\n",
    "                                                  self.h1_new_idx[\"index\"], self.h2_new_idx[\"index\"],\n",
    "                                                  self.h1_new_idx[\"confident\"], self.h2_new_idx[\"confident\"],\n",
    "                                                  plotSavingPath = plot_save_path, name = dataset_name+\"_\"+v_name)\n",
    "            ''' \n",
    "            Notice here dv1_proba and dv2_proba's index is index for u' (Unlabeled data only)\n",
    "            We use index of u' to find index (position) of data in U where U and L is all data index\n",
    "            '''\n",
    "            # rank class probabilities for unlabeled sample for it's confidence measure\n",
    "            dv1_proba, dv2_proba = self.get_confidence_score(clf_h1=iter_clf1, clf_h2=iter_clf2, \n",
    "                                                             dv1=iter_unlabeled_d1, dv2=iter_unlabeled_d2)\n",
    "            proba_sample_idx_map = iter_unlabeled_d1.index\n",
    "            dv1_proba_rank = []\n",
    "            dv2_proba_rank = []\n",
    "            # proba1_rank[i] is label i's confidence measure\n",
    "            for class_proba in dv1_proba.T:\n",
    "                dv1_proba_rank.append((-class_proba).argsort())\n",
    "            for class_proba in dv2_proba.T:\n",
    "                dv2_proba_rank.append((-class_proba).argsort())\n",
    "            # print(dv1_proba)\n",
    "            # print(dv1_proba_rank)\n",
    "            # print(dv2_proba)\n",
    "            # print(dv2_proba_rank)\n",
    "            # h1 classifier self label data\n",
    "            h1_new_sample, h1_new_probs = self.label_p_n_samples(dv1_proba, dv1_proba_rank, proba_sample_idx_map)\n",
    "            # h2 classifier\n",
    "            h2_new_sample, h2_new_probs = self.label_p_n_samples(dv2_proba, dv2_proba_rank, proba_sample_idx_map)\n",
    "            \n",
    "            '''Case if h1's new class 1 is h2's new class 2\n",
    "            Example: Iteration 45  h1 new:  [[95], [150]]  probs:  [[0.89], [0.82]]\n",
    "                     Iteration 45  h2 new:  [[147], [95]]  probs:  [[0.89], [0.92]]\n",
    "            '''\n",
    "            for i, h1_class_new in enumerate(h1_new_sample):\n",
    "                temp_pop_list = []\n",
    "                for j, h1_item in enumerate(h1_class_new):\n",
    "                    for k, h2_class_new in enumerate(h2_new_sample):\n",
    "                        for l, h2_item in enumerate(h2_class_new):\n",
    "                            # conflict\n",
    "                            if h1_item == h2_item:\n",
    "                                if h1_new_probs[i][j]> h2_new_probs[k][l]:\n",
    "                                    h2_new_sample[k].pop(l)\n",
    "                                    h2_new_probs[k].pop(l)\n",
    "                                else:\n",
    "                                    temp_pop_list.append(j)\n",
    "                temp_pop_list = list(set(temp_pop_list))\n",
    "                temp_pop_list.sort(reverse=True)\n",
    "                for item_idx in temp_pop_list:\n",
    "                    h1_new_sample[i].pop(item_idx)\n",
    "                    h1_new_probs[i].pop(item_idx)\n",
    "            # collect statistic for plot only (before remove self-labeled sample from u')\n",
    "            iter_h1_for_plot = list(itertools.chain(*h1_new_sample))\n",
    "            iter_h2_for_plot = list(itertools.chain(*h2_new_sample))\n",
    "            iter_h1_prob = list(itertools.chain(*h1_new_probs))\n",
    "            iter_h2_prob = list(itertools.chain(*h2_new_probs))\n",
    "            self.h1_new_idx[\"index\"].append(iter_h1_for_plot)\n",
    "            self.h1_new_idx[\"confident\"].append(iter_h1_prob)\n",
    "            self.h2_new_idx[\"index\"].append(iter_h2_for_plot)\n",
    "            self.h2_new_idx[\"confident\"].append(iter_h2_prob)\n",
    "            '''\n",
    "            Add most confidence samples as new training samples, auto label the samples and remove it from U_prime\n",
    "            Special Case: if two view classifier give same most confident sample and p is 1, only 1 datapoint self-labeled\n",
    "            '''\n",
    "            roundNew = list(zip(h1_new_sample, h2_new_sample))\n",
    "            roundNew_flatten_unique = []\n",
    "            for label, round_new in enumerate(roundNew):\n",
    "                round_new = set([item for sublist in round_new for item in sublist])\n",
    "                round_new = [idx for idx in round_new]\n",
    "                self.new_labeled_idx[self.classes_[label]].append(round_new)\n",
    "                roundNew_flatten_unique.extend(round_new)\n",
    "                # add label to those new samples\n",
    "                #print(labels[\"authorID\"][round_new])\n",
    "                labels[\"authorID\"][round_new] = self.classes_[label]\n",
    "                #print(labels[\"authorID\"][round_new])\n",
    "                #print(self.classes_[label],\" (u' idx): \",round_new)\n",
    "            #print(roundNew_flatten_unique)\n",
    "            # extend the labeled sample\n",
    "            L.extend(roundNew_flatten_unique)\n",
    "            # remove the labeled sample from U_prime\n",
    "            U_prime = [x for x in U_prime if x not in roundNew_flatten_unique]\n",
    "            #print(U_prime)\n",
    "            # randomly choice 2p+2n examples from u to replenish u_prime\n",
    "            replenishItem = U[-(2*self.p+2*self.n):]\n",
    "            U_prime.extend(replenishItem)\n",
    "            U = U[:-len(replenishItem)]\n",
    "            self.iterCounter +=1\n",
    "            \n",
    "            ''' # ----------- plot the last iteration of co-training process -------------- # '''\n",
    "            if self.iterCounter == self.k:\n",
    "                new_train_label = labels.iloc[L]\n",
    "                print(\"Iteration\",self.iterCounter,\" h1 new: \",self.h1_new_idx[\"index\"][-1],\" probs: \", self.h1_new_idx[\"confident\"][-1])\n",
    "                print(\"Iteration\",self.iterCounter,\" h2 new: \", self.h2_new_idx[\"index\"][-1], \" probs: \", self.h2_new_idx[\"confident\"][-1])\n",
    "                # ----- save pca reduced plot ------ #\n",
    "                if plot_save_path != None:\n",
    "                    for pca_view,v_name,clf in [(pca_dv1,\"dv1\",copy.deepcopy(self.clf1)),(pca_dv2,\"dv2\",copy.deepcopy(self.clf2))]:\n",
    "                        print(pca_view)\n",
    "                        print(len(pca_view))\n",
    "                        print(len(pca_view[0]))\n",
    "                        self.plot_co_training_process(self.iterCounter, pca_view, new_train_label, U_prime,\n",
    "                                                      self.h1_new_idx[\"index\"], self.h2_new_idx[\"index\"],\n",
    "                                                      self.h1_new_idx[\"confident\"], self.h2_new_idx[\"confident\"],\n",
    "                                                      plotSavingPath = plot_save_path, name = dataset_name+\"_\"+v_name)\n",
    "            \n",
    "        print(\"Total Labeled number: \", len(L), \" Still unlabeled number: \", len(U_prime))\n",
    "        #print(self.f1_on_validation_dv1)\n",
    "        #print(self.f1_on_validation_dv2)\n",
    "        # final train\n",
    "        newtrain_d1 = dv1.iloc[L]\n",
    "        newtrain_d2 = dv2.iloc[L]\n",
    "        self.clf1.fit(newtrain_d1, labels.iloc[L].values.ravel())\n",
    "        self.clf2.fit(newtrain_d2, labels.iloc[L].values.ravel())\n",
    "        '''\n",
    "        Evalutation plot for co-training process, save f1 score vs number of iteration plot\n",
    "        '''\n",
    "        if (dataset_name != None) and (plot_save_path != None):\n",
    "            default_text_based = [self.f1_on_validation_dv1[0]] * self.iterCounter\n",
    "            default_citation_based = [self.f1_on_validation_dv2[0]] * self.iterCounter\n",
    "            default_step = np.arange(0,self.iterCounter)\n",
    "            co_train_text_based = self.f1_on_validation_dv1[1:]\n",
    "            co_train_citation_based = self.f1_on_validation_dv2[1:]\n",
    "            co_training_step = np.arange(1,self.iterCounter)\n",
    "\n",
    "            fig = plt.figure()\n",
    "            ax = plt.axes()\n",
    "            plt.plot(default_step, default_text_based, linestyle='dashed', label=\"Text based default\")\n",
    "            plt.plot(default_step, default_citation_based, linestyle='dashdot', label=\"Citation based default\")\n",
    "            plt.plot(co_training_step, co_train_text_based, linestyle='solid', marker = \"*\", label=\"Text based\")\n",
    "            plt.plot(co_training_step, co_train_citation_based, linestyle='dotted', marker = \"+\", label=\"Citation based\")\n",
    "            ax.autoscale_view()\n",
    "            legend = ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.2), ncol=2)\n",
    "            plt.xlabel('Co-Training Iterations')\n",
    "            plt.ylabel('F1 score')\n",
    "            plt.savefig((plot_save_path+dataset_name+\"_co_train_iteration_f1.png\"), dpi=150, bbox_extra_artists=(legend,), bbox_inches='tight')\n",
    "            # plt.show()\n",
    "            plt.close(\"all\")\n",
    "        return \"Training Done\"\n",
    "\n",
    "    def co_train_process_f1(self):\n",
    "        return self.f1_on_validation_dv1, self.f1_on_validation_dv2\n",
    "\n",
    "    def get_iter_count(self):\n",
    "        return self.iterCounter\n",
    "\n",
    "    def predict(self, data):\n",
    "        dv1=data[0]\n",
    "        dv2=data[1]\n",
    "        dv1_predict = self.clf1.predict(dv1)\n",
    "        dv2_predict = self.clf2.predict(dv2)\n",
    "        #fill pred with -1 so we can identify the samples in which error occur\n",
    "        y_pred = [\"-1\"] * dv1.shape[0]\n",
    "        for i, (dv1_y, dv2_y) in enumerate(zip(dv1_predict, dv2_predict)):\n",
    "            # if both agree on label\n",
    "            if dv1_y == dv2_y:\n",
    "                y_pred[i] = dv1_y\n",
    "            # If disagree on label and support proba method: We times probability together, choice class have higher probabilities\n",
    "            elif hasattr(self.clf1, \"predict_proba\") and hasattr(self.clf2, \"predict_proba\"):\n",
    "                h1_probas = self.clf1.predict_proba([dv1.iloc[i]])[0]\n",
    "                h2_probas = self.clf2.predict_proba([dv2.iloc[i]])[0]\n",
    "                final_y_probas = [proba_y1 * proba_y2 for (proba_y1, proba_y2) in zip(h1_probas, h2_probas)]\n",
    "                #print(\"h1 and h2 disagree on\",i, \" h1 Proba : \",h1_probas, \" h2 Proba: \", h2_probas)\n",
    "                #print(\"product probas:\",final_y_probas)\n",
    "                max_prob_idx = final_y_probas.index(max(final_y_probas))\n",
    "                y_pred[i] = self.classes_[max_prob_idx]\n",
    "                #print(\"result idx: \", max_prob_idx, \" result: \",y_pred[i])\n",
    "            # If disagree on label and not support proba method: We use decision_function to first calculate probability,\n",
    "            # then we times calculate probability together, choice class have higher probabilities\n",
    "            elif hasattr(self.clf1, \"decision_function\") and hasattr(self.clf2, \"decision_function\"):\n",
    "                dv1_distance = self.clf1.decision_function([dv1.iloc[i]])\n",
    "                dv2_distance = self.clf2.decision_function([dv2.iloc[i]])\n",
    "                if len(self.clf1.classes_)==2:\n",
    "                    h1_probas = np.array([1-self.sigmoid(dv1_distance), self.sigmoid(dv1_distance)])\n",
    "                    h2_probas = np.array([1-self.sigmoid(dv2_distance), self.sigmoid(dv2_distance)])\n",
    "                else:\n",
    "                    h1_probas = self.softmax(dv1_distance)\n",
    "                    h2_probas = self.softmax(dv2_distance)\n",
    "                final_y_probas = [proba_y1 * proba_y2 for (proba_y1, proba_y2) in zip(h1_probas, h2_probas)]\n",
    "                #print(\"h1 and h2 disagree on\",i, \" h1 Proba : \",h1_probas, \" h2 Proba: \", h2_probas)\n",
    "                #print(\"product probas:\",final_y_probas)\n",
    "                max_prob_idx = final_y_probas.index(max(final_y_probas))\n",
    "                y_pred[i] = self.classes_[max_prob_idx]\n",
    "                #print(\"result idx: \", max_prob_idx, \" result: \",y_pred[i])\n",
    "            # disagree and not support any confidence measure\n",
    "            else:\n",
    "                sys.exit(\"Error occur\")\n",
    "\n",
    "        # convert final result to np array\n",
    "        y_pred_np_array = np.asarray(y_pred)\n",
    "        return y_pred_np_array\n",
    "\n",
    "    def predict_proba(self, dv1, dv2):\n",
    "        # the predicted probabilities is simply a product (*) of probabilities given from each classifier trained\n",
    "        h1_probas = self.clf1.predict_proba(dv1)\n",
    "        h2_probas = self.clf2.predict_proba(dv2)\n",
    "        \n",
    "        proba = (h1_probas*h2_probas)\n",
    "        return proba\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved co-training\n",
    "\n",
    "Possible improvement:\n",
    "\n",
    "Part 1: Parameter input\n",
    "1. Using two different algorithm for view one and view two.\n",
    "2. Instead of using parameter k to control number of iteration, using a stop criterion where if unlabeled sample can't get more than 95% of confidence on their confidence score, then stop iteration and finish co-training.\n",
    "3. Parameter p, n are not needed, use input data class ratio replace it (unlabeled distribution may different from labeled, this is unlikely works in reality).\n",
    "4. u prime not needed, use all unlabeled data.\n",
    "\n",
    "Part 2: Change confident measure method\n",
    "1. Changes method used when determine which class unlabeled sample belones to, instead of using confident score, we could use classifiers saved during each iteration of training to perform an majority voting. (Need to careful about number of iteration in co-training, since adding bad unlabel data as label data is easy to occur) (Source: 2004_Co-training and Self-training for Word Sense Disambiguation)\n",
    "\n",
    "Part 3: Change details in confident score method\n",
    "1. Add an third classifier and only train with original labeled samples(no-self-labeled sample), then use third classifier to evaluate self-labeled samples and get it's confidence score. Combine three classifier's confidence score together to get final decision.\n",
    "2. COTRADE method: construct undirected neighborhood graph using all samples, after finding most confident sample with it's corresponding class. Then, calculate distance between all labeled samples in this class and most confident sample in this class. This distance from graph + probability from supervised algorithm will be used together as confidence score.\n",
    "\n",
    "Part 4: Add checking to reduce the chance of adding mislabeled data\n",
    "1. Add an check on validation after new label sample is added, if not improving h1/h2, remove new labeled sample.\n",
    "\n",
    "Part 5: Change algorithm and make it work for muti-class\n",
    "1. Using same concept of muti-class SVM, train many OVR binary classifier. (To expensive)\n",
    "2. Allow algorithm directly take muti-class case and using same concept as binary co-training.\n",
    "\n",
    "\n",
    "My idea:\n",
    "1. Use one classifier (check)\n",
    "2. Have parameter k as default, but can be stoped early (check)\n",
    "3. Parameter p, n are replace with sl_base which set as input data class ratio*sl_base. (Allow muti-class case and will maintaining the class distribution in L. However, unlabeled distribution may different from labeled, this is unlikely works in reality) (check)\n",
    "4. u prime not needed, use all unlabeled data. (check)\n",
    "5. Adopt filter view disagreed samples, instead directly remove disagreed sample, we will give a score to evaluate disagreement between views and add this score as part of confident score. (2008-Multi-view learning in the presence of view disagreement)\n",
    "6. We will accumulate the probability of first iteration and current iteration; add them together as part of confident score. (check)\n",
    "7. Adopt COTRADE method, their assumption where that a correctly labeled example should be very close to samples in L with corresponding label same to be very useful. Thus a k-mean clustering is train with labeled, then transform unlabel data into same instance space as labeled samples, then calculate distance to each cluster's centroid, then use distance perfrom an softmax to get reverse of probability of sample belone to class. After that 1- reversed probability get probability and add it as part of co-train confident score. (check)\n",
    "8. We can think confident score as quality of unlabeled data, after each co-training iteration, it should always increase in best case. However, when we see a decrease in confident score compare to pervious iteration, it means the data quality level have decresed, therefore we need to check whether heighest probability of samples is greater than 0.8 or not. \n",
    "\n",
    "\n",
    "9. Notice that our main evaluator for whether a new sample should be added to L is current iteration probability, not the confident score, confident score is only used for sample quality check. (check)\n",
    "10. When no sample been added for both view, we stop co-training iteration since data left may provide more noise than information. (check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-07T23:18:14.348511Z",
     "start_time": "2020-08-07T23:17:57.273494Z"
    },
    "code_folding": [
     16,
     32,
     36,
     39,
     55,
     70,
     90,
     184,
     195,
     198,
     536,
     539,
     542,
     588
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import warnings\n",
    "import copy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import precision_score, recall_score,f1_score,accuracy_score\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "\n",
    "# create co training classifier\n",
    "class Improved_co_training_clf(object):\n",
    "    \n",
    "    def __init__(self, clf=[], view_num=2, sl_class_max=1, k=30, **args):\n",
    "        if len(clf) == view_num:\n",
    "            self.clf = clf\n",
    "        elif len(clf) == 1:\n",
    "            self.clf = [copy.deepcopy(clf[0]) for i in range(view_num)]\n",
    "        else:\n",
    "            sys.exit(\"Classifier doesn't match with view number.\")\n",
    "        # self-label step for each iteration\n",
    "        if sl_class_max<1:\n",
    "            sl_class_max=1\n",
    "        self.sl_class_max=sl_class_max\n",
    "        self.view_num = view_num\n",
    "        self.iter_new_size =[]\n",
    "        # number of iteration\n",
    "        self.k = k\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return np.around(1 / (1 + np.exp(-x)), decimals=4).item()\n",
    "\n",
    "    def init_L_U_U_prime(self, labels):\n",
    "        # index of the samples that are initially labeled\n",
    "        L = labels.index[labels[\"authorID\"] != -1].tolist()\n",
    "        # index of unlabeled samples\n",
    "        U = labels.index[labels[\"authorID\"] == -1].tolist()\n",
    "        print(\"Initial L size: \", len(L))\n",
    "        print(\"Initial U size: \", len(U))\n",
    "        # random drawing sample from U\n",
    "        random.shuffle(U)\n",
    "        U_prime = U[-min(len(U), self.u):]\n",
    "        # remove the samples in U_prime from U\n",
    "        U = U[:-len(U_prime)]\n",
    "        print(\"U size after drawing sample to U prime:\",len(U))\n",
    "        print(\"Initial U prime size: \", len(U_prime))\n",
    "        return L, U, U_prime\n",
    "    \n",
    "    def check_iter_label_mapping(self, iter_clf):\n",
    "        '''\n",
    "        In theory, it shouldn't occur that label not mapping since it trained on same dataset but different view\n",
    "        But add a check to make sure it won't occur and save the class mapping for later label unlabeled sample\n",
    "        '''\n",
    "        class_label = defaultdict(list)\n",
    "        for idx,clf in enumerate(iter_clf):\n",
    "            if idx ==0:\n",
    "                self.classes_ = clf.classes_\n",
    "            \n",
    "            if not all(self.classes_ == clf.classes_):\n",
    "                sys.exit(\"Two view classifier label not mapping\")\n",
    "        #print(self.classes_)\n",
    "        return \"checked\"\n",
    "\n",
    "    def get_confidence_score(self, clf, data):\n",
    "        if hasattr(clf, \"predict_proba\"):\n",
    "            proba = clf.predict_proba(data)\n",
    "        elif hasattr(clf, \"decision_function\"):    # use decision function\n",
    "            all_distance = np.array(clf.decision_function(data))\n",
    "            # if binary case, use sigmoid function to calculate probability\n",
    "            if iter_train_label.nunique()==2:\n",
    "                proba = []\n",
    "                for distance in all_distance:\n",
    "                    proba.append([1-self.sigmoid(distance), self.sigmoid(distance)])\n",
    "            else:\n",
    "                proba = self.softmax(dv1_distance)\n",
    "            proba = np.array(proba)\n",
    "            #print(\"Distance to hyperplane: \",distance)\n",
    "            #print(\"Probability: \",proba)\n",
    "        else:\n",
    "            sys.exit(\"No confident score for process\")\n",
    "        \n",
    "        return proba\n",
    "\n",
    "    def get_cluster_distance_as_proba(self, label_train, iter_train_label, unlabeled):\n",
    "        kmeans = KMeans(n_clusters=len(self.classes_)).fit(label_train)\n",
    "        # map correct idx to label\n",
    "        kmeans_predict = kmeans.labels_\n",
    "        label_mapping = list(zip(iter_train_label, kmeans_predict))\n",
    "        \n",
    "        lmc = Counter(label_mapping)\n",
    "        final_label_mapping = []\n",
    "        for label in np.unique(iter_train_label):\n",
    "            for (item,index),count in lmc.most_common():\n",
    "                if item == label:\n",
    "                    #print(\"label:\",label, \" idx:\",index, \" count:\",count)\n",
    "                    final_label_mapping.append((item,index))\n",
    "                    break\n",
    "        #print(final_label_mapping)\n",
    "        \n",
    "        # convert distance to centroid to probability of belone to class\n",
    "        dist_to_centroid = kmeans.transform(unlabeled)\n",
    "        cluster_proba = []\n",
    "        for distance in dist_to_centroid:\n",
    "            reverse_proba = self.softmax(distance)\n",
    "            proba = [1-x for x in reverse_proba]\n",
    "            cluster_proba.append(proba)\n",
    "        cluster_proba = np.array(cluster_proba)\n",
    "        \n",
    "        # map the proba same order as train in supervised learning\n",
    "        new_permutation = [idx for label, idx in final_label_mapping]\n",
    "        #print(new_permutation)\n",
    "        idx = np.empty_like(new_permutation)\n",
    "        idx[new_permutation] = np.arange(len(new_permutation))\n",
    "        cluster_proba[:] = cluster_proba[:, idx]\n",
    "        \n",
    "        return cluster_proba\n",
    "\n",
    "    def label_samples(self, score, rank, score_sample_idx_map,curr_iter_proba):\n",
    "        U_prime_size = len(score)\n",
    "        self_trained_sample_idx = []\n",
    "        self_trained_confident = []\n",
    "        self_trained_iter_proba = []\n",
    "        self.iter_new_size = defaultdict(list)\n",
    "        '''Automatic estimate the class ratio and try to get correct data class ratio'''\n",
    "        # if one class have lots of high confident samples while other don't, change class ratio\n",
    "        class_ratio = []\n",
    "        for class_idx in range(len(self.classes_)):\n",
    "            high_confident = [proba for proba in curr_iter_proba if proba[class_idx]>0.95]\n",
    "            high_confident_count = max(len(high_confident), 1)\n",
    "            class_ratio.append(high_confident_count)\n",
    "        for class_idx in range(len(self.classes_)):\n",
    "            class_new_sample_size = class_ratio[class_idx]\n",
    "            if class_new_sample_size>self.sl_class_max:\n",
    "                class_new_sample_size=self.sl_class_max\n",
    "            self.iter_new_size[self.classes_[class_idx]].append(class_new_sample_size)\n",
    "        #print(self.iter_new_size)\n",
    "        \n",
    "        '''Search for samples that can be labeled'''\n",
    "        for label, confident_rank in enumerate(rank):\n",
    "            # ---------- find heightest current iteration probability for all class ----- #\n",
    "            class_curr_iter_proba = [row[label] for row in curr_iter_proba]\n",
    "            class_curr_iter_proba.sort()\n",
    "            class_heightest_proba = max(class_curr_iter_proba)\n",
    "            # ------------------- find last iteration max confidence score -------------- #\n",
    "            pervious_iter_confidence_score = self.new_labeled_idx[self.classes_[label]+\"_score\"]\n",
    "            if len(pervious_iter_confidence_score)==0:\n",
    "                last_iter_max_confidence_score=0\n",
    "            else:\n",
    "                last_iter_max_confidence_score = max(self.new_labeled_idx[self.classes_[label]+\"_score\"][-1], default=0)\n",
    "            \n",
    "            new_label_sample_size = [size for size in self.iter_new_size[self.classes_[label]]][0]\n",
    "            new_sample_idx = []\n",
    "            new_sample_confident_score = []\n",
    "            new_sample_proba = []\n",
    "            index = 0\n",
    "            while(len(new_sample_idx) < new_label_sample_size):\n",
    "                max_conf_sample_index = confident_rank[index]\n",
    "                #print('Select idx: ', max_conf_sample_index,\":\",score_sample_idx_map[max_conf_sample_index],\" have score: \",score[max_conf_sample_index], \" probability \",curr_iter_proba[max_conf_sample_index])\n",
    "                if (score_sample_idx_map[max_conf_sample_index]<last_iter_max_confidence_score) and (class_heightest_proba<0.8):\n",
    "                    print('Most confident idx: ', max_conf_sample_index,\":\",score_sample_idx_map[max_conf_sample_index],\" have score: \",score[max_conf_sample_index],\" smaller than last iteration max score: \",last_iter_max_confidence_score,\" probability \",curr_iter_proba[max_conf_sample_index], \" no bigger than 0.8, sample not used\")\n",
    "                    break\n",
    "                elif curr_iter_proba[max_conf_sample_index][label] <0.7:\n",
    "                    #print('Most confident idx: ', max_conf_sample_index,\":\",score_sample_idx_map[max_conf_sample_index],\" have score: \",score[max_conf_sample_index], \"have probability \",curr_iter_proba[max_conf_sample_index], \" not in top 3, thus sample not used\")\n",
    "                    index +=1\n",
    "                else:\n",
    "                    #print('Class:', self.classes_[label],' idx: ',max_conf_sample_index,\":\",score_sample_idx_map[max_conf_sample_index], \" score: \", score[max_conf_sample_index], \" iter proba: \", curr_iter_proba[max_conf_sample_index])\n",
    "                    new_sample_idx.append(score_sample_idx_map[max_conf_sample_index])\n",
    "                    new_sample_confident_score.append(score[max_conf_sample_index][label])\n",
    "                    new_sample_proba.append(curr_iter_proba[max_conf_sample_index][label])\n",
    "                    index +=1\n",
    "                if (index>=U_prime_size) :\n",
    "                    break\n",
    "            self_trained_sample_idx.append(new_sample_idx)\n",
    "            self_trained_confident.append(new_sample_confident_score)\n",
    "            self_trained_iter_proba.append(new_sample_proba)\n",
    "        return self_trained_sample_idx, self_trained_confident, self_trained_iter_proba\n",
    "\n",
    "    def get_self_labeled_sample(self):\n",
    "        '''\n",
    "        return:\n",
    "            self-labeled new positive, self-labeled new negative (Index)\n",
    "        '''\n",
    "        temp = defaultdict(list)\n",
    "        for label in self.classes_:\n",
    "            temp[label]=self.new_labeled_idx[label]\n",
    "        \n",
    "        return temp\n",
    "\n",
    "    def set_PCA(self, pca=None):\n",
    "        self.pca = pca\n",
    "\n",
    "    def plot_co_training_process(self, iterCount, data, iter_train_label, unlabeled_idx, h1_new = [], h2_new = [],\n",
    "                                 h1_new_prob = [], h2_new_prob = [], plotSavingPath=None, name=None):\n",
    "        if not os.path.exists(plotSavingPath):\n",
    "            os.makedirs(plotSavingPath)\n",
    "        pca_one = data[:,0]\n",
    "        pca_two = data[:,1]\n",
    "        # Layer 1. plot unlabel samples in u_prime\n",
    "        fig, ax = plt.subplots(figsize=(9,7))\n",
    "        ax.scatter(x=pca_one[unlabeled_idx], y=pca_two[unlabeled_idx], color='grey', label=\"unlabeled\", s=50, alpha=0.5)\n",
    "        # Layer 2. plot the labeled samples\n",
    "        for author in np.unique(iter_train_label[\"authorID\"]):\n",
    "            ix = iter_train_label.index[iter_train_label[\"authorID\"] == author].tolist()\n",
    "            ax.scatter(x=pca_one[ix], y=pca_two[ix], cmap='viridis', label=author, s=50, alpha=0.9)\n",
    "        # layer 3. mark self labeled samples\n",
    "        if iterCount != 0:\n",
    "            #all_h1_new = list(itertools.chain(*h1_new))\n",
    "            #all_h2_new = list(itertools.chain(*h2_new))\n",
    "            last_iter_h1_new = h1_new[-1]\n",
    "            last_iter_h2_new = h2_new[-1]\n",
    "            temp_h1 = ax.scatter(pca_one[last_iter_h1_new], pca_two[last_iter_h1_new], edgecolor='black', linewidth=1, s=50)\n",
    "            temp_h1.set_facecolor(\"none\")\n",
    "            temp_h1.set_label(\"h1 self-labeled\")\n",
    "            temp_h2 = ax.scatter(pca_one[last_iter_h2_new], pca_two[last_iter_h2_new], edgecolor='red', linewidth=1, s=50)\n",
    "            temp_h2.set_facecolor(\"none\")\n",
    "            temp_h2.set_label(\"h2 self-labeled\")\n",
    "            # layer 4. mark new samples confidence and which view produce it\n",
    "            last_iter_h1_new_prob = h1_new_prob[-1]\n",
    "            last_iter_h2_new_prob = h2_new_prob[-1]\n",
    "            text = []\n",
    "            for i, idx in enumerate(last_iter_h1_new):\n",
    "                text.append(plt.text(pca_one[idx], pca_two[idx], \"{:.2f}\".format(last_iter_h1_new_prob[i]), color='black'))\n",
    "            for i, idx in enumerate(last_iter_h2_new):\n",
    "                text.append(plt.text(pca_one[idx], pca_two[idx], \"{:.2f}\".format(last_iter_h2_new_prob[i]), color='red'))\n",
    "            adjust_text(text, x=pca_one, y=pca_two, force_points=0.3, force_text=0.3, expand_points=(2, 2), \n",
    "                        expand_text=(2, 2), arrowprops=dict(arrowstyle='Simple', color='red'))\n",
    "        legend = ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.2), ncol=3,prop={'size': 13})\n",
    "        plt.title('Co-training iteration: '+ str(iterCount), fontsize=14)\n",
    "        plt.xlabel(\"First principal component\",fontsize=14)\n",
    "        plt.ylabel(\"Second principal component\",fontsize=14)\n",
    "        plt.savefig(fname=plotSavingPath+name+\"_PCA_i-\"+str(iterCount)+\".png\", dpi=100, bbox_extra_artists=(legend,), bbox_inches='tight')\n",
    "        # plt.show()\n",
    "        plt.close(\"all\")\n",
    "        \n",
    "\n",
    "    def fit(self, train_data, validation_data, dataset_name=None, plot_save_path=None):\n",
    "        # sync input datatype\n",
    "        for idx, item in enumerate(train_data):\n",
    "            if not isinstance(item, pd.DataFrame):\n",
    "                item = pd.DataFrame(item)\n",
    "            item.reset_index(drop=True,inplace=True)\n",
    "            train_data[idx] = item\n",
    "        \n",
    "        labels = train_data[-1]\n",
    "        label_validation = validation_data[-1]\n",
    "        \n",
    "        # using all unlabeled sample instead of pool of unlabeled sample\n",
    "        self.u = len(labels)\n",
    "        L, U, U_prime = self.init_L_U_U_prime(labels)\n",
    "        #print(\"All data index: \",train_data[0].index.values)\n",
    "        #print(\"L: \", L)\n",
    "        #print(\"U: \", U)\n",
    "        #print(\"U_prime: \", U_prime)\n",
    "        \n",
    "        # index of self labeled samples\n",
    "        self.new_labeled_idx = defaultdict(list)\n",
    "        self.new_labeled_details = defaultdict(list)\n",
    "        self.f1_on_validation = defaultdict(list)\n",
    "        # save clf in each iteration\n",
    "        self.all_iter_clf = []\n",
    "        # when fit co-train, we collect f1 on validation samples wrt each iteration\n",
    "        for view_idx in range(self.view_num):\n",
    "            self.f1_on_validation[\"dv\"+str(view_idx)]=[]\n",
    "        self.iterCounter = 0\n",
    "        # --------- plot PCA reduced plot for initial stage of train -------------- #\n",
    "        pca = self.pca\n",
    "        '''Notice pca will change input datatype dataframe to datatype list, keep it's order,\n",
    "           or re-assign dv1.index.values to the output'''\n",
    "        pca_dv1 = pca.fit_transform(X=train_data[0])\n",
    "        pca_dv2 = pca.fit_transform(X=train_data[1])\n",
    "        #loop until we have assigned labels to every sample in U and U_prime or we hit our iteration break condition\n",
    "        while self.iterCounter < self.k and U_prime:\n",
    "            '''\n",
    "            Extract labeled training sample from training sample and train classifier, then make prediction\n",
    "            Evaluate performance of current iteration classifier\n",
    "            '''\n",
    "            # print(\"Iteration:\",self.iterCounter, \" L: \",L)\n",
    "            # print(\"Iteration:\",self.iterCounter, \" U_prime: \",U_prime)\n",
    "            iter_train = defaultdict(list)\n",
    "            iter_unlabeled = defaultdict(list)\n",
    "            iter_clf = copy.deepcopy(self.clf) \n",
    "            iter_train_label = labels.iloc[L]\n",
    "            for view_idx in range(self.view_num):\n",
    "                iter_train[\"dv\"+str(view_idx)]=train_data[view_idx].iloc[L]\n",
    "                iter_unlabeled[\"dv\"+str(view_idx)]=train_data[view_idx].iloc[U_prime]\n",
    "                \n",
    "                iter_clf[view_idx].fit(iter_train[\"dv\"+str(view_idx)], iter_train_label.values.ravel())\n",
    "\n",
    "            self.check_iter_label_mapping(iter_clf)\n",
    "            # --------- test error on validation data --------------------- #\n",
    "            y_predict = defaultdict(list)\n",
    "            f1 = defaultdict(list)\n",
    "            for view_idx in range(self.view_num):\n",
    "                y_predict[view_idx]=iter_clf[view_idx].predict(validation_data[view_idx])\n",
    "                f1[\"dv\"+str(view_idx)]=f1_score(label_validation,y_predict[view_idx],average='macro')\n",
    "                self.f1_on_validation[\"dv\"+str(view_idx)].append(f1[\"dv\"+str(view_idx)])\n",
    "            \n",
    "            self.all_iter_clf.append((self.iterCounter,iter_clf))\n",
    "            \n",
    "            new_train_label = labels.iloc[L]\n",
    "            if len(self.new_labeled_details[\"h0_new_idx\"])==0:\n",
    "                print(\"Iteration 0, strat self label.\")\n",
    "            else:\n",
    "                print(\"Iteration\",self.iterCounter,\" h1 new: \", self.new_labeled_details[\"h0_new_idx\"][-1], \" score: \", self.new_labeled_details[\"h0_new_proba\"][-1])\n",
    "                print(\"Iteration\",self.iterCounter,\" h2 new: \", self.new_labeled_details[\"h1_new_idx\"][-1], \" score: \", self.new_labeled_details[\"h1_new_proba\"][-1])\n",
    "            \n",
    "            ''' # ----------- plot the co-training process -------------- # '''\n",
    "            if (plot_save_path != None) and (dataset_name != None):\n",
    "                for pca_view,v_name in [(pca_dv1,\"dv1\"),(pca_dv2,\"dv2\")]:\n",
    "                    self.plot_co_training_process(self.iterCounter, pca_view, new_train_label, U_prime,\n",
    "                                                  self.new_labeled_details[\"h0_new_idx\"], self.new_labeled_details[\"h1_new_idx\"],\n",
    "                                                  self.new_labeled_details[\"h0_new_proba\"], self.new_labeled_details[\"h1_new_proba\"],\n",
    "                                                  plotSavingPath = plot_save_path, name = dataset_name+\"_\"+v_name)\n",
    "            '''\n",
    "            Confidence measure:\n",
    "            Part 1: View agreement score is probability of all view multiply together\n",
    "            Case 1: Two view agree with very large confidence score (check)\n",
    "            Case 2: Two view disagree, one view have large confidence, scoring towards large confident view's class (check)\n",
    "            Case 3: Two view disagree and both have large confidence score\n",
    "            Case 4: Two view agree/disagree with small confidence score\n",
    "            '''\n",
    "            unlabeled_pred_proba = []\n",
    "            for view_idx in range(self.view_num):\n",
    "                view_pred_proba = self.get_confidence_score(clf=iter_clf[view_idx],data=iter_unlabeled[\"dv\"+str(view_idx)])\n",
    "                unlabeled_pred_proba.append(view_pred_proba)\n",
    "            view_agreement_score = np.array([np.multiply(x[0],x[1]) for x in zip(*unlabeled_pred_proba)])\n",
    "            #print(unlabeled_pred_proba)\n",
    "            #print(view_agreement_score)\n",
    "            ''' \n",
    "            # Part 2: Accumulate predicted probability on each iteration as score\n",
    "            '''\n",
    "            diff_iter_score = defaultdict(list)\n",
    "            curr_iter_proba = defaultdict(list)\n",
    "            final_score = defaultdict(list)\n",
    "            cluster_proba = defaultdict(list)\n",
    "            score_rank = defaultdict(list)\n",
    "            proba_sample_idx_map=iter_unlabeled[\"dv0\"].index\n",
    "            for view_idx in range(self.view_num):\n",
    "                # ------ Part 1: Accumulate predicted probability on first and current iteration as score ------ #\n",
    "                for iteration, clf in self.all_iter_clf:\n",
    "                    if (iteration ==0) or (iteration ==self.iterCounter):\n",
    "                        diff_iter_score[view_idx].append(self.get_confidence_score(clf=clf[view_idx], data=iter_unlabeled[\"dv\"+str(view_idx)]))\n",
    "                # add score in each iteration together\n",
    "                final_score[\"dv\"+str(view_idx)] = np.array([sum(x) for x in zip(*diff_iter_score[view_idx])])\n",
    "                # save last iteration probability\n",
    "                curr_iter_proba[\"dv\"+str(view_idx)] = diff_iter_score[view_idx][-1]\n",
    "                \n",
    "                # ----------- Part 2: Incorporate clustering distance as part of score ------------ #\n",
    "                cluster_proba[view_idx] = self.get_cluster_distance_as_proba(label_train=iter_train[\"dv\"+str(view_idx)], iter_train_label=iter_train_label[\"authorID\"],\n",
    "                                                                             unlabeled=iter_unlabeled[\"dv\"+str(view_idx)])\n",
    "                final_score[\"dv\"+str(view_idx)]=np.array([sum(x) for x in zip(final_score[\"dv\"+str(view_idx)],cluster_proba[view_idx])])\n",
    "                \n",
    "                # ----------------- Part 3: Incorporate view agreement score ---------------------- #\n",
    "                final_score[\"dv\"+str(view_idx)]=np.array([sum(x) for x in zip(final_score[\"dv\"+str(view_idx)],view_agreement_score)])\n",
    "                \n",
    "                # -------------- Part 4: Rank confidence and start self labeling ------------------ #\n",
    "                # proba1_rank[i] is label i's confidence measure\n",
    "                for class_proba in final_score[\"dv\"+str(view_idx)].T:\n",
    "                    score_rank[\"dv\"+str(view_idx)].append((-class_proba).argsort())\n",
    "                \n",
    "            #print(\"All: \", diff_iter_score)\n",
    "            #print(\"Current: \",curr_iter_proba)\n",
    "            #print(\"Cluster proba: \", cluster_proba)\n",
    "            #print(\"Final score: \",final_score)\n",
    "            #print(\"Score rank: \", score_rank)\n",
    "            \n",
    "            '''\n",
    "            Start labelling\n",
    "            '''\n",
    "            new_sample = defaultdict(list)\n",
    "            new_sample_score = defaultdict(list)\n",
    "            new_sample_proba = defaultdict(list)\n",
    "            flatten_new_idx = defaultdict(list)\n",
    "            flatten_score = defaultdict(list)\n",
    "            flatten_proba = defaultdict(list)\n",
    "            self.new_labeled_details[\"Iteration\"].append(self.iterCounter)\n",
    "            \n",
    "            for view_idx in range(self.view_num):\n",
    "                (new_sample[\"dv\"+str(view_idx)], \n",
    "                 new_sample_score[\"dv\"+str(view_idx)], \n",
    "                 new_sample_proba[\"dv\"+str(view_idx)]) = self.label_samples(final_score[\"dv\"+str(view_idx)], \n",
    "                                                                            score_rank[\"dv\"+str(view_idx)], \n",
    "                                                                            proba_sample_idx_map, \n",
    "                                                                            curr_iter_proba[\"dv\"+str(view_idx)])\n",
    "            '''Case if h1's new class 1 is h2's new class 2\n",
    "            Example: Iteration 45  h1 new:  [[95], [150]]  probs:  [[0.89], [0.82]]\n",
    "                     Iteration 45  h2 new:  [[147], [95]]  probs:  [[0.89], [0.92]]\n",
    "            '''\n",
    "            #print(\"before fix: \",new_sample)\n",
    "            #print(\"before fix: \",new_sample_proba)\n",
    "            full_views_list = list(new_sample.keys())\n",
    "            temp_views_list = list(new_sample.keys())\n",
    "            for curr_view in full_views_list:\n",
    "                #print(\"t1 \",new_sample[curr_view])\n",
    "                #print(\"t1 \",new_sample_proba[curr_view])\n",
    "                temp_views_list.remove(curr_view)\n",
    "                for i, curr_class_new in enumerate(new_sample[curr_view]):\n",
    "                    temp_pop_list = []\n",
    "                    for j, class_item in enumerate(curr_class_new):\n",
    "                        #print(class_item)\n",
    "                        for other_view in temp_views_list:\n",
    "                            #print(\"t2 org\",new_sample[other_view])\n",
    "                            #print(\"t2 org\",new_sample_proba[other_view])\n",
    "                            for l, other_view_class_new in enumerate(new_sample[other_view]):\n",
    "                                for m, other_class_item in enumerate(other_view_class_new):\n",
    "                                    # only perform delete if current view contain item, if it's deleted, move on\n",
    "                                    if new_sample[curr_view][i][j] == new_sample[other_view][l][m]:\n",
    "                                        if new_sample_proba[curr_view][i][j]>new_sample_proba[other_view][l][m]:\n",
    "                                            new_sample[other_view][l].pop(m)\n",
    "                                            new_sample_proba[other_view][l].pop(m)\n",
    "                                            #print(\"t2 update\",new_sample)\n",
    "                                            #print(\"t2 update\",new_sample_proba)\n",
    "                                        else:\n",
    "                                            temp_pop_list.append(j)\n",
    "                                        #print(class_item, \"check\")\n",
    "                    temp_pop_list.sort(reverse=True)\n",
    "                    for idx in temp_pop_list:\n",
    "                        new_sample[curr_view][i].pop(idx)\n",
    "                        new_sample_proba[curr_view][i].pop(idx)\n",
    "            #print(\"apply fix: \",new_sample)\n",
    "            #print(\"apply fix: \",new_sample_proba)\n",
    "            \n",
    "            for view_idx in range(self.view_num):\n",
    "                flatten_new_idx[view_idx] = list(itertools.chain(*new_sample[\"dv\"+str(view_idx)]))\n",
    "                flatten_score[view_idx] = list(itertools.chain(*new_sample_score[\"dv\"+str(view_idx)]))\n",
    "                flatten_proba[view_idx] = list(itertools.chain(*new_sample_proba[\"dv\"+str(view_idx)]))\n",
    "            \n",
    "            # if no new label added to L, stop iteration\n",
    "            if not any(flatten_new_idx):\n",
    "                self.iterCounter += 1\n",
    "                print(\"Remaining unlabel sample is uncertain.\")\n",
    "                break\n",
    "                \n",
    "            for view_idx in range(self.view_num):\n",
    "                self.new_labeled_details[\"h\"+str(view_idx)+\"_new_idx\"].append(flatten_new_idx[view_idx])\n",
    "                self.new_labeled_details[\"h\"+str(view_idx)+\"_new_score\"].append(flatten_score[view_idx])\n",
    "                self.new_labeled_details[\"h\"+str(view_idx)+\"_new_proba\"].append(flatten_proba[view_idx])\n",
    "                \n",
    "            #print(self.new_labeled_details)\n",
    "            '''\n",
    "            Add most confidence samples as new training samples, auto label the samples and remove it from U_prime\n",
    "            Special Case: if two view classifier give same most confident sample, only 1 datapoint self-labeled\n",
    "            '''\n",
    "            roundNewIdx = list(zip(*[value for key, value in new_sample.items()]))\n",
    "            roundNewScore = list(zip(*[value for key, value in new_sample_score.items()]))\n",
    "            roundNewProba = list(zip(*[value for key, value in new_sample_proba.items()]))\n",
    "            roundNew_flatten_unique = []\n",
    "            for label, round_new in enumerate(zip(roundNewIdx,roundNewScore,roundNewProba)):\n",
    "                #print(round_new)\n",
    "                round_new = list(zip(*round_new))\n",
    "                temp = []\n",
    "                for index, (idx,score,proba) in enumerate(round_new):\n",
    "                    for i in range(len(idx)):\n",
    "                        temp.append((idx[i],score[i],proba[i]))\n",
    "                #print(temp)\n",
    "                round_new = set(temp)\n",
    "                round_new_idx = [item[0] for item in round_new]\n",
    "                round_new_score = [item[1] for item in round_new]\n",
    "                round_new_proba = [item[2] for item in round_new]\n",
    "                self.new_labeled_idx[self.classes_[label]].append(round_new_idx)\n",
    "                self.new_labeled_idx[self.classes_[label]+\"_score\"].append(round_new_score)\n",
    "                self.new_labeled_idx[self.classes_[label]+\"_proba\"].append(round_new_proba)\n",
    "                roundNew_flatten_unique.extend(round_new_idx)\n",
    "                # add label to those new samples\n",
    "                #print(labels[round_new_idx])\n",
    "                labels[\"authorID\"][round_new_idx] = self.classes_[label]\n",
    "                #print(labels[round_new_idx])\n",
    "                #print(self.classes_[label],\" (u' idx): \",round_new_idx)\n",
    "            #print(roundNew_flatten_unique)\n",
    "            # extend the labeled sample\n",
    "            L.extend(roundNew_flatten_unique)\n",
    "            # remove the labeled sample from U_prime\n",
    "            U_prime = [x for x in U_prime if x not in roundNew_flatten_unique]\n",
    "            #print(U_prime)\n",
    "            # randomly choice view_num*self-label-class-max*class_num examples from u to replenish u_prime\n",
    "            replenishItem = U[-((self.view_num)*(self.sl_class_max)*len(self.classes_)):]\n",
    "            U_prime.extend(replenishItem)\n",
    "            U = U[:-len(replenishItem)]\n",
    "            self.iterCounter +=1\n",
    "            \n",
    "            '''\n",
    "            # ----------- plot the last iteration of co-training process -------------- #\n",
    "            '''\n",
    "            if self.iterCounter == self.k:\n",
    "                new_train_label = labels.iloc[L]\n",
    "                print(\"Iteration\",self.iterCounter,\" h1 new: \", self.new_labeled_details[\"h0_new_idx\"][-1], \" score: \", self.new_labeled_details[\"h0_new_proba\"][-1])\n",
    "                print(\"Iteration\",self.iterCounter,\" h2 new: \", self.new_labeled_details[\"h1_new_idx\"][-1], \" score: \", self.new_labeled_details[\"h1_new_proba\"][-1])\n",
    "                # ----- save pca reduced plot ------ #\n",
    "                if plot_save_path != None:\n",
    "                    for pca_view,v_name in [(pca_dv1,\"dv1\"),(pca_dv2,\"dv2\")]:\n",
    "                        self.plot_co_training_process(self.iterCounter, pca_view, new_train_label, U_prime,\n",
    "                                                      self.new_labeled_details[\"h0_new_idx\"], self.new_labeled_details[\"h1_new_idx\"],\n",
    "                                                      self.new_labeled_details[\"h0_new_proba\"], self.new_labeled_details[\"h1_new_proba\"],\n",
    "                                                      plotSavingPath = plot_save_path, name = dataset_name+\"_\"+v_name)\n",
    "        \n",
    "        print(\"Total Labeled number: \", len(L), \" Still unlabeled number: \", len(U_prime))\n",
    "        #print(self.f1_on_validation[\"dv0\"])\n",
    "        #print(self.f1_on_validation[\"dv1\"])\n",
    "        # final train\n",
    "        final_train = defaultdict(list)\n",
    "        for view_idx in range(self.view_num):\n",
    "            final_train[view_idx] = train_data[view_idx].iloc[L]\n",
    "            self.clf[view_idx].fit(final_train[view_idx], labels.iloc[L].values.ravel())\n",
    "        '''\n",
    "        Evalutation plot for co-training process, save f1 score vs number of iteration plot\n",
    "        '''\n",
    "        if (dataset_name != None) and (plot_save_path != None):\n",
    "            default_text_based = [self.f1_on_validation[\"dv0\"][0]] * self.iterCounter\n",
    "            default_citation_based = [self.f1_on_validation[\"dv1\"][0]] * self.iterCounter\n",
    "            default_step = np.arange(0,self.iterCounter)\n",
    "            co_train_text_based = self.f1_on_validation[\"dv0\"][1:]\n",
    "            co_train_citation_based = self.f1_on_validation[\"dv1\"][1:]\n",
    "            co_training_step = np.arange(1,self.iterCounter)\n",
    "\n",
    "            fig = plt.figure()\n",
    "            ax = plt.axes()\n",
    "            plt.plot(default_step, default_text_based, linestyle='dashed', label=\"Text based default\")\n",
    "            plt.plot(default_step, default_citation_based, linestyle='dashdot', label=\"Citation based default\")\n",
    "            plt.plot(co_training_step, co_train_text_based, linestyle='solid', marker = \"*\", label=\"Text based\")\n",
    "            plt.plot(co_training_step, co_train_citation_based, linestyle='dotted', marker = \"+\", label=\"Citation based\")\n",
    "            ax.autoscale_view()\n",
    "            legend = ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.2), ncol=2)\n",
    "            plt.xlabel('Co-Training Iterations')\n",
    "            plt.ylabel('F1 score')\n",
    "            plt.savefig((plot_save_path+dataset_name+\"_co_train_iteration_f1.png\"), dpi=150, bbox_extra_artists=(legend,), bbox_inches='tight')\n",
    "            # plt.show()\n",
    "            plt.close(\"all\")\n",
    "        return \"Training Done\"\n",
    "\n",
    "    def co_train_process_f1(self):\n",
    "        return self.f1_on_validation[\"dv0\"], self.f1_on_validation[\"dv1\"]\n",
    "\n",
    "    def get_iter_count(self):\n",
    "        return self.iterCounter\n",
    "\n",
    "    def predict(self, data):\n",
    "        y_predict = defaultdict(list)\n",
    "        for view_idx in range(self.view_num):\n",
    "            y_predict[view_idx]=self.clf[view_idx].predict(data[view_idx])\n",
    "        y_predict = [value for key, value in y_predict.items()]\n",
    "        #fill pred with -1 so we can identify the samples in which error occur\n",
    "        y_pred = [\"-1\"] * data[0].shape[0]\n",
    "        for i, (dv1_y, dv2_y) in enumerate(zip(*y_predict)):\n",
    "            # if both agree on label\n",
    "            if dv1_y == dv2_y:\n",
    "                y_pred[i] = dv1_y\n",
    "            # If disagree on label and support proba method: We times probability together, choice class have higher probabilities\n",
    "            elif hasattr(self.clf[0], \"predict_proba\") and hasattr(self.clf[1], \"predict_proba\"):\n",
    "                h1_probas = self.clf[0].predict_proba([data[0].iloc[i]])[0]\n",
    "                h2_probas = self.clf[1].predict_proba([data[1].iloc[i]])[0]\n",
    "                final_y_probas = [proba_y1 * proba_y2 for (proba_y1, proba_y2) in zip(h1_probas, h2_probas)]\n",
    "                #print(\"h1 and h2 disagree on\",i, \" h1 Proba : \",h1_probas, \" h2 Proba: \", h2_probas)\n",
    "                #print(\"product probas:\",final_y_probas)\n",
    "                max_prob_idx = final_y_probas.index(max(final_y_probas))\n",
    "                y_pred[i] = self.classes_[max_prob_idx]\n",
    "                #print(\"result idx: \", max_prob_idx, \" result: \",y_pred[i])\n",
    "            # If disagree on label and not support proba method: We use decision_function to first calculate probability,\n",
    "            # then we times calculate probability together, choice class have higher probabilities\n",
    "            elif hasattr(self.clf[0], \"decision_function\") and hasattr(self.clf[1], \"decision_function\"):\n",
    "                dv1_distance = self.clf[0].decision_function([data[0].iloc[i]])\n",
    "                dv2_distance = self.clf[1].decision_function([data[1].iloc[i]])\n",
    "                if len(self.clf1.classes_)==2:\n",
    "                    h1_probas = np.array([1-self.sigmoid(dv1_distance), self.sigmoid(dv1_distance)])\n",
    "                    h2_probas = np.array([1-self.sigmoid(dv2_distance), self.sigmoid(dv2_distance)])\n",
    "                else:\n",
    "                    h1_probas = self.softmax(dv1_distance)\n",
    "                    h2_probas = self.softmax(dv2_distance)\n",
    "                final_y_probas = [proba_y1 * proba_y2 for (proba_y1, proba_y2) in zip(h1_probas, h2_probas)]\n",
    "                #print(\"h1 and h2 disagree on\",i, \" h1 Proba : \",h1_probas, \" h2 Proba: \", h2_probas)\n",
    "                #print(\"product probas:\",final_y_probas)\n",
    "                max_prob_idx = final_y_probas.index(max(final_y_probas))\n",
    "                y_pred[i] = self.classes_[max_prob_idx]\n",
    "                #print(\"result idx: \", max_prob_idx, \" result: \",y_pred[i])\n",
    "            # disagree and not support any confidence measure\n",
    "            else:\n",
    "                sys.exit(\"Error occur\")\n",
    "\n",
    "        # convert final result to np array\n",
    "        y_pred_np_array = np.asarray(y_pred)\n",
    "        return y_pred_np_array\n",
    "\n",
    "    def predict_proba(self, dv1, dv2):\n",
    "        # the predicted probabilities is simply a product (*) of probabilities given from each classifier trained\n",
    "        h1_probas = self.clf1.predict_proba(dv1)\n",
    "        h2_probas = self.clf2.predict_proba(dv2)\n",
    "        \n",
    "        proba = (h1_probas*h2_probas)\n",
    "        return proba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-07T23:18:14.407542Z",
     "start_time": "2020-08-07T23:18:14.350572Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simulate_co_training_situation(all_train_label,init_labeled_size,init_validation_size):\n",
    "    # ----------- set some labeled data as unlabeled ------------ #\n",
    "    # 1. obtain data ratio\n",
    "    c = Counter(all_train_label)\n",
    "    data_ratio = [(i, c[i] / len(all_train_label)) for i in c]\n",
    "    print(data_ratio)\n",
    "    # 2. calculate per class size \n",
    "    # co_train_per_class_size contain (label,initial train size for each class, initial validation data for each class)\n",
    "    co_train_per_class_size = [(label, round(ratio*init_labeled_size),round(ratio*init_validation_size)) for label, ratio in data_ratio]\n",
    "    print(co_train_per_class_size)\n",
    "    # 3.Initialize train and validation sample index list save it for later use\n",
    "    temp_train_label = all_train_label.copy()\n",
    "    label_train_sample_idx = []\n",
    "    validation_sample_idx = []\n",
    "    # 4. random draw both train labeled samples and validation samples, mark other as unlabeled\n",
    "    # we could also use validation samples to improve performance\n",
    "    for unique_label, training_size, validation_size in co_train_per_class_size:\n",
    "        curr_label_idx = [idx for (idx, label) in temp_train_label.iteritems() if label == unique_label]\n",
    "        curr_label_size = len(curr_label_idx)\n",
    "        # 1. get train sample idx\n",
    "        keep_as_labelled_train_p1 = random.sample(curr_label_idx, training_size)\n",
    "        curr_label_idx_no_train_p2 = [x for x in curr_label_idx if x not in keep_as_labelled_train_p1]\n",
    "        label_train_sample_idx += keep_as_labelled_train_p1\n",
    "        # 2. get validation sample idx\n",
    "        temp_validation_sample_idx = random.sample(curr_label_idx_no_train_p2, validation_size)\n",
    "        validation_sample_idx += temp_validation_sample_idx\n",
    "        unlabel_item_idx = [x for x in curr_label_idx_no_train_p2 if x not in temp_validation_sample_idx]\n",
    "        # 3. set other samples to -1 as unlabeled\n",
    "        for unlabel_idx in unlabel_item_idx:\n",
    "            temp_train_label[unlabel_idx]=-1\n",
    "    return label_train_sample_idx, temp_train_label, validation_sample_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-07T23:17:46.816Z"
    },
    "code_folding": [
     37,
     72,
     161,
     231,
     238
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import collections\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score,f1_score,accuracy_score\n",
    "\n",
    "# cross validation\n",
    "def k_fold_cv_all_algorithm(dv1, dv2, label, init_labeled_size, muti_view_clf=[], combined_clf=[],\n",
    "                            num_fold=10, dataset_name=None, plot_save_path=None, validation=False):\n",
    "    # set validation dataset as 10% of all data\n",
    "    if validation:\n",
    "        init_validation_size = len(label)*0.1\n",
    "    else:\n",
    "        init_validation_size = 0\n",
    "    kf = StratifiedKFold(n_splits=num_fold)\n",
    "    allTrueLabel = []\n",
    "    co_train_algorithm = [name for clf,name in muti_view_clf]\n",
    "    baseline_algorithm = [name for clf,name in combined_clf]\n",
    "    allPredLabel = collections.defaultdict(list)\n",
    "    all_fold_coTrain_diff_iteration = collections.defaultdict(list)\n",
    "    co_train_iteration = collections.defaultdict(list)\n",
    "    \n",
    "    all_fold_statistic = []\n",
    "    fold = 0\n",
    "    # convert different input type to dataframe for consistency\n",
    "    dv1 = pd.DataFrame(dv1)\n",
    "    dv2 = pd.DataFrame(dv2)\n",
    "    pca = PCA(n_components=2)\n",
    "    plot_new_dir = plot_save_path+dataset_name+\"/\"\n",
    "    if not os.path.exists(plot_new_dir):\n",
    "        os.makedirs(plot_new_dir)\n",
    "    for view,name in [(dv1,\"dv1\"),(dv2,\"dv2\")]:\n",
    "        pca_view = pca.fit_transform(X=view)\n",
    "        first_principal_component = pca_view[:,0]\n",
    "        second_principal_component = pca_view[:,1]\n",
    "        fig, ax = plt.subplots(figsize=(9,7))\n",
    "        for author in np.unique(label):\n",
    "            ix = label.index[label == author].tolist()\n",
    "            ax.scatter(x=first_principal_component[ix], y=second_principal_component[ix],\n",
    "                       cmap='viridis', label = author, s = 50, alpha = 0.8)\n",
    "        legend = ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.2), ncol=2,prop={'size': 13})\n",
    "        plt.title('True label', fontsize=14)\n",
    "        plt.xlabel(\"First principal component\",fontsize=14)\n",
    "        plt.ylabel(\"Second principal component\",fontsize=14)\n",
    "        plt.savefig(fname=plot_new_dir+dataset_name+\"_PCA_true_label_\"+name+\".png\",\n",
    "                    dpi=100, bbox_extra_artists=(legend,), bbox_inches='tight')\n",
    "        #plt.show()\n",
    "        plt.close(\"all\")\n",
    "    \n",
    "    for train_index, test_index in kf.split(dv1, label):\n",
    "        fold +=1\n",
    "        # print(\"TRAIN:\", train_index, \" \\n TEST:\", test_index)\n",
    "        # ---------------split train and test -------------------- #\n",
    "        dv1_train, dv1_test = dv1.iloc[train_index], dv1.iloc[test_index]\n",
    "        dv2_train, dv2_test = dv2.iloc[train_index], dv2.iloc[test_index]\n",
    "        all_train_label, test_label = label.iloc[train_index], label.iloc[test_index]\n",
    "        '''\n",
    "        Plot true labeled result for different view on each fold, use PCA to reduce views to 2d\n",
    "        Real training process will not using PCA to reduce it's dimension \n",
    "        '''\n",
    "        if (plot_save_path == None) or (dataset_name == None):\n",
    "            print(\"plot off\")\n",
    "        else:\n",
    "            detailed_plot_path = plot_save_path+dataset_name+\"/fold\"+str(fold)+\"/\"\n",
    "            if not os.path.exists(detailed_plot_path):\n",
    "                os.makedirs(detailed_plot_path)\n",
    "            for view,name in [(dv1_train,\"dv1\"),(dv2_train,\"dv2\")]:\n",
    "                pca_view = pca.fit_transform(X=view)\n",
    "                first_principal_component = pca_view[:,0]\n",
    "                second_principal_component = pca_view[:,1]\n",
    "                fig, ax = plt.subplots(figsize=(9,7))\n",
    "                for author in np.unique(label):\n",
    "                    # real idx get earsed by PCA function\n",
    "                    temp = all_train_label.index[all_train_label == author].tolist()\n",
    "                    pos_idx = []\n",
    "                    for idx in temp:\n",
    "                        pos_idx.append(all_train_label.index.get_loc(idx))\n",
    "                    #print(temp)\n",
    "                    #print(pos_idx)\n",
    "                    ax.scatter(x=first_principal_component[pos_idx], y=second_principal_component[pos_idx],\n",
    "                               cmap='viridis', label = author, s = 50, alpha = 0.8)\n",
    "                legend = ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.2), ncol=2,prop={'size': 13})\n",
    "                plt.title('True label', fontsize=14)\n",
    "                plt.xlabel(\"First principal component\",fontsize=14)\n",
    "                plt.ylabel(\"Second principal component\",fontsize=14)\n",
    "                plt.savefig(fname=detailed_plot_path+dataset_name+\"_PCA_true_label_\"+name+\".png\",\n",
    "                            dpi=100, bbox_extra_artists=(legend,), bbox_inches='tight')\n",
    "                #plt.show()\n",
    "                plt.close(\"all\")\n",
    "        ''' \n",
    "        Comparison Part 1: Use two view concatenated features and train with supervise learning (up bound)\n",
    "        Notice we train supervise learning with all labeled data, no data has been mark as unlabeled\n",
    "        '''\n",
    "        # ---------- collect per fold statistic ---------------------- #\n",
    "        curr_fold_statistic = {'author': dataset_name, 'fold':fold, 'test_size': dv1_test.shape[0]} \n",
    "        ub_concatenated_train = pd.concat([dv1_train,dv2_train], axis=1, ignore_index=True)\n",
    "        concatenated_test = pd.concat([dv1_test,dv2_test], axis=1, ignore_index=True)\n",
    "        for in_clf, clf_name in combined_clf:\n",
    "            per_fold_clf = copy.deepcopy(in_clf) \n",
    "            per_fold_clf.fit(ub_concatenated_train, all_train_label)\n",
    "            ub_pred = per_fold_clf.predict(concatenated_test)\n",
    "            print(clf_name+\"-UB f1: \", metrics.classification_report(test_label, ub_pred))\n",
    "            print(metrics.confusion_matrix(test_label, ub_pred).ravel())\n",
    "            curr_fold_statistic[clf_name+\"-UB f1\"] = f1_score(test_label.values.tolist(), ub_pred,average='macro')\n",
    "            allPredLabel[clf_name+\"-UB predict label\"].append(ub_pred)\n",
    "\n",
    "        ''' \n",
    "        Comparison Part 2: Use two view and train with co-training (Check effective of co-training)\n",
    "        Notice here we are simulating situation for co-training by set most of data to unlabelled\n",
    "        '''\n",
    "        label_train_sample_idx, final_train_label, validation_sample_idx = simulate_co_training_situation(all_train_label,init_labeled_size,init_validation_size)\n",
    "        curr_fold_statistic[\"train_size\"]=len(label_train_sample_idx)\n",
    "        # -------------------- train with co-training ------------------- #\n",
    "        for in_clf, clf_name in muti_view_clf: \n",
    "            if plot_save_path == None:\n",
    "                print(\"plot off\")\n",
    "                per_clf_plot_save_path = None\n",
    "            else:\n",
    "                per_clf_plot_save_path = detailed_plot_path+clf_name+\"/\"\n",
    "                if not os.path.exists(per_clf_plot_save_path):\n",
    "                    os.makedirs(per_clf_plot_save_path)\n",
    "            \n",
    "            unlabeled_sample_size = len(all_train_label)-len(label_train_sample_idx)\n",
    "            per_fold_clf = copy.deepcopy(in_clf)\n",
    "            per_fold_clf.set_PCA(pca)\n",
    "            per_fold_clf.fit(train_data=[dv1_train.copy(), dv2_train.copy(), final_train_label.copy()], \n",
    "                             validation_data=[dv1_test, dv2_test, test_label],\n",
    "                             dataset_name=dataset_name, plot_save_path=per_clf_plot_save_path)\n",
    "            curr_fold_statistic[\"unlabel_size\"]=unlabeled_sample_size\n",
    "            curr_fold_statistic['validation_size']=len(validation_sample_idx)\n",
    "            curr_fold_statistic[\"total_iteration\"]= per_fold_clf.get_iter_count()\n",
    "            # -------------- get self-labeled sample index --------------- #\n",
    "            self_labeled_index = per_fold_clf.get_self_labeled_sample()\n",
    "            #print(\"Self labeled sample index: \", self_labeled_index)\n",
    "            self_labeled_idx_temp = [idx for idx in self_labeled_index.values()]\n",
    "            all_self_labeled_index = [val for sublist in self_labeled_idx_temp for subsublist in sublist for val in subsublist]\n",
    "            curr_fold_statistic[clf_name+'_total_self_labeled']= len(all_self_labeled_index)\n",
    "            # ------------- get predicted label for test set ------------- #\n",
    "            co_train_predict = per_fold_clf.predict([dv1_test, dv2_test])\n",
    "            print(clf_name+\" f1: \", metrics.classification_report(test_label, co_train_predict))\n",
    "            print(metrics.confusion_matrix(test_label, co_train_predict).ravel())\n",
    "            curr_fold_statistic[clf_name+\" f1\"] = f1_score(test_label.values.tolist(), co_train_predict,average='macro')\n",
    "            allPredLabel[clf_name+\" predict label\"].append(co_train_predict)\n",
    "            # ------------- get co-training iterations f1 score ---------- #\n",
    "            co_train_iteration[clf_name].append(per_fold_clf.get_iter_count())\n",
    "            coTrain_diff_iter_on_test_dv1, coTrain_diff_iter_on_test_dv2 = per_fold_clf.co_train_process_f1()\n",
    "            all_fold_coTrain_diff_iteration[clf_name+\"_dv1\"].append(coTrain_diff_iter_on_test_dv1)\n",
    "            all_fold_coTrain_diff_iteration[clf_name+\"_dv2\"].append(coTrain_diff_iter_on_test_dv2)\n",
    "        ''' \n",
    "        Comparison Part 3: Use two view concatenated features and train with supervise learning (lower bound)\n",
    "        Notice we only train supervise learning with labeled train, data mark as unlabeled is not used\n",
    "        '''\n",
    "        lb_concatenated_train = pd.concat([dv1_train.loc[label_train_sample_idx],\n",
    "                                           dv2_train.loc[label_train_sample_idx]],axis=1, ignore_index=True)\n",
    "        lb_train_label = [final_train_label[i] for i in label_train_sample_idx]\n",
    "        for in_clf, clf_name in combined_clf:\n",
    "            per_fold_clf = copy.deepcopy(in_clf)\n",
    "            per_fold_clf.fit(lb_concatenated_train, lb_train_label)\n",
    "            lb_pred = per_fold_clf.predict(concatenated_test)\n",
    "            print(clf_name+\"-LB f1: \", metrics.classification_report(test_label, lb_pred))\n",
    "            print(metrics.confusion_matrix(test_label, lb_pred).ravel())\n",
    "            curr_fold_statistic[clf_name+\"-LB f1\"] = f1_score(test_label.values.tolist(), lb_pred,average='macro')\n",
    "            allPredLabel[clf_name+\"-LB predict label\"].append(lb_pred)\n",
    "        \n",
    "        allTrueLabel.extend(test_label.values.tolist())\n",
    "        all_fold_statistic.append(curr_fold_statistic)\n",
    "\n",
    "    if (plot_save_path !=None) and (dataset_name != None):\n",
    "        ''' # --------------- plot per fold result f1 variance --------------- # '''\n",
    "        all_per_fold_f1_score_variance_plot = pd.DataFrame(all_fold_statistic)\n",
    "        #print(all_per_fold_f1_score_variance_plot)\n",
    "        sns.set(rc={'figure.figsize':(10,8)})\n",
    "        plot_temp_f1 = pd.DataFrame()\n",
    "        for clf_name in co_train_algorithm:\n",
    "            clf_temp_f1 = all_per_fold_f1_score_variance_plot[[clf_name+' f1']].values\n",
    "            plot_temp_f1[clf_name]=clf_temp_f1.flatten()\n",
    "        for clf_name in baseline_algorithm:\n",
    "            for clf_type in [\"-UB\",\"-LB\"]:\n",
    "                clf_temp_f1 = all_per_fold_f1_score_variance_plot[[clf_name+clf_type+' f1']].values\n",
    "                plot_temp_f1[clf_name+clf_type]=clf_temp_f1.flatten()\n",
    "        plot_temp_f1 = pd.melt(plot_temp_f1, var_name='methods', value_name='f1')\n",
    "        #print(plot_temp_f1)\n",
    "        ax = sns.boxplot(x=\"methods\", y=\"f1\", data=plot_temp_f1)\n",
    "        ax = sns.swarmplot(x=\"methods\", y=\"f1\", data=plot_temp_f1, color=\".25\")\n",
    "        ax.set_title(dataset_name+\" result variance within \"+str(num_fold)+\" fold\")\n",
    "        plt.savefig(plot_save_path+dataset_name+\"/all_method_result_variance.png\", dpi=150)\n",
    "        #plt.show()\n",
    "        plt.close(\"all\")\n",
    "        \n",
    "        '''  plot averaged f1 score wrt different fold in different iterations in co-training process '''\n",
    "        for clf_name in co_train_algorithm:\n",
    "            print(clf_name,\" per fold iteration: \", co_train_iteration[clf_name])\n",
    "            # each fold have different iteration, find minimum of iteration and \n",
    "            co_train_min_iteration_num = min(co_train_iteration[clf_name])\n",
    "            for sublist in all_fold_coTrain_diff_iteration[clf_name+\"_dv1\"]:\n",
    "                sublist[:] = sublist[:co_train_min_iteration_num]\n",
    "            for sublist in all_fold_coTrain_diff_iteration[clf_name+\"_dv2\"]:\n",
    "                sublist[:] = sublist[:co_train_min_iteration_num]\n",
    "            # -------- mean with respect to all fold --------- #\n",
    "            averaged_coTrain_diff_iter_dv1 = np.mean(all_fold_coTrain_diff_iteration[clf_name+\"_dv1\"], axis=0)\n",
    "            averaged_coTrain_diff_iter_dv2 = np.mean(all_fold_coTrain_diff_iteration[clf_name+\"_dv2\"], axis=0)\n",
    "            # -------- initial variables for plot ------------ #\n",
    "            default_text_based = [averaged_coTrain_diff_iter_dv1[0]] * co_train_min_iteration_num\n",
    "            default_citation_based = [averaged_coTrain_diff_iter_dv2[0]] * co_train_min_iteration_num\n",
    "            default_step = np.arange(0, co_train_min_iteration_num)\n",
    "            co_train_text_based = averaged_coTrain_diff_iter_dv1[1:]\n",
    "            co_train_citation_based = averaged_coTrain_diff_iter_dv2[1:]\n",
    "            co_training_step = np.arange(1, co_train_min_iteration_num)\n",
    "            # ----------- plot details ----------------------- #\n",
    "            fig = plt.figure()\n",
    "            ax = plt.axes()\n",
    "            plt.plot(default_step, default_text_based, linestyle='dashed', label=\"Text based default\")\n",
    "            plt.plot(default_step, default_citation_based, linestyle='dashdot', label=\"Citation based default\")\n",
    "            plt.plot(co_training_step, co_train_text_based, linestyle='solid', marker = \"*\", label=\"Text based\")\n",
    "            plt.plot(co_training_step, co_train_citation_based, linestyle='dotted', marker = \"+\", label=\"Citation based\")\n",
    "            ax.autoscale_view()\n",
    "            legend = ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.2), ncol=2)\n",
    "            plt.xlabel('Co-Training Iterations')\n",
    "            plt.ylabel('F1 score')\n",
    "            plt.savefig((plot_save_path+dataset_name+\"/\"+clf_name+\"_mean_diff_iter_f1.png\"), dpi=150, bbox_extra_artists=(legend,), bbox_inches='tight')\n",
    "            #plt.show()\n",
    "            plt.close(\"all\")\n",
    "    \n",
    "    ''' The results of a k-fold cross-validation run are often summarized with the mean of the model scores.'''\n",
    "    final_f1_score = []\n",
    "    for clf_name in co_train_algorithm:\n",
    "        clf_all_fold_f1 =[]\n",
    "        for per_fold_statistic in all_fold_statistic:\n",
    "            clf_per_fold_f1 = per_fold_statistic[clf_name+\" f1\"]\n",
    "            clf_all_fold_f1.append(clf_per_fold_f1)\n",
    "        clf_mean_f1 = np.mean(clf_all_fold_f1, axis=0)\n",
    "        final_f1_score.append((clf_name,clf_mean_f1))\n",
    "    for clf_name in baseline_algorithm:\n",
    "        for clf_type in [\"-UB\",\"-LB\"]:\n",
    "            clf_all_fold_f1 =[]\n",
    "            for per_fold_statistic in all_fold_statistic:\n",
    "                clf_per_fold_f1 = per_fold_statistic[clf_name+clf_type+\" f1\"]\n",
    "                clf_all_fold_f1.append(clf_per_fold_f1)\n",
    "            clf_mean_f1 = np.mean(clf_all_fold_f1, axis=0)\n",
    "            final_f1_score.append((clf_name+clf_type,clf_mean_f1))\n",
    "    #print(final_f1_score)\n",
    "    \n",
    "    return final_f1_score, all_fold_statistic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-07T23:17:47.005Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load text embedding:  pv_dbow\n",
      "Total text vector records: 135796\n",
      "Vector dimension:  100\n",
      "Load citation embedding:  n2v\n",
      "Total citation vector records: 124922\n",
      "Vector dimension:  100\n",
      "For name:  j_read\n",
      "(136, 2)\n",
      "j_read  pass\n",
      "For name:  f_esteves\n",
      "(34, 2)\n",
      "f_esteves  pass\n",
      "For name:  c_miller\n",
      "(252, 2)\n",
      "c_miller  pass\n",
      "For name:  r_jha\n",
      "(11, 2)\n",
      "r_jha  pass\n",
      "For name:  a_lowe\n",
      "(102, 2)\n",
      "a_lowe  pass\n",
      "For name:  a_vega\n",
      "(20, 2)\n",
      "a_vega  pass\n",
      "For name:  k_smith\n",
      "(338, 2)\n",
      "k_smith  pass\n",
      "For name:  j_gordon\n",
      "(19, 2)\n",
      "j_gordon  pass\n",
      "For name:  s_liao\n",
      "(104, 2)\n",
      "s_liao  pass\n",
      "For name:  j_qian\n",
      "(17, 2)\n",
      "j_qian  pass\n",
      "For name:  s_bernardi\n",
      "(91, 2)\n",
      "s_bernardi  pass\n",
      "For name:  t_hill\n",
      "(15, 2)\n",
      "t_hill  pass\n",
      "For name:  s_schindler\n",
      "(51, 2)\n",
      "s_schindler  pass\n",
      "For name:  j_williams\n",
      "(625, 2)\n",
      "j_williams  pass\n",
      "For name:  s_jacobson\n",
      "(28, 2)\n",
      "s_jacobson  pass\n",
      "For name:  e_andrade\n",
      "(17, 2)\n",
      "e_andrade  pass\n",
      "For name:  t_santos\n",
      "(45, 2)\n",
      "t_santos  pass\n",
      "For name:  k_kim\n",
      "(1111, 2)\n",
      "Total sample size before apply threshold:  1111\n",
      "Counter({'0000-0002-6929-5359': 211, '0000-0001-9498-284X': 154, '0000-0002-5878-8895': 139, '0000-0002-1864-3392': 92, '0000-0002-7045-8004': 57, '0000-0001-7896-6751': 57, '0000-0002-7991-9428': 55, '0000-0002-4010-1063': 45, '0000-0002-2186-3484': 28, '0000-0002-4899-1929': 25, '0000-0003-0487-4242': 24, '0000-0002-3642-1486': 22, '0000-0001-9965-3535': 17, '0000-0002-4168-757X': 17, '0000-0001-6525-3744': 14, '0000-0002-3897-0278': 14, '0000-0002-1181-5112': 12, '0000-0003-1447-9385': 11, '0000-0002-7305-8786': 11, '0000-0002-2655-7806': 10, '0000-0003-3466-5353': 9, '0000-0002-7359-663X': 8, '0000-0003-4600-8668': 6, '0000-0002-1382-7088': 5, '0000-0002-9505-4882': 5, '0000-0003-3667-9900': 4, '0000-0001-9714-6038': 4, '0000-0002-4760-0228': 3, '0000-0003-4188-7915': 3, '0000-0001-9454-0427': 3, '0000-0002-0333-6808': 3, '0000-0003-2134-4964': 3, '0000-0002-6658-047X': 3, '0000-0003-1273-379X': 3, '0000-0002-7047-3183': 3, '0000-0002-1814-9546': 3, '0000-0003-4812-6297': 2, '0000-0001-6597-578X': 2, '0000-0002-5285-9138': 2, '0000-0002-6796-7844': 2, '0000-0002-1130-8698': 2, '0000-0001-8518-8150': 2, '0000-0002-7103-924X': 2, '0000-0002-5407-0202': 1, '0000-0001-6220-8411': 1, '0000-0002-7440-6703': 1, '0000-0002-1603-7559': 1, '0000-0003-0257-1707': 1, '0000-0001-8532-6517': 1, '0000-0001-6626-316X': 1, '0000-0002-3246-9861': 1, '0000-0002-7207-4389': 1, '0000-0001-9682-9654': 1, '0000-0002-0196-3832': 1, '0000-0001-8063-6081': 1, '0000-0003-2037-3333': 1, '0000-0001-8872-6751': 1})\n",
      "Total author before apply threshoid:  57\n",
      "Total author after apply threshoid:  3\n",
      "Total sample size after apply threshold:  504\n",
      "Total missing sample:  0\n",
      "(504, 101)\n",
      "Total missing sample:  47\n",
      "(504, 101)\n",
      "Labeled:  504  :  504\n",
      "(504, 103)\n",
      "(504, 103)\n",
      "k_kim is multi-class case, ignored\n",
      "For name:  d_ricci\n",
      "(40, 2)\n",
      "d_ricci  pass\n",
      "For name:  s_cameron\n",
      "(66, 2)\n",
      "s_cameron  pass\n",
      "For name:  t_wright\n",
      "(31, 2)\n",
      "t_wright  pass\n",
      "For name:  r_cunha\n",
      "(209, 2)\n",
      "r_cunha  pass\n",
      "For name:  s_fuchs\n",
      "(32, 2)\n",
      "s_fuchs  pass\n",
      "For name:  m_nawaz\n",
      "(9, 2)\n",
      "m_nawaz  pass\n",
      "For name:  k_harris\n",
      "(47, 2)\n",
      "k_harris  pass\n",
      "For name:  r_daniel\n",
      "(173, 2)\n",
      "r_daniel  pass\n",
      "For name:  k_xu\n",
      "(37, 2)\n",
      "k_xu  pass\n",
      "For name:  s_antunes\n",
      "(54, 2)\n",
      "s_antunes  pass\n",
      "For name:  k_cho\n",
      "(126, 2)\n",
      "k_cho  pass\n",
      "For name:  j_sanderson\n",
      "(31, 2)\n",
      "j_sanderson  pass\n",
      "For name:  s_uddin\n",
      "(39, 2)\n",
      "s_uddin  pass\n",
      "For name:  a_batista\n",
      "(48, 2)\n",
      "a_batista  pass\n",
      "For name:  h_pereira\n",
      "(70, 2)\n",
      "h_pereira  pass\n",
      "For name:  a_patel\n",
      "(262, 2)\n",
      "a_patel  pass\n",
      "For name:  r_graham\n",
      "(52, 2)\n",
      "r_graham  pass\n",
      "For name:  a_nilsson\n",
      "(42, 2)\n",
      "a_nilsson  pass\n",
      "For name:  m_soto\n",
      "(97, 2)\n",
      "m_soto  pass\n",
      "For name:  g_guidi\n",
      "(37, 2)\n",
      "g_guidi  pass\n",
      "For name:  e_andersson\n",
      "(138, 2)\n",
      "e_andersson  pass\n",
      "For name:  s_reid\n",
      "(132, 2)\n",
      "s_reid  pass\n",
      "For name:  a_maleki\n",
      "(25, 2)\n",
      "a_maleki  pass\n",
      "For name:  j_moon\n",
      "(203, 2)\n",
      "j_moon  pass\n",
      "For name:  t_abe\n",
      "(50, 2)\n",
      "t_abe  pass\n",
      "For name:  x_fu\n",
      "(16, 2)\n",
      "x_fu  pass\n",
      "For name:  f_ortega\n",
      "(368, 2)\n",
      "f_ortega  pass\n",
      "For name:  r_morris\n",
      "(409, 2)\n",
      "r_morris  pass\n",
      "For name:  w_fang\n",
      "(43, 2)\n",
      "w_fang  pass\n",
      "For name:  m_amaral\n",
      "(134, 2)\n",
      "m_amaral  pass\n",
      "For name:  h_song\n",
      "(210, 2)\n",
      "h_song  pass\n",
      "For name:  h_dai\n",
      "(6, 2)\n",
      "h_dai  pass\n",
      "For name:  y_nakajima\n",
      "(12, 2)\n",
      "y_nakajima  pass\n",
      "For name:  t_warner\n",
      "(68, 2)\n",
      "t_warner  pass\n",
      "For name:  s_saha\n",
      "(111, 2)\n",
      "s_saha  pass\n",
      "For name:  j_fernandez\n",
      "(28, 2)\n",
      "j_fernandez  pass\n",
      "For name:  m_pan\n",
      "(146, 2)\n",
      "m_pan  pass\n",
      "For name:  a_simon\n",
      "(117, 2)\n",
      "a_simon  pass\n",
      "For name:  r_freitas\n",
      "(73, 2)\n",
      "r_freitas  pass\n",
      "For name:  c_yun\n",
      "(284, 2)\n",
      "c_yun  pass\n",
      "For name:  j_huang\n",
      "(443, 2)\n",
      "j_huang  pass\n",
      "For name:  p_santos\n",
      "(92, 2)\n",
      "p_santos  pass\n",
      "For name:  n_young\n",
      "(182, 2)\n",
      "n_young  pass\n",
      "For name:  d_ross\n",
      "(25, 2)\n",
      "d_ross  pass\n",
      "For name:  q_wang\n",
      "(348, 2)\n",
      "q_wang  pass\n",
      "For name:  c_cardoso\n",
      "(52, 2)\n",
      "c_cardoso  pass\n",
      "For name:  j_matthews\n",
      "(65, 2)\n",
      "j_matthews  pass\n",
      "For name:  g_lee\n",
      "(202, 2)\n",
      "g_lee  pass\n",
      "For name:  m_salem\n",
      "(25, 2)\n",
      "m_salem  pass\n",
      "For name:  h_lai\n",
      "(165, 2)\n",
      "h_lai  pass\n",
      "For name:  r_harris\n",
      "(50, 2)\n",
      "r_harris  pass\n",
      "For name:  c_vaughan\n",
      "(83, 2)\n",
      "c_vaughan  pass\n",
      "For name:  e_thompson\n",
      "(181, 2)\n",
      "e_thompson  pass\n",
      "For name:  r_gomes\n",
      "(52, 2)\n",
      "r_gomes  pass\n",
      "For name:  r_bennett\n",
      "(93, 2)\n",
      "r_bennett  pass\n",
      "For name:  m_collins\n",
      "(57, 2)\n",
      "m_collins  pass\n",
      "For name:  m_cowley\n",
      "(132, 2)\n",
      "m_cowley  pass\n",
      "For name:  p_teixeira\n",
      "(213, 2)\n",
      "p_teixeira  pass\n",
      "For name:  c_cox\n",
      "(48, 2)\n",
      "c_cox  pass\n",
      "For name:  s_hsu\n",
      "(204, 2)\n",
      "s_hsu  pass\n",
      "For name:  f_williams\n",
      "(149, 2)\n",
      "f_williams  pass\n",
      "For name:  d_parsons\n",
      "(30, 2)\n",
      "d_parsons  pass\n",
      "For name:  a_choudhury\n",
      "(56, 2)\n",
      "a_choudhury  pass\n",
      "For name:  c_richter\n",
      "(11, 2)\n",
      "c_richter  pass\n",
      "For name:  m_hossain\n",
      "(102, 2)\n",
      "m_hossain  pass\n",
      "For name:  v_alves\n",
      "(24, 2)\n",
      "v_alves  pass\n",
      "For name:  j_becker\n",
      "(177, 2)\n",
      "j_becker  pass\n",
      "For name:  m_soares\n",
      "(247, 2)\n",
      "m_soares  pass\n",
      "For name:  j_yi\n",
      "(29, 2)\n",
      "j_yi  pass\n",
      "For name:  s_khan\n",
      "(193, 2)\n",
      "s_khan  pass\n",
      "For name:  a_rao\n",
      "(93, 2)\n",
      "a_rao  pass\n",
      "For name:  d_cameron\n",
      "(49, 2)\n",
      "d_cameron  pass\n",
      "For name:  c_morgan\n",
      "(43, 2)\n",
      "c_morgan  pass\n",
      "For name:  h_cui\n",
      "(40, 2)\n",
      "h_cui  pass\n",
      "For name:  p_zhang\n",
      "(137, 2)\n",
      "p_zhang  pass\n",
      "For name:  j_fernandes\n",
      "(208, 2)\n",
      "j_fernandes  pass\n",
      "For name:  a_jain\n",
      "(67, 2)\n",
      "a_jain  pass\n",
      "For name:  d_zhang\n",
      "(94, 2)\n",
      "d_zhang  pass\n",
      "For name:  b_huang\n",
      "(48, 2)\n",
      "b_huang  pass\n",
      "For name:  m_chong\n",
      "(43, 2)\n",
      "m_chong  pass\n",
      "For name:  m_cerqueira\n",
      "(41, 2)\n",
      "m_cerqueira  pass\n",
      "For name:  p_yang\n",
      "(227, 2)\n",
      "p_yang  pass\n",
      "For name:  j_marques\n",
      "(183, 2)\n",
      "j_marques  pass\n",
      "For name:  n_ali\n",
      "(14, 2)\n",
      "n_ali  pass\n",
      "For name:  h_ng\n",
      "(109, 2)\n",
      "h_ng  pass\n",
      "For name:  m_viana\n",
      "(139, 2)\n",
      "m_viana  pass\n",
      "For name:  t_inoue\n",
      "(70, 2)\n",
      "t_inoue  pass\n",
      "For name:  b_meyer\n",
      "(92, 2)\n",
      "b_meyer  pass\n",
      "For name:  c_liao\n",
      "(35, 2)\n",
      "c_liao  pass\n",
      "For name:  k_wheeler\n",
      "(28, 2)\n",
      "k_wheeler  pass\n",
      "For name:  m_rizzo\n",
      "(152, 2)\n",
      "m_rizzo  pass\n",
      "For name:  y_shi\n",
      "(67, 2)\n",
      "y_shi  pass\n",
      "For name:  c_luo\n",
      "(78, 2)\n",
      "c_luo  pass\n",
      "For name:  j_arthur\n",
      "(42, 2)\n",
      "j_arthur  pass\n",
      "For name:  m_ansari\n",
      "(34, 2)\n",
      "m_ansari  pass\n",
      "For name:  g_anderson\n",
      "(103, 2)\n",
      "g_anderson  pass\n",
      "For name:  m_hidalgo\n",
      "(279, 2)\n",
      "m_hidalgo  pass\n",
      "For name:  k_jacobsen\n",
      "(113, 2)\n",
      "k_jacobsen  pass\n",
      "For name:  s_kelly\n",
      "(102, 2)\n",
      "s_kelly  pass\n",
      "For name:  s_james\n",
      "(59, 2)\n",
      "s_james  pass\n",
      "For name:  p_persson\n",
      "(80, 2)\n",
      "p_persson  pass\n",
      "For name:  y_tanaka\n",
      "(20, 2)\n",
      "y_tanaka  pass\n",
      "For name:  c_gao\n",
      "(189, 2)\n",
      "c_gao  pass\n",
      "For name:  w_jung\n",
      "(33, 2)\n",
      "w_jung  pass\n",
      "For name:  s_lewis\n",
      "(306, 2)\n",
      "s_lewis  pass\n",
      "For name:  w_han\n",
      "(34, 2)\n",
      "w_han  pass\n",
      "For name:  m_shah\n",
      "(17, 2)\n",
      "m_shah  pass\n",
      "For name:  c_arango\n",
      "(185, 2)\n",
      "c_arango  pass\n",
      "For name:  r_young\n",
      "(361, 2)\n",
      "r_young  pass\n",
      "For name:  r_coleman\n",
      "(34, 2)\n",
      "r_coleman  pass\n",
      "For name:  b_kang\n",
      "(20, 2)\n",
      "b_kang  pass\n",
      "For name:  s_carter\n",
      "(205, 2)\n",
      "s_carter  pass\n",
      "For name:  c_thomas\n",
      "(102, 2)\n",
      "c_thomas  pass\n",
      "For name:  m_gutierrez\n",
      "(32, 2)\n",
      "m_gutierrez  pass\n",
      "For name:  s_moon\n",
      "(85, 2)\n",
      "s_moon  pass\n",
      "For name:  r_pereira\n",
      "(202, 2)\n",
      "r_pereira  pass\n",
      "For name:  a_nielsen\n",
      "(132, 2)\n",
      "a_nielsen  pass\n",
      "For name:  j_conde\n",
      "(84, 2)\n",
      "j_conde  pass\n",
      "For name:  k_wright\n",
      "(59, 2)\n",
      "k_wright  pass\n",
      "For name:  m_parker\n",
      "(280, 2)\n",
      "m_parker  pass\n",
      "For name:  h_huang\n",
      "(224, 2)\n",
      "h_huang  pass\n",
      "For name:  j_terry\n",
      "(57, 2)\n",
      "j_terry  pass\n",
      "For name:  y_xu\n",
      "(137, 2)\n",
      "y_xu  pass\n",
      "For name:  a_melo\n",
      "(48, 2)\n",
      "a_melo  pass\n",
      "For name:  r_doyle\n",
      "(11, 2)\n",
      "r_doyle  pass\n",
      "For name:  m_bernardo\n",
      "(250, 2)\n",
      "m_bernardo  pass\n",
      "For name:  j_soares\n",
      "(49, 2)\n",
      "j_soares  pass\n",
      "For name:  j_richard\n",
      "(179, 2)\n",
      "j_richard  pass\n",
      "For name:  p_robinson\n",
      "(275, 2)\n",
      "Total sample size before apply threshold:  275\n",
      "Counter({'0000-0002-7878-0313': 133, '0000-0002-0736-9199': 119, '0000-0002-3156-3418': 19, '0000-0002-0577-3147': 4})\n",
      "Total author before apply threshoid:  4\n",
      "Total author after apply threshoid:  2\n",
      "Total sample size after apply threshold:  252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing sample:  0\n",
      "(252, 101)\n",
      "Total missing sample:  6\n",
      "(252, 101)\n",
      "Labeled:  252  :  252\n",
      "(252, 103)\n",
      "(252, 103)\n",
      "p_robinson is binary case\n",
      "LR-UB f1:                precision    recall  f1-score   support\n",
      "\n",
      "p_robinson_1       1.00      0.96      0.98        24\n",
      "p_robinson_2       0.96      1.00      0.98        27\n",
      "\n",
      "    accuracy                           0.98        51\n",
      "   macro avg       0.98      0.98      0.98        51\n",
      "weighted avg       0.98      0.98      0.98        51\n",
      "\n",
      "[23  1  0 27]\n",
      "SVM-UB f1:                precision    recall  f1-score   support\n",
      "\n",
      "p_robinson_1       1.00      0.96      0.98        24\n",
      "p_robinson_2       0.96      1.00      0.98        27\n",
      "\n",
      "    accuracy                           0.98        51\n",
      "   macro avg       0.98      0.98      0.98        51\n",
      "weighted avg       0.98      0.98      0.98        51\n",
      "\n",
      "[23  1  0 27]\n",
      "[('p_robinson_1', 0.472636815920398), ('p_robinson_2', 0.527363184079602)]\n",
      "[('p_robinson_1', 5, 0), ('p_robinson_2', 5, 0)]\n",
      "Initial L size:  10\n",
      "Initial U size:  191\n",
      "U size after drawing sample to U prime: 0\n",
      "Initial U prime size:  191\n",
      "P value:  10  N value:  10\n",
      "Iteration 0, strat self label.\n",
      "Iteration 1  h1 new:  [31, 60, 177, 80, 184, 28, 30, 197, 151, 24, 13, 6, 17, 134, 148, 150, 26]  probs:  [0.9266788209134866, 0.9154138300086317, 0.9127705406529802, 0.9073360772777836, 0.904176003684843, 0.8847245130218885, 0.8839966608662329, 0.8804357190461839, 0.9374941656049933, 0.9127147464595119, 0.9064805670883891, 0.8972203358016616, 0.8934507133209533, 0.8882011334678371, 0.8741882885793661, 0.8738247226804531, 0.873572324559562]\n",
      "Iteration 1  h2 new:  [43, 112, 106, 93, 143, 25, 99, 158, 192, 20, 69, 145, 3, 84, 57, 85, 52, 18, 129, 194]  probs:  [0.9824665543375816, 0.9645549745637982, 0.9483143984504531, 0.9482470705352412, 0.9466755767366206, 0.9320952932110999, 0.9254291493465528, 0.9254031767797776, 0.9237490648599664, 0.9234405590577113, 0.9789597270261505, 0.9694153200134349, 0.9505919675227976, 0.9255366305492014, 0.9237731556086964, 0.9173616217218471, 0.9156583687105961, 0.914286700946806, 0.9099305552973977, 0.9095070946083286]\n",
      "Iteration 2  h1 new:  [62, 133, 130, 140, 122, 108, 27, 187, 76, 68, 179, 101, 165, 125, 91]  probs:  [0.9791685376551412, 0.9724526020315171, 0.9688458456866637, 0.9654499840894263, 0.9652694690686905, 0.9652567169088613, 0.9629642672332441, 0.9590057725888997, 0.9835292125893618, 0.9768981748936507, 0.9673494133497914, 0.9629437562145646, 0.9627656664019083, 0.9612329218395345, 0.9594735517329605]\n",
      "Iteration 2  h2 new:  [74, 21, 16, 104, 12, 23, 94, 189, 161, 87, 141, 182, 138, 109, 78, 70, 63, 64, 156, 196]  probs:  [0.985004936471869, 0.984870214827746, 0.9813674875955749, 0.9807779335558848, 0.9793355453554278, 0.9759966541404806, 0.9720819820518553, 0.9698472388198269, 0.9679045925736603, 0.9676157023740746, 0.9859706434129232, 0.9844232345670694, 0.9839569079894532, 0.9808026261405728, 0.980238048933739, 0.9799407887622535, 0.9795011073416141, 0.9787565810994736, 0.9783799517805782, 0.97742642047675]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "# loop through all files in directory add name to name list\n",
    "fileDir = \"../Data/\"+Dataset+\"/canopies_labeled/\"\n",
    "listfiles = os.listdir(fileDir)\n",
    "\n",
    "init_labeled_size = 10\n",
    "SLCM = 10\n",
    "SLP=10\n",
    "SLN=10\n",
    "\n",
    "co_lr_diff_embedding_final_result = collections.defaultdict(list)\n",
    "\n",
    "#---------------- load different embedding combination ---------------#\n",
    "for v1_emb, v2_emb in zip(pp_text, pp_citation):\n",
    "    # read embeddings\n",
    "    print(\"Load text embedding: \", v1_emb)\n",
    "    viewone_text_emb = com_func.read_text_embedding(emb_type=v1_emb, training_size = \"140k\")\n",
    "    print(\"Load citation embedding: \", v2_emb)\n",
    "    viewtwo_citation_embedding = com_func.read_citation_embedding_sorted(emb_type = v2_emb, labeled_only = True)\n",
    "    # print(viewone_text_emb[0])\n",
    "    # print(viewtwo_citation_embedding[0])\n",
    "    \n",
    "    threshold_change_all_method_f1s = collections.defaultdict(list)\n",
    "    # -------------- different threshold (step by 10) -----------------------#\n",
    "    for step_threshold in range(threshold_lower, threshold_upper, 10):\n",
    "        plot_save_path = \"../../plot/3_co_train_detail_plots/threshold=\"+str(step_threshold)+\"/V1=\"+v1_emb+\"_V2=\"+v2_emb+\"(P=\"+str(SLP)+\"_N=\"+str(SLN)+\"_SLCM=\"+str(SLCM)+\")/\"\n",
    "        #plot_save_path = None\n",
    "        threshold_change_all_method_f1s[\"threshold\"].append(step_threshold)\n",
    "        # collect statistic to output\n",
    "        statistic_detail = collections.defaultdict(list)\n",
    "        total_selected_group = 0\n",
    "        selected_binary_case_group = 0\n",
    "\n",
    "        # ------- different name group in all name group --------------------#\n",
    "        for file in listfiles:\n",
    "            # group name\n",
    "            temp = file.split(\"_\")\n",
    "            name = str(temp[1]+\"_\"+temp[-1])\n",
    "            print(\"For name: \",name)\n",
    "            # read label (pid : author ORCID) from file\n",
    "            data = com_func.read_pid_aid(fileDir+file)\n",
    "            labeled_mask = data[\"authorID\"] != \"-1\"\n",
    "            labeled_data = data[labeled_mask]\n",
    "            print(labeled_data.shape)\n",
    "            # ---------------- collect all labeled sample -------------------- #\n",
    "            # ---------------- if use all samples as negative --------------- #\n",
    "            all_labeled_samples = labeled_data[\"paperID\"].tolist()\n",
    "            authorCounter = com_func.select_productive_groups(labeled_data=labeled_data, \n",
    "                                                              threshold=threshold_select_name_group)\n",
    "            ''' \n",
    "            Case 1: no author under this name have written more than threshold number of papers, dataset not used\n",
    "            Case 2: only one author under this name written more than threshold number of papers, dataset not used\n",
    "            Case 3: 2 or more author under this name written more than threshold number of papers, dataset used\n",
    "            '''\n",
    "            if(len(authorCounter)==0) or (len(authorCounter)==1):\n",
    "                print(name,\" pass\")\n",
    "            else:\n",
    "                total_selected_group+= 1\n",
    "                '''\n",
    "                Case 1: All papers under name group is used. We include authors written less than threshold of paper,\n",
    "                        and treat it as negative class. This will be OVR.(Not used)\n",
    "                Case 2: All papers under name group is used. We include authors written less than threshold of paper,\n",
    "                        and perform muti-class classification (Not used)\n",
    "                Case 3: Only Include author with more than threshold number of paper and perform muti-class classification (Not used)\n",
    "                Case 5: Only Include author with more than threshold number of paper and perform OVR(used)\n",
    "                Case 4: Only Include author with more than threshold number of paper and only select binary case(used)\n",
    "                '''\n",
    "                if apply_threshold_to_name_group_samples == True:\n",
    "                    # ---------- only use sample pass threshold ------- #\n",
    "                    #-------- only select authors in name group are very productive (more than threshold)---------#\n",
    "                    labeled_data, author_list, _= com_func.only_select_productive_authors(labeled_data, step_threshold)\n",
    "                    # ----------------- if use filtered samples as negative  --------- #\n",
    "                    filtered_all_labeled_samples = labeled_data[\"paperID\"].tolist()\n",
    "                else:\n",
    "                    # ----------- use all sample in name group --------- #\n",
    "                    author_list = com_func.productive_authors_list(labeled_data, step_threshold)\n",
    "                    print(name, \" name group sample size: \",labeled_data.shape)\n",
    "                # -------------- extract all samples for name group -------------- #\n",
    "                # for each name group\n",
    "                # read in labeled data\n",
    "                labeled_viewone_text = com_func.extract_sorted_embedding(viewone_text_emb, labeled_data[\"paperID\"])\n",
    "                print(labeled_viewone_text.shape)\n",
    "                labeled_viewtwo_citation = com_func.extract_sorted_embedding(viewtwo_citation_embedding, labeled_data[\"paperID\"])\n",
    "                print(labeled_viewtwo_citation.shape)\n",
    "                print(\"Labeled: \",len(labeled_viewone_text), \" : \", len(labeled_viewtwo_citation))\n",
    "                # ---------------- shuffle the data ----------------- #\n",
    "                labeled_data = labeled_data.sample(frac=1).reset_index(drop=True)\n",
    "                ''' Alignment and deal with missing data rows: \n",
    "                Case 1: fill missing data with 0, using labeled_viewtwo_citation.fillna(0, inplace=True)\n",
    "                Case 2: fill missing data with average of other data\n",
    "                Case 3: drop the sample contains missing data\n",
    "                '''\n",
    "                labeled_viewone_text = pd.merge(labeled_data, labeled_viewone_text, left_on=\"paperID\", right_on = [0], how = \"left\")\n",
    "                labeled_viewtwo_citation = pd.merge(labeled_data, labeled_viewtwo_citation, left_on=\"paperID\", right_on = [0], how = \"left\")\n",
    "                #rows_with_nan = [index for index, row in labeled_viewtwo_citation.iterrows() if row.isnull().any()]\n",
    "                #print(rows_with_nan)\n",
    "                #labeled_viewone_text = labeled_viewone_text.drop(rows_with_nan).reset_index(drop=True)\n",
    "                #labeled_viewtwo_citation = labeled_viewtwo_citation.drop(rows_with_nan).reset_index(drop=True)\n",
    "                labeled_viewtwo_citation.fillna(0, inplace=True)\n",
    "                unique_labels = labeled_viewone_text.authorID.unique()\n",
    "                map_dict = {}\n",
    "                for idx, unique_label in enumerate(unique_labels):\n",
    "                    map_dict[unique_label] = name+\"_\"+str(idx+1)\n",
    "                true_label = labeled_viewone_text[\"authorID\"].replace(map_dict)\n",
    "                \n",
    "                print(labeled_viewone_text.shape)\n",
    "                print(labeled_viewtwo_citation.shape)\n",
    "                '''\n",
    "                only work on binary case, ignored multi-class case\n",
    "                We need to check whether the name group only contain binary case or not\n",
    "                '''\n",
    "                if (len(author_list)==2):\n",
    "                    #if name in [\"p_robinson\",\"t_smith\",\"d_richardson\",\"y_wang\",\"w_lee\",\"k_becker\"]:\n",
    "                    #   print(name, \" Pass for error checking\")\n",
    "                    #    continue\n",
    "                    selected_binary_case_group +=1\n",
    "                    print(name + \" is binary case\")\n",
    "                    viewone_text_final = labeled_viewone_text.drop([\"paperID\", \"authorID\", 0], axis=1)\n",
    "                    viewtwo_citation_final = labeled_viewtwo_citation.drop([\"paperID\", \"authorID\", 0], axis=1)\n",
    "                    \n",
    "                    ''' Apply different algorithm:\n",
    "                    Part 1: Basic supervised algorithm \n",
    "                    Part 2: Basic co-training algorithm \n",
    "                    Part 3: 2 clf co-training \n",
    "                    Part 4: Improved co-training algorithm (Self-proposed with all different improvement)\n",
    "                    '''\n",
    "                    # -------------------- part 1 ------------------------- #\n",
    "                    LR_clf = LogisticRegression(solver= \"liblinear\")\n",
    "                    SVM_clf = SVC(gamma=\"auto\", kernel='linear')\n",
    "                    # -------------------- part 2 ------------------------- #\n",
    "                    initial_cotrain_parameters = {\"p\":SLP,\"n\":SLN,\"k\":30}\n",
    "                    co_LR_clf = Co_training_clf(clf1=LogisticRegression(solver= \"liblinear\"),**initial_cotrain_parameters)\n",
    "                    ''' For co-training with SVM\n",
    "                    Case 1: Using Scikit-learn where we set probability=True\n",
    "                    Case 1 details: Scikit-learn uses LibSVM internally, and this in turn uses Platt scaling\n",
    "                    Platt scaling requires first training the SVM as usual, then optimizing parameter \n",
    "                    vectors A and B such that: P(y|X) = 1 / (1 + exp(A * f(X) + B))\n",
    "                    where f(X) is the signed distance of a sample from the hyperplane\n",
    "                    (scikit-learn's decision_function method).\n",
    "                    Case 2: Using decision_function to get signed distance of sample from the hyperplane, calculate proba\n",
    "                    Case 2 details: use sigmoid (binary)/ softmax (muti-class) for calculate probability based on \n",
    "                    signed distance of sample from the hyperplane. Then follow same step as case 1. The result is relatively\n",
    "                    bad\n",
    "                    '''\n",
    "                    # ----------- Case 1 Using Scikit-learn where we set probability=True ------ #\n",
    "                    co_SVM_clf = Co_training_clf(clf1=SVC(gamma=\"auto\", kernel='linear',probability=True),**initial_cotrain_parameters)\n",
    "                    # ------- Case 2 Using decision_function get distance, calculate proba------ #\n",
    "                    #co_svm_clf = Co_training_clf(clf1=SVC(gamma=\"auto\", kernel='linear'),**initial_cotrain_parameters)\n",
    "                    #co_train_clfs = [(co_svm_clf,\"co_train_SVM\")]\n",
    "                    # -------------------- part 3 ------------------------- #\n",
    "                    # two different clf with basic co-training\n",
    "                    co_LR_SVM_clf = Co_training_clf(clf1 = LogisticRegression(solver= \"liblinear\"),\n",
    "                                                    clf2 = SVC(gamma=\"auto\", kernel='linear',probability=True),\n",
    "                                                    **initial_cotrain_parameters)\n",
    "                    # -------------------- part 4 ------------------------- #\n",
    "                    improved_cotrain_parameters = {\"view_num\":2,\"sl_class_max\":SLCM,\"k\":30}\n",
    "                    improved_co_LR = Improved_co_training_clf(clf = [LogisticRegression(solver= \"liblinear\")], \n",
    "                                                              **improved_cotrain_parameters)\n",
    "                    # -------------------- train together ----------------- #\n",
    "                    baseline_clfs = [(LR_clf,\"LR\"),(SVM_clf,\"SVM\")]\n",
    "                    final_co_train_clfs = [(co_LR_clf,\"CO_LR\"), (co_SVM_clf,\"CO_SVM\"),\n",
    "                                           (co_LR_SVM_clf, \"CO_LR_SVM\"), (improved_co_LR, \"ICO_LR\")]\n",
    "                    #baseline_clfs = [(LR_clf,\"LR\")]\n",
    "                    #final_co_train_clfs = [(co_LR_clf,\"co_LR\"),(improved_co_LR, \"Improved_co_LR\")]\n",
    "                    final_f1_score, cv_per_fold_status= k_fold_cv_all_algorithm(dv1=viewone_text_final,\n",
    "                                                                                dv2=viewtwo_citation_final,\n",
    "                                                                                label=true_label,\n",
    "                                                                                init_labeled_size=init_labeled_size,\n",
    "                                                                                muti_view_clf=final_co_train_clfs,\n",
    "                                                                                combined_clf=baseline_clfs,\n",
    "                                                                                num_fold=5,\n",
    "                                                                                dataset_name=name,\n",
    "                                                                                plot_save_path=plot_save_path)\n",
    "                    #print(final_f1_score)\n",
    "                    #print(cv_per_fold_status)\n",
    "                    statistic_detail['Name'].append(name)\n",
    "                    statistic_detail['Total_sample_size'].append(len(true_label))\n",
    "                    statistic_detail['Train_size'].append(cv_per_fold_status[0][\"train_size\"])\n",
    "                    statistic_detail['Test_size'].append(cv_per_fold_status[0][\"test_size\"])\n",
    "                    statistic_detail[\"All_fold_details\"].append(cv_per_fold_status)\n",
    "                    if len(final_co_train_clfs)!=0:\n",
    "                        statistic_detail['Unlabel_size'].append(cv_per_fold_status[0][\"unlabel_size\"])\n",
    "                        statistic_detail['Validation_size'].append(cv_per_fold_status[0][\"validation_size\"])\n",
    "                        for clf, clf_name in final_co_train_clfs:\n",
    "                            statistic_detail[clf_name+'_total_self_labeled'].append(cv_per_fold_status[0][clf_name+\"_total_self_labeled\"])\n",
    "\n",
    "                    for clf_name, clf_f1 in final_f1_score:\n",
    "                        statistic_detail[clf_name+\"_f1\"].append(clf_f1)\n",
    "                else:\n",
    "                    print(name+ \" is multi-class case, ignored\")\n",
    "\n",
    "        # print(statistic_detail)\n",
    "        print(\"Total number of selected group:\",total_selected_group)\n",
    "        print(\"Total number of selected binary group:\",selected_binary_case_group)\n",
    "        print(\"Total number of selected muti-class group:\",(total_selected_group-selected_binary_case_group))\n",
    "        # write evaluation result to excel\n",
    "        output = pd.DataFrame(statistic_detail)\n",
    "        print(output)\n",
    "        savePath = \"../../result/\"+Dataset+\"/3_co_train_sample=140k/\"\n",
    "        filename = \"(P=\"+str(SLP)+\"_N=\"+str(SLN)+\"_SLCM=\"+str(SLCM)+\"_init_labeled_size=\"+str(init_labeled_size)+\") V1=\"+v1_emb+\"_V2=\"+v2_emb+\"_threshold=\"+str(step_threshold)+\".csv\"\n",
    "        com_func.write_csv_df(savePath, filename, output)\n",
    "        print(\"v1:\",v1_emb,\" v2:\",v2_emb, \"threshold\",step_threshold,\" Done\")\n",
    "        \n",
    "        '''Save result with respect to threshold change'''\n",
    "        threshold_change_all_method_f1s['Name'].append(statistic_detail['Name'])\n",
    "        threshold_change_all_method_f1s['All_details'].append(statistic_detail[\"All_fold_details\"])\n",
    "        for col in output.columns: \n",
    "            if \"f1\" in col:\n",
    "                threshold_change_all_method_f1s[col].append(statistic_detail[col])\n",
    "\n",
    "    co_lr_diff_embedding_final_result[\"v1:\"+v1_emb+\" v2:\"+v2_emb].append(threshold_change_all_method_f1s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-07T18:24:47.784727Z",
     "start_time": "2020-08-07T18:24:14.284Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots: 1. All method per fold variance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T01:28:55.181521Z",
     "start_time": "2020-07-08T01:28:03.301Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_author_details = co_lr_diff_embedding_final_result['v1:pv_dbow v2:n2v'][0]['All_details'][0]\n",
    "\n",
    "cotrain_all_per_fold_result= {}\n",
    "\n",
    "for author in all_author_details:\n",
    "    per_fold_f1 = collections.defaultdict(list)\n",
    "    for per_fold_details in author:\n",
    "        #print(per_fold_details)\n",
    "        for key in per_fold_details:\n",
    "            if key in ['co_LR f1','co_SVM f1','co_LR_SVM f1','Improved_co_LR f1','LR-LB f1','SVM-LB f1','LR-UB f1','SVM-UB f1']:\n",
    "                per_fold_f1[key].append(per_fold_details[key])\n",
    "    cotrain_all_per_fold_result[author[0]['author']]=(per_fold_f1)\n",
    "#print(cotrain_all_per_fold_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T01:28:55.184200Z",
     "start_time": "2020-07-08T01:28:03.348Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name_list = list(cotrain_all_per_fold_result.keys())\n",
    "method_list = []\n",
    "f1_mean = collections.defaultdict(list)\n",
    "f1_min = collections.defaultdict(list)\n",
    "f1_max = collections.defaultdict(list)\n",
    "for author, author_result in cotrain_all_per_fold_result.items():\n",
    "    #print(author)\n",
    "    for method, method_result in author_result.items():\n",
    "        if method.replace(' f1','') not in method_list:\n",
    "            method_list.append(method.replace(' f1',''))\n",
    "        f1_mean[method.replace(' f1','')].append(np.mean(method_result))\n",
    "        f1_min[method.replace(' f1','')].append(np.min(method_result))\n",
    "        f1_max[method.replace(' f1','')].append(np.max(method_result))\n",
    "        #print(method_result, \" mean: \", np.mean(method_result), \" min: \",np.min(method_result),\" max: \", np.max(method_result))\n",
    "print(name_list)\n",
    "print(method_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T01:28:55.186465Z",
     "start_time": "2020-07-08T01:28:03.394Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = name_list\n",
    "random_color = np.random.rand(len(method_list),3)\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "for idx, method in enumerate(method_list):\n",
    "    y = f1_mean[method]\n",
    "    y_min = f1_min[method]\n",
    "    y_max = f1_max[method]\n",
    "    \n",
    "    plt.plot(x, y, 'k', color = random_color[idx], label=method)\n",
    "    plt.fill_between(x, y_min, y_max, alpha=0.5, edgecolor = random_color[idx], facecolor=random_color[idx])\n",
    "    plt.xticks(rotation=45, horizontalalignment='center')\n",
    "\n",
    "legend = plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=2)\n",
    "plt.xlabel('Datasets')\n",
    "plt.ylabel('F1 score')\n",
    "#plt.savefig(plot_save_path+\"/all_method_result_variance.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T01:28:55.188740Z",
     "start_time": "2020-07-08T01:28:03.440Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = name_list\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "\n",
    "for idx, method in enumerate(method_list):\n",
    "    y = f1_mean[method]\n",
    "    y_min_error = [a - b for a, b in zip(f1_mean[method], f1_min[method])]\n",
    "    y_max_error = [a - b for a, b in zip(f1_max[method], f1_mean[method])]\n",
    "    y_error = [y_min_error,y_max_error]\n",
    "\n",
    "    (_, caps, _) = ax.errorbar(x, y,yerr=y_error, fmt='-o',capsize=10, label=method)\n",
    "    for cap in caps:\n",
    "        cap.set_markeredgewidth(1)\n",
    "    plt.xticks(rotation=45, horizontalalignment='center')\n",
    "\n",
    "legend = plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=2)\n",
    "plt.xlabel('Datasets')\n",
    "plt.ylabel('F1 score')\n",
    "#plt.savefig(plot_save_path+\"/all_method_result_variance.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Plot: Each method per fold variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T01:28:55.191178Z",
     "start_time": "2020-07-08T01:28:03.502Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for idx, method in enumerate(method_list):\n",
    "    plt.figure(figsize=(15,10))\n",
    "    x = name_list\n",
    "    y = f1_mean[method]\n",
    "    y_min = f1_min[method]\n",
    "    y_max = f1_max[method]\n",
    "    \n",
    "    plt.plot(x, y, 'k', color = random_color[idx],marker='o', label=method)\n",
    "    plt.fill_between(x, y_min, y_max, alpha=0.5, edgecolor = random_color[idx], facecolor=random_color[idx])\n",
    "    plt.xticks(rotation=45, horizontalalignment='center')\n",
    "    legend = plt.legend()\n",
    "    plt.xlabel('Datasets')\n",
    "    plt.ylabel('F1 score')\n",
    "    #plt.savefig(plot_save_path+\"/\"+method+\"_result_variance.png\", dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T01:28:55.193593Z",
     "start_time": "2020-07-08T01:28:03.548Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for idx, method in enumerate(method_list):\n",
    "    fig, ax = plt.subplots(figsize=(15,10))\n",
    "    x = name_list\n",
    "    y = f1_mean[method]\n",
    "    y_min_error = [a - b for a, b in zip(f1_mean[method], f1_min[method])]\n",
    "    y_max_error = [a - b for a, b in zip(f1_max[method], f1_mean[method])]\n",
    "    y_error = [y_min_error,y_max_error]\n",
    "\n",
    "    (_, caps, _) = ax.errorbar(x, y,yerr=y_error, fmt='-o',capsize=10, label=method)\n",
    "    for cap in caps:\n",
    "        cap.set_markeredgewidth(1)\n",
    "    plt.xticks(rotation=45, horizontalalignment='center')\n",
    "    legend = plt.legend()\n",
    "    plt.xlabel('Datasets')\n",
    "    plt.ylabel('F1 score')\n",
    "    #plt.savefig(plot_save_path+\"/\"+method+\"_result_variance.png\", dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T01:28:55.195963Z",
     "start_time": "2020-07-08T01:28:03.583Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#         # --------------- plot overall result f1 variance --------------- #\n",
    "#         all_per_fold_f1_score_variance_plot = pd.DataFrame(all_per_fold_f1_score_variance)\n",
    "#         ax = sns.boxplot(x=\"author\", y=\"f1\", data=all_per_fold_f1_score_variance_plot)\n",
    "#         ax = sns.swarmplot(x=\"author\", y=\"f1\", data=all_per_fold_f1_score_variance_plot, color=\".25\")\n",
    "#         plt.savefig(plot_save_path+\"all_result_variance.png\", dpi=300)\n",
    "#         # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
