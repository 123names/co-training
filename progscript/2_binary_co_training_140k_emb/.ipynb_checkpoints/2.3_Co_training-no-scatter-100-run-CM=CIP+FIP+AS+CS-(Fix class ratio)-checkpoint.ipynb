{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T23:21:06.691252Z",
     "start_time": "2020-07-10T23:21:05.110978Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T23:21:06.719288Z",
     "start_time": "2020-07-10T23:21:06.699278Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import logging\n",
    "# logging.basicConfig(filename='./test.log', level=logging.DEBUG, \n",
    "#                     format='%(asctime)s %(levelname)s %(name)s %(message)s')\n",
    "# logger=logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T23:21:06.739938Z",
     "start_time": "2020-07-10T23:21:06.729505Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from IPython.core.magic import register_cell_magic\n",
    "\n",
    "# @register_cell_magic('handle')\n",
    "# def handle(line, cell):\n",
    "#     try:\n",
    "#         exec(cell)\n",
    "#     except Exception as e:\n",
    "#         logger.error(e)\n",
    "#         raise # if you want the full trace-back in the notebook\n",
    "\n",
    "\n",
    "# use %%handle when want to output error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T23:21:06.779274Z",
     "start_time": "2020-07-10T23:21:06.749668Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time, os, fnmatch, shutil\n",
    "t = time.localtime()\n",
    "timestamp = time.strftime('%Y-%b-%d-%H-%M-%S', t)\n",
    "print(timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co-training\n",
    "\n",
    "For visualization of co-training process, we apply PCA to feature before training. This will make co-training process clear, but the result will be not accuracy because apply PCA will loss lots of information.\n",
    "\n",
    "1. We assume only part of label exist\n",
    "\n",
    "2. We only select binary case (Only when one name indicate two and only two author)\n",
    "\n",
    "3. When we apply 10 fold with co-training, each fold of first iteration will be baseline compare to co-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-14T02:52:47.749578Z",
     "start_time": "2020-08-14T02:52:47.699473Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "#warnings.filterwarnings('error')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import com_func\n",
    "\n",
    "#----- threshold for selecting set of name group -----------#\n",
    "threshold_select_name_group = 100\n",
    "#----- threshold for selecting min sample in name group ----#\n",
    "threshold_lower = 100\n",
    "threshold_upper = 110\n",
    "\n",
    "apply_threshold_to_name_group_samples = True\n",
    "\n",
    "pp_text = [\"pv_dbow\"]\n",
    "pp_citation = [\"n2v\"]\n",
    "\n",
    "Dataset = \"pubmed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Co-training details\n",
    "1. Basic co-training algorithm required parameter p,n,k,u. Since we have 15 different dataset, we will assume p and n is 1, k is 30. (We are simulate real world situation where we do not know the distribution of unlabeled data amount 15 different dataset)\n",
    "2. We set the parameter u as size of input train data (labeled+unlabeled) since our ublabeled data is not that large.\n",
    "3. During co-training process, the confidence measure is using probability as confident score to evaluate whether unlabel sample should be label or not.\n",
    "4. Probability only have few issues: \n",
    "Case 1: View diagreement in early iteration. The view could disagree with each other in early stage of co-training which will introduce noise; Case 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-14T02:53:00.817460Z",
     "start_time": "2020-08-14T02:52:48.085561Z"
    },
    "code_folding": [
     12,
     25,
     29,
     32,
     48,
     60,
     61,
     87,
     130,
     137,
     140,
     188,
     277,
     338,
     393,
     470,
     516,
     525
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import warnings\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import lines\n",
    "from adjustText import adjust_text\n",
    "from sklearn.metrics import precision_score, recall_score,f1_score,accuracy_score\n",
    "from collections import defaultdict\n",
    "\n",
    "# create co training classifier\n",
    "class Co_training_clf(object):\n",
    "    \n",
    "    def __init__(self, clf1, clf2=None, u = 75, k=30):\n",
    "        \n",
    "        self.clf1 = clf1\n",
    "        # assume co_training on one classifier\n",
    "        if clf2 == None:\n",
    "            self.clf2 = copy.deepcopy(clf1)\n",
    "        else:\n",
    "            self.clf2 = clf2\n",
    "        # size of pool of unlabeled samples\n",
    "        self.u = u\n",
    "        # number of iteration\n",
    "        self.k = k\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=1)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return np.around(1 / (1 + np.exp(-x)), decimals=4).item()\n",
    "\n",
    "    def init_L_U_U_prime(self, labels):\n",
    "        # index of the samples that are initially labeled\n",
    "        L = labels.index[labels[\"authorID\"] != -1].tolist()\n",
    "        # index of unlabeled samples\n",
    "        U = labels.index[labels[\"authorID\"] == -1].tolist()\n",
    "        print(\"Initial L size: \", len(L))\n",
    "        print(\"Initial U size: \", len(U))\n",
    "        # random drawing sample from U\n",
    "        random.shuffle(U)\n",
    "        U_prime = U[-min(len(U), self.u):]\n",
    "        # remove the samples in U_prime from U\n",
    "        U = U[:-len(U_prime)]\n",
    "        print(\"U size after drawing sample to U prime:\",len(U))\n",
    "        print(\"Initial U prime size: \", len(U_prime))\n",
    "        return L, U, U_prime\n",
    "    \n",
    "    def check_iter_label_mapping(self, iter_clf1, iter_clf2):\n",
    "        '''\n",
    "        In theory, it shouldn't occur that label not mapping since it trained on same dataset but different view\n",
    "        But add a check to make sure it won't occur and save the class mapping for later label unlabeled sample\n",
    "        '''\n",
    "        dv1_class_label = iter_clf1.classes_\n",
    "        dv2_class_label = iter_clf2.classes_\n",
    "        if all(dv1_class_label == dv2_class_label):\n",
    "            self.classes_ = dv1_class_label\n",
    "        else:\n",
    "            sys.exit(\"Two view classifier label not mapping\")\n",
    "\n",
    "    def get_confidence_score(self, clf_h1, clf_h2, dv1, dv2):\n",
    "        if hasattr(clf_h1, \"predict_proba\"):\n",
    "            dv1_proba = clf_h1.predict_proba(dv1)\n",
    "            dv2_proba = clf_h2.predict_proba(dv2)\n",
    "        elif hasattr(clf_h1, \"decision_function\"):    # use decision function\n",
    "            dv1_distance = np.array(clf_h1.decision_function(dv1))\n",
    "            dv2_distance = np.array(clf_h2.decision_function(dv2))\n",
    "            # if binary case, use sigmoid function to calculate probability\n",
    "            if iter_train_label.nunique()==2:\n",
    "                dv1_proba = []\n",
    "                dv2_proba = []\n",
    "                for distance in dv1_distance:\n",
    "                    dv1_proba.append([1-self.sigmoid(distance), self.sigmoid(distance)])\n",
    "                for distance in dv2_distance:\n",
    "                    dv2_proba.append([1-self.sigmoid(distance), self.sigmoid(distance)])\n",
    "            else:\n",
    "                dv1_proba = self.softmax(dv1_distance)\n",
    "                dv2_proba = self.softmax(dv2_distance)\n",
    "            dv1_proba = np.array(dv1_proba)\n",
    "            dv2_proba = np.array(dv2_proba)\n",
    "            #print(\"Distance to hyperplane (dv1): \",dv1_distance)\n",
    "            #print(\"Probability (dv1): \",dv1_proba)\n",
    "        else:\n",
    "            sys.exit(\"No confident score for process\")\n",
    "        \n",
    "        return dv1_proba, dv2_proba\n",
    "\n",
    "    def label_p_n_samples(self, proba, rank, proba_sample_idx_map):\n",
    "        U_prime_size = len(proba)\n",
    "        self_trained_sample_idx = []\n",
    "        self_trained_confident = []\n",
    "        for label, confident_rank in enumerate(rank):\n",
    "            # 0 positive sample\n",
    "            if label==0:\n",
    "                p = []\n",
    "                p_confident = []\n",
    "                index = 0\n",
    "                while(len(p) < self.p):\n",
    "                    max_conf_sample_index = confident_rank[index]\n",
    "                    # ---- if positive predict proba is more than 50% ------- #\n",
    "                    if (proba[max_conf_sample_index][label] > 0.5):\n",
    "                        #print('Sample idx: P: ', proba_sample_idx_map[max_conf_sample_index], \" : \", proba[max_conf_sample_index])\n",
    "                        p.append(proba_sample_idx_map[max_conf_sample_index])\n",
    "                        p_confident.append(proba[max_conf_sample_index][label])\n",
    "                    index +=1\n",
    "                    if (index>=U_prime_size):\n",
    "                        break\n",
    "                self_trained_sample_idx.append(p)\n",
    "                self_trained_confident.append(p_confident)\n",
    "            # 1 negative sample\n",
    "            elif label == 1:\n",
    "                n = []\n",
    "                n_confident = []\n",
    "                index = 0\n",
    "                while(len(n) < self.n):\n",
    "                    max_conf_sample_index = confident_rank[index]\n",
    "                    # ---- if negative predict proba is more than 50% ------- #\n",
    "                    if (proba[max_conf_sample_index][label] > 0.5):\n",
    "                        #print('Sample idx: N: ', proba_sample_idx_map[max_conf_sample_index], \" : \", proba[max_conf_sample_index])\n",
    "                        n.append(proba_sample_idx_map[max_conf_sample_index])\n",
    "                        n_confident.append(proba[max_conf_sample_index][label])\n",
    "                    index +=1\n",
    "                    if (index>=U_prime_size):\n",
    "                        break\n",
    "                self_trained_sample_idx.append(n)\n",
    "                self_trained_confident.append(n_confident)\n",
    "            else:\n",
    "                print(\"Class label error\")\n",
    "        return self_trained_sample_idx, self_trained_confident\n",
    "\n",
    "    def get_self_labeled_sample(self):\n",
    "        '''\n",
    "        return:\n",
    "            self-labeled new positive, self-labeled new negative (Index)\n",
    "        '''\n",
    "        return self.self_new_all, self.self_new_at_k\n",
    "\n",
    "    def set_PCA(self, pca=None):\n",
    "        self.pca = pca\n",
    "\n",
    "    def fit(self, train_data, validation_data, dataset_name=None, plot_save_path=None):\n",
    "        # sync input datatype\n",
    "        for idx, item in enumerate(train_data):\n",
    "            if not isinstance(item, pd.DataFrame):\n",
    "                item = pd.DataFrame(item)\n",
    "            item.reset_index(drop=True,inplace=True)\n",
    "            train_data[idx] = item\n",
    "        dv1 = train_data[0]\n",
    "        dv2 = train_data[1]\n",
    "        labels = train_data[-1]\n",
    "        dv1_validation = validation_data[0]\n",
    "        dv2_validation = validation_data[1] \n",
    "        label_validation = validation_data[-1]\n",
    "        # using all unlabeled sample instead of pool of unlabeled sample\n",
    "        self.u = len(labels)\n",
    "        L_plot, U_plot, U_prime_plot = self.init_L_U_U_prime(labels)\n",
    "        L, U, U_prime = ([] for i in range(3)) \n",
    "        #print(\"All data index: \",dv1.index.values)\n",
    "        #print(\"L_plot: \", L_plot)\n",
    "        #print(\"U_plot: \", U_plot)\n",
    "        #print(\"U_prime_plot: \", U_prime_plot)\n",
    "        \n",
    "        # index of self labeled samples\n",
    "        self.self_new_all = defaultdict(list)\n",
    "        self.self_new_at_k = defaultdict(list)\n",
    "        self.h1_new_idx = defaultdict(list)\n",
    "        self.h2_new_idx = defaultdict(list)\n",
    "        # when fit co-train, we collect f1 on validation samples wrt each iteration\n",
    "        self.f1_on_validation_dv1 = []\n",
    "        self.f1_on_validation_dv2 = []\n",
    "        self.f1_on_validation_combined = []\n",
    "        self.iterCounter = 0\n",
    "        ''' train clf with concatenated features for k=0'''\n",
    "        temp_d1 = dv1.iloc[L_plot]\n",
    "        temp_d2 = dv2.iloc[L_plot]\n",
    "        concatenate_data = pd.concat([temp_d1,temp_d2], axis=1, ignore_index=True)\n",
    "        concatenate_vaildation = pd.concat([dv1_validation,dv2_validation], axis=1, ignore_index=True)\n",
    "        concatenate_clf = copy.deepcopy(self.clf1)\n",
    "        concatenate_clf.fit(concatenate_data, labels.iloc[L_plot].values.ravel())\n",
    "        baseline_predict_result = concatenate_clf.predict(concatenate_vaildation)\n",
    "        self.f1_baseline = f1_score(label_validation, baseline_predict_result, average='macro')\n",
    "        # --------- plot PCA reduced plot for initial stage of train -------------- #\n",
    "        pca = self.pca\n",
    "        '''Notice pca will change input datatype dataframe to datatype list, keep it's order,\n",
    "           or re-assign dv1.index.values to the output'''\n",
    "        pca_dv1 = pca.fit_transform(X=dv1)\n",
    "        pca_dv2 = pca.fit_transform(X=dv2)\n",
    "        #loop until we have assigned labels to every sample in U and U_prime or we hit our iteration break condition\n",
    "        while U_prime_plot:\n",
    "            '''\n",
    "            Extract labeled training sample from training sample and train classifier, then make prediction\n",
    "            Evaluate performance of current iteration classifier\n",
    "            '''\n",
    "            # print(\"Iteration:\",self.iterCounter, \" L_plot: \",L_plot)\n",
    "            # print(\"Iteration:\",self.iterCounter, \" U_prime_plot: \",U_prime_plot)\n",
    "            # ------------- get labeled samples for train ----------- # \n",
    "            iter_train_d1 = dv1.iloc[L_plot]\n",
    "            iter_train_d2 = dv2.iloc[L_plot]\n",
    "            iter_train_label = labels.iloc[L_plot]\n",
    "            # ----------- get U_prime unlabeled samples  ------------ #\n",
    "            iter_unlabeled_d1 = dv1.iloc[U_prime_plot]\n",
    "            iter_unlabeled_d2 = dv2.iloc[U_prime_plot]\n",
    "            # ------------ train different view with classifier ----------- #\n",
    "            iter_clf1 = copy.deepcopy(self.clf1) \n",
    "            iter_clf2 = copy.deepcopy(self.clf2)\n",
    "            iter_clf1.fit(iter_train_d1, iter_train_label.values.ravel())\n",
    "            iter_clf2.fit(iter_train_d2, iter_train_label.values.ravel())\n",
    "            self.check_iter_label_mapping(iter_clf1, iter_clf2)\n",
    "            #obtain input data class ratio\n",
    "            temp = labels.iloc[L_plot]['authorID'].value_counts(normalize=True)\n",
    "            data_ratio = []\n",
    "            for classes in self.classes_:\n",
    "                data_ratio.append(temp[classes]*10)\n",
    "            data_ratio = [i/min(data_ratio) for i in data_ratio]\n",
    "            self.p = int(data_ratio[0])\n",
    "            self.n = int(data_ratio[1])\n",
    "            print(\"P value: \", self.p, \" N value: \", self.n)\n",
    "            # ------------- test error on validation data --------------- #\n",
    "            # make prediction on validation data\n",
    "            y1 = iter_clf1.predict(dv1_validation)\n",
    "            y2 = iter_clf2.predict(dv2_validation)\n",
    "            # f1 score on each iteration\n",
    "            f1_dv1 = f1_dv2 = 0\n",
    "            f1_dv1 = f1_score(label_validation, y1, average='macro')\n",
    "            f1_dv2 = f1_score(label_validation, y2, average='macro')\n",
    "            # collect f1 for current iteration\n",
    "            self.f1_on_validation_dv1.append(f1_dv1)\n",
    "            self.f1_on_validation_dv2.append(f1_dv2)\n",
    "            \n",
    "            # ----------- collect the result of different k value ----------- #\n",
    "            temp_d1 = dv1.iloc[L_plot]\n",
    "            temp_d2 = dv2.iloc[L_plot]\n",
    "            temp_validation = [dv1_validation,dv2_validation]\n",
    "            \n",
    "            temp_clf1 = copy.deepcopy(self.clf1) \n",
    "            temp_clf2 = copy.deepcopy(self.clf2)\n",
    "            temp_clf1.fit(temp_d1, labels.iloc[L_plot].values.ravel())\n",
    "            temp_clf2.fit(temp_d2, labels.iloc[L_plot].values.ravel())\n",
    "            \n",
    "            predict_result = self.combined_predict(temp_clf1,temp_clf2,temp_validation)\n",
    "            f1_combined = f1_score(label_validation, predict_result, average='macro')\n",
    "            self.f1_on_validation_combined.append(f1_combined)\n",
    "            \n",
    "            new_train_label = labels.iloc[L_plot]\n",
    "            if len(self.h1_new_idx[\"index\"])==0:\n",
    "                print(\"Iteration 0, strat self label.\")\n",
    "            else:\n",
    "                print(\"Iteration\",self.iterCounter,\" h1 new: \",self.h1_new_idx[\"index\"][-1],\" probs: \", self.h1_new_idx[\"confident\"][-1])\n",
    "                print(\"Iteration\",self.iterCounter,\" h2 new: \", self.h2_new_idx[\"index\"][-1], \" probs: \", self.h2_new_idx[\"confident\"][-1])\n",
    "            ''' \n",
    "            Notice here dv1_proba and dv2_proba's index is index for u' (Unlabeled data only)\n",
    "            We use index of u' to find index (position) of data in U where U and L is all data index\n",
    "            '''\n",
    "            # rank class probabilities for unlabeled sample for it's confidence measure\n",
    "            dv1_proba, dv2_proba = self.get_confidence_score(clf_h1=iter_clf1, clf_h2=iter_clf2, \n",
    "                                                             dv1=iter_unlabeled_d1, dv2=iter_unlabeled_d2)\n",
    "            proba_sample_idx_map = iter_unlabeled_d1.index\n",
    "            dv1_proba_rank = []\n",
    "            dv2_proba_rank = []\n",
    "            # proba1_rank[i] is label i's confidence measure\n",
    "            for class_proba in dv1_proba.T:\n",
    "                dv1_proba_rank.append((-class_proba).argsort())\n",
    "            for class_proba in dv2_proba.T:\n",
    "                dv2_proba_rank.append((-class_proba).argsort())\n",
    "            # print(dv1_proba)\n",
    "            # print(dv1_proba_rank)\n",
    "            # print(dv2_proba)\n",
    "            # print(dv2_proba_rank)\n",
    "            # h1 classifier self label data\n",
    "            h1_new_sample, h1_new_probs = self.label_p_n_samples(dv1_proba, dv1_proba_rank, proba_sample_idx_map)\n",
    "            # h2 classifier\n",
    "            h2_new_sample, h2_new_probs = self.label_p_n_samples(dv2_proba, dv2_proba_rank, proba_sample_idx_map)\n",
    "            \n",
    "            '''Case if h1's new class 1 is h2's new class 2\n",
    "            Example: Iteration 45  h1 new:  [[95], [150]]  probs:  [[0.89], [0.82]]\n",
    "                     Iteration 45  h2 new:  [[147], [95]]  probs:  [[0.89], [0.92]]\n",
    "            '''\n",
    "            for i, h1_class_new in enumerate(h1_new_sample):\n",
    "                temp_pop_list = []\n",
    "                for j, h1_item in enumerate(h1_class_new):\n",
    "                    for k, h2_class_new in enumerate(h2_new_sample):\n",
    "                        for l, h2_item in enumerate(h2_class_new):\n",
    "                            # conflict\n",
    "                            if h1_item == h2_item:\n",
    "                                if h1_new_probs[i][j]> h2_new_probs[k][l]:\n",
    "                                    h2_new_sample[k].pop(l)\n",
    "                                    h2_new_probs[k].pop(l)\n",
    "                                else:\n",
    "                                    temp_pop_list.append(j)\n",
    "                temp_pop_list = list(set(temp_pop_list))\n",
    "                temp_pop_list.sort(reverse=True)\n",
    "                for item_idx in temp_pop_list:\n",
    "                    h1_new_sample[i].pop(item_idx)\n",
    "                    h1_new_probs[i].pop(item_idx)\n",
    "            # collect statistic for plot only (before remove self-labeled sample from u')\n",
    "            iter_h1_for_plot = list(itertools.chain(*h1_new_sample))\n",
    "            iter_h2_for_plot = list(itertools.chain(*h2_new_sample))\n",
    "            iter_h1_prob = list(itertools.chain(*h1_new_probs))\n",
    "            iter_h2_prob = list(itertools.chain(*h2_new_probs))\n",
    "            \n",
    "            self.h1_new_idx[\"index\"].append(iter_h1_for_plot)\n",
    "            self.h1_new_idx[\"confident\"].append(iter_h1_prob)\n",
    "            self.h2_new_idx[\"index\"].append(iter_h2_for_plot)\n",
    "            self.h2_new_idx[\"confident\"].append(iter_h2_prob)\n",
    "            '''\n",
    "            Add most confidence samples as new training samples, auto label the samples and remove it from U_prime\n",
    "            Special Case: if two view classifier give same most confident sample and p is 1, only 1 datapoint self-labeled\n",
    "            '''\n",
    "            roundNew = list(zip(h1_new_sample, h2_new_sample))\n",
    "            roundNew_flatten_unique = []\n",
    "            for label, round_new in enumerate(roundNew):\n",
    "                round_new = set([item for sublist in round_new for item in sublist])\n",
    "                round_new = [idx for idx in round_new]\n",
    "                self.self_new_all[self.classes_[label]].append(round_new)\n",
    "                roundNew_flatten_unique.extend(round_new)\n",
    "                # add label to those new samples\n",
    "                #print(labels[\"authorID\"][round_new])\n",
    "                labels[\"authorID\"][round_new] = self.classes_[label]\n",
    "                #print(labels[\"authorID\"][round_new])\n",
    "                #print(self.classes_[label],\" (u' idx): \",round_new)\n",
    "            #print(roundNew_flatten_unique)\n",
    "            # extend the labeled sample\n",
    "            L_plot.extend(roundNew_flatten_unique)\n",
    "            # remove the labeled sample from U_prime\n",
    "            U_prime_plot = [x for x in U_prime_plot if x not in roundNew_flatten_unique]\n",
    "            #print(U_prime_plot)\n",
    "            # randomly choice 2p+2n examples from u to replenish u_prime\n",
    "            replenishItem = U_plot[-(2*self.p+2*self.n):]\n",
    "            U_prime_plot.extend(replenishItem)\n",
    "            U_plot = U_plot[:-len(replenishItem)]\n",
    "            self.iterCounter +=1\n",
    "            ''' --------- if reach loop condition k, we extract L from L_plot --------- '''\n",
    "            if self.iterCounter <= self.k:\n",
    "                L = copy.deepcopy(L_plot)\n",
    "                U = copy.deepcopy(U_plot)\n",
    "                U_prime = copy.deepcopy(U_prime_plot)\n",
    "                self.self_new_at_k = copy.deepcopy(self.self_new_all)\n",
    "            ''' # ----------- plot the last iteration of co-training process -------------- # '''\n",
    "            if not U_prime_plot:\n",
    "                self.iterCounter +=1\n",
    "                # --------- test error on validation data --------------------- #\n",
    "                # make prediction on validation data\n",
    "                y1 = iter_clf1.predict(dv1_validation)\n",
    "                y2 = iter_clf2.predict(dv2_validation)\n",
    "                # f1 score on each iteration\n",
    "                f1_dv1 = f1_dv2 = 0\n",
    "                f1_dv1 = f1_score(label_validation, y1, average='macro')\n",
    "                f1_dv2 = f1_score(label_validation, y2, average='macro')\n",
    "                # collect f1 for current iteration\n",
    "                self.f1_on_validation_dv1.append(f1_dv1)\n",
    "                self.f1_on_validation_dv2.append(f1_dv2)\n",
    "                \n",
    "                # ----------- collect the result of different k value ----------- #\n",
    "                temp_d1 = dv1.iloc[L_plot]\n",
    "                temp_d2 = dv2.iloc[L_plot]\n",
    "                temp_validation = [dv1_validation,dv2_validation]\n",
    "\n",
    "                temp_clf1 = copy.deepcopy(self.clf1) \n",
    "                temp_clf2 = copy.deepcopy(self.clf2)\n",
    "                temp_clf1.fit(temp_d1, labels.iloc[L_plot].values.ravel())\n",
    "                temp_clf2.fit(temp_d2, labels.iloc[L_plot].values.ravel())\n",
    "\n",
    "                predict_result = self.combined_predict(temp_clf1,temp_clf2,temp_validation)\n",
    "                f1_combined = f1_score(label_validation, predict_result, average='macro')\n",
    "                self.f1_on_validation_combined.append(f1_combined)\n",
    "                \n",
    "                new_train_label = labels.iloc[L_plot]\n",
    "                print(\"Iteration\",self.iterCounter,\" h1 new: \",self.h1_new_idx[\"index\"][-1],\" probs: \", self.h1_new_idx[\"confident\"][-1])\n",
    "                print(\"Iteration\",self.iterCounter,\" h2 new: \", self.h2_new_idx[\"index\"][-1], \" probs: \", self.h2_new_idx[\"confident\"][-1])\n",
    "        \n",
    "        selected_return_iter = self.k\n",
    "        if self.iterCounter <= self.k:\n",
    "            selected_return_iter = self.iterCounter\n",
    "            self.k=self.iterCounter\n",
    "        print(self.k,\"iterations total Labeled number: \", len(L), \" Still unlabeled number: \", len(U_prime))\n",
    "        print(self.iterCounter,\"iteration total Labeled number: \", len(L_plot), \" Still unlabeled number: \", len(U_prime_plot))\n",
    "        counter = Counter(L_plot)\n",
    "        print(counter.most_common(3))\n",
    "        if len(set(L_plot)) == len(L_plot):\n",
    "            print(\"success\")\n",
    "        else:\n",
    "            print(\"duplicate found\")\n",
    "            sys.exit(\"error\")\n",
    "        #print(self.f1_on_validation_dv1)\n",
    "        #print(self.f1_on_validation_dv2)\n",
    "        # final train\n",
    "        newtrain_d1 = dv1.iloc[L]\n",
    "        newtrain_d2 = dv2.iloc[L]\n",
    "        self.clf1.fit(newtrain_d1, labels.iloc[L].values.ravel())\n",
    "        self.clf2.fit(newtrain_d2, labels.iloc[L].values.ravel())\n",
    "        '''\n",
    "        Evalutation plot for co-training process, save f1 score vs number of iteration plot\n",
    "        '''\n",
    "        if (dataset_name != None) and (plot_save_path != None):\n",
    "            # ----------------- plot h1/h2 result separately -------------------- #\n",
    "            default_text_based = [self.f1_on_validation_dv1[0]] * self.iterCounter\n",
    "            default_citation_based = [self.f1_on_validation_dv2[0]] * self.iterCounter\n",
    "            co_train_text_based = self.f1_on_validation_dv1\n",
    "            co_train_citation_based = self.f1_on_validation_dv2\n",
    "            step = np.arange(start=0, stop=self.iterCounter, step=1)\n",
    "            \n",
    "            #print(\"step:\",len(step))\n",
    "            #print(\"default:\",self.iterCounter)\n",
    "            #print(\"co-training v1:\",len(co_train_text_based))\n",
    "            #print(\"co-training v2:\",len(co_train_citation_based))\n",
    "            #print(\"default concatenate:\",self.iterCounter)\n",
    "            #print(\"default combined:\",self.iterCounter)\n",
    "            #print(\"co-training combined:\",len(self.f1_on_validation_combined))\n",
    "\n",
    "            fig = plt.figure(figsize=(8,6))\n",
    "            ax = plt.axes()\n",
    "            plt.plot(step, default_text_based, linestyle='dashed', label=\"Text based default\")\n",
    "            plt.plot(step, default_citation_based, linestyle='dashdot', label=\"Citation based default\")\n",
    "            plt.plot(step, co_train_text_based, linestyle='solid', marker = \"*\", label=\"Text based\")\n",
    "            plt.plot(step, co_train_citation_based, linestyle='dotted', marker = \"+\", label=\"Citation based\")\n",
    "            handles, _  = ax.get_legend_handles_labels()\n",
    "            plt.axvline(x=(selected_return_iter-1), color='r', label=\"k\")\n",
    "            _, labels = ax.get_legend_handles_labels()\n",
    "            vertical_line = lines.Line2D([], [],  marker='|', linestyle='None', color=\"r\", markersize=10, markeredgewidth=1.5)\n",
    "            handles.append(vertical_line)\n",
    "            ax.autoscale_view()\n",
    "            legend = ax.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 1.2), ncol=2)\n",
    "            plt.xlabel('Co-Training Iterations')\n",
    "            plt.ylabel('F1 score')\n",
    "            f1_vs_iteration_plot_name = plot_save_path+dataset_name+\"_co_train_iteration_f1(1).png\"\n",
    "            counter = 1\n",
    "            while os.path.exists(plot_save_path+dataset_name+\"_co_train_iteration_f1(%s).png\" % counter):\n",
    "                counter+=1\n",
    "                f1_vs_iteration_plot_name = plot_save_path+dataset_name+\"_co_train_iteration_f1(%s).png\" % counter\n",
    "            plt.savefig(f1_vs_iteration_plot_name, dpi=150, bbox_extra_artists=(legend,), bbox_inches='tight')\n",
    "            # plt.show()\n",
    "            plt.close(\"all\")\n",
    "            \n",
    "            # -------------------- plot combined result ------------------------- #\n",
    "            default_concatenate = [self.f1_baseline] * self.iterCounter\n",
    "            default_combined = [self.f1_on_validation_combined[0]] * self.iterCounter\n",
    "            co_train_combined = self.f1_on_validation_combined\n",
    "            fig = plt.figure(figsize=(8,6))\n",
    "            ax = plt.axes()\n",
    "            plt.plot(step, default_concatenate, linestyle='dashed', label=\"Concatenate default\")\n",
    "            plt.plot(step, default_combined, linestyle='dotted', label=\"Combined default\")\n",
    "            plt.plot(step, co_train_combined, linestyle='solid', marker = \"*\", label=\"Combined co-train\")\n",
    "            handles, _  = ax.get_legend_handles_labels()\n",
    "            plt.axvline(x=(selected_return_iter-1), color='r', label=\"k\")\n",
    "            _, labels = ax.get_legend_handles_labels()\n",
    "            vertical_line = lines.Line2D([], [],  marker='|', linestyle='None', color=\"r\", markersize=10, markeredgewidth=1.5)\n",
    "            handles.append(vertical_line)\n",
    "            ax.autoscale_view()\n",
    "            legend = ax.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=2)\n",
    "            plt.xlabel('Co-Training Iterations')\n",
    "            plt.ylabel('F1 score')\n",
    "            f1_vs_iteration_plot_name = plot_save_path+dataset_name+\"_co_train_combined_iteration_f1(1).png\"\n",
    "            counter = 1\n",
    "            while os.path.exists(plot_save_path+dataset_name+\"_co_train_combined_iteration_f1(%s).png\" % counter):\n",
    "                counter+=1\n",
    "                f1_vs_iteration_plot_name = plot_save_path+dataset_name+\"_co_train_combined_iteration_f1(%s).png\" % counter\n",
    "            plt.savefig(f1_vs_iteration_plot_name, dpi=150, bbox_extra_artists=(legend,), bbox_inches='tight')\n",
    "            # plt.show()\n",
    "            plt.close(\"all\")\n",
    "        return \"Training Done\"\n",
    "\n",
    "    def co_train_process_f1(self):\n",
    "        return self.f1_on_validation_dv1, self.f1_on_validation_dv2, self.f1_on_validation_combined, self.f1_baseline\n",
    "\n",
    "    def get_iter_count(self):\n",
    "        return self.iterCounter\n",
    "    \n",
    "    def get_k(self):\n",
    "        return self.k\n",
    "\n",
    "    def predict(self, data):\n",
    "        dv1=data[0]\n",
    "        dv2=data[1]\n",
    "        dv1_predict = self.clf1.predict(dv1)\n",
    "        dv2_predict = self.clf2.predict(dv2)\n",
    "        #fill pred with -1 so we can identify the samples in which error occur\n",
    "        y_pred = [\"-1\"] * dv1.shape[0]\n",
    "        for i, (dv1_y, dv2_y) in enumerate(zip(dv1_predict, dv2_predict)):\n",
    "            # if both agree on label\n",
    "            if dv1_y == dv2_y:\n",
    "                y_pred[i] = dv1_y\n",
    "            # If disagree on label and support proba method: We times probability together, choice class have higher probabilities\n",
    "            elif hasattr(self.clf1, \"predict_proba\") and hasattr(self.clf2, \"predict_proba\"):\n",
    "                h1_probas = self.clf1.predict_proba([dv1.iloc[i]])[0]\n",
    "                h2_probas = self.clf2.predict_proba([dv2.iloc[i]])[0]\n",
    "                final_y_probas = [proba_y1 * proba_y2 for (proba_y1, proba_y2) in zip(h1_probas, h2_probas)]\n",
    "                #print(\"h1 and h2 disagree on\",i, \" h1 Proba : \",h1_probas, \" h2 Proba: \", h2_probas)\n",
    "                #print(\"product probas:\",final_y_probas)\n",
    "                max_prob_idx = final_y_probas.index(max(final_y_probas))\n",
    "                y_pred[i] = self.classes_[max_prob_idx]\n",
    "                #print(\"result idx: \", max_prob_idx, \" result: \",y_pred[i])\n",
    "            # If disagree on label and not support proba method: We use decision_function to first calculate probability,\n",
    "            # then we times calculate probability together, choice class have higher probabilities\n",
    "            elif hasattr(self.clf1, \"decision_function\") and hasattr(self.clf2, \"decision_function\"):\n",
    "                dv1_distance = self.clf1.decision_function([dv1.iloc[i]])\n",
    "                dv2_distance = self.clf2.decision_function([dv2.iloc[i]])\n",
    "                if len(self.clf1.classes_)==2:\n",
    "                    h1_probas = np.array([1-self.sigmoid(dv1_distance), self.sigmoid(dv1_distance)])\n",
    "                    h2_probas = np.array([1-self.sigmoid(dv2_distance), self.sigmoid(dv2_distance)])\n",
    "                else:\n",
    "                    h1_probas = self.softmax(dv1_distance)\n",
    "                    h2_probas = self.softmax(dv2_distance)\n",
    "                final_y_probas = [proba_y1 * proba_y2 for (proba_y1, proba_y2) in zip(h1_probas, h2_probas)]\n",
    "                #print(\"h1 and h2 disagree on\",i, \" h1 Proba : \",h1_probas, \" h2 Proba: \", h2_probas)\n",
    "                #print(\"product probas:\",final_y_probas)\n",
    "                max_prob_idx = final_y_probas.index(max(final_y_probas))\n",
    "                y_pred[i] = self.classes_[max_prob_idx]\n",
    "                #print(\"result idx: \", max_prob_idx, \" result: \",y_pred[i])\n",
    "            # disagree and not support any confidence measure\n",
    "            else:\n",
    "                sys.exit(\"Error occur\")\n",
    "\n",
    "        # convert final result to np array\n",
    "        y_pred_np_array = np.asarray(y_pred)\n",
    "        return y_pred_np_array\n",
    "\n",
    "    def predict_proba(self, dv1, dv2):\n",
    "        # the predicted probabilities is simply a product (*) of probabilities given from each classifier trained\n",
    "        h1_probas = self.clf1.predict_proba(dv1)\n",
    "        h2_probas = self.clf2.predict_proba(dv2)\n",
    "        \n",
    "        proba = (h1_probas*h2_probas)\n",
    "        return proba\n",
    "    \n",
    "    ''' ------------- function for optimize parameter k only ---------------- '''\n",
    "    def combined_predict(self, dv1_clf, dv2_clf, data):\n",
    "        dv1=data[0]\n",
    "        dv2=data[1]\n",
    "        dv1_predict = dv1_clf.predict(dv1)\n",
    "        dv2_predict = dv2_clf.predict(dv2)\n",
    "        #fill pred with -1 so we can identify the samples in which error occur\n",
    "        y_pred = [\"-1\"] * dv1.shape[0]\n",
    "        for i, (dv1_y, dv2_y) in enumerate(zip(dv1_predict, dv2_predict)):\n",
    "            # if both agree on label\n",
    "            if dv1_y == dv2_y:\n",
    "                y_pred[i] = dv1_y\n",
    "            # If disagree on label and support proba method: We times probability together, choice class have higher probabilities\n",
    "            elif hasattr(dv1_clf, \"predict_proba\") and hasattr(dv2_clf, \"predict_proba\"):\n",
    "                h1_probas = dv1_clf.predict_proba([dv1.iloc[i]])[0]\n",
    "                h2_probas = dv2_clf.predict_proba([dv2.iloc[i]])[0]\n",
    "                final_y_probas = [proba_y1 * proba_y2 for (proba_y1, proba_y2) in zip(h1_probas, h2_probas)]\n",
    "                #print(\"h1 and h2 disagree on\",i, \" h1 Proba : \",h1_probas, \" h2 Proba: \", h2_probas)\n",
    "                #print(\"product probas:\",final_y_probas)\n",
    "                max_prob_idx = final_y_probas.index(max(final_y_probas))\n",
    "                y_pred[i] = self.classes_[max_prob_idx]\n",
    "                #print(\"result idx: \", max_prob_idx, \" result: \",y_pred[i])\n",
    "            # If disagree on label and not support proba method: We use decision_function to first calculate probability,\n",
    "            # then we times calculate probability together, choice class have higher probabilities\n",
    "            elif hasattr(dv1_clf, \"decision_function\") and hasattr(dv2_clf, \"decision_function\"):\n",
    "                dv1_distance = dv1_clf.decision_function([dv1.iloc[i]])\n",
    "                dv2_distance = dv2_clf.decision_function([dv2.iloc[i]])\n",
    "                if len(dv1_clf.classes_)==2:\n",
    "                    h1_probas = np.array([1-self.sigmoid(dv1_distance), self.sigmoid(dv1_distance)])\n",
    "                    h2_probas = np.array([1-self.sigmoid(dv2_distance), self.sigmoid(dv2_distance)])\n",
    "                else:\n",
    "                    h1_probas = self.softmax(dv1_distance)\n",
    "                    h2_probas = self.softmax(dv2_distance)\n",
    "                final_y_probas = [proba_y1 * proba_y2 for (proba_y1, proba_y2) in zip(h1_probas, h2_probas)]\n",
    "                #print(\"h1 and h2 disagree on\",i, \" h1 Proba : \",h1_probas, \" h2 Proba: \", h2_probas)\n",
    "                #print(\"product probas:\",final_y_probas)\n",
    "                max_prob_idx = final_y_probas.index(max(final_y_probas))\n",
    "                y_pred[i] = self.classes_[max_prob_idx]\n",
    "                #print(\"result idx: \", max_prob_idx, \" result: \",y_pred[i])\n",
    "            # disagree and not support any confidence measure\n",
    "            else:\n",
    "                sys.exit(\"Error occur\")\n",
    "\n",
    "        # convert final result to np array\n",
    "        y_pred_np_array = np.asarray(y_pred)\n",
    "        return y_pred_np_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved co-training\n",
    "\n",
    "Possible improvement:\n",
    "\n",
    "Part 1: Parameter input\n",
    "1. Using two different algorithm for view one and view two.\n",
    "2. Instead of using parameter k to control number of iteration, using a stop criterion where if unlabeled sample can't get more than 95% of confidence on their confidence score, then stop iteration and finish co-training.\n",
    "3. Parameter p, n are not needed, use input data class ratio replace it (unlabeled distribution may different from labeled, this is unlikely works in reality).\n",
    "4. u prime not needed, use all unlabeled data.\n",
    "\n",
    "Part 2: Change confident measure method\n",
    "1. Changes method used when determine which class unlabeled sample belones to, instead of using confident score, we could use classifiers saved during each iteration of training to perform an majority voting. (Need to careful about number of iteration in co-training, since adding bad unlabel data as label data is easy to occur) (Source: 2004_Co-training and Self-training for Word Sense Disambiguation)\n",
    "\n",
    "Part 3: Change details in confident score method\n",
    "1. Add an third classifier and only train with original labeled samples(no-self-labeled sample), then use third classifier to evaluate self-labeled samples and get it's confidence score. Combine three classifier's confidence score together to get final decision.\n",
    "2. COTRADE method: construct undirected neighborhood graph using all samples, after finding most confident sample with it's corresponding class. Then, calculate distance between all labeled samples in this class and most confident sample in this class. This distance from graph + probability from supervised algorithm will be used together as confidence score.\n",
    "\n",
    "Part 4: Add checking to reduce the chance of adding mislabeled data\n",
    "1. Add an check on validation after new label sample is added, if not improving h1/h2, remove new labeled sample.\n",
    "\n",
    "Part 5: Change algorithm and make it work for muti-class\n",
    "1. Using same concept of muti-class SVM, train many OVR binary classifier. (To expensive)\n",
    "2. Allow algorithm directly take muti-class case and using same concept as binary co-training.\n",
    "\n",
    "\n",
    "My idea:\n",
    "1. Use one classifier (check)\n",
    "2. Have parameter k as default, but can be stoped early (check)\n",
    "3. Parameter p, n are replace with sl_base which set as input data class ratio*sl_base. (Allow muti-class case and will maintaining the class distribution in L. However, unlabeled distribution may different from labeled, this is unlikely works in reality) (check)\n",
    "4. u prime not needed, use all unlabeled data. (check)\n",
    "5. Adopt filter view disagreed samples, instead directly remove disagreed sample, we will give a score to evaluate disagreement between views and add this score as part of confident score. (2008-Multi-view learning in the presence of view disagreement)\n",
    "6. We will accumulate the probability of first iteration and current iteration; add them together as part of confident score. (check)\n",
    "7. Adopt COTRADE method, their assumption where that a correctly labeled example should be very close to samples in L with corresponding label same to be very useful. Thus a k-mean clustering is train with labeled, then transform unlabel data into same instance space as labeled samples, then calculate distance to each cluster's centroid, then use distance perfrom an softmax to get reverse of probability of sample belone to class. After that 1- reversed probability get probability and add it as part of co-train confident score. (check)\n",
    "8. We can think confident score as quality of unlabeled data, after each co-training iteration, it should always increase in best case. However, when we see a decrease in confident score compare to pervious iteration, it means the data quality level have decresed, therefore we need to check whether heighest probability of samples is greater than 0.8 or not. \n",
    "\n",
    "\n",
    "9. Notice that our main evaluator for whether a new sample should be added to L is current iteration probability, not the confident score, confident score is only used for sample quality check. (check)\n",
    "10. When no sample been added for both view, we stop co-training iteration since data left may provide more noise than information. (check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-14T02:53:21.425695Z",
     "start_time": "2020-08-14T02:53:00.820245Z"
    },
    "code_folding": [
     16,
     28,
     32,
     35,
     51,
     66,
     86,
     120,
     171,
     184,
     187,
     424,
     464,
     510,
     585,
     588,
     591,
     594,
     640,
     649
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import warnings\n",
    "import copy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import precision_score, recall_score,f1_score,accuracy_score\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "\n",
    "# create co training classifier\n",
    "class Improved_co_training_clf(object):\n",
    "    \n",
    "    def __init__(self, clf=[], view_num=2, k=30, **args):\n",
    "        if len(clf) == view_num:\n",
    "            self.clf = clf\n",
    "        elif len(clf) == 1:\n",
    "            self.clf = [copy.deepcopy(clf[0]) for i in range(view_num)]\n",
    "        else:\n",
    "            sys.exit(\"Classifier doesn't match with view number.\")\n",
    "        self.view_num = view_num\n",
    "        self.iter_new_size =[]\n",
    "        # number of iteration\n",
    "        self.k = k\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return np.around(1 / (1 + np.exp(-x)), decimals=4).item()\n",
    "\n",
    "    def init_L_U_U_prime(self, labels):\n",
    "        # index of the samples that are initially labeled\n",
    "        L = labels.index[labels[\"authorID\"] != -1].tolist()\n",
    "        # index of unlabeled samples\n",
    "        U = labels.index[labels[\"authorID\"] == -1].tolist()\n",
    "        print(\"Initial L size: \", len(L))\n",
    "        print(\"Initial U size: \", len(U))\n",
    "        # random drawing sample from U\n",
    "        random.shuffle(U)\n",
    "        U_prime = U[-min(len(U), self.u):]\n",
    "        # remove the samples in U_prime from U\n",
    "        U = U[:-len(U_prime)]\n",
    "        print(\"U size after drawing sample to U prime:\",len(U))\n",
    "        print(\"Initial U prime size: \", len(U_prime))\n",
    "        return L, U, U_prime\n",
    "    \n",
    "    def check_iter_label_mapping(self, iter_clf):\n",
    "        '''\n",
    "        In theory, it shouldn't occur that label not mapping since it trained on same dataset but different view\n",
    "        But add a check to make sure it won't occur and save the class mapping for later label unlabeled sample\n",
    "        '''\n",
    "        class_label = defaultdict(list)\n",
    "        for idx,clf in enumerate(iter_clf):\n",
    "            if idx ==0:\n",
    "                self.classes_ = clf.classes_\n",
    "            \n",
    "            if not all(self.classes_ == clf.classes_):\n",
    "                sys.exit(\"Two view classifier label not mapping\")\n",
    "        #print(self.classes_)\n",
    "        return \"checked\"\n",
    "\n",
    "    def get_confidence_score(self, clf, data):\n",
    "        if hasattr(clf, \"predict_proba\"):\n",
    "            proba = clf.predict_proba(data)\n",
    "        elif hasattr(clf, \"decision_function\"):    # use decision function\n",
    "            all_distance = np.array(clf.decision_function(data))\n",
    "            # if binary case, use sigmoid function to calculate probability\n",
    "            if iter_train_label.nunique()==2:\n",
    "                proba = []\n",
    "                for distance in all_distance:\n",
    "                    proba.append([1-self.sigmoid(distance), self.sigmoid(distance)])\n",
    "            else:\n",
    "                proba = self.softmax(dv1_distance)\n",
    "            proba = np.array(proba)\n",
    "            #print(\"Distance to hyperplane: \",distance)\n",
    "            #print(\"Probability: \",proba)\n",
    "        else:\n",
    "            sys.exit(\"No confident score for process\")\n",
    "        \n",
    "        return proba\n",
    "\n",
    "    def get_cluster_distance_as_proba(self, label_train, iter_train_label, unlabeled):\n",
    "        kmeans = KMeans(n_clusters=len(self.classes_)).fit(label_train)\n",
    "        # map correct idx to label\n",
    "        kmeans_predict = kmeans.labels_\n",
    "        label_mapping = list(zip(iter_train_label, kmeans_predict))\n",
    "        \n",
    "        lmc = Counter(label_mapping)\n",
    "        final_label_mapping = []\n",
    "        for label in np.unique(iter_train_label):\n",
    "            for (item,index),count in lmc.most_common():\n",
    "                if item == label:\n",
    "                    #print(\"label:\",label, \" idx:\",index, \" count:\",count)\n",
    "                    final_label_mapping.append((item,index))\n",
    "                    break\n",
    "        #print(final_label_mapping)\n",
    "        \n",
    "        # convert distance to centroid to probability of belone to class\n",
    "        dist_to_centroid = kmeans.transform(unlabeled)\n",
    "        cluster_proba = []\n",
    "        for distance in dist_to_centroid:\n",
    "            reverse_proba = self.softmax(distance)\n",
    "            proba = [1-x for x in reverse_proba]\n",
    "            cluster_proba.append(proba)\n",
    "        cluster_proba = np.array(cluster_proba)\n",
    "        \n",
    "        # map the proba same order as train in supervised learning\n",
    "        new_permutation = [idx for label, idx in final_label_mapping]\n",
    "        #print(new_permutation)\n",
    "        idx = np.empty_like(new_permutation)\n",
    "        idx[new_permutation] = np.arange(len(new_permutation))\n",
    "        cluster_proba[:] = cluster_proba[:, idx]\n",
    "        \n",
    "        return cluster_proba\n",
    "\n",
    "    def label_samples(self, score, rank, score_sample_idx_map,curr_iter_proba):\n",
    "        U_prime_size = len(score)\n",
    "        self_trained_sample_idx = []\n",
    "        self_trained_confident = []\n",
    "        self_trained_iter_proba = []\n",
    "        self.iter_new_size = defaultdict(list)\n",
    "\n",
    "        for class_idx in range(len(self.classes_)):\n",
    "            self.iter_new_size[self.classes_[class_idx]].append(int(self.class_ratio[class_idx]))\n",
    "        #print(self.iter_new_size)\n",
    "        \n",
    "        '''Search for samples that can be labeled'''\n",
    "        for label, confident_rank in enumerate(rank):\n",
    "            # ---------- find heightest current iteration probability for all class ----- #\n",
    "            class_curr_iter_proba = [row[label] for row in curr_iter_proba]\n",
    "            class_curr_iter_proba.sort()\n",
    "            class_heightest_proba = max(class_curr_iter_proba)\n",
    "            # ------------------- find last iteration max confidence score -------------- #\n",
    "            pervious_iter_confidence_score = self.self_new_all[self.classes_[label]+\"_score\"]\n",
    "            if len(pervious_iter_confidence_score)==0:\n",
    "                last_iter_max_confidence_score=0\n",
    "            else:\n",
    "                last_iter_max_confidence_score = max(self.self_new_all[self.classes_[label]+\"_score\"][-1], default=0)\n",
    "            \n",
    "            new_label_sample_size = [size for size in self.iter_new_size[self.classes_[label]]][0]\n",
    "            new_sample_idx = []\n",
    "            new_sample_confident_score = []\n",
    "            new_sample_proba = []\n",
    "            index = 0\n",
    "            while(len(new_sample_idx) < new_label_sample_size):\n",
    "                max_conf_sample_index = confident_rank[index]\n",
    "                #print('Select idx: ', max_conf_sample_index,\":\",score_sample_idx_map[max_conf_sample_index],\" have score: \",score[max_conf_sample_index], \" probability \",curr_iter_proba[max_conf_sample_index])\n",
    "                if (score_sample_idx_map[max_conf_sample_index]<last_iter_max_confidence_score) and (class_heightest_proba<0.8):\n",
    "                    print('Most confident idx: ', max_conf_sample_index,\":\",score_sample_idx_map[max_conf_sample_index],\" have score: \",score[max_conf_sample_index],\" smaller than last iteration max score: \",last_iter_max_confidence_score,\" probability \",curr_iter_proba[max_conf_sample_index], \" no bigger than 0.8, sample not used\")\n",
    "                    break\n",
    "                elif curr_iter_proba[max_conf_sample_index][label] <0.5:\n",
    "                    #print('Most confident idx: ', max_conf_sample_index,\":\",score_sample_idx_map[max_conf_sample_index],\" have score: \",score[max_conf_sample_index], \"have probability \",curr_iter_proba[max_conf_sample_index], \" not in top 3, thus sample not used\")\n",
    "                    index +=1\n",
    "                else:\n",
    "                    #print('Class:', self.classes_[label],' idx: ',max_conf_sample_index,\":\",score_sample_idx_map[max_conf_sample_index], \" score: \", score[max_conf_sample_index], \" iter proba: \", curr_iter_proba[max_conf_sample_index])\n",
    "                    new_sample_idx.append(score_sample_idx_map[max_conf_sample_index])\n",
    "                    new_sample_confident_score.append(score[max_conf_sample_index][label])\n",
    "                    new_sample_proba.append(curr_iter_proba[max_conf_sample_index][label])\n",
    "                    index +=1\n",
    "                if (index>=U_prime_size) :\n",
    "                    break\n",
    "            self_trained_sample_idx.append(new_sample_idx)\n",
    "            self_trained_confident.append(new_sample_confident_score)\n",
    "            self_trained_iter_proba.append(new_sample_proba)\n",
    "        return self_trained_sample_idx, self_trained_confident, self_trained_iter_proba\n",
    "\n",
    "    def get_self_labeled_sample(self):\n",
    "        '''\n",
    "        return:\n",
    "            self-labeled new positive, self-labeled new negative (Index)\n",
    "        '''\n",
    "        temp = defaultdict(list)\n",
    "        temp_k = defaultdict(list)\n",
    "        for label in self.classes_:\n",
    "            temp[label]=self.self_new_all[label]\n",
    "            temp_k[label] = self.self_new_at_k[label]\n",
    "        \n",
    "        return temp, temp_k\n",
    "\n",
    "    def set_PCA(self, pca=None):\n",
    "        self.pca = pca\n",
    "\n",
    "    def fit(self, train_data, validation_data, dataset_name=None, plot_save_path=None):\n",
    "        # sync input datatype\n",
    "        for idx, item in enumerate(train_data):\n",
    "            if not isinstance(item, pd.DataFrame):\n",
    "                item = pd.DataFrame(item)\n",
    "            item.reset_index(drop=True,inplace=True)\n",
    "            train_data[idx] = item\n",
    "        \n",
    "        labels = train_data[-1]\n",
    "        label_validation = validation_data[-1]\n",
    "        \n",
    "        # using all unlabeled sample instead of pool of unlabeled sample\n",
    "        self.u = len(labels)\n",
    "        L_plot, U_plot, U_prime_plot = self.init_L_U_U_prime(labels)\n",
    "        L, U, U_prime = ([] for i in range(3))\n",
    "        #print(\"All data index: \",train_data[0].index.values)\n",
    "        #print(\"L_plot: \", L_plot)\n",
    "        #print(\"U_plot: \", U_plot)\n",
    "        #print(\"U_prime_plot: \", U_prime_plot)\n",
    "        \n",
    "        # index of self labeled samples\n",
    "        self.self_new_all = defaultdict(list)\n",
    "        self.self_new_at_k = defaultdict(list)\n",
    "        self.new_labeled_details = defaultdict(list)\n",
    "        self.f1_on_validation = defaultdict(list)\n",
    "        self.f1_on_validation_combined = []\n",
    "        ''' train clf with concatenated features for k=0'''\n",
    "        temp_train = defaultdict(list)\n",
    "        for view_idx in range(self.view_num):\n",
    "            temp_train[view_idx] = train_data[view_idx].iloc[L_plot]\n",
    "        concatenate_data = pd.concat([temp_train[0],temp_train[1]], axis=1, ignore_index=True)\n",
    "        concatenate_vaildation = pd.concat([validation_data[0],validation_data[1]], axis=1, ignore_index=True)\n",
    "        concatenate_clf = copy.deepcopy(self.clf[0])\n",
    "        concatenate_clf.fit(concatenate_data, labels.iloc[L_plot].values.ravel())\n",
    "        baseline_predict_result = concatenate_clf.predict(concatenate_vaildation)\n",
    "        self.f1_baseline = f1_score(label_validation, baseline_predict_result, average='macro')\n",
    "        # save clf in each iteration\n",
    "        self.all_iter_clf = []\n",
    "        # when fit co-train, we collect f1 on validation samples wrt each iteration\n",
    "        for view_idx in range(self.view_num):\n",
    "            self.f1_on_validation[\"dv\"+str(view_idx)]=[]\n",
    "        self.iterCounter = 0\n",
    "        # --------- plot PCA reduced plot for initial stage of train -------------- #\n",
    "        pca = self.pca\n",
    "        '''Notice pca will change input datatype dataframe to datatype list, keep it's order,\n",
    "           or re-assign dv1.index.values to the output'''\n",
    "        pca_dv1 = pca.fit_transform(X=train_data[0])\n",
    "        pca_dv2 = pca.fit_transform(X=train_data[1])\n",
    "        #loop until we have assigned labels to every sample in U and U_prime\n",
    "        while U_prime_plot:\n",
    "            '''\n",
    "            Extract labeled training sample from training sample and train classifier, then make prediction\n",
    "            Evaluate performance of current iteration classifier\n",
    "            '''\n",
    "            # print(\"Iteration:\",self.iterCounter, \" L_plot: \",L_plot)\n",
    "            # print(\"Iteration:\",self.iterCounter, \" U_prime_plot: \",U_prime_plot)\n",
    "            iter_train = defaultdict(list)\n",
    "            iter_unlabeled = defaultdict(list)\n",
    "            iter_clf = copy.deepcopy(self.clf) \n",
    "            iter_train_label = labels.iloc[L_plot]\n",
    "            for view_idx in range(self.view_num):\n",
    "                iter_train[\"dv\"+str(view_idx)]=train_data[view_idx].iloc[L_plot]\n",
    "                iter_unlabeled[\"dv\"+str(view_idx)]=train_data[view_idx].iloc[U_prime_plot]\n",
    "                \n",
    "                iter_clf[view_idx].fit(iter_train[\"dv\"+str(view_idx)], iter_train_label.values.ravel())\n",
    "\n",
    "            self.check_iter_label_mapping(iter_clf)\n",
    "            # ----------------- obtain input data class ratio --------------- #\n",
    "            temp = labels.iloc[L_plot]['authorID'].value_counts(normalize=True)\n",
    "            self.class_ratio = []\n",
    "            for classes in self.classes_:\n",
    "                self.class_ratio.append(temp[classes]*10)\n",
    "            self.class_ratio = [i/min(self.class_ratio) for i in self.class_ratio]\n",
    "            # ---------------------- test error on validation data --------------------- #\n",
    "            y_predict = defaultdict(list)\n",
    "            f1 = defaultdict(list)\n",
    "            for view_idx in range(self.view_num):\n",
    "                y_predict[view_idx]=iter_clf[view_idx].predict(validation_data[view_idx])\n",
    "                f1[\"dv\"+str(view_idx)]=f1_score(label_validation,y_predict[view_idx],average='macro')\n",
    "                self.f1_on_validation[\"dv\"+str(view_idx)].append(f1[\"dv\"+str(view_idx)])\n",
    "            \n",
    "            self.all_iter_clf.append((self.iterCounter,iter_clf))\n",
    "                        \n",
    "            # ---------------- collect the result of different k value ----------------- #\n",
    "            # --------------- collect result for concatenated features ----------------- #\n",
    "            temp_train = defaultdict(list)\n",
    "            temp_clf = defaultdict(list)\n",
    "            for view_idx in range(self.view_num):\n",
    "                temp_train[view_idx] = train_data[view_idx].iloc[L_plot]\n",
    "                temp_clf[view_idx] = copy.deepcopy(self.clf[view_idx])\n",
    "                temp_clf[view_idx].fit(temp_train[view_idx], labels.iloc[L_plot].values.ravel())\n",
    "            \n",
    "            predict_result = self.combined_predict(temp_clf[0],temp_clf[1],validation_data)\n",
    "            f1_combined = f1_score(label_validation, predict_result, average='macro')\n",
    "            self.f1_on_validation_combined.append(f1_combined)\n",
    "            \n",
    "            # ----------------------- start print details ------------------------ #\n",
    "            new_train_label = labels.iloc[L_plot]\n",
    "            if len(self.new_labeled_details[\"h0_new_idx\"])==0:\n",
    "                print(\"Iteration 0, strat self label.\")\n",
    "            else:\n",
    "                print(\"Iteration\",self.iterCounter,\" h1 new: \", self.new_labeled_details[\"h0_new_idx\"][-1], \" score: \", self.new_labeled_details[\"h0_new_proba\"][-1])\n",
    "                print(\"Iteration\",self.iterCounter,\" h2 new: \", self.new_labeled_details[\"h1_new_idx\"][-1], \" score: \", self.new_labeled_details[\"h1_new_proba\"][-1])\n",
    "            \n",
    "            '''\n",
    "            Confidence measure:\n",
    "            Part 1: View agreement score is probability of all view multiply together\n",
    "            Case 1: Two view agree with very large confidence score (check)\n",
    "            Case 2: Two view disagree, one view have large confidence, scoring towards large confident view's class (check)\n",
    "            Case 3: Two view disagree and both have large confidence score\n",
    "            Case 4: Two view agree/disagree with small confidence score\n",
    "            '''\n",
    "            unlabeled_pred_proba = []\n",
    "            for view_idx in range(self.view_num):\n",
    "                view_pred_proba = self.get_confidence_score(clf=iter_clf[view_idx],data=iter_unlabeled[\"dv\"+str(view_idx)])\n",
    "                unlabeled_pred_proba.append(view_pred_proba)\n",
    "            view_agreement_score = np.array([np.multiply(x[0],x[1]) for x in zip(*unlabeled_pred_proba)])\n",
    "            #print(unlabeled_pred_proba)\n",
    "            #print(view_agreement_score)\n",
    "            ''' \n",
    "            # Part 2: Accumulate predicted probability on each iteration as score\n",
    "            '''\n",
    "            diff_iter_score = defaultdict(list)\n",
    "            curr_iter_proba = defaultdict(list)\n",
    "            final_score = defaultdict(list)\n",
    "            cluster_proba = defaultdict(list)\n",
    "            score_rank = defaultdict(list)\n",
    "            proba_sample_idx_map=iter_unlabeled[\"dv0\"].index\n",
    "            for view_idx in range(self.view_num):\n",
    "                # ------ Part 1: Accumulate predicted probability on first and current iteration as score ------ #\n",
    "                for iteration, clf in self.all_iter_clf:\n",
    "                    if (iteration ==0) or (iteration ==self.iterCounter):\n",
    "                        diff_iter_score[view_idx].append(self.get_confidence_score(clf=clf[view_idx], data=iter_unlabeled[\"dv\"+str(view_idx)]))\n",
    "                # add score in each iteration together\n",
    "                final_score[\"dv\"+str(view_idx)] = np.array([sum(x) for x in zip(*diff_iter_score[view_idx])])\n",
    "                # save last iteration probability\n",
    "                curr_iter_proba[\"dv\"+str(view_idx)] = diff_iter_score[view_idx][-1]\n",
    "                \n",
    "                # ----------- Part 2: Incorporate clustering distance as part of score ------------ #\n",
    "                cluster_proba[view_idx] = self.get_cluster_distance_as_proba(label_train=iter_train[\"dv\"+str(view_idx)], iter_train_label=iter_train_label[\"authorID\"],\n",
    "                                                                             unlabeled=iter_unlabeled[\"dv\"+str(view_idx)])\n",
    "                final_score[\"dv\"+str(view_idx)]=np.array([sum(x) for x in zip(final_score[\"dv\"+str(view_idx)],cluster_proba[view_idx])])\n",
    "                \n",
    "                # ----------------- Part 3: Incorporate view agreement score ---------------------- #\n",
    "                final_score[\"dv\"+str(view_idx)]=np.array([sum(x) for x in zip(final_score[\"dv\"+str(view_idx)],view_agreement_score)])\n",
    "                \n",
    "                # -------------- Part 4: Rank confidence and start self labeling ------------------ #\n",
    "                # proba1_rank[i] is label i's confidence measure\n",
    "                for class_proba in final_score[\"dv\"+str(view_idx)].T:\n",
    "                    score_rank[\"dv\"+str(view_idx)].append((-class_proba).argsort())\n",
    "                \n",
    "            #print(\"All: \", diff_iter_score)\n",
    "            #print(\"Current: \",curr_iter_proba)\n",
    "            #print(\"Cluster proba: \", cluster_proba)\n",
    "            #print(\"Final score: \",final_score)\n",
    "            #print(\"Score rank: \", score_rank)\n",
    "            \n",
    "            ''' Start labelling '''\n",
    "            new_sample = defaultdict(list)\n",
    "            new_sample_score = defaultdict(list)\n",
    "            new_sample_proba = defaultdict(list)\n",
    "            flatten_new_idx = defaultdict(list)\n",
    "            flatten_score = defaultdict(list)\n",
    "            flatten_proba = defaultdict(list)\n",
    "            self.new_labeled_details[\"Iteration\"].append(self.iterCounter)\n",
    "            \n",
    "            for view_idx in range(self.view_num):\n",
    "                (new_sample[\"dv\"+str(view_idx)], \n",
    "                 new_sample_score[\"dv\"+str(view_idx)], \n",
    "                 new_sample_proba[\"dv\"+str(view_idx)]) = self.label_samples(final_score[\"dv\"+str(view_idx)], \n",
    "                                                                            score_rank[\"dv\"+str(view_idx)], \n",
    "                                                                            proba_sample_idx_map, \n",
    "                                                                            curr_iter_proba[\"dv\"+str(view_idx)])\n",
    "            '''Case if h1's new class 1 is h2's new class 2\n",
    "            Example: Iteration 45  h1 new:  [[95], [150]]  probs:  [[0.89], [0.82]]\n",
    "                     Iteration 45  h2 new:  [[147], [95]]  probs:  [[0.89], [0.92]]\n",
    "            '''\n",
    "            #print(\"before fix: \",new_sample)\n",
    "            #print(\"before fix: \",new_sample_proba)\n",
    "            full_views_list = list(new_sample.keys())\n",
    "            temp_views_list = list(new_sample.keys())\n",
    "            for curr_view in full_views_list:\n",
    "                #print(\"t1 \",new_sample[curr_view])\n",
    "                #print(\"t1 \",new_sample_proba[curr_view])\n",
    "                temp_views_list.remove(curr_view)\n",
    "                for i, curr_class_new in enumerate(new_sample[curr_view]):\n",
    "                    temp_pop_list = []\n",
    "                    for j, class_item in enumerate(curr_class_new):\n",
    "                        #print(class_item)\n",
    "                        for other_view in temp_views_list:\n",
    "                            #print(\"t2 org\",new_sample[other_view])\n",
    "                            #print(\"t2 org\",new_sample_proba[other_view])\n",
    "                            for l, other_view_class_new in enumerate(new_sample[other_view]):\n",
    "                                for m, other_class_item in enumerate(other_view_class_new):\n",
    "                                    # only perform delete if current view contain item, if it's deleted, move on\n",
    "                                    if new_sample[curr_view][i][j] == new_sample[other_view][l][m]:\n",
    "                                        if new_sample_proba[curr_view][i][j]>new_sample_proba[other_view][l][m]:\n",
    "                                            new_sample[other_view][l].pop(m)\n",
    "                                            new_sample_proba[other_view][l].pop(m)\n",
    "                                            #print(\"t2 update\",new_sample)\n",
    "                                            #print(\"t2 update\",new_sample_proba)\n",
    "                                        else:\n",
    "                                            temp_pop_list.append(j)\n",
    "                                        #print(class_item, \"check\")\n",
    "                    temp_pop_list.sort(reverse=True)\n",
    "                    for idx in temp_pop_list:\n",
    "                        new_sample[curr_view][i].pop(idx)\n",
    "                        new_sample_proba[curr_view][i].pop(idx)\n",
    "            #print(\"apply fix: \",new_sample)\n",
    "            #print(\"apply fix: \",new_sample_proba)\n",
    "            \n",
    "            for view_idx in range(self.view_num):\n",
    "                flatten_new_idx[view_idx] = list(itertools.chain(*new_sample[\"dv\"+str(view_idx)]))\n",
    "                flatten_score[view_idx] = list(itertools.chain(*new_sample_score[\"dv\"+str(view_idx)]))\n",
    "                flatten_proba[view_idx] = list(itertools.chain(*new_sample_proba[\"dv\"+str(view_idx)]))\n",
    "                \n",
    "            #print(\"check:\", flatten_new_idx)\n",
    "            # if no new label added to L, stop iteration\n",
    "            if not any(flatten_new_idx.values()):\n",
    "                self.iterCounter +=1\n",
    "                print(\"Remaining unlabel sample is uncertain.\")\n",
    "                break\n",
    "                \n",
    "            for view_idx in range(self.view_num):\n",
    "                self.new_labeled_details[\"h\"+str(view_idx)+\"_new_idx\"].append(flatten_new_idx[view_idx])\n",
    "                self.new_labeled_details[\"h\"+str(view_idx)+\"_new_score\"].append(flatten_score[view_idx])\n",
    "                self.new_labeled_details[\"h\"+str(view_idx)+\"_new_proba\"].append(flatten_proba[view_idx])\n",
    "                \n",
    "            #print(self.new_labeled_details)\n",
    "            '''\n",
    "            Add most confidence samples as new training samples, auto label the samples and remove it from U_prime\n",
    "            Special Case: if two view classifier give same most confident sample, only 1 datapoint self-labeled\n",
    "            '''\n",
    "            roundNewIdx = list(zip(*[value for key, value in new_sample.items()]))\n",
    "            roundNewScore = list(zip(*[value for key, value in new_sample_score.items()]))\n",
    "            roundNewProba = list(zip(*[value for key, value in new_sample_proba.items()]))\n",
    "            roundNew_flatten_unique = []\n",
    "            for label, round_new in enumerate(zip(roundNewIdx,roundNewScore,roundNewProba)):\n",
    "                #print(round_new)\n",
    "                round_new = list(zip(*round_new))\n",
    "                temp = []\n",
    "                for index, (idx,score,proba) in enumerate(round_new):\n",
    "                    for i in range(len(idx)):\n",
    "                        temp.append((idx[i],score[i],proba[i]))\n",
    "                #print(temp)\n",
    "                round_new = set(temp)\n",
    "                round_new_idx = [item[0] for item in round_new]\n",
    "                round_new_score = [item[1] for item in round_new]\n",
    "                round_new_proba = [item[2] for item in round_new]\n",
    "                self.self_new_all[self.classes_[label]].append(round_new_idx)\n",
    "                self.self_new_all[self.classes_[label]+\"_score\"].append(round_new_score)\n",
    "                self.self_new_all[self.classes_[label]+\"_proba\"].append(round_new_proba)\n",
    "                roundNew_flatten_unique.extend(round_new_idx)\n",
    "                # add label to those new samples\n",
    "                #print(labels[round_new_idx])\n",
    "                labels[\"authorID\"][round_new_idx] = self.classes_[label]\n",
    "                #print(labels[round_new_idx])\n",
    "                #print(self.classes_[label],\" (u' idx): \",round_new_idx)\n",
    "            #print(roundNew_flatten_unique)\n",
    "            # extend the labeled sample\n",
    "            L_plot.extend(roundNew_flatten_unique)\n",
    "            # remove the labeled sample from U_prime\n",
    "            U_prime_plot = [x for x in U_prime_plot if x not in roundNew_flatten_unique]\n",
    "            #print(U_prime_plot)\n",
    "            # randomly choice view_num*self-label-class-max*class_num examples from u to replenish u_prime\n",
    "            replenishItem = U_plot[-(int(sum(self.class_ratio))):]\n",
    "            U_prime_plot.extend(replenishItem)\n",
    "            U_plot = U_plot[:-len(replenishItem)]\n",
    "            self.iterCounter +=1\n",
    "            ''' --------- if reach loop condition k, we extract L from L_plot --------- '''\n",
    "            if self.iterCounter <= self.k:\n",
    "                L = copy.deepcopy(L_plot)\n",
    "                U = copy.deepcopy(U_plot)\n",
    "                U_prime = copy.deepcopy(U_prime_plot)\n",
    "                self.self_new_at_k = copy.deepcopy(self.self_new_all)\n",
    "            \n",
    "            ''' ----------- if the unlabelled list is running empty -------------- '''\n",
    "            if not U_prime_plot:\n",
    "                self.iterCounter +=1\n",
    "                # ---------------------- test error on validation data --------------------- #\n",
    "                y_predict = defaultdict(list)\n",
    "                f1 = defaultdict(list)\n",
    "                for view_idx in range(self.view_num):\n",
    "                    y_predict[view_idx]=iter_clf[view_idx].predict(validation_data[view_idx])\n",
    "                    f1[\"dv\"+str(view_idx)]=f1_score(label_validation,y_predict[view_idx],average='macro')\n",
    "                    self.f1_on_validation[\"dv\"+str(view_idx)].append(f1[\"dv\"+str(view_idx)])\n",
    "\n",
    "                self.all_iter_clf.append((self.iterCounter,iter_clf))\n",
    "\n",
    "                # ---------------- collect the result of different k value ----------------- #\n",
    "                # --------------- collect result for concatenated features ----------------- #\n",
    "                temp_train = defaultdict(list)\n",
    "                temp_clf = defaultdict(list)\n",
    "                for view_idx in range(self.view_num):\n",
    "                    temp_train[view_idx] = train_data[view_idx].iloc[L_plot]\n",
    "                    temp_clf[view_idx] = copy.deepcopy(self.clf[view_idx])\n",
    "                    temp_clf[view_idx].fit(temp_train[view_idx], labels.iloc[L_plot].values.ravel())\n",
    "\n",
    "                predict_result = self.combined_predict(temp_clf[0],temp_clf[1],validation_data)\n",
    "                f1_combined = f1_score(label_validation, predict_result, average='macro')\n",
    "                self.f1_on_validation_combined.append(f1_combined)\n",
    "                \n",
    "                # --------------- print last iteration details ----------- #\n",
    "                new_train_label = labels.iloc[L_plot]\n",
    "                print(\"Iteration\",self.iterCounter,\" h1 new: \", self.new_labeled_details[\"h0_new_idx\"][-1], \" score: \", self.new_labeled_details[\"h0_new_proba\"][-1])\n",
    "                print(\"Iteration\",self.iterCounter,\" h2 new: \", self.new_labeled_details[\"h1_new_idx\"][-1], \" score: \", self.new_labeled_details[\"h1_new_proba\"][-1])\n",
    "        \n",
    "        selected_return_iter = self.k\n",
    "        if self.iterCounter <= self.k:\n",
    "            selected_return_iter = self.iterCounter\n",
    "            self.k = self.iterCounter\n",
    "        print(self.k,\"iterations total Labeled number: \", len(L), \" Still unlabeled number: \", len(U_prime))\n",
    "        print(self.iterCounter,\"iteration total Labeled number: \", len(L_plot), \" Still unlabeled number: \", len(U_prime_plot))\n",
    "        counter = Counter(L_plot)\n",
    "        print(counter.most_common(3))\n",
    "        if len(set(L_plot)) == len(L_plot):\n",
    "            print(\"success\")\n",
    "        else:\n",
    "            print(\"duplicate found\")\n",
    "            sys.exit(\"error\")\n",
    "        \n",
    "        # final train with only k iteration\n",
    "        final_train = defaultdict(list)\n",
    "        for view_idx in range(self.view_num):\n",
    "            final_train[view_idx] = train_data[view_idx].iloc[L]\n",
    "            self.clf[view_idx].fit(final_train[view_idx], labels.iloc[L].values.ravel())\n",
    "        ''' Evalutation plot for co-training process, save f1 score vs number of iteration plot '''\n",
    "        #print(self.f1_on_validation[\"dv0\"])\n",
    "        #print(self.f1_on_validation[\"dv1\"])\n",
    "        if (dataset_name != None) and (plot_save_path != None):\n",
    "            # ----------------- plot h1/h2 result separately -------------------- #\n",
    "            default_text_based = [self.f1_on_validation[\"dv0\"][0]] * self.iterCounter\n",
    "            default_citation_based = [self.f1_on_validation[\"dv1\"][0]] * self.iterCounter\n",
    "            co_train_text_based = self.f1_on_validation[\"dv0\"]\n",
    "            co_train_citation_based = self.f1_on_validation[\"dv1\"]\n",
    "            step = np.arange(start=0, stop=self.iterCounter, step=1)\n",
    "            \n",
    "            #print(\"step:\",len(step))\n",
    "            #print(\"default:\",self.iterCounter)\n",
    "            #print(\"co-training v1:\",len(co_train_text_based))\n",
    "            #print(\"co-training v2:\",len(co_train_citation_based))\n",
    "            #print(\"default concatenate:\",self.iterCounter)\n",
    "            #print(\"default combined:\",self.iterCounter)\n",
    "            #print(\"co-training combined:\",len(self.f1_on_validation_combined))\n",
    "\n",
    "            fig = plt.figure(figsize=(8,6))\n",
    "            ax = plt.axes()\n",
    "            plt.plot(step, default_text_based, linestyle='dashed', label=\"Text based default\")\n",
    "            plt.plot(step, default_citation_based, linestyle='dashdot', label=\"Citation based default\")\n",
    "            plt.plot(step, co_train_text_based, linestyle='solid', marker = \"*\", label=\"Text based\")\n",
    "            plt.plot(step, co_train_citation_based, linestyle='dotted', marker = \"+\", label=\"Citation based\")\n",
    "            handles, _  = ax.get_legend_handles_labels()\n",
    "            plt.axvline(x=(selected_return_iter-1), color='r', label=\"k\")\n",
    "            _, labels = ax.get_legend_handles_labels()\n",
    "            vertical_line = lines.Line2D([], [],  marker='|', linestyle='None', color=\"r\", markersize=10, markeredgewidth=1.5)\n",
    "            handles.append(vertical_line)\n",
    "            ax.autoscale_view()\n",
    "            legend = ax.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 1.2), ncol=2)\n",
    "            plt.xlabel('Co-Training Iterations')\n",
    "            plt.ylabel('F1 score')\n",
    "            f1_vs_iteration_plot_name = plot_save_path+dataset_name+\"_co_train_iteration_f1(1).png\"\n",
    "            counter = 1\n",
    "            while os.path.exists(plot_save_path+dataset_name+\"_co_train_iteration_f1(%s).png\" % counter):\n",
    "                counter+=1\n",
    "                f1_vs_iteration_plot_name = plot_save_path+dataset_name+\"_co_train_iteration_f1(%s).png\" % counter\n",
    "            plt.savefig(f1_vs_iteration_plot_name, dpi=150, bbox_extra_artists=(legend,), bbox_inches='tight')\n",
    "            # plt.show()\n",
    "            plt.close(\"all\")\n",
    "            \n",
    "            # -------------------- plot combined result ------------------------- #\n",
    "            default_concatenate = [self.f1_baseline] * self.iterCounter\n",
    "            default_combined = [self.f1_on_validation_combined[0]] * self.iterCounter\n",
    "            co_train_combined = self.f1_on_validation_combined\n",
    "            fig = plt.figure(figsize=(8,6))\n",
    "            ax = plt.axes()\n",
    "            plt.plot(step, default_concatenate, linestyle='dashed', label=\"Concatenate default\")\n",
    "            plt.plot(step, default_combined, linestyle='dotted', label=\"Combined default\")\n",
    "            plt.plot(step, co_train_combined, linestyle='solid', marker = \"*\", label=\"Combined co-train\")\n",
    "            handles, _  = ax.get_legend_handles_labels()\n",
    "            plt.axvline(x=(selected_return_iter-1), color='r', label=\"k\")\n",
    "            _, labels = ax.get_legend_handles_labels()\n",
    "            vertical_line = lines.Line2D([], [],  marker='|', linestyle='None', color=\"r\", markersize=10, markeredgewidth=1.5)\n",
    "            handles.append(vertical_line)\n",
    "            ax.autoscale_view()\n",
    "            legend = ax.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=2)\n",
    "            plt.xlabel('Co-Training Iterations')\n",
    "            plt.ylabel('F1 score')\n",
    "            f1_vs_iteration_plot_name = plot_save_path+dataset_name+\"_co_train_combined_iteration_f1(1).png\"\n",
    "            counter = 1\n",
    "            while os.path.exists(plot_save_path+dataset_name+\"_co_train_combined_iteration_f1(%s).png\" % counter):\n",
    "                counter+=1\n",
    "                f1_vs_iteration_plot_name = plot_save_path+dataset_name+\"_co_train_combined_iteration_f1(%s).png\" % counter\n",
    "            plt.savefig(f1_vs_iteration_plot_name, dpi=150, bbox_extra_artists=(legend,), bbox_inches='tight')\n",
    "            # plt.show()\n",
    "            plt.close(\"all\")\n",
    "            \n",
    "        return \"Training Done\"\n",
    "\n",
    "    def co_train_process_f1(self):\n",
    "        return self.f1_on_validation[\"dv0\"], self.f1_on_validation[\"dv1\"], self.f1_on_validation_combined, self.f1_baseline\n",
    "\n",
    "    def get_iter_count(self):\n",
    "        return self.iterCounter\n",
    "    \n",
    "    def get_k(self):\n",
    "        return self.k\n",
    "\n",
    "    def predict(self, data):\n",
    "        y_predict = defaultdict(list)\n",
    "        for view_idx in range(self.view_num):\n",
    "            y_predict[view_idx]=self.clf[view_idx].predict(data[view_idx])\n",
    "        y_predict = [value for key, value in y_predict.items()]\n",
    "        #fill pred with -1 so we can identify the samples in which error occur\n",
    "        y_pred = [\"-1\"] * data[0].shape[0]\n",
    "        for i, (dv1_y, dv2_y) in enumerate(zip(*y_predict)):\n",
    "            # if both agree on label\n",
    "            if dv1_y == dv2_y:\n",
    "                y_pred[i] = dv1_y\n",
    "            # If disagree on label and support proba method: We times probability together, choice class have higher probabilities\n",
    "            elif hasattr(self.clf[0], \"predict_proba\") and hasattr(self.clf[1], \"predict_proba\"):\n",
    "                h1_probas = self.clf[0].predict_proba([data[0].iloc[i]])[0]\n",
    "                h2_probas = self.clf[1].predict_proba([data[1].iloc[i]])[0]\n",
    "                final_y_probas = [proba_y1 * proba_y2 for (proba_y1, proba_y2) in zip(h1_probas, h2_probas)]\n",
    "                #print(\"h1 and h2 disagree on\",i, \" h1 Proba : \",h1_probas, \" h2 Proba: \", h2_probas)\n",
    "                #print(\"product probas:\",final_y_probas)\n",
    "                max_prob_idx = final_y_probas.index(max(final_y_probas))\n",
    "                y_pred[i] = self.classes_[max_prob_idx]\n",
    "                #print(\"result idx: \", max_prob_idx, \" result: \",y_pred[i])\n",
    "            # If disagree on label and not support proba method: We use decision_function to first calculate probability,\n",
    "            # then we times calculate probability together, choice class have higher probabilities\n",
    "            elif hasattr(self.clf[0], \"decision_function\") and hasattr(self.clf[1], \"decision_function\"):\n",
    "                dv1_distance = self.clf[0].decision_function([data[0].iloc[i]])\n",
    "                dv2_distance = self.clf[1].decision_function([data[1].iloc[i]])\n",
    "                if len(self.clf1.classes_)==2:\n",
    "                    h1_probas = np.array([1-self.sigmoid(dv1_distance), self.sigmoid(dv1_distance)])\n",
    "                    h2_probas = np.array([1-self.sigmoid(dv2_distance), self.sigmoid(dv2_distance)])\n",
    "                else:\n",
    "                    h1_probas = self.softmax(dv1_distance)\n",
    "                    h2_probas = self.softmax(dv2_distance)\n",
    "                final_y_probas = [proba_y1 * proba_y2 for (proba_y1, proba_y2) in zip(h1_probas, h2_probas)]\n",
    "                #print(\"h1 and h2 disagree on\",i, \" h1 Proba : \",h1_probas, \" h2 Proba: \", h2_probas)\n",
    "                #print(\"product probas:\",final_y_probas)\n",
    "                max_prob_idx = final_y_probas.index(max(final_y_probas))\n",
    "                y_pred[i] = self.classes_[max_prob_idx]\n",
    "                #print(\"result idx: \", max_prob_idx, \" result: \",y_pred[i])\n",
    "            # disagree and not support any confidence measure\n",
    "            else:\n",
    "                sys.exit(\"Error occur\")\n",
    "\n",
    "        # convert final result to np array\n",
    "        y_pred_np_array = np.asarray(y_pred)\n",
    "        return y_pred_np_array\n",
    "\n",
    "    def predict_proba(self, dv1, dv2):\n",
    "        # the predicted probabilities is simply a product (*) of probabilities given from each classifier trained\n",
    "        h1_probas = self.clf1.predict_proba(dv1)\n",
    "        h2_probas = self.clf2.predict_proba(dv2)\n",
    "        \n",
    "        proba = (h1_probas*h2_probas)\n",
    "        return proba\n",
    "    \n",
    "    ''' ------------- function for optimize parameter k only ---------------- '''\n",
    "    def combined_predict(self, dv1_clf, dv2_clf, data):\n",
    "        dv1=data[0]\n",
    "        dv2=data[1]\n",
    "        dv1_predict = dv1_clf.predict(dv1)\n",
    "        dv2_predict = dv2_clf.predict(dv2)\n",
    "        #fill pred with -1 so we can identify the samples in which error occur\n",
    "        y_pred = [\"-1\"] * dv1.shape[0]\n",
    "        for i, (dv1_y, dv2_y) in enumerate(zip(dv1_predict, dv2_predict)):\n",
    "            # if both agree on label\n",
    "            if dv1_y == dv2_y:\n",
    "                y_pred[i] = dv1_y\n",
    "            # If disagree on label and support proba method: We times probability together, choice class have higher probabilities\n",
    "            elif hasattr(dv1_clf, \"predict_proba\") and hasattr(dv2_clf, \"predict_proba\"):\n",
    "                h1_probas = dv1_clf.predict_proba([dv1.iloc[i]])[0]\n",
    "                h2_probas = dv2_clf.predict_proba([dv2.iloc[i]])[0]\n",
    "                final_y_probas = [proba_y1 * proba_y2 for (proba_y1, proba_y2) in zip(h1_probas, h2_probas)]\n",
    "                #print(\"h1 and h2 disagree on\",i, \" h1 Proba : \",h1_probas, \" h2 Proba: \", h2_probas)\n",
    "                #print(\"product probas:\",final_y_probas)\n",
    "                max_prob_idx = final_y_probas.index(max(final_y_probas))\n",
    "                y_pred[i] = self.classes_[max_prob_idx]\n",
    "                #print(\"result idx: \", max_prob_idx, \" result: \",y_pred[i])\n",
    "            # If disagree on label and not support proba method: We use decision_function to first calculate probability,\n",
    "            # then we times calculate probability together, choice class have higher probabilities\n",
    "            elif hasattr(dv1_clf, \"decision_function\") and hasattr(dv2_clf, \"decision_function\"):\n",
    "                dv1_distance = dv1_clf.decision_function([dv1.iloc[i]])\n",
    "                dv2_distance = dv2_clf.decision_function([dv2.iloc[i]])\n",
    "                if len(dv1_clf.classes_)==2:\n",
    "                    h1_probas = np.array([1-self.sigmoid(dv1_distance), self.sigmoid(dv1_distance)])\n",
    "                    h2_probas = np.array([1-self.sigmoid(dv2_distance), self.sigmoid(dv2_distance)])\n",
    "                else:\n",
    "                    h1_probas = self.softmax(dv1_distance)\n",
    "                    h2_probas = self.softmax(dv2_distance)\n",
    "                final_y_probas = [proba_y1 * proba_y2 for (proba_y1, proba_y2) in zip(h1_probas, h2_probas)]\n",
    "                #print(\"h1 and h2 disagree on\",i, \" h1 Proba : \",h1_probas, \" h2 Proba: \", h2_probas)\n",
    "                #print(\"product probas:\",final_y_probas)\n",
    "                max_prob_idx = final_y_probas.index(max(final_y_probas))\n",
    "                y_pred[i] = self.classes_[max_prob_idx]\n",
    "                #print(\"result idx: \", max_prob_idx, \" result: \",y_pred[i])\n",
    "            # disagree and not support any confidence measure\n",
    "            else:\n",
    "                sys.exit(\"Error occur\")\n",
    "\n",
    "        # convert final result to np array\n",
    "        y_pred_np_array = np.asarray(y_pred)\n",
    "        return y_pred_np_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-14T02:53:21.477859Z",
     "start_time": "2020-08-14T02:53:21.428237Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simulate_co_training_situation(all_train_label,init_labeled_size,init_validation_size):\n",
    "    # ----------- set some labeled data as unlabeled ------------ #\n",
    "    # 1. obtain data ratio\n",
    "    c = Counter(all_train_label)\n",
    "    data_ratio = [(i, c[i] / len(all_train_label)) for i in c]\n",
    "    print(data_ratio)\n",
    "    # 2. calculate per class size \n",
    "    # co_train_per_class_size contain (label,initial train size for each class, initial validation data for each class)\n",
    "    co_train_per_class_size = [(label, round(ratio*init_labeled_size),round(ratio*init_validation_size)) for label, ratio in data_ratio]\n",
    "    print(co_train_per_class_size)\n",
    "    # 3.Initialize train and validation sample index list save it for later use\n",
    "    temp_train_label = all_train_label.copy()\n",
    "    label_train_sample_idx = []\n",
    "    validation_sample_idx = []\n",
    "    # 4. random draw both train labeled samples and validation samples, mark other as unlabeled\n",
    "    # we could also use validation samples to improve performance\n",
    "    for unique_label, training_size, validation_size in co_train_per_class_size:\n",
    "        curr_label_idx = [idx for (idx, label) in temp_train_label.iteritems() if label == unique_label]\n",
    "        curr_label_size = len(curr_label_idx)\n",
    "        # 1. get train sample idx\n",
    "        keep_as_labelled_train_p1 = random.sample(curr_label_idx, training_size)\n",
    "        curr_label_idx_no_train_p2 = [x for x in curr_label_idx if x not in keep_as_labelled_train_p1]\n",
    "        label_train_sample_idx += keep_as_labelled_train_p1\n",
    "        # 2. get validation sample idx\n",
    "        temp_validation_sample_idx = random.sample(curr_label_idx_no_train_p2, validation_size)\n",
    "        validation_sample_idx += temp_validation_sample_idx\n",
    "        unlabel_item_idx = [x for x in curr_label_idx_no_train_p2 if x not in temp_validation_sample_idx]\n",
    "        # 3. set other samples to -1 as unlabeled\n",
    "        for unlabel_idx in unlabel_item_idx:\n",
    "            temp_train_label[unlabel_idx]=-1\n",
    "    return label_train_sample_idx, temp_train_label, validation_sample_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-14T02:53:26.540872Z",
     "start_time": "2020-08-14T02:53:21.480131Z"
    },
    "code_folding": [
     35,
     55,
     76,
     149,
     221,
     243,
     310,
     317
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import collections\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from statistics import mean \n",
    "from collections import Counter\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score,f1_score,accuracy_score\n",
    "\n",
    "# cross validation\n",
    "def k_fold_cv_all_algorithm(dv1, dv2, label, init_labeled_size, muti_view_clf=[], combined_clf=[],\n",
    "                            num_fold=10, dataset_name=None, plot_save_path=None, validation=False):\n",
    "    # set validation dataset as 10% of all data\n",
    "    if validation:\n",
    "        init_validation_size = len(label)*0.1\n",
    "    else:\n",
    "        init_validation_size = 0\n",
    "    kf = StratifiedKFold(n_splits=num_fold)\n",
    "    allTrueLabel = []\n",
    "    co_train_algorithm = [name for clf,name in muti_view_clf]\n",
    "    baseline_algorithm = [name for clf,name in combined_clf]\n",
    "    allPredLabel = collections.defaultdict(list)\n",
    "    all_fold_co_train_iter_count = collections.defaultdict(list)\n",
    "    all_fold_co_train_iter_result = collections.defaultdict(list)\n",
    "    \n",
    "    all_fold_statistic = []\n",
    "    fold = 0\n",
    "    # convert different input type to dataframe for consistency\n",
    "    dv1 = pd.DataFrame(dv1)\n",
    "    dv2 = pd.DataFrame(dv2)\n",
    "    \n",
    "    for train_index, test_index in kf.split(dv1, label):\n",
    "        fold +=1\n",
    "        # print(\"TRAIN:\", train_index, \" \\n TEST:\", test_index)\n",
    "        # ---------------split train and test -------------------- #\n",
    "        dv1_train, dv1_test = dv1.iloc[train_index], dv1.iloc[test_index]\n",
    "        dv2_train, dv2_test = dv2.iloc[train_index], dv2.iloc[test_index]\n",
    "        all_train_label, test_label = label.iloc[train_index], label.iloc[test_index]\n",
    "        '''\n",
    "        Plot true labeled result for different view on each fold, use PCA to reduce views to 2d\n",
    "        Real training process will not using PCA to reduce it's dimension \n",
    "        '''\n",
    "        pca = PCA(n_components=2)\n",
    "        ''' \n",
    "        Comparison Part 1: Use two view concatenated features and train with supervise learning (up bound)\n",
    "        Notice we train supervise learning with all labeled data, no data has been mark as unlabeled\n",
    "        '''\n",
    "        # ---------- collect per fold statistic ---------------------- #\n",
    "        curr_fold_statistic = {'author': dataset_name, 'fold':fold, 'test_size': dv1_test.shape[0]} \n",
    "        ub_concatenated_train = pd.concat([dv1_train,dv2_train], axis=1, ignore_index=True)\n",
    "        concatenated_test = pd.concat([dv1_test,dv2_test], axis=1, ignore_index=True)\n",
    "        for in_clf, clf_name in combined_clf:\n",
    "            UB_clf = copy.deepcopy(in_clf) \n",
    "            UB_clf.fit(ub_concatenated_train, all_train_label)\n",
    "            ub_pred = UB_clf.predict(concatenated_test)\n",
    "            print(clf_name+\"-UB f1: \", metrics.classification_report(test_label, ub_pred))\n",
    "            print(metrics.confusion_matrix(test_label, ub_pred).ravel())\n",
    "            curr_fold_statistic[clf_name+\"-UB f1\"] = f1_score(test_label.values.tolist(), ub_pred,average='macro')\n",
    "            allPredLabel[clf_name+\"-UB predict label\"].append(ub_pred)\n",
    "\n",
    "        ''' \n",
    "        ############# Notice, part 2 and part 3 will run 10 times to obtain an average (stable result) ###############\n",
    "        Comparison Part 2: Use two view and train with co-training (Check effective of co-training)\n",
    "        Notice here we are simulating situation for co-training by set most of data to unlabelled\n",
    "        '''\n",
    "        curr_fold_statistic[\"train_size\"] = init_labeled_size\n",
    "        curr_fold_statistic['validation_size']=init_validation_size\n",
    "        curr_fold_statistic[\"unlabel_size\"] = len(all_train_label) - init_labeled_size\n",
    "        # ------------ collect statistic for all runs ---------------- #\n",
    "        curr_fold_all_runs = collections.defaultdict(list)\n",
    "        curr_fold_co_train_iter_count = collections.defaultdict(list)\n",
    "        curr_fold_co_train_iter_result = collections.defaultdict(list)\n",
    "        for i in range(20):\n",
    "            label_train_sample_idx, final_train_label, validation_sample_idx = simulate_co_training_situation(all_train_label,init_labeled_size,init_validation_size)\n",
    "            print(label_train_sample_idx)\n",
    "            # -------------------- train with co-training ------------------- #\n",
    "            for in_clf, clf_name in muti_view_clf: \n",
    "                if plot_save_path == None:\n",
    "                    print(\"plot off\")\n",
    "                    per_clf_plot_save_path = None\n",
    "                else:\n",
    "                    per_clf_plot_save_path = plot_save_path+dataset_name+\"/fold\"+str(fold)+\"/\"+clf_name+\"/\"\n",
    "                    if not os.path.exists(per_clf_plot_save_path):\n",
    "                        os.makedirs(per_clf_plot_save_path)\n",
    "                co_train_clf = copy.deepcopy(in_clf)\n",
    "                co_train_clf.set_PCA(pca)\n",
    "                co_train_clf.fit(train_data=[dv1_train.copy(), dv2_train.copy(), final_train_label.copy()], \n",
    "                                 validation_data=[dv1_test, dv2_test, test_label],\n",
    "                                 dataset_name=dataset_name, plot_save_path=per_clf_plot_save_path)\n",
    "                curr_fold_all_runs[clf_name+\"_total_iteration\"].append(co_train_clf.get_iter_count())\n",
    "                curr_fold_all_runs[clf_name+\"_select_iteration\"].append(co_train_clf.get_k())\n",
    "                # -------------- get self-labeled sample index --------------- #\n",
    "                self_new_all, self_new_k_iter = co_train_clf.get_self_labeled_sample()\n",
    "                #print(\"Self labeled at iter k sample idx: \", self_new_k_iter)\n",
    "                #print(\"Self labeled idx: \", self_new_all)\n",
    "                \n",
    "                self_new_all_idx = [idx for idx in self_new_all.values()]\n",
    "                all_self_new_idx = list(set([val for sublist in self_new_all_idx for subsublist in sublist for val in subsublist]))\n",
    "                curr_fold_all_runs[clf_name+'_total_self_labeled'].append(len(all_self_new_idx))\n",
    "                \n",
    "                self_new_k_iter_idx = [idx for idx in self_new_k_iter.values()]\n",
    "                iter_k_self_new_idx = list(set([val for sublist in self_new_k_iter_idx for subsublist in sublist for val in subsublist]))\n",
    "                curr_fold_all_runs[clf_name+'_iter_k_self_labeled'].append(len(iter_k_self_new_idx))\n",
    "                #print(\"Self labeled idx: \", all_self_new_idx, \" Size: \", len(all_self_new_idx))\n",
    "                #print(\"Self labeled at iter k sample idx: \", iter_k_self_new_idx, \" Size: \", len(iter_k_self_new_idx))\n",
    "                # ------------- get predicted label for test set ------------- #\n",
    "                co_train_predict = co_train_clf.predict([dv1_test, dv2_test])\n",
    "                print(clf_name+\" f1: \", metrics.classification_report(test_label, co_train_predict))\n",
    "                print(metrics.confusion_matrix(test_label, co_train_predict).ravel())\n",
    "                curr_fold_all_runs[clf_name+\" f1\"].append(f1_score(test_label.values.tolist(), co_train_predict,average='macro'))\n",
    "                allPredLabel[clf_name+\" predict label\"].append(co_train_predict)\n",
    "                # ------------- get co-training per iterations f1 score for plot  ---------- #\n",
    "                curr_fold_co_train_iter_count[clf_name].append(co_train_clf.get_iter_count())\n",
    "                # --------------- h1/h2 plot ----------------- #\n",
    "                diff_iter_f1_dv1, diff_iter_f1_dv2, diff_iter_f1_combined, concatenate_f1 = co_train_clf.co_train_process_f1()\n",
    "                curr_fold_co_train_iter_result[clf_name+\"_dv1\"].append(diff_iter_f1_dv1)\n",
    "                curr_fold_co_train_iter_result[clf_name+\"_dv2\"].append(diff_iter_f1_dv2)\n",
    "                # ------------- combined plot ---------------- #\n",
    "                curr_fold_co_train_iter_result[clf_name+\"_combined\"].append(diff_iter_f1_combined)\n",
    "                curr_fold_co_train_iter_result[clf_name+\"_concatenate_baseline\"].append(concatenate_f1)\n",
    "\n",
    "            ''' \n",
    "            Comparison Part 3: Use two view concatenated features and train with supervise learning (lower bound)\n",
    "            Notice we only train supervise learning with labeled train, data mark as unlabeled is not used\n",
    "            '''\n",
    "            lb_concatenated_train = pd.concat([dv1_train.loc[label_train_sample_idx],\n",
    "                                               dv2_train.loc[label_train_sample_idx]],axis=1, ignore_index=True)\n",
    "            lb_train_label = [final_train_label[idx] for idx in label_train_sample_idx]\n",
    "            for in_clf, clf_name in combined_clf:\n",
    "                LB_clf = copy.deepcopy(in_clf)\n",
    "                LB_clf.fit(lb_concatenated_train, lb_train_label)\n",
    "                lb_pred = LB_clf.predict(concatenated_test)\n",
    "                print(clf_name+\"-LB f1: \", metrics.classification_report(test_label, lb_pred))\n",
    "                print(metrics.confusion_matrix(test_label, lb_pred).ravel())\n",
    "                curr_fold_all_runs[clf_name+\"-LB f1\"].append(f1_score(test_label.values.tolist(), lb_pred,average='macro'))\n",
    "                allPredLabel[clf_name+\"-LB predict label\"].append(lb_pred)\n",
    "\n",
    "        # ---------- calculate average of all runs to obtain a stable result ------------- #\n",
    "        for key,value in curr_fold_all_runs.items():\n",
    "            #print(key,\":\",value,\":\",value,\":\",mean(value))\n",
    "            curr_fold_statistic[key] = mean(value)\n",
    "        allTrueLabel.extend(test_label.values.tolist())\n",
    "        all_fold_statistic.append(curr_fold_statistic)\n",
    "        \n",
    "        # -------- plot averaged result for 20 runs in each fold ------------- #\n",
    "        for clf_name in co_train_algorithm:\n",
    "            # each run have different iteration, find minimum of iteration in all runs.\n",
    "            min_iter_in_runs = min(curr_fold_co_train_iter_count[clf_name])\n",
    "            print(clf_name,\" per run iteration: \", curr_fold_co_train_iter_count[clf_name], \"min iter is: \", min_iter_in_runs)\n",
    "            for idx, sublist in enumerate(curr_fold_co_train_iter_result[clf_name+\"_dv1\"]):\n",
    "                curr_fold_co_train_iter_result[clf_name+\"_dv1\"][idx] = sublist[:min_iter_in_runs]\n",
    "            for idx, sublist in enumerate(curr_fold_co_train_iter_result[clf_name+\"_dv2\"]):\n",
    "                curr_fold_co_train_iter_result[clf_name+\"_dv2\"][idx] = sublist[:min_iter_in_runs]\n",
    "            for idx, sublist in enumerate(curr_fold_co_train_iter_result[clf_name+\"_combined\"]):\n",
    "                curr_fold_co_train_iter_result[clf_name+\"_combined\"][idx] = sublist[:min_iter_in_runs]\n",
    "            # -------- mean with respect to all run --------- #\n",
    "            mean_curr_fold_diff_iter_dv1 = np.mean(curr_fold_co_train_iter_result[clf_name+\"_dv1\"], axis=0)\n",
    "            mean_curr_fold_diff_iter_dv2 = np.mean(curr_fold_co_train_iter_result[clf_name+\"_dv2\"], axis=0)\n",
    "            mean_curr_fold_diff_iter_combined = np.mean(curr_fold_co_train_iter_result[clf_name+\"_combined\"], axis=0)\n",
    "            mean_curr_fold_diff_iter_concatenate_baseline = np.mean(curr_fold_co_train_iter_result[clf_name+\"_concatenate_baseline\"])\n",
    "            # -------- initial variables for plot ------------ #\n",
    "            default_text_based = [mean_curr_fold_diff_iter_dv1[0]] * min_iter_in_runs\n",
    "            default_citation_based = [mean_curr_fold_diff_iter_dv2[0]] * min_iter_in_runs\n",
    "            co_train_text_based = mean_curr_fold_diff_iter_dv1\n",
    "            co_train_citation_based = mean_curr_fold_diff_iter_dv2\n",
    "            step = np.arange(start=0, stop=min_iter_in_runs, step=1)\n",
    "            \n",
    "            # ----------- plot h1/h2 details ----------------------- #\n",
    "            fig = plt.figure(figsize=(8,6))\n",
    "            ax = plt.axes()\n",
    "            plt.plot(step, default_text_based, linestyle='dashed', label=\"Text based default\")\n",
    "            plt.plot(step, default_citation_based, linestyle='dashdot', label=\"Citation based default\")\n",
    "            plt.plot(step, co_train_text_based, linestyle='solid', marker = \"*\", label=\"Text based\")\n",
    "            plt.plot(step, co_train_citation_based, linestyle='dotted', marker = \"+\", label=\"Citation based\")\n",
    "            handles, _  = ax.get_legend_handles_labels()\n",
    "            plt.axvline(x=(min(min_iter_in_runs-1,30)), color='r', label=\"k\")\n",
    "            _, labels = ax.get_legend_handles_labels()\n",
    "            vertical_line = lines.Line2D([], [],  marker='|', linestyle='None', color=\"r\", markersize=10, markeredgewidth=1.5)\n",
    "            handles.append(vertical_line)\n",
    "            ax.autoscale_view()\n",
    "            legend = ax.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 1.2), ncol=2)\n",
    "            plt.xlabel('Co-Training Iterations')\n",
    "            plt.ylabel('F1 score')\n",
    "            figure_name = plot_save_path+dataset_name+\"/fold\"+str(fold)+\"/\"+clf_name+\"_mean_diff_iter_f1.png\"\n",
    "            plt.savefig(fname=figure_name, dpi=150, bbox_extra_artists=(legend,), bbox_inches='tight')\n",
    "            #plt.show()\n",
    "            plt.close(\"all\")\n",
    "            # ---------------- plot combined details ------------------ #\n",
    "            default_concatenated = [mean_curr_fold_diff_iter_concatenate_baseline] * min_iter_in_runs\n",
    "            default_combined = [mean_curr_fold_diff_iter_combined[0]] * min_iter_in_runs\n",
    "            co_train_combined = mean_curr_fold_diff_iter_combined\n",
    "            fig = plt.figure(figsize=(8,6))\n",
    "            ax = plt.axes()\n",
    "            plt.plot(step, default_concatenated, linestyle='dashed', label=\"Concatenate default\")\n",
    "            plt.plot(step, default_combined, linestyle='dotted', label=\"Combined default\")\n",
    "            plt.plot(step, co_train_combined, linestyle='solid', marker = \"+\", label=\"Combined Co-train\")\n",
    "            handles, _  = ax.get_legend_handles_labels()\n",
    "            plt.axvline(x=(min(min_iter_in_runs-1,30)), color='r', label=\"k\")\n",
    "            _, labels = ax.get_legend_handles_labels()\n",
    "            vertical_line = lines.Line2D([], [],  marker='|', linestyle='None', color=\"r\", markersize=10, markeredgewidth=1.5)\n",
    "            handles.append(vertical_line)\n",
    "            ax.autoscale_view()\n",
    "            legend = ax.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=2)\n",
    "            plt.xlabel('Co-Training Iterations')\n",
    "            plt.ylabel('F1 score')\n",
    "            figure_name = plot_save_path+dataset_name+\"/fold\"+str(fold)+\"/\"+clf_name+\"_mean_combined_diff_iter_f1.png\"\n",
    "            plt.savefig(fname=figure_name, dpi=150, bbox_extra_artists=(legend,), bbox_inches='tight')\n",
    "            #plt.show()\n",
    "            plt.close(\"all\")\n",
    "            # ---------- collect everyfold average statistic for all fold average ----------- #\n",
    "            all_fold_co_train_iter_count[clf_name].append(min_iter_in_runs)\n",
    "            all_fold_co_train_iter_result[clf_name+\"_dv1\"].append(mean_curr_fold_diff_iter_dv1)\n",
    "            all_fold_co_train_iter_result[clf_name+\"_dv2\"].append(mean_curr_fold_diff_iter_dv2)\n",
    "            all_fold_co_train_iter_result[clf_name+\"_combined\"].append(mean_curr_fold_diff_iter_combined)\n",
    "            all_fold_co_train_iter_result[clf_name+\"_concatenated_baseline\"].append(mean_curr_fold_diff_iter_concatenate_baseline)\n",
    "    \n",
    "    ''' # --------------- plot per fold result f1 variance --------------- # '''\n",
    "    if (plot_save_path !=None) and (dataset_name != None):\n",
    "        all_per_fold_f1_score_variance_plot = pd.DataFrame(all_fold_statistic)\n",
    "        #print(all_per_fold_f1_score_variance_plot)\n",
    "        sns.set(rc={'figure.figsize':(10,8)})\n",
    "        plot_temp_f1 = pd.DataFrame()\n",
    "        for clf_name in co_train_algorithm:\n",
    "            clf_temp_f1 = all_per_fold_f1_score_variance_plot[[clf_name+' f1']].values\n",
    "            plot_temp_f1[clf_name]=clf_temp_f1.flatten()\n",
    "        for clf_name in baseline_algorithm:\n",
    "            for clf_type in [\"-LB\"]:\n",
    "                clf_temp_f1 = all_per_fold_f1_score_variance_plot[[clf_name+clf_type+' f1']].values\n",
    "                plot_temp_f1[clf_name]=clf_temp_f1.flatten()\n",
    "        plot_temp_f1 = pd.melt(plot_temp_f1, var_name='methods', value_name='f1')\n",
    "        #print(plot_temp_f1)\n",
    "        ax = sns.boxplot(x=\"methods\", y=\"f1\", data=plot_temp_f1)\n",
    "        ax = sns.swarmplot(x=\"methods\", y=\"f1\", data=plot_temp_f1, color=\".25\")\n",
    "        ax.set_title(dataset_name+\" result variance within \"+str(num_fold)+\" fold\")\n",
    "        plt.savefig(plot_save_path+dataset_name+\"/all_method_result_variance.png\", dpi=150)\n",
    "        #plt.show()\n",
    "        plt.close(\"all\")\n",
    "\n",
    "        '''  plot averaged f1 score wrt different fold in different iterations in co-training process '''\n",
    "        for clf_name in co_train_algorithm:\n",
    "            # each fold have different iteration, find minimum of iteration and \n",
    "            min_iter_in_folds = min(all_fold_co_train_iter_count[clf_name])\n",
    "            print(clf_name,\" per fold iteration: \", all_fold_co_train_iter_count[clf_name],\" min iter: \",min_iter_in_folds)\n",
    "            for idx, sublist in enumerate(all_fold_co_train_iter_result[clf_name+\"_dv1\"]):\n",
    "                all_fold_co_train_iter_result[clf_name+\"_dv1\"][idx] = sublist[:min_iter_in_folds]\n",
    "            for idx, sublist in enumerate(all_fold_co_train_iter_result[clf_name+\"_dv2\"]):\n",
    "                all_fold_co_train_iter_result[clf_name+\"_dv2\"][idx] = sublist[:min_iter_in_folds]\n",
    "            for idx, sublist in enumerate(all_fold_co_train_iter_result[clf_name+\"_combined\"]):\n",
    "                all_fold_co_train_iter_result[clf_name+\"_combined\"][idx] = sublist[:min_iter_in_folds]\n",
    "            \n",
    "            # -------- mean with respect to all fold --------- #\n",
    "            mean_all_fold_diff_iter_dv1 = np.mean(all_fold_co_train_iter_result[clf_name+\"_dv1\"], axis=0)\n",
    "            mean_all_fold_diff_iter_dv2 = np.mean(all_fold_co_train_iter_result[clf_name+\"_dv2\"], axis=0)\n",
    "            mean_all_fold_diff_iter_combined = np.mean(all_fold_co_train_iter_result[clf_name+\"_combined\"], axis=0)\n",
    "            mean_all_fold_diff_iter_baseline = np.mean(all_fold_co_train_iter_result[clf_name+\"_concatenated_baseline\"], axis=0)\n",
    "            # -------- initial variables for plot ------------ #\n",
    "            step = np.arange(start=0, stop=min_iter_in_folds, step=1)\n",
    "            default_text_based = [mean_all_fold_diff_iter_dv1[0]] * min_iter_in_folds\n",
    "            default_citation_based = [mean_all_fold_diff_iter_dv2[0]] * min_iter_in_folds\n",
    "            co_train_text_based = mean_all_fold_diff_iter_dv1\n",
    "            co_train_citation_based = mean_all_fold_diff_iter_dv2\n",
    "            # --------------------- plot averaged training details ----------------------- #\n",
    "            # -------------- h1/h2 ---------------- #\n",
    "            fig = plt.figure(figsize=(8,6))\n",
    "            ax = plt.axes()\n",
    "            plt.plot(step, default_text_based, linestyle='dashed', label=\"Text based default\")\n",
    "            plt.plot(step, default_citation_based, linestyle='dashdot', label=\"Citation based default\")\n",
    "            plt.plot(step, co_train_text_based, linestyle='solid', marker = \"*\", label=\"Text based\")\n",
    "            plt.plot(step, co_train_citation_based, linestyle='dotted', marker = \"+\", label=\"Citation based\")\n",
    "            handles, _  = ax.get_legend_handles_labels()\n",
    "            plt.axvline(x=min(min_iter_in_folds-1,30), color='r', label=\"k\")\n",
    "            _, labels = ax.get_legend_handles_labels()\n",
    "            vertical_line = lines.Line2D([], [],  marker='|', linestyle='None', color=\"r\", markersize=10, markeredgewidth=1.5)\n",
    "            handles.append(vertical_line)\n",
    "            ax.autoscale_view()\n",
    "            legend = ax.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 1.2), ncol=2)\n",
    "            plt.xlabel('Co-Training Iterations')\n",
    "            plt.ylabel('F1 score')\n",
    "            plt.savefig((plot_save_path+dataset_name+\"/\"+clf_name+\"_mean_diff_iter_f1.png\"), dpi=150, bbox_extra_artists=(legend,), bbox_inches='tight')\n",
    "            #plt.show()\n",
    "            plt.close(\"all\")\n",
    "            # ----------- combined ------------- #\n",
    "            default_concatenated = [mean_all_fold_diff_iter_baseline] * min_iter_in_folds\n",
    "            default_combined = [mean_all_fold_diff_iter_combined[0]] * min_iter_in_folds\n",
    "            co_train_combined = mean_all_fold_diff_iter_combined\n",
    "            fig = plt.figure(figsize=(8,6))\n",
    "            ax = plt.axes()\n",
    "            plt.plot(step, default_concatenated, linestyle='dashed', label=\"Concatenate default\")\n",
    "            plt.plot(step, default_combined, linestyle='dotted', label=\"Combined default\")\n",
    "            plt.plot(step, co_train_combined, linestyle='solid', marker = \"+\", label=\"Combined Co-train\")\n",
    "            handles, _  = ax.get_legend_handles_labels()\n",
    "            plt.axvline(x=min(min_iter_in_folds-1,30), color='r', label=\"k\")\n",
    "            _, labels = ax.get_legend_handles_labels()\n",
    "            vertical_line = lines.Line2D([], [],  marker='|', linestyle='None', color=\"r\", markersize=10, markeredgewidth=1.5)\n",
    "            handles.append(vertical_line)\n",
    "            ax.autoscale_view()\n",
    "            legend = ax.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=2)\n",
    "            plt.xlabel('Co-Training Iterations')\n",
    "            plt.ylabel('F1 score')\n",
    "            figure_name = plot_save_path+dataset_name+\"/\"+clf_name+\"_mean_combined_diff_iter_f1.png\"\n",
    "            plt.savefig(fname=figure_name, dpi=150, bbox_extra_artists=(legend,), bbox_inches='tight')\n",
    "            #plt.show()\n",
    "            plt.close(\"all\")\n",
    "    \n",
    "    ''' The results of a k-fold cross-validation run are often summarized with the mean of the model scores.'''\n",
    "    final_f1_score = []\n",
    "    for clf_name in co_train_algorithm:\n",
    "        clf_all_fold_f1 =[]\n",
    "        for per_fold_statistic in all_fold_statistic:\n",
    "            clf_per_fold_f1 = per_fold_statistic[clf_name+\" f1\"]\n",
    "            clf_all_fold_f1.append(clf_per_fold_f1)\n",
    "        clf_mean_f1 = np.mean(clf_all_fold_f1, axis=0)\n",
    "        final_f1_score.append((clf_name,clf_mean_f1))\n",
    "    for clf_name in baseline_algorithm:\n",
    "        for clf_type in [\"-UB\",\"-LB\"]:\n",
    "            clf_all_fold_f1 =[]\n",
    "            for per_fold_statistic in all_fold_statistic:\n",
    "                clf_per_fold_f1 = per_fold_statistic[clf_name+clf_type+\" f1\"]\n",
    "                clf_all_fold_f1.append(clf_per_fold_f1)\n",
    "            clf_mean_f1 = np.mean(clf_all_fold_f1, axis=0)\n",
    "            final_f1_score.append((clf_name+clf_type,clf_mean_f1))\n",
    "    \n",
    "    return final_f1_score, all_fold_statistic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-14T03:19:53.447850Z",
     "start_time": "2020-08-14T03:19:11.024778Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load text embedding:  pv_dbow\n",
      "Total text vector records: 135796\n",
      "Vector dimension:  100\n",
      "Load citation embedding:  n2v\n",
      "Total citation vector records: 124922\n",
      "Vector dimension:  100\n",
      "For name:  j_read\n",
      "(136, 2)\n",
      "j_read  pass\n",
      "For name:  f_esteves\n",
      "(34, 2)\n",
      "f_esteves  pass\n",
      "For name:  c_miller\n",
      "(252, 2)\n",
      "c_miller  pass\n",
      "For name:  r_jha\n",
      "(11, 2)\n",
      "r_jha  pass\n",
      "For name:  a_lowe\n",
      "(102, 2)\n",
      "a_lowe  pass\n",
      "For name:  a_vega\n",
      "(20, 2)\n",
      "a_vega  pass\n",
      "For name:  k_smith\n",
      "(338, 2)\n",
      "k_smith  pass\n",
      "For name:  j_gordon\n",
      "(19, 2)\n",
      "j_gordon  pass\n",
      "For name:  s_liao\n",
      "(104, 2)\n",
      "s_liao  pass\n",
      "For name:  j_qian\n",
      "(17, 2)\n",
      "j_qian  pass\n",
      "For name:  s_bernardi\n",
      "(91, 2)\n",
      "s_bernardi  pass\n",
      "For name:  t_hill\n",
      "(15, 2)\n",
      "t_hill  pass\n",
      "For name:  s_schindler\n",
      "(51, 2)\n",
      "s_schindler  pass\n",
      "For name:  j_williams\n",
      "(625, 2)\n",
      "j_williams  pass\n",
      "For name:  s_jacobson\n",
      "(28, 2)\n",
      "s_jacobson  pass\n",
      "For name:  e_andrade\n",
      "(17, 2)\n",
      "e_andrade  pass\n",
      "For name:  t_santos\n",
      "(45, 2)\n",
      "t_santos  pass\n",
      "For name:  k_kim\n",
      "(1111, 2)\n",
      "Total sample size before apply threshold:  1111\n",
      "Counter({'0000-0002-6929-5359': 211, '0000-0001-9498-284X': 154, '0000-0002-5878-8895': 139, '0000-0002-1864-3392': 92, '0000-0002-7045-8004': 57, '0000-0001-7896-6751': 57, '0000-0002-7991-9428': 55, '0000-0002-4010-1063': 45, '0000-0002-2186-3484': 28, '0000-0002-4899-1929': 25, '0000-0003-0487-4242': 24, '0000-0002-3642-1486': 22, '0000-0001-9965-3535': 17, '0000-0002-4168-757X': 17, '0000-0001-6525-3744': 14, '0000-0002-3897-0278': 14, '0000-0002-1181-5112': 12, '0000-0003-1447-9385': 11, '0000-0002-7305-8786': 11, '0000-0002-2655-7806': 10, '0000-0003-3466-5353': 9, '0000-0002-7359-663X': 8, '0000-0003-4600-8668': 6, '0000-0002-1382-7088': 5, '0000-0002-9505-4882': 5, '0000-0003-3667-9900': 4, '0000-0001-9714-6038': 4, '0000-0002-4760-0228': 3, '0000-0003-4188-7915': 3, '0000-0001-9454-0427': 3, '0000-0002-0333-6808': 3, '0000-0003-2134-4964': 3, '0000-0002-6658-047X': 3, '0000-0003-1273-379X': 3, '0000-0002-7047-3183': 3, '0000-0002-1814-9546': 3, '0000-0003-4812-6297': 2, '0000-0001-6597-578X': 2, '0000-0002-5285-9138': 2, '0000-0002-6796-7844': 2, '0000-0002-1130-8698': 2, '0000-0001-8518-8150': 2, '0000-0002-7103-924X': 2, '0000-0002-5407-0202': 1, '0000-0001-6220-8411': 1, '0000-0002-7440-6703': 1, '0000-0002-1603-7559': 1, '0000-0003-0257-1707': 1, '0000-0001-8532-6517': 1, '0000-0001-6626-316X': 1, '0000-0002-3246-9861': 1, '0000-0002-7207-4389': 1, '0000-0001-9682-9654': 1, '0000-0002-0196-3832': 1, '0000-0001-8063-6081': 1, '0000-0003-2037-3333': 1, '0000-0001-8872-6751': 1})\n",
      "Total author before apply threshoid:  57\n",
      "Total author after apply threshoid:  3\n",
      "Total sample size after apply threshold:  504\n",
      "Total missing sample:  0\n",
      "(504, 101)\n",
      "Total missing sample:  47\n",
      "(504, 101)\n",
      "Papers with no citation:  [1, 6, 14, 19, 28, 30, 35, 43, 57, 74, 76, 82, 88, 91, 127, 154, 165, 175, 192, 202, 218, 221, 226, 242, 245, 259, 260, 267, 294, 315, 323, 327, 357, 364, 393, 410, 414, 422, 424, 427, 434, 438, 444, 446, 468, 470, 478]\n",
      "Total paper with citation:  457\n",
      "k_kim is multi-class case, ignored\n",
      "For name:  d_ricci\n",
      "(40, 2)\n",
      "d_ricci  pass\n",
      "For name:  s_cameron\n",
      "(66, 2)\n",
      "s_cameron  pass\n",
      "For name:  t_wright\n",
      "(31, 2)\n",
      "t_wright  pass\n",
      "For name:  r_cunha\n",
      "(209, 2)\n",
      "r_cunha  pass\n",
      "For name:  s_fuchs\n",
      "(32, 2)\n",
      "s_fuchs  pass\n",
      "For name:  m_nawaz\n",
      "(9, 2)\n",
      "m_nawaz  pass\n",
      "For name:  k_harris\n",
      "(47, 2)\n",
      "k_harris  pass\n",
      "For name:  r_daniel\n",
      "(173, 2)\n",
      "r_daniel  pass\n",
      "For name:  k_xu\n",
      "(37, 2)\n",
      "k_xu  pass\n",
      "For name:  s_antunes\n",
      "(54, 2)\n",
      "s_antunes  pass\n",
      "For name:  k_cho\n",
      "(126, 2)\n",
      "k_cho  pass\n",
      "For name:  j_sanderson\n",
      "(31, 2)\n",
      "j_sanderson  pass\n",
      "For name:  s_uddin\n",
      "(39, 2)\n",
      "s_uddin  pass\n",
      "For name:  a_batista\n",
      "(48, 2)\n",
      "a_batista  pass\n",
      "For name:  h_pereira\n",
      "(70, 2)\n",
      "h_pereira  pass\n",
      "For name:  a_patel\n",
      "(262, 2)\n",
      "a_patel  pass\n",
      "For name:  r_graham\n",
      "(52, 2)\n",
      "r_graham  pass\n",
      "For name:  a_nilsson\n",
      "(42, 2)\n",
      "a_nilsson  pass\n",
      "For name:  m_soto\n",
      "(97, 2)\n",
      "m_soto  pass\n",
      "For name:  g_guidi\n",
      "(37, 2)\n",
      "g_guidi  pass\n",
      "For name:  e_andersson\n",
      "(138, 2)\n",
      "e_andersson  pass\n",
      "For name:  s_reid\n",
      "(132, 2)\n",
      "s_reid  pass\n",
      "For name:  a_maleki\n",
      "(25, 2)\n",
      "a_maleki  pass\n",
      "For name:  j_moon\n",
      "(203, 2)\n",
      "j_moon  pass\n",
      "For name:  t_abe\n",
      "(50, 2)\n",
      "t_abe  pass\n",
      "For name:  x_fu\n",
      "(16, 2)\n",
      "x_fu  pass\n",
      "For name:  f_ortega\n",
      "(368, 2)\n",
      "f_ortega  pass\n",
      "For name:  r_morris\n",
      "(409, 2)\n",
      "r_morris  pass\n",
      "For name:  w_fang\n",
      "(43, 2)\n",
      "w_fang  pass\n",
      "For name:  m_amaral\n",
      "(134, 2)\n",
      "m_amaral  pass\n",
      "For name:  h_song\n",
      "(210, 2)\n",
      "h_song  pass\n",
      "For name:  h_dai\n",
      "(6, 2)\n",
      "h_dai  pass\n",
      "For name:  y_nakajima\n",
      "(12, 2)\n",
      "y_nakajima  pass\n",
      "For name:  t_warner\n",
      "(68, 2)\n",
      "t_warner  pass\n",
      "For name:  s_saha\n",
      "(111, 2)\n",
      "s_saha  pass\n",
      "For name:  j_fernandez\n",
      "(28, 2)\n",
      "j_fernandez  pass\n",
      "For name:  m_pan\n",
      "(146, 2)\n",
      "m_pan  pass\n",
      "For name:  a_simon\n",
      "(117, 2)\n",
      "a_simon  pass\n",
      "For name:  r_freitas\n",
      "(73, 2)\n",
      "r_freitas  pass\n",
      "For name:  c_yun\n",
      "(284, 2)\n",
      "c_yun  pass\n",
      "For name:  j_huang\n",
      "(443, 2)\n",
      "j_huang  pass\n",
      "For name:  p_santos\n",
      "(92, 2)\n",
      "p_santos  pass\n",
      "For name:  n_young\n",
      "(182, 2)\n",
      "n_young  pass\n",
      "For name:  d_ross\n",
      "(25, 2)\n",
      "d_ross  pass\n",
      "For name:  q_wang\n",
      "(348, 2)\n",
      "q_wang  pass\n",
      "For name:  c_cardoso\n",
      "(52, 2)\n",
      "c_cardoso  pass\n",
      "For name:  j_matthews\n",
      "(65, 2)\n",
      "j_matthews  pass\n",
      "For name:  g_lee\n",
      "(202, 2)\n",
      "g_lee  pass\n",
      "For name:  m_salem\n",
      "(25, 2)\n",
      "m_salem  pass\n",
      "For name:  h_lai\n",
      "(165, 2)\n",
      "h_lai  pass\n",
      "For name:  r_harris\n",
      "(50, 2)\n",
      "r_harris  pass\n",
      "For name:  c_vaughan\n",
      "(83, 2)\n",
      "c_vaughan  pass\n",
      "For name:  e_thompson\n",
      "(181, 2)\n",
      "e_thompson  pass\n",
      "For name:  r_gomes\n",
      "(52, 2)\n",
      "r_gomes  pass\n",
      "For name:  r_bennett\n",
      "(93, 2)\n",
      "r_bennett  pass\n",
      "For name:  m_collins\n",
      "(57, 2)\n",
      "m_collins  pass\n",
      "For name:  m_cowley\n",
      "(132, 2)\n",
      "m_cowley  pass\n",
      "For name:  p_teixeira\n",
      "(213, 2)\n",
      "p_teixeira  pass\n",
      "For name:  c_cox\n",
      "(48, 2)\n",
      "c_cox  pass\n",
      "For name:  s_hsu\n",
      "(204, 2)\n",
      "s_hsu  pass\n",
      "For name:  f_williams\n",
      "(149, 2)\n",
      "f_williams  pass\n",
      "For name:  d_parsons\n",
      "(30, 2)\n",
      "d_parsons  pass\n",
      "For name:  a_choudhury\n",
      "(56, 2)\n",
      "a_choudhury  pass\n",
      "For name:  c_richter\n",
      "(11, 2)\n",
      "c_richter  pass\n",
      "For name:  m_hossain\n",
      "(102, 2)\n",
      "m_hossain  pass\n",
      "For name:  v_alves\n",
      "(24, 2)\n",
      "v_alves  pass\n",
      "For name:  j_becker\n",
      "(177, 2)\n",
      "j_becker  pass\n",
      "For name:  m_soares\n",
      "(247, 2)\n",
      "m_soares  pass\n",
      "For name:  j_yi\n",
      "(29, 2)\n",
      "j_yi  pass\n",
      "For name:  s_khan\n",
      "(193, 2)\n",
      "s_khan  pass\n",
      "For name:  a_rao\n",
      "(93, 2)\n",
      "a_rao  pass\n",
      "For name:  d_cameron\n",
      "(49, 2)\n",
      "d_cameron  pass\n",
      "For name:  c_morgan\n",
      "(43, 2)\n",
      "c_morgan  pass\n",
      "For name:  h_cui\n",
      "(40, 2)\n",
      "h_cui  pass\n",
      "For name:  p_zhang\n",
      "(137, 2)\n",
      "p_zhang  pass\n",
      "For name:  j_fernandes\n",
      "(208, 2)\n",
      "j_fernandes  pass\n",
      "For name:  a_jain\n",
      "(67, 2)\n",
      "a_jain  pass\n",
      "For name:  d_zhang\n",
      "(94, 2)\n",
      "d_zhang  pass\n",
      "For name:  b_huang\n",
      "(48, 2)\n",
      "b_huang  pass\n",
      "For name:  m_chong\n",
      "(43, 2)\n",
      "m_chong  pass\n",
      "For name:  m_cerqueira\n",
      "(41, 2)\n",
      "m_cerqueira  pass\n",
      "For name:  p_yang\n",
      "(227, 2)\n",
      "p_yang  pass\n",
      "For name:  j_marques\n",
      "(183, 2)\n",
      "j_marques  pass\n",
      "For name:  n_ali\n",
      "(14, 2)\n",
      "n_ali  pass\n",
      "For name:  h_ng\n",
      "(109, 2)\n",
      "h_ng  pass\n",
      "For name:  m_viana\n",
      "(139, 2)\n",
      "m_viana  pass\n",
      "For name:  t_inoue\n",
      "(70, 2)\n",
      "t_inoue  pass\n",
      "For name:  b_meyer\n",
      "(92, 2)\n",
      "b_meyer  pass\n",
      "For name:  c_liao\n",
      "(35, 2)\n",
      "c_liao  pass\n",
      "For name:  k_wheeler\n",
      "(28, 2)\n",
      "k_wheeler  pass\n",
      "For name:  m_rizzo\n",
      "(152, 2)\n",
      "m_rizzo  pass\n",
      "For name:  y_shi\n",
      "(67, 2)\n",
      "y_shi  pass\n",
      "For name:  c_luo\n",
      "(78, 2)\n",
      "c_luo  pass\n",
      "For name:  j_arthur\n",
      "(42, 2)\n",
      "j_arthur  pass\n",
      "For name:  m_ansari\n",
      "(34, 2)\n",
      "m_ansari  pass\n",
      "For name:  g_anderson\n",
      "(103, 2)\n",
      "g_anderson  pass\n",
      "For name:  m_hidalgo\n",
      "(279, 2)\n",
      "m_hidalgo  pass\n",
      "For name:  k_jacobsen\n",
      "(113, 2)\n",
      "k_jacobsen  pass\n",
      "For name:  s_kelly\n",
      "(102, 2)\n",
      "s_kelly  pass\n",
      "For name:  s_james\n",
      "(59, 2)\n",
      "s_james  pass\n",
      "For name:  p_persson\n",
      "(80, 2)\n",
      "p_persson  pass\n",
      "For name:  y_tanaka\n",
      "(20, 2)\n",
      "y_tanaka  pass\n",
      "For name:  c_gao\n",
      "(189, 2)\n",
      "c_gao  pass\n",
      "For name:  w_jung\n",
      "(33, 2)\n",
      "w_jung  pass\n",
      "For name:  s_lewis\n",
      "(306, 2)\n",
      "s_lewis  pass\n",
      "For name:  w_han\n",
      "(34, 2)\n",
      "w_han  pass\n",
      "For name:  m_shah\n",
      "(17, 2)\n",
      "m_shah  pass\n",
      "For name:  c_arango\n",
      "(185, 2)\n",
      "c_arango  pass\n",
      "For name:  r_young\n",
      "(361, 2)\n",
      "r_young  pass\n",
      "For name:  r_coleman\n",
      "(34, 2)\n",
      "r_coleman  pass\n",
      "For name:  b_kang\n",
      "(20, 2)\n",
      "b_kang  pass\n",
      "For name:  s_carter\n",
      "(205, 2)\n",
      "s_carter  pass\n",
      "For name:  c_thomas\n",
      "(102, 2)\n",
      "c_thomas  pass\n",
      "For name:  m_gutierrez\n",
      "(32, 2)\n",
      "m_gutierrez  pass\n",
      "For name:  s_moon\n",
      "(85, 2)\n",
      "s_moon  pass\n",
      "For name:  r_pereira\n",
      "(202, 2)\n",
      "r_pereira  pass\n",
      "For name:  a_nielsen\n",
      "(132, 2)\n",
      "a_nielsen  pass\n",
      "For name:  j_conde\n",
      "(84, 2)\n",
      "j_conde  pass\n",
      "For name:  k_wright\n",
      "(59, 2)\n",
      "k_wright  pass\n",
      "For name:  m_parker\n",
      "(280, 2)\n",
      "m_parker  pass\n",
      "For name:  h_huang\n",
      "(224, 2)\n",
      "h_huang  pass\n",
      "For name:  j_terry\n",
      "(57, 2)\n",
      "j_terry  pass\n",
      "For name:  y_xu\n",
      "(137, 2)\n",
      "y_xu  pass\n",
      "For name:  a_melo\n",
      "(48, 2)\n",
      "a_melo  pass\n",
      "For name:  r_doyle\n",
      "(11, 2)\n",
      "r_doyle  pass\n",
      "For name:  m_bernardo\n",
      "(250, 2)\n",
      "m_bernardo  pass\n",
      "For name:  j_soares\n",
      "(49, 2)\n",
      "j_soares  pass\n",
      "For name:  j_richard\n",
      "(179, 2)\n",
      "j_richard  pass\n",
      "For name:  p_robinson\n",
      "(275, 2)\n",
      "Total sample size before apply threshold:  275\n",
      "Counter({'0000-0002-7878-0313': 133, '0000-0002-0736-9199': 119, '0000-0002-3156-3418': 19, '0000-0002-0577-3147': 4})\n",
      "Total author before apply threshoid:  4\n",
      "Total author after apply threshoid:  2\n",
      "Total sample size after apply threshold:  252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing sample:  0\n",
      "(252, 101)\n",
      "Total missing sample:  6\n",
      "(252, 101)\n",
      "Papers with no citation:  [12, 61, 73, 140, 165, 168]\n",
      "Total paper with citation:  246\n",
      "For name:  c_zou\n",
      "(32, 2)\n",
      "c_zou  pass\n",
      "For name:  s_rana\n",
      "(42, 2)\n",
      "s_rana  pass\n",
      "For name:  a_nunes\n",
      "(61, 2)\n",
      "a_nunes  pass\n",
      "For name:  s_jeong\n",
      "(93, 2)\n",
      "s_jeong  pass\n",
      "For name:  b_olsen\n",
      "(213, 2)\n",
      "b_olsen  pass\n",
      "For name:  m_reilly\n",
      "(20, 2)\n",
      "m_reilly  pass\n",
      "For name:  d_nguyen\n",
      "(25, 2)\n",
      "d_nguyen  pass\n",
      "For name:  r_santos\n",
      "(184, 2)\n",
      "r_santos  pass\n",
      "For name:  f_ferreira\n",
      "(224, 2)\n",
      "f_ferreira  pass\n",
      "For name:  y_ng\n",
      "(19, 2)\n",
      "y_ng  pass\n",
      "For name:  j_madsen\n",
      "(69, 2)\n",
      "j_madsen  pass\n",
      "For name:  d_collins\n",
      "(31, 2)\n",
      "d_collins  pass\n",
      "For name:  l_davies\n",
      "(96, 2)\n",
      "l_davies  pass\n",
      "For name:  m_mora\n",
      "(131, 2)\n",
      "m_mora  pass\n",
      "For name:  a_fontana\n",
      "(203, 2)\n",
      "a_fontana  pass\n",
      "For name:  r_chen\n",
      "(367, 2)\n",
      "r_chen  pass\n",
      "For name:  s_krause\n",
      "(70, 2)\n",
      "s_krause  pass\n",
      "For name:  t_smith\n",
      "(603, 2)\n",
      "Total sample size before apply threshold:  603\n",
      "Counter({'0000-0002-3650-9381': 154, '0000-0003-1673-2954': 113, '0000-0002-2120-2766': 85, '0000-0002-6279-9685': 84, '0000-0003-3528-6793': 65, '0000-0003-4453-9713': 32, '0000-0002-5197-5030': 26, '0000-0002-3945-630X': 10, '0000-0001-7894-6814': 9, '0000-0002-5750-0706': 6, '0000-0002-5495-8906': 4, '0000-0003-3762-6253': 4, '0000-0002-0479-4261': 3, '0000-0003-2389-461X': 2, '0000-0001-6272-8871': 2, '0000-0001-7683-2653': 1, '0000-0002-2104-2264': 1, '0000-0001-9068-4642': 1, '0000-0002-1881-2766': 1})\n",
      "Total author before apply threshoid:  19\n",
      "Total author after apply threshoid:  2\n",
      "Total sample size after apply threshold:  267\n",
      "Total missing sample:  0\n",
      "(267, 101)\n",
      "Total missing sample:  12\n",
      "(267, 101)\n",
      "Papers with no citation:  [8, 10, 34, 48, 61, 72, 135, 143, 147, 158, 214, 239]\n",
      "Total paper with citation:  255\n",
      "For name:  a_biswas\n",
      "(10, 2)\n",
      "a_biswas  pass\n",
      "For name:  j_day\n",
      "(16, 2)\n",
      "j_day  pass\n",
      "For name:  d_truong\n",
      "(13, 2)\n",
      "d_truong  pass\n",
      "For name:  s_pan\n",
      "(101, 2)\n",
      "s_pan  pass\n",
      "For name:  a_andrade\n",
      "(52, 2)\n",
      "a_andrade  pass\n",
      "For name:  t_oliveira\n",
      "(41, 2)\n",
      "t_oliveira  pass\n",
      "For name:  n_romano\n",
      "(11, 2)\n",
      "n_romano  pass\n",
      "For name:  t_hara\n",
      "(23, 2)\n",
      "t_hara  pass\n",
      "For name:  t_wong\n",
      "(14, 2)\n",
      "t_wong  pass\n",
      "For name:  s_ross\n",
      "(25, 2)\n",
      "s_ross  pass\n",
      "For name:  d_richardson\n",
      "(456, 2)\n",
      "Total sample size before apply threshold:  456\n",
      "Counter({'0000-0003-0960-6415': 231, '0000-0002-7751-1058': 167, '0000-0002-3992-8610': 22, '0000-0003-0247-9118': 17, '0000-0002-3189-2190': 12, '0000-0002-0054-6850': 7})\n",
      "Total author before apply threshoid:  6\n",
      "Total author after apply threshoid:  2\n",
      "Total sample size after apply threshold:  398\n",
      "Total missing sample:  0\n",
      "(398, 101)\n",
      "Total missing sample:  54\n",
      "(398, 101)\n",
      "Papers with no citation:  [0, 2, 4, 6, 17, 25, 29, 36, 40, 46, 53, 56, 59, 64, 76, 80, 96, 99, 105, 107, 117, 119, 122, 145, 150, 200, 201, 202, 239, 244, 250, 251, 252, 253, 267, 274, 289, 291, 292, 294, 305, 306, 318, 332, 334, 342, 343, 344, 345, 354, 378, 382, 384, 391]\n",
      "Total paper with citation:  344\n",
      "For name:  j_moraes\n",
      "(26, 2)\n",
      "j_moraes  pass\n",
      "For name:  e_moreno\n",
      "(83, 2)\n",
      "e_moreno  pass\n",
      "For name:  r_little\n",
      "(4, 2)\n",
      "r_little  pass\n",
      "For name:  t_kobayashi\n",
      "(150, 2)\n",
      "t_kobayashi  pass\n",
      "For name:  a_lin\n",
      "(46, 2)\n",
      "a_lin  pass\n",
      "For name:  a_miranda\n",
      "(70, 2)\n",
      "a_miranda  pass\n",
      "For name:  h_vogel\n",
      "(15, 2)\n",
      "h_vogel  pass\n",
      "For name:  m_campos\n",
      "(148, 2)\n",
      "m_campos  pass\n",
      "For name:  d_stewart\n",
      "(294, 2)\n",
      "d_stewart  pass\n",
      "For name:  j_abrantes\n",
      "(57, 2)\n",
      "j_abrantes  pass\n",
      "For name:  j_arroyo\n",
      "(109, 2)\n",
      "j_arroyo  pass\n",
      "For name:  a_giuliani\n",
      "(196, 2)\n",
      "a_giuliani  pass\n",
      "For name:  f_campos\n",
      "(49, 2)\n",
      "f_campos  pass\n",
      "For name:  a_mitchell\n",
      "(436, 2)\n",
      "a_mitchell  pass\n",
      "For name:  c_murray\n",
      "(112, 2)\n",
      "c_murray  pass\n",
      "For name:  m_grant\n",
      "(39, 2)\n",
      "m_grant  pass\n",
      "For name:  d_scott\n",
      "(145, 2)\n",
      "d_scott  pass\n",
      "For name:  s_mohan\n",
      "(50, 2)\n",
      "s_mohan  pass\n",
      "For name:  n_wong\n",
      "(24, 2)\n",
      "n_wong  pass\n",
      "For name:  k_anderson\n",
      "(171, 2)\n",
      "k_anderson  pass\n",
      "For name:  m_king\n",
      "(58, 2)\n",
      "m_king  pass\n",
      "For name:  a_srivastava\n",
      "(49, 2)\n",
      "a_srivastava  pass\n",
      "For name:  m_scholz\n",
      "(42, 2)\n",
      "m_scholz  pass\n",
      "For name:  y_ju\n",
      "(27, 2)\n",
      "y_ju  pass\n",
      "For name:  d_stanley\n",
      "(6, 2)\n",
      "d_stanley  pass\n",
      "For name:  c_nogueira\n",
      "(303, 2)\n",
      "c_nogueira  pass\n",
      "For name:  j_cooper\n",
      "(147, 2)\n",
      "j_cooper  pass\n",
      "For name:  k_lau\n",
      "(121, 2)\n",
      "k_lau  pass\n",
      "For name:  s_hussein\n",
      "(33, 2)\n",
      "s_hussein  pass\n",
      "For name:  z_luo\n",
      "(25, 2)\n",
      "z_luo  pass\n",
      "For name:  c_pimentel\n",
      "(22, 2)\n",
      "c_pimentel  pass\n",
      "For name:  s_ito\n",
      "(59, 2)\n",
      "s_ito  pass\n",
      "For name:  f_zhang\n",
      "(103, 2)\n",
      "f_zhang  pass\n",
      "For name:  s_chapman\n",
      "(71, 2)\n",
      "s_chapman  pass\n",
      "For name:  j_rosa\n",
      "(29, 2)\n",
      "j_rosa  pass\n",
      "For name:  y_yin\n",
      "(152, 2)\n",
      "y_yin  pass\n",
      "For name:  p_tavares\n",
      "(53, 2)\n",
      "p_tavares  pass\n",
      "For name:  a_palma\n",
      "(61, 2)\n",
      "a_palma  pass\n",
      "For name:  e_shaw\n",
      "(16, 2)\n",
      "e_shaw  pass\n",
      "For name:  m_cameron\n",
      "(28, 2)\n",
      "m_cameron  pass\n",
      "For name:  a_reid\n",
      "(44, 2)\n",
      "a_reid  pass\n",
      "For name:  d_gil\n",
      "(60, 2)\n",
      "d_gil  pass\n",
      "For name:  s_morgan\n",
      "(83, 2)\n",
      "s_morgan  pass\n",
      "For name:  p_ross\n",
      "(27, 2)\n",
      "p_ross  pass\n",
      "For name:  l_simon\n",
      "(14, 2)\n",
      "l_simon  pass\n",
      "For name:  k_thomas\n",
      "(60, 2)\n",
      "k_thomas  pass\n",
      "For name:  l_torres\n",
      "(65, 2)\n",
      "l_torres  pass\n",
      "For name:  p_ding\n",
      "(17, 2)\n",
      "p_ding  pass\n",
      "For name:  g_morris\n",
      "(128, 2)\n",
      "g_morris  pass\n",
      "For name:  s_andrews\n",
      "(60, 2)\n",
      "s_andrews  pass\n",
      "For name:  b_yan\n",
      "(129, 2)\n",
      "b_yan  pass\n",
      "For name:  r_hu\n",
      "(128, 2)\n",
      "r_hu  pass\n",
      "For name:  j_braun\n",
      "(72, 2)\n",
      "j_braun  pass\n",
      "For name:  c_he\n",
      "(49, 2)\n",
      "c_he  pass\n",
      "For name:  w_lu\n",
      "(138, 2)\n",
      "w_lu  pass\n",
      "For name:  r_radhakrishnan\n",
      "(62, 2)\n",
      "r_radhakrishnan  pass\n",
      "For name:  k_saito\n",
      "(61, 2)\n",
      "k_saito  pass\n",
      "For name:  y_wang\n",
      "(1689, 2)\n",
      "Total sample size before apply threshold:  1689\n",
      "Counter({'0000-0001-8592-0698': 121, '0000-0003-0852-0767': 117, '0000-0002-6227-6112': 69, '0000-0001-5803-5343': 60, '0000-0002-1211-2822': 57, '0000-0002-3063-3066': 55, '0000-0003-2067-382X': 54, '0000-0003-0773-1212': 42, '0000-0002-6574-6706': 40, '0000-0001-9574-2194': 37, '0000-0001-5764-6740': 35, '0000-0001-6046-2934': 31, '0000-0001-8043-5757': 31, '0000-0003-2533-865X': 31, '0000-0001-8619-0455': 30, '0000-0003-0764-2279': 30, '0000-0002-9893-8296': 29, '0000-0001-7076-8312': 29, '0000-0001-5291-9826': 28, '0000-0002-0921-0122': 27, '0000-0003-3557-5085': 26, '0000-0002-0474-4790': 25, '0000-0003-2540-2199': 24, '0000-0003-0513-9039': 22, '0000-0003-3011-1919': 18, '0000-0002-1241-6252': 17, '0000-0002-5845-5150': 17, '0000-0001-9753-5535': 16, '0000-0003-0961-1716': 16, '0000-0001-6321-9542': 15, '0000-0002-0768-1676': 15, '0000-0002-7851-1623': 14, '0000-0003-1360-8931': 14, '0000-0001-7042-9804': 14, '0000-0002-5985-5244': 13, '0000-0001-5716-3183': 13, '0000-0002-7243-441X': 13, '0000-0002-0363-926X': 13, '0000-0001-6790-1311': 12, '0000-0003-0266-0224': 12, '0000-0001-8440-9388': 12, '0000-0002-2110-623X': 11, '0000-0002-2626-478X': 11, '0000-0001-8021-5180': 11, '0000-0001-8697-9165': 11, '0000-0002-1786-5970': 11, '0000-0003-0144-1388': 11, '0000-0002-3002-8069': 10, '0000-0002-6822-4778': 9, '0000-0002-9659-977X': 9, '0000-0002-8601-8302': 9, '0000-0001-9032-9990': 9, '0000-0002-1851-3483': 9, '0000-0002-1255-0937': 9, '0000-0002-7209-585X': 9, '0000-0002-5111-1443': 9, '0000-0002-6295-6492': 8, '0000-0002-4847-6273': 8, '0000-0002-0002-2467': 8, '0000-0002-7389-5066': 8, '0000-0003-2561-1855': 7, '0000-0003-1286-2401': 7, '0000-0002-2900-5126': 7, '0000-0003-3594-2658': 7, '0000-0003-4816-9182': 6, '0000-0001-5580-7766': 6, '0000-0002-0582-0855': 6, '0000-0002-3034-7377': 6, '0000-0002-2188-383X': 6, '0000-0003-1567-3358': 6, '0000-0001-5020-2020': 6, '0000-0001-9997-7636': 5, '0000-0002-6401-7464': 5, '0000-0003-3620-8455': 5, '0000-0002-2532-4832': 5, '0000-0002-3823-2136': 5, '0000-0002-5300-7121': 4, '0000-0002-7986-4500': 4, '0000-0003-3430-2210': 4, '0000-0002-3769-0020': 4, '0000-0001-8925-5277': 4, '0000-0001-6232-0382': 4, '0000-0003-2763-1008': 3, '0000-0001-5231-6283': 3, '0000-0003-3222-0211': 3, '0000-0002-5590-5881': 3, '0000-0002-3729-2743': 3, '0000-0002-1769-1966': 3, '0000-0003-1786-5767': 3, '0000-0003-0708-1950': 2, '0000-0002-1609-2523': 2, '0000-0001-8518-6745': 2, '0000-0001-5495-5839': 2, '0000-0003-1681-9566': 2, '0000-0001-9474-6396': 2, '0000-0001-6108-5157': 2, '0000-0001-5500-1228': 2, '0000-0002-8648-2172': 2, '0000-0002-3184-4201': 2, '0000-0003-3432-0603': 2, '0000-0002-8937-3000': 2, '0000-0002-0676-5886': 2, '0000-0003-1154-820X': 2, '0000-0002-5223-4074': 2, '0000-0001-6264-650X': 2, '0000-0002-6066-2634': 2, '0000-0003-1404-8526': 2, '0000-0003-3928-6926': 2, '0000-0002-5399-2803': 2, '0000-0002-1288-8997': 2, '0000-0001-6085-5615': 2, '0000-0002-3656-4284': 2, '0000-0002-5187-3755': 2, '0000-0002-9628-1382': 2, '0000-0002-2244-1742': 2, '0000-0003-1009-2087': 1, '0000-0001-6823-1225': 1, '0000-0002-5692-3117': 1, '0000-0001-6981-7797': 1, '0000-0001-7956-3102': 1, '0000-0002-2657-7057': 1, '0000-0002-2665-0365': 1, '0000-0002-4336-0474': 1, '0000-0002-7629-4178': 1, '0000-0001-5918-7525': 1, '0000-0002-0891-1517': 1, '0000-0002-9684-1730': 1, '0000-0002-2932-6042': 1, '0000-0001-8538-5998': 1, '0000-0002-4506-4230': 1, '0000-0003-3120-827X': 1, '0000-0002-9640-0871': 1, '0000-0003-3511-0288': 1, '0000-0001-9156-0377': 1, '0000-0002-7281-1908': 1, '0000-0003-2540-5824': 1, '0000-0002-9365-1851': 1, '0000-0002-2333-157X': 1})\n",
      "Total author before apply threshoid:  138\n",
      "Total author after apply threshoid:  2\n",
      "Total sample size after apply threshold:  238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing sample:  0\n",
      "(238, 101)\n",
      "Total missing sample:  11\n",
      "(238, 101)\n",
      "Papers with no citation:  [8, 24, 50, 54, 90, 92, 100, 147, 180, 198, 236]\n",
      "Total paper with citation:  227\n",
      "For name:  j_gao\n",
      "(222, 2)\n",
      "j_gao  pass\n",
      "For name:  d_fernandes\n",
      "(40, 2)\n",
      "d_fernandes  pass\n",
      "For name:  c_silva\n",
      "(148, 2)\n",
      "c_silva  pass\n",
      "For name:  t_fitzgerald\n",
      "(54, 2)\n",
      "t_fitzgerald  pass\n",
      "For name:  j_mitchell\n",
      "(143, 2)\n",
      "j_mitchell  pass\n",
      "For name:  a_gomes\n",
      "(244, 2)\n",
      "a_gomes  pass\n",
      "For name:  t_weber\n",
      "(71, 2)\n",
      "t_weber  pass\n",
      "For name:  j_shim\n",
      "(188, 2)\n",
      "j_shim  pass\n",
      "For name:  k_kang\n",
      "(128, 2)\n",
      "k_kang  pass\n",
      "For name:  i_ferreira\n",
      "(344, 2)\n",
      "i_ferreira  pass\n",
      "For name:  y_jia\n",
      "(46, 2)\n",
      "y_jia  pass\n",
      "For name:  p_gaspar\n",
      "(93, 2)\n",
      "p_gaspar  pass\n",
      "For name:  r_o'connor\n",
      "(82, 2)\n",
      "r_o'connor  pass\n",
      "For name:  k_larsen\n",
      "(47, 2)\n",
      "k_larsen  pass\n",
      "For name:  s_das\n",
      "(197, 2)\n",
      "s_das  pass\n",
      "For name:  f_rodriguez\n",
      "(6, 2)\n",
      "f_rodriguez  pass\n",
      "For name:  w_peng\n",
      "(38, 2)\n",
      "w_peng  pass\n",
      "For name:  c_torres\n",
      "(300, 2)\n",
      "c_torres  pass\n",
      "For name:  s_rossi\n",
      "(199, 2)\n",
      "s_rossi  pass\n",
      "For name:  s_alavi\n",
      "(38, 2)\n",
      "s_alavi  pass\n",
      "For name:  r_marques\n",
      "(41, 2)\n",
      "r_marques  pass\n",
      "For name:  m_wheeler\n",
      "(163, 2)\n",
      "m_wheeler  pass\n",
      "For name:  l_rasmussen\n",
      "(249, 2)\n",
      "l_rasmussen  pass\n",
      "For name:  m_saad\n",
      "(4, 2)\n",
      "m_saad  pass\n",
      "For name:  j_carr\n",
      "(271, 2)\n",
      "j_carr  pass\n",
      "For name:  j_fraser\n",
      "(101, 2)\n",
      "j_fraser  pass\n",
      "For name:  s_woo\n",
      "(25, 2)\n",
      "s_woo  pass\n",
      "For name:  s_bartlett\n",
      "(104, 2)\n",
      "s_bartlett  pass\n",
      "For name:  m_lucas\n",
      "(75, 2)\n",
      "m_lucas  pass\n",
      "For name:  w_lee\n",
      "(590, 2)\n",
      "Total sample size before apply threshold:  590\n",
      "Counter({'0000-0003-3171-7672': 108, '0000-0001-5833-989X': 100, '0000-0003-3231-9764': 82, '0000-0002-1082-7592': 62, '0000-0003-3267-4811': 40, '0000-0001-7805-869X': 36, '0000-0003-2883-0391': 21, '0000-0002-0607-038X': 21, '0000-0002-5461-6770': 16, '0000-0002-3912-6095': 11, '0000-0001-6757-885X': 11, '0000-0001-6408-7668': 10, '0000-0002-9873-1033': 9, '0000-0001-7801-083X': 8, '0000-0001-8430-4797': 7, '0000-0002-2572-7287': 5, '0000-0002-6766-8481': 5, '0000-0001-8706-6026': 4, '0000-0002-0036-2859': 4, '0000-0002-9624-0505': 3, '0000-0002-3413-4029': 3, '0000-0003-1817-8395': 3, '0000-0003-1744-8525': 3, '0000-0001-8052-2420': 2, '0000-0003-0853-8561': 2, '0000-0001-7285-4054': 2, '0000-0001-9645-8179': 2, '0000-0002-4383-756X': 2, '0000-0003-1911-3454': 2, '0000-0003-4333-5444': 1, '0000-0002-7324-5792': 1, '0000-0002-2152-7210': 1, '0000-0003-4040-1100': 1, '0000-0003-0133-9076': 1, '0000-0002-7696-5517': 1})\n",
      "Total author before apply threshoid:  35\n",
      "Total author after apply threshoid:  2\n",
      "Total sample size after apply threshold:  208\n",
      "Total missing sample:  0\n",
      "(208, 101)\n",
      "Total missing sample:  12\n",
      "(208, 101)\n",
      "Papers with no citation:  [8, 42, 48, 70, 77, 103, 133, 138, 157, 168, 169, 200]\n",
      "Total paper with citation:  196\n",
      "For name:  j_cheng\n",
      "(66, 2)\n",
      "j_cheng  pass\n",
      "For name:  g_lewis\n",
      "(367, 2)\n",
      "g_lewis  pass\n",
      "For name:  j_albert\n",
      "(78, 2)\n",
      "j_albert  pass\n",
      "For name:  k_goh\n",
      "(42, 2)\n",
      "k_goh  pass\n",
      "For name:  n_harris\n",
      "(14, 2)\n",
      "n_harris  pass\n",
      "For name:  s_hill\n",
      "(152, 2)\n",
      "s_hill  pass\n",
      "For name:  p_pathak\n",
      "(9, 2)\n",
      "p_pathak  pass\n",
      "For name:  h_zeng\n",
      "(82, 2)\n",
      "h_zeng  pass\n",
      "For name:  h_liu\n",
      "(439, 2)\n",
      "h_liu  pass\n",
      "For name:  s_bae\n",
      "(83, 2)\n",
      "s_bae  pass\n",
      "For name:  s_fernandes\n",
      "(38, 2)\n",
      "s_fernandes  pass\n",
      "For name:  a_miller\n",
      "(109, 2)\n",
      "a_miller  pass\n",
      "For name:  a_eklund\n",
      "(118, 2)\n",
      "a_eklund  pass\n",
      "For name:  r_moore\n",
      "(221, 2)\n",
      "r_moore  pass\n",
      "For name:  m_thomsen\n",
      "(98, 2)\n",
      "m_thomsen  pass\n",
      "For name:  l_ng\n",
      "(44, 2)\n",
      "l_ng  pass\n",
      "For name:  a_phillips\n",
      "(170, 2)\n",
      "a_phillips  pass\n",
      "For name:  y_ye\n",
      "(85, 2)\n",
      "y_ye  pass\n",
      "For name:  m_guerreiro\n",
      "(36, 2)\n",
      "m_guerreiro  pass\n",
      "For name:  g_alves\n",
      "(60, 2)\n",
      "g_alves  pass\n",
      "For name:  m_mohammed\n",
      "(6, 2)\n",
      "m_mohammed  pass\n",
      "For name:  s_mohammadi\n",
      "(59, 2)\n",
      "s_mohammadi  pass\n",
      "For name:  c_chao\n",
      "(155, 2)\n",
      "c_chao  pass\n",
      "For name:  s_teixeira\n",
      "(36, 2)\n",
      "s_teixeira  pass\n",
      "For name:  l_almeida\n",
      "(133, 2)\n",
      "l_almeida  pass\n",
      "For name:  y_tseng\n",
      "(61, 2)\n",
      "y_tseng  pass\n",
      "For name:  a_ferro\n",
      "(125, 2)\n",
      "a_ferro  pass\n",
      "For name:  d_he\n",
      "(29, 2)\n",
      "d_he  pass\n",
      "For name:  k_ko\n",
      "(168, 2)\n",
      "k_ko  pass\n",
      "For name:  t_mori\n",
      "(104, 2)\n",
      "t_mori  pass\n",
      "For name:  p_lima\n",
      "(24, 2)\n",
      "p_lima  pass\n",
      "For name:  d_ferguson\n",
      "(217, 2)\n",
      "d_ferguson  pass\n",
      "For name:  h_moreira\n",
      "(28, 2)\n",
      "h_moreira  pass\n",
      "For name:  s_yi\n",
      "(67, 2)\n",
      "s_yi  pass\n",
      "For name:  q_liu\n",
      "(264, 2)\n",
      "q_liu  pass\n",
      "For name:  m_ibrahim\n",
      "(146, 2)\n",
      "m_ibrahim  pass\n",
      "For name:  s_collins\n",
      "(163, 2)\n",
      "s_collins  pass\n",
      "For name:  d_franco\n",
      "(115, 2)\n",
      "d_franco  pass\n",
      "For name:  h_brown\n",
      "(48, 2)\n",
      "h_brown  pass\n",
      "For name:  s_martins\n",
      "(84, 2)\n",
      "s_martins  pass\n",
      "For name:  m_ruiz\n",
      "(111, 2)\n",
      "m_ruiz  pass\n",
      "For name:  a_levy\n",
      "(23, 2)\n",
      "a_levy  pass\n",
      "For name:  j_murray\n",
      "(213, 2)\n",
      "j_murray  pass\n",
      "For name:  y_hou\n",
      "(162, 2)\n",
      "y_hou  pass\n",
      "For name:  m_sahin\n",
      "(48, 2)\n",
      "m_sahin  pass\n",
      "For name:  c_feng\n",
      "(88, 2)\n",
      "c_feng  pass\n",
      "For name:  j_coutinho\n",
      "(129, 2)\n",
      "j_coutinho  pass\n",
      "For name:  s_huber\n",
      "(44, 2)\n",
      "s_huber  pass\n",
      "For name:  a_rocha\n",
      "(73, 2)\n",
      "a_rocha  pass\n",
      "For name:  a_white\n",
      "(386, 2)\n",
      "a_white  pass\n",
      "For name:  j_scott\n",
      "(342, 2)\n",
      "j_scott  pass\n",
      "For name:  s_hosseini\n",
      "(25, 2)\n",
      "s_hosseini  pass\n",
      "For name:  d_vieira\n",
      "(68, 2)\n",
      "d_vieira  pass\n",
      "For name:  j_kang\n",
      "(200, 2)\n",
      "j_kang  pass\n",
      "For name:  j_jensen\n",
      "(388, 2)\n",
      "j_jensen  pass\n",
      "For name:  k_lai\n",
      "(52, 2)\n",
      "k_lai  pass\n",
      "For name:  j_gonzalez\n",
      "(39, 2)\n",
      "j_gonzalez  pass\n",
      "For name:  m_zakaria\n",
      "(24, 2)\n",
      "m_zakaria  pass\n",
      "For name:  c_campos\n",
      "(48, 2)\n",
      "c_campos  pass\n",
      "For name:  a_gad\n",
      "(29, 2)\n",
      "a_gad  pass\n",
      "For name:  y_zhao\n",
      "(338, 2)\n",
      "y_zhao  pass\n",
      "For name:  s_hussain\n",
      "(52, 2)\n",
      "s_hussain  pass\n",
      "For name:  k_scott\n",
      "(16, 2)\n",
      "k_scott  pass\n",
      "For name:  a_martinez\n",
      "(180, 2)\n",
      "a_martinez  pass\n",
      "For name:  r_luz\n",
      "(20, 2)\n",
      "r_luz  pass\n",
      "For name:  p_tran\n",
      "(18, 2)\n",
      "p_tran  pass\n",
      "For name:  e_romero\n",
      "(23, 2)\n",
      "e_romero  pass\n",
      "For name:  j_stevens\n",
      "(161, 2)\n",
      "j_stevens  pass\n",
      "For name:  l_you\n",
      "(32, 2)\n",
      "l_you  pass\n",
      "For name:  p_stevenson\n",
      "(122, 2)\n",
      "p_stevenson  pass\n",
      "For name:  t_kang\n",
      "(26, 2)\n",
      "t_kang  pass\n",
      "For name:  s_mohanty\n",
      "(67, 2)\n",
      "s_mohanty  pass\n",
      "For name:  m_amorim\n",
      "(95, 2)\n",
      "m_amorim  pass\n",
      "For name:  y_kamiya\n",
      "(161, 2)\n",
      "y_kamiya  pass\n",
      "For name:  w_he\n",
      "(48, 2)\n",
      "w_he  pass\n",
      "For name:  t_kato\n",
      "(249, 2)\n",
      "t_kato  pass\n",
      "For name:  a_ward\n",
      "(164, 2)\n",
      "a_ward  pass\n",
      "For name:  j_chen\n",
      "(1139, 2)\n",
      "j_chen  pass\n",
      "For name:  m_tseng\n",
      "(141, 2)\n",
      "m_tseng  pass\n",
      "For name:  c_henderson\n",
      "(107, 2)\n",
      "c_henderson  pass\n",
      "For name:  j_mcdonald\n",
      "(21, 2)\n",
      "j_mcdonald  pass\n",
      "For name:  m_ismail\n",
      "(24, 2)\n",
      "m_ismail  pass\n",
      "For name:  x_xu\n",
      "(408, 2)\n",
      "x_xu  pass\n",
      "For name:  f_liu\n",
      "(185, 2)\n",
      "f_liu  pass\n",
      "For name:  a_rego\n",
      "(78, 2)\n",
      "a_rego  pass\n",
      "For name:  s_hammad\n",
      "(32, 2)\n",
      "s_hammad  pass\n",
      "For name:  k_johansson\n",
      "(26, 2)\n",
      "k_johansson  pass\n",
      "For name:  m_barreto\n",
      "(201, 2)\n",
      "m_barreto  pass\n",
      "For name:  j_moore\n",
      "(154, 2)\n",
      "j_moore  pass\n",
      "For name:  a_gray\n",
      "(121, 2)\n",
      "a_gray  pass\n",
      "For name:  v_martins\n",
      "(104, 2)\n",
      "v_martins  pass\n",
      "For name:  t_zhou\n",
      "(76, 2)\n",
      "t_zhou  pass\n",
      "For name:  s_howell\n",
      "(31, 2)\n",
      "s_howell  pass\n",
      "For name:  m_larsson\n",
      "(61, 2)\n",
      "m_larsson  pass\n",
      "For name:  s_morris\n",
      "(33, 2)\n",
      "s_morris  pass\n",
      "For name:  s_biswas\n",
      "(37, 2)\n",
      "s_biswas  pass\n",
      "For name:  s_patel\n",
      "(416, 2)\n",
      "s_patel  pass\n",
      "For name:  m_white\n",
      "(292, 2)\n",
      "m_white  pass\n",
      "For name:  s_sherman\n",
      "(125, 2)\n",
      "s_sherman  pass\n",
      "For name:  j_dai\n",
      "(31, 2)\n",
      "j_dai  pass\n",
      "For name:  m_fischer\n",
      "(48, 2)\n",
      "m_fischer  pass\n",
      "For name:  y_zeng\n",
      "(26, 2)\n",
      "y_zeng  pass\n",
      "For name:  j_turner\n",
      "(178, 2)\n",
      "j_turner  pass\n",
      "For name:  c_cai\n",
      "(57, 2)\n",
      "c_cai  pass\n",
      "For name:  f_pereira\n",
      "(86, 2)\n",
      "f_pereira  pass\n",
      "For name:  a_vitale\n",
      "(56, 2)\n",
      "a_vitale  pass\n",
      "For name:  q_yang\n",
      "(102, 2)\n",
      "q_yang  pass\n",
      "For name:  d_xue\n",
      "(111, 2)\n",
      "d_xue  pass\n",
      "For name:  m_sadeghi\n",
      "(98, 2)\n",
      "m_sadeghi  pass\n",
      "For name:  h_chang\n",
      "(182, 2)\n",
      "h_chang  pass\n",
      "For name:  a_lombardi\n",
      "(90, 2)\n",
      "a_lombardi  pass\n",
      "For name:  c_correia\n",
      "(55, 2)\n",
      "c_correia  pass\n",
      "For name:  j_you\n",
      "(239, 2)\n",
      "j_you  pass\n",
      "For name:  c_lopez\n",
      "(53, 2)\n",
      "c_lopez  pass\n",
      "For name:  y_oh\n",
      "(91, 2)\n",
      "y_oh  pass\n",
      "For name:  s_yoon\n",
      "(73, 2)\n",
      "s_yoon  pass\n",
      "For name:  a_lima\n",
      "(85, 2)\n",
      "a_lima  pass\n",
      "For name:  h_singh\n",
      "(45, 2)\n",
      "h_singh  pass\n",
      "For name:  s_scott\n",
      "(143, 2)\n",
      "s_scott  pass\n",
      "For name:  z_he\n",
      "(160, 2)\n",
      "z_he  pass\n",
      "For name:  s_mukherjee\n",
      "(125, 2)\n",
      "s_mukherjee  pass\n",
      "For name:  j_yue\n",
      "(62, 2)\n",
      "j_yue  pass\n",
      "For name:  f_dias\n",
      "(68, 2)\n",
      "f_dias  pass\n",
      "For name:  r_walker\n",
      "(87, 2)\n",
      "r_walker  pass\n",
      "For name:  l_campos\n",
      "(12, 2)\n",
      "l_campos  pass\n",
      "For name:  m_iqbal\n",
      "(8, 2)\n",
      "m_iqbal  pass\n",
      "For name:  s_lim\n",
      "(136, 2)\n",
      "s_lim  pass\n",
      "For name:  p_li\n",
      "(118, 2)\n",
      "p_li  pass\n",
      "For name:  f_andrade\n",
      "(37, 2)\n",
      "f_andrade  pass\n",
      "For name:  c_schmitt\n",
      "(13, 2)\n",
      "c_schmitt  pass\n",
      "For name:  t_tan\n",
      "(73, 2)\n",
      "t_tan  pass\n",
      "For name:  h_gomes\n",
      "(11, 2)\n",
      "h_gomes  pass\n",
      "For name:  m_matos\n",
      "(82, 2)\n",
      "m_matos  pass\n",
      "For name:  k_ryan\n",
      "(182, 2)\n",
      "k_ryan  pass\n",
      "For name:  w_zheng\n",
      "(93, 2)\n",
      "w_zheng  pass\n",
      "For name:  j_franco\n",
      "(85, 2)\n",
      "j_franco  pass\n",
      "For name:  l_walker\n",
      "(194, 2)\n",
      "l_walker  pass\n",
      "For name:  a_gordon\n",
      "(126, 2)\n",
      "a_gordon  pass\n",
      "For name:  z_yin\n",
      "(52, 2)\n",
      "z_yin  pass\n",
      "For name:  c_gu\n",
      "(65, 2)\n",
      "c_gu  pass\n",
      "For name:  a_soto\n",
      "(32, 2)\n",
      "a_soto  pass\n",
      "For name:  h_hsieh\n",
      "(70, 2)\n",
      "h_hsieh  pass\n",
      "For name:  m_crespo\n",
      "(49, 2)\n",
      "m_crespo  pass\n",
      "For name:  s_phillips\n",
      "(183, 2)\n",
      "s_phillips  pass\n",
      "For name:  r_rodrigues\n",
      "(74, 2)\n",
      "r_rodrigues  pass\n",
      "For name:  a_mansour\n",
      "(15, 2)\n",
      "a_mansour  pass\n",
      "For name:  a_lau\n",
      "(35, 2)\n",
      "a_lau  pass\n",
      "For name:  j_berg\n",
      "(171, 2)\n",
      "j_berg  pass\n",
      "For name:  l_wilson\n",
      "(59, 2)\n",
      "l_wilson  pass\n",
      "For name:  c_park\n",
      "(360, 2)\n",
      "c_park  pass\n",
      "For name:  r_thomas\n",
      "(368, 2)\n",
      "r_thomas  pass\n",
      "For name:  j_fonseca\n",
      "(170, 2)\n",
      "j_fonseca  pass\n",
      "For name:  s_henderson\n",
      "(82, 2)\n",
      "s_henderson  pass\n",
      "For name:  m_coelho\n",
      "(93, 2)\n",
      "m_coelho  pass\n",
      "For name:  j_pearson\n",
      "(119, 2)\n",
      "j_pearson  pass\n",
      "For name:  z_xie\n",
      "(99, 2)\n",
      "z_xie  pass\n",
      "For name:  m_wright\n",
      "(379, 2)\n",
      "m_wright  pass\n",
      "For name:  j_song\n",
      "(248, 2)\n",
      "j_song  pass\n",
      "For name:  k_becker\n",
      "(394, 2)\n",
      "Total sample size before apply threshold:  394\n",
      "Counter({'0000-0002-6794-6656': 180, '0000-0002-6391-1341': 112, '0000-0002-6801-4498': 80, '0000-0003-4231-2590': 19, '0000-0001-6317-1884': 3})\n",
      "Total author before apply threshoid:  5\n",
      "Total author after apply threshoid:  2\n",
      "Total sample size after apply threshold:  292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing sample:  0\n",
      "(292, 101)\n",
      "Total missing sample:  7\n",
      "(292, 101)\n",
      "Papers with no citation:  [56, 104, 210, 247, 270, 272, 276]\n",
      "Total paper with citation:  285\n",
      "For name:  r_sinha\n",
      "(27, 2)\n",
      "r_sinha  pass\n",
      "For name:  c_turner\n",
      "(88, 2)\n",
      "c_turner  pass\n",
      "For name:  y_su\n",
      "(190, 2)\n",
      "y_su  pass\n",
      "For name:  a_popov\n",
      "(135, 2)\n",
      "a_popov  pass\n",
      "For name:  w_liao\n",
      "(79, 2)\n",
      "w_liao  pass\n",
      "For name:  j_zhong\n",
      "(280, 2)\n",
      "j_zhong  pass\n",
      "For name:  a_wheeler\n",
      "(138, 2)\n",
      "a_wheeler  pass\n",
      "For name:  m_walsh\n",
      "(37, 2)\n",
      "m_walsh  pass\n",
      "For name:  r_figueiredo\n",
      "(48, 2)\n",
      "r_figueiredo  pass\n",
      "For name:  y_lin\n",
      "(785, 2)\n",
      "Total sample size before apply threshold:  785\n",
      "Counter({'0000-0003-3791-7587': 146, '0000-0001-8153-1441': 115, '0000-0003-1224-6561': 64, '0000-0002-4192-3165': 49, '0000-0002-2499-8632': 39, '0000-0001-8667-0811': 33, '0000-0002-5887-0880': 24, '0000-0001-5227-2663': 23, '0000-0002-4350-7755': 23, '0000-0003-4913-8003': 22, '0000-0001-6460-2877': 21, '0000-0003-1954-334X': 20, '0000-0001-8572-649X': 20, '0000-0001-5574-7062': 15, '0000-0002-0352-2694': 15, '0000-0002-9390-795X': 13, '0000-0001-8904-1287': 13, '0000-0003-3410-3588': 12, '0000-0003-4384-8354': 9, '0000-0001-6833-8276': 9, '0000-0002-8746-3387': 9, '0000-0002-0796-0130': 8, '0000-0002-0435-7694': 8, '0000-0001-6454-0901': 7, '0000-0002-0123-9836': 6, '0000-0001-7120-4690': 6, '0000-0001-5100-6072': 6, '0000-0003-3913-5298': 6, '0000-0003-3177-5186': 5, '0000-0003-1240-7011': 5, '0000-0003-1470-4159': 5, '0000-0001-7910-1223': 4, '0000-0003-4289-894X': 4, '0000-0002-7289-5347': 4, '0000-0003-1328-1641': 2, '0000-0002-2229-6354': 2, '0000-0002-6835-7116': 2, '0000-0001-6819-1235': 2, '0000-0003-2656-3613': 1, '0000-0002-5379-5359': 1, '0000-0003-4327-7432': 1, '0000-0001-7923-0789': 1, '0000-0002-7492-9985': 1, '0000-0002-7639-9594': 1, '0000-0002-2502-2412': 1, '0000-0001-7243-0980': 1, '0000-0002-8287-1429': 1})\n",
      "Total author before apply threshoid:  47\n",
      "Total author after apply threshoid:  2\n",
      "Total sample size after apply threshold:  261\n",
      "Total missing sample:  0\n",
      "(261, 101)\n",
      "Total missing sample:  26\n",
      "(261, 101)\n",
      "Papers with no citation:  [0, 18, 19, 21, 33, 79, 81, 91, 94, 95, 97, 111, 130, 137, 139, 146, 147, 150, 166, 181, 191, 206, 211, 214, 221, 239]\n",
      "Total paper with citation:  235\n",
      "For name:  k_sato\n",
      "(65, 2)\n",
      "k_sato  pass\n",
      "For name:  f_ahmed\n",
      "(25, 2)\n",
      "f_ahmed  pass\n",
      "For name:  y_watanabe\n",
      "(98, 2)\n",
      "y_watanabe  pass\n",
      "For name:  k_singh\n",
      "(25, 2)\n",
      "k_singh  pass\n",
      "For name:  j_mcevoy\n",
      "(65, 2)\n",
      "j_mcevoy  pass\n",
      "For name:  g_singh\n",
      "(36, 2)\n",
      "g_singh  pass\n",
      "For name:  e_ford\n",
      "(54, 2)\n",
      "e_ford  pass\n",
      "For name:  s_chou\n",
      "(39, 2)\n",
      "s_chou  pass\n",
      "For name:  s_hughes\n",
      "(106, 2)\n",
      "s_hughes  pass\n",
      "For name:  m_thomas\n",
      "(225, 2)\n",
      "m_thomas  pass\n",
      "For name:  j_liang\n",
      "(105, 2)\n",
      "j_liang  pass\n",
      "For name:  t_wu\n",
      "(168, 2)\n",
      "t_wu  pass\n",
      "For name:  b_ahmed\n",
      "(23, 2)\n",
      "b_ahmed  pass\n",
      "For name:  m_takahashi\n",
      "(54, 2)\n",
      "m_takahashi  pass\n",
      "For name:  i_lee\n",
      "(73, 2)\n",
      "i_lee  pass\n",
      "For name:  a_figueiredo\n",
      "(150, 2)\n",
      "a_figueiredo  pass\n",
      "For name:  s_clark\n",
      "(39, 2)\n",
      "s_clark  pass\n",
      "For name:  a_schmid\n",
      "(61, 2)\n",
      "a_schmid  pass\n",
      "For name:  k_cheung\n",
      "(16, 2)\n",
      "k_cheung  pass\n",
      "For name:  s_ma\n",
      "(136, 2)\n",
      "s_ma  pass\n",
      "For name:  m_marino\n",
      "(69, 2)\n",
      "m_marino  pass\n",
      "For name:  a_kirby\n",
      "(64, 2)\n",
      "a_kirby  pass\n",
      "For name:  d_roberts\n",
      "(105, 2)\n",
      "d_roberts  pass\n",
      "For name:  b_thompson\n",
      "(83, 2)\n",
      "b_thompson  pass\n",
      "For name:  j_blanco\n",
      "(362, 2)\n",
      "j_blanco  pass\n",
      "For name:  x_cai\n",
      "(79, 2)\n",
      "x_cai  pass\n",
      "For name:  r_menezes\n",
      "(29, 2)\n",
      "r_menezes  pass\n",
      "For name:  s_tsang\n",
      "(20, 2)\n",
      "s_tsang  pass\n",
      "For name:  c_king\n",
      "(218, 2)\n",
      "c_king  pass\n",
      "For name:  h_kobayashi\n",
      "(28, 2)\n",
      "h_kobayashi  pass\n",
      "For name:  k_yang\n",
      "(70, 2)\n",
      "k_yang  pass\n",
      "For name:  b_zheng\n",
      "(90, 2)\n",
      "b_zheng  pass\n",
      "For name:  f_xu\n",
      "(94, 2)\n",
      "f_xu  pass\n",
      "For name:  r_day\n",
      "(202, 2)\n",
      "r_day  pass\n",
      "For name:  j_young\n",
      "(267, 2)\n",
      "j_young  pass\n",
      "For name:  c_black\n",
      "(41, 2)\n",
      "c_black  pass\n",
      "For name:  s_joseph\n",
      "(20, 2)\n",
      "s_joseph  pass\n",
      "For name:  z_fan\n",
      "(38, 2)\n",
      "z_fan  pass\n",
      "For name:  j_matos\n",
      "(25, 2)\n",
      "j_matos  pass\n",
      "For name:  l_santos\n",
      "(172, 2)\n",
      "l_santos  pass\n",
      "For name:  g_taylor\n",
      "(44, 2)\n",
      "g_taylor  pass\n",
      "For name:  x_yang\n",
      "(164, 2)\n",
      "x_yang  pass\n",
      "For name:  s_bianchi\n",
      "(45, 2)\n",
      "s_bianchi  pass\n",
      "For name:  a_morales\n",
      "(77, 2)\n",
      "a_morales  pass\n",
      "For name:  p_wong\n",
      "(36, 2)\n",
      "p_wong  pass\n",
      "For name:  a_cooper\n",
      "(265, 2)\n",
      "a_cooper  pass\n",
      "For name:  j_nguyen\n",
      "(27, 2)\n",
      "j_nguyen  pass\n",
      "For name:  t_lang\n",
      "(107, 2)\n",
      "t_lang  pass\n",
      "For name:  s_russo\n",
      "(45, 2)\n",
      "s_russo  pass\n",
      "For name:  r_arora\n",
      "(64, 2)\n",
      "r_arora  pass\n",
      "For name:  c_porter\n",
      "(157, 2)\n",
      "c_porter  pass\n",
      "For name:  m_moore\n",
      "(112, 2)\n",
      "m_moore  pass\n",
      "For name:  c_johnson\n",
      "(300, 2)\n",
      "c_johnson  pass\n",
      "For name:  e_henry\n",
      "(31, 2)\n",
      "e_henry  pass\n",
      "For name:  x_xie\n",
      "(24, 2)\n",
      "x_xie  pass\n",
      "For name:  x_jin\n",
      "(62, 2)\n",
      "x_jin  pass\n",
      "For name:  s_singh\n",
      "(344, 2)\n",
      "s_singh  pass\n",
      "For name:  m_reid\n",
      "(62, 2)\n",
      "m_reid  pass\n",
      "For name:  m_wallace\n",
      "(144, 2)\n",
      "m_wallace  pass\n",
      "For name:  y_zhang\n",
      "(1244, 2)\n",
      "y_zhang  pass\n",
      "For name:  m_young\n",
      "(101, 2)\n",
      "m_young  pass\n",
      "For name:  s_saraf\n",
      "(54, 2)\n",
      "s_saraf  pass\n",
      "For name:  r_pinto\n",
      "(85, 2)\n",
      "r_pinto  pass\n",
      "For name:  m_brito\n",
      "(86, 2)\n",
      "m_brito  pass\n",
      "For name:  s_goel\n",
      "(31, 2)\n",
      "s_goel  pass\n",
      "For name:  y_park\n",
      "(627, 2)\n",
      "y_park  pass\n",
      "For name:  p_melo\n",
      "(28, 2)\n",
      "p_melo  pass\n",
      "For name:  c_lemos\n",
      "(52, 2)\n",
      "c_lemos  pass\n",
      "For name:  b_liu\n",
      "(298, 2)\n",
      "b_liu  pass\n",
      "For name:  k_turner\n",
      "(62, 2)\n",
      "k_turner  pass\n",
      "For name:  r_rao\n",
      "(94, 2)\n",
      "r_rao  pass\n",
      "For name:  b_barker\n",
      "(35, 2)\n",
      "b_barker  pass\n",
      "For name:  a_wright\n",
      "(149, 2)\n",
      "a_wright  pass\n",
      "For name:  z_ma\n",
      "(111, 2)\n",
      "z_ma  pass\n",
      "For name:  s_bose\n",
      "(28, 2)\n",
      "s_bose  pass\n",
      "For name:  j_dyer\n",
      "(61, 2)\n",
      "j_dyer  pass\n",
      "For name:  f_blanco\n",
      "(128, 2)\n",
      "f_blanco  pass\n",
      "For name:  s_ferreira\n",
      "(70, 2)\n",
      "s_ferreira  pass\n",
      "For name:  j_ren\n",
      "(102, 2)\n",
      "j_ren  pass\n",
      "For name:  j_muller\n",
      "(113, 2)\n",
      "j_muller  pass\n",
      "For name:  h_tanaka\n",
      "(28, 2)\n",
      "h_tanaka  pass\n",
      "For name:  j_pierce\n",
      "(39, 2)\n",
      "j_pierce  pass\n",
      "For name:  j_guerrero\n",
      "(15, 2)\n",
      "j_guerrero  pass\n",
      "For name:  r_coelho\n",
      "(26, 2)\n",
      "r_coelho  pass\n",
      "For name:  a_masi\n",
      "(39, 2)\n",
      "a_masi  pass\n",
      "For name:  b_jackson\n",
      "(29, 2)\n",
      "b_jackson  pass\n",
      "For name:  a_jha\n",
      "(39, 2)\n",
      "a_jha  pass\n",
      "For name:  m_mosquera\n",
      "(60, 2)\n",
      "m_mosquera  pass\n",
      "For name:  a_silva\n",
      "(786, 2)\n",
      "Total sample size before apply threshold:  786\n",
      "Counter({'0000-0003-2861-8286': 158, '0000-0001-5525-0494': 156, '0000-0002-8984-8600': 74, '0000-0001-5790-5116': 41, '0000-0002-7524-9914': 39, '0000-0002-7802-8690': 39, '0000-0003-4968-5138': 30, '0000-0002-7713-1813': 22, '0000-0002-9968-3707': 18, '0000-0002-6332-5182': 16, '0000-0002-5668-7134': 16, '0000-0001-5554-7714': 14, '0000-0002-4839-8279': 14, '0000-0002-1112-1209': 11, '0000-0003-0423-2514': 10, '0000-0002-4386-5851': 10, '0000-0002-9679-8357': 10, '0000-0003-3786-2889': 10, '0000-0002-1673-2164': 10, '0000-0001-7604-792X': 8, '0000-0002-1840-1473': 8, '0000-0003-0393-1655': 7, '0000-0003-4212-5955': 7, '0000-0002-0067-0288': 5, '0000-0002-0634-0546': 5, '0000-0003-2002-4774': 4, '0000-0001-5470-9523': 4, '0000-0002-4364-4979': 4, '0000-0002-5388-1732': 3, '0000-0001-5203-5908': 3, '0000-0001-7231-7021': 3, '0000-0002-5334-0047': 3, '0000-0002-1718-0744': 2, '0000-0003-0384-4447': 2, '0000-0002-2100-7223': 2, '0000-0003-4504-0607': 2, '0000-0003-3576-9023': 2, '0000-0002-3403-5792': 2, '0000-0003-2092-801X': 1, '0000-0002-9595-0038': 1, '0000-0003-4734-6538': 1, '0000-0001-6365-1407': 1, '0000-0002-5842-643X': 1, '0000-0002-8363-0109': 1, '0000-0002-7029-1048': 1, '0000-0002-4904-7470': 1, '0000-0002-3254-2598': 1, '0000-0002-5957-2711': 1, '0000-0002-1724-7777': 1, '0000-0001-6939-8430': 1})\n",
      "Total author before apply threshoid:  50\n",
      "Total author after apply threshoid:  2\n",
      "Total sample size after apply threshold:  314\n",
      "Total missing sample:  0\n",
      "(314, 101)\n",
      "Total missing sample:  22\n",
      "(314, 101)\n",
      "Papers with no citation:  [7, 30, 34, 41, 85, 87, 92, 94, 130, 139, 146, 193, 201, 209, 231, 246, 253, 266, 273, 287, 301, 305]\n",
      "Total paper with citation:  292\n",
      "For name:  m_guerra\n",
      "(18, 2)\n",
      "m_guerra  pass\n",
      "For name:  h_suzuki\n",
      "(82, 2)\n",
      "h_suzuki  pass\n",
      "For name:  m_cohen\n",
      "(251, 2)\n",
      "m_cohen  pass\n",
      "For name:  m_kobayashi\n",
      "(51, 2)\n",
      "m_kobayashi  pass\n",
      "For name:  s_wright\n",
      "(61, 2)\n",
      "s_wright  pass\n",
      "For name:  a_mills\n",
      "(169, 2)\n",
      "a_mills  pass\n",
      "For name:  c_west\n",
      "(181, 2)\n",
      "c_west  pass\n",
      "For name:  a_marino\n",
      "(15, 2)\n",
      "a_marino  pass\n",
      "For name:  r_jiang\n",
      "(102, 2)\n",
      "r_jiang  pass\n",
      "For name:  t_becker\n",
      "(21, 2)\n",
      "t_becker  pass\n",
      "For name:  s_pedersen\n",
      "(322, 2)\n",
      "s_pedersen  pass\n",
      "For name:  a_ali\n",
      "(62, 2)\n",
      "a_ali  pass\n",
      "For name:  k_jones\n",
      "(607, 2)\n",
      "k_jones  pass\n",
      "For name:  m_becker\n",
      "(67, 2)\n",
      "m_becker  pass\n",
      "For name:  c_marshall\n",
      "(106, 2)\n",
      "c_marshall  pass\n",
      "For name:  s_rafiq\n",
      "(33, 2)\n",
      "s_rafiq  pass\n",
      "For name:  h_liang\n",
      "(104, 2)\n",
      "h_liang  pass\n",
      "For name:  c_davis\n",
      "(43, 2)\n",
      "c_davis  pass\n",
      "For name:  e_hall\n",
      "(115, 2)\n",
      "e_hall  pass\n",
      "For name:  g_volpe\n",
      "(31, 2)\n",
      "g_volpe  pass\n",
      "For name:  r_lewis\n",
      "(427, 2)\n",
      "Total sample size before apply threshold:  427\n",
      "Counter({'0000-0003-3470-923X': 185, '0000-0002-2002-4339': 175, '0000-0003-4044-9104': 41, '0000-0002-4598-7553': 7, '0000-0003-1395-3276': 6, '0000-0003-1859-0021': 4, '0000-0001-9929-2629': 3, '0000-0001-6642-5771': 3, '0000-0002-2680-6235': 1, '0000-0002-6644-6385': 1, '0000-0003-1046-811X': 1})\n",
      "Total author before apply threshoid:  11\n",
      "Total author after apply threshoid:  2\n",
      "Total sample size after apply threshold:  360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing sample:  0\n",
      "(360, 101)\n",
      "Total missing sample:  16\n",
      "(360, 101)\n",
      "Papers with no citation:  [13, 43, 94, 100, 129, 130, 143, 147, 185, 201, 249, 263, 272, 277, 321, 343]\n",
      "Total paper with citation:  344\n",
      "For name:  c_rodriguez\n",
      "(43, 2)\n",
      "c_rodriguez  pass\n",
      "For name:  p_hall\n",
      "(22, 2)\n",
      "p_hall  pass\n",
      "For name:  r_srivastava\n",
      "(184, 2)\n",
      "r_srivastava  pass\n",
      "For name:  a_macedo\n",
      "(29, 2)\n",
      "a_macedo  pass\n",
      "For name:  m_schultz\n",
      "(40, 2)\n",
      "m_schultz  pass\n",
      "For name:  s_jacobs\n",
      "(21, 2)\n",
      "s_jacobs  pass\n",
      "For name:  c_hong\n",
      "(32, 2)\n",
      "c_hong  pass\n",
      "For name:  r_mohan\n",
      "(7, 2)\n",
      "r_mohan  pass\n",
      "For name:  r_hill\n",
      "(90, 2)\n",
      "r_hill  pass\n",
      "For name:  q_shen\n",
      "(57, 2)\n",
      "q_shen  pass\n",
      "For name:  l_schmidt\n",
      "(13, 2)\n",
      "l_schmidt  pass\n",
      "For name:  s_qin\n",
      "(42, 2)\n",
      "s_qin  pass\n",
      "For name:  a_fabbri\n",
      "(64, 2)\n",
      "a_fabbri  pass\n",
      "For name:  l_robinson\n",
      "(93, 2)\n",
      "l_robinson  pass\n",
      "For name:  r_gross\n",
      "(71, 2)\n",
      "r_gross  pass\n",
      "For name:  j_ahn\n",
      "(130, 2)\n",
      "j_ahn  pass\n",
      "For name:  j_john\n",
      "(43, 2)\n",
      "j_john  pass\n",
      "For name:  d_lloyd\n",
      "(157, 2)\n",
      "d_lloyd  pass\n",
      "For name:  a_mohammadi\n",
      "(8, 2)\n",
      "a_mohammadi  pass\n",
      "For name:  d_dean\n",
      "(189, 2)\n",
      "d_dean  pass\n",
      "For name:  s_chang\n",
      "(592, 2)\n",
      "s_chang  pass\n",
      "For name:  m_conte\n",
      "(118, 2)\n",
      "m_conte  pass\n",
      "For name:  i_wilson\n",
      "(220, 2)\n",
      "i_wilson  pass\n",
      "For name:  h_yoo\n",
      "(22, 2)\n",
      "h_yoo  pass\n",
      "For name:  d_das\n",
      "(17, 2)\n",
      "d_das  pass\n",
      "For name:  d_carr\n",
      "(34, 2)\n",
      "d_carr  pass\n",
      "For name:  s_sahu\n",
      "(18, 2)\n",
      "s_sahu  pass\n",
      "For name:  m_tsai\n",
      "(110, 2)\n",
      "m_tsai  pass\n",
      "For name:  m_vitale\n",
      "(217, 2)\n",
      "m_vitale  pass\n",
      "For name:  r_castro\n",
      "(116, 2)\n",
      "r_castro  pass\n",
      "For name:  a_hassan\n",
      "(16, 2)\n",
      "a_hassan  pass\n",
      "For name:  w_martin\n",
      "(259, 2)\n",
      "w_martin  pass\n",
      "For name:  a_krishnan\n",
      "(46, 2)\n",
      "a_krishnan  pass\n",
      "For name:  l_tavares\n",
      "(41, 2)\n",
      "l_tavares  pass\n",
      "For name:  t_murakami\n",
      "(63, 2)\n",
      "t_murakami  pass\n",
      "For name:  x_xiao\n",
      "(31, 2)\n",
      "x_xiao  pass\n",
      "For name:  j_davies\n",
      "(122, 2)\n",
      "j_davies  pass\n",
      "For name:  a_schmidt\n",
      "(90, 2)\n",
      "a_schmidt  pass\n",
      "For name:  j_nieto\n",
      "(56, 2)\n",
      "j_nieto  pass\n",
      "For name:  s_hasan\n",
      "(12, 2)\n",
      "s_hasan  pass\n",
      "For name:  m_teixeira\n",
      "(313, 2)\n",
      "m_teixeira  pass\n",
      "For name:  j_koh\n",
      "(62, 2)\n",
      "j_koh  pass\n",
      "For name:  m_amin\n",
      "(13, 2)\n",
      "m_amin  pass\n",
      "For name:  h_cho\n",
      "(73, 2)\n",
      "h_cho  pass\n",
      "For name:  s_lam\n",
      "(90, 2)\n",
      "s_lam  pass\n",
      "For name:  t_tran\n",
      "(54, 2)\n",
      "t_tran  pass\n",
      "For name:  c_su\n",
      "(297, 2)\n",
      "c_su  pass\n",
      "For name:  s_george\n",
      "(78, 2)\n",
      "s_george  pass\n",
      "For name:  j_hong\n",
      "(143, 2)\n",
      "j_hong  pass\n",
      "For name:  p_baptista\n",
      "(114, 2)\n",
      "p_baptista  pass\n",
      "For name:  p_thompson\n",
      "(148, 2)\n",
      "p_thompson  pass\n",
      "For name:  a_castro\n",
      "(126, 2)\n",
      "a_castro  pass\n",
      "For name:  j_zhang\n",
      "(965, 2)\n",
      "j_zhang  pass\n",
      "For name:  j_rodrigues\n",
      "(264, 2)\n",
      "j_rodrigues  pass\n",
      "For name:  s_watson\n",
      "(117, 2)\n",
      "s_watson  pass\n",
      "For name:  c_barros\n",
      "(34, 2)\n",
      "c_barros  pass\n",
      "For name:  f_cardoso\n",
      "(178, 2)\n",
      "f_cardoso  pass\n",
      "For name:  m_pinto\n",
      "(201, 2)\n",
      "m_pinto  pass\n",
      "For name:  j_cuevas\n",
      "(78, 2)\n",
      "j_cuevas  pass\n",
      "For name:  j_chang\n",
      "(360, 2)\n",
      "j_chang  pass\n",
      "For name:  a_dias\n",
      "(90, 2)\n",
      "a_dias  pass\n",
      "For name:  j_choi\n",
      "(441, 2)\n",
      "j_choi  pass\n",
      "For name:  m_ahmed\n",
      "(27, 2)\n",
      "m_ahmed  pass\n",
      "For name:  j_jo\n",
      "(13, 2)\n",
      "j_jo  pass\n",
      "For name:  n_dawson\n",
      "(31, 2)\n",
      "n_dawson  pass\n",
      "For name:  j_barbosa\n",
      "(35, 2)\n",
      "j_barbosa  pass\n",
      "For name:  e_o'connor\n",
      "(18, 2)\n",
      "e_o'connor  pass\n",
      "For name:  c_zheng\n",
      "(46, 2)\n",
      "c_zheng  pass\n",
      "For name:  r_hall\n",
      "(144, 2)\n",
      "r_hall  pass\n",
      "For name:  d_hwang\n",
      "(52, 2)\n",
      "d_hwang  pass\n",
      "For name:  c_shen\n",
      "(111, 2)\n",
      "c_shen  pass\n",
      "For name:  v_lopes\n",
      "(26, 2)\n",
      "v_lopes  pass\n",
      "For name:  m_quintana\n",
      "(68, 2)\n",
      "m_quintana  pass\n",
      "For name:  j_nunes\n",
      "(62, 2)\n",
      "j_nunes  pass\n",
      "For name:  z_nagy\n",
      "(110, 2)\n",
      "z_nagy  pass\n",
      "For name:  e_brown\n",
      "(71, 2)\n",
      "e_brown  pass\n",
      "For name:  j_nielsen\n",
      "(913, 2)\n",
      "Total sample size before apply threshold:  913\n",
      "Counter({'0000-0002-9955-6003': 487, '0000-0001-5568-2916': 105, '0000-0001-9414-1653': 104, '0000-0002-8747-6938': 57, '0000-0002-2831-7718': 39, '0000-0002-2854-8188': 35, '0000-0003-2228-5994': 24, '0000-0002-2058-3579': 23, '0000-0003-1730-3094': 13, '0000-0001-8521-7353': 9, '0000-0002-5211-948X': 8, '0000-0002-8112-8449': 6, '0000-0002-3418-4907': 2, '0000-0002-4760-3875': 1})\n",
      "Total author before apply threshoid:  14\n",
      "Total author after apply threshoid:  3\n",
      "Total sample size after apply threshold:  696\n",
      "Total missing sample:  0\n",
      "(696, 101)\n",
      "Total missing sample:  46\n",
      "(696, 101)\n",
      "Papers with no citation:  [24, 60, 85, 90, 96, 110, 116, 132, 137, 158, 162, 179, 197, 217, 223, 233, 256, 283, 301, 316, 337, 356, 389, 401, 402, 408, 414, 417, 427, 428, 432, 452, 455, 471, 472, 475, 518, 542, 558, 567, 574, 631, 652, 661, 676, 679]\n",
      "Total paper with citation:  650\n",
      "j_nielsen is multi-class case, ignored\n",
      "For name:  w_choi\n",
      "(118, 2)\n",
      "w_choi  pass\n",
      "For name:  d_tavares\n",
      "(13, 2)\n",
      "d_tavares  pass\n",
      "For name:  l_alves\n",
      "(51, 2)\n",
      "l_alves  pass\n",
      "For name:  s_chan\n",
      "(176, 2)\n",
      "s_chan  pass\n",
      "For name:  b_ferreira\n",
      "(29, 2)\n",
      "b_ferreira  pass\n",
      "For name:  r_neves\n",
      "(25, 2)\n",
      "r_neves  pass\n",
      "For name:  m_cardoso\n",
      "(105, 2)\n",
      "m_cardoso  pass\n",
      "For name:  c_shao\n",
      "(96, 2)\n",
      "c_shao  pass\n",
      "For name:  h_yeo\n",
      "(10, 2)\n",
      "h_yeo  pass\n",
      "For name:  m_goodman\n",
      "(100, 2)\n",
      "m_goodman  pass\n",
      "For name:  r_dias\n",
      "(26, 2)\n",
      "r_dias  pass\n",
      "For name:  s_sengupta\n",
      "(149, 2)\n",
      "s_sengupta  pass\n",
      "For name:  y_jung\n",
      "(180, 2)\n",
      "y_jung  pass\n",
      "For name:  c_franco\n",
      "(64, 2)\n",
      "c_franco  pass\n",
      "For name:  v_wong\n",
      "(35, 2)\n",
      "v_wong  pass\n",
      "For name:  j_feng\n",
      "(147, 2)\n",
      "j_feng  pass\n",
      "For name:  s_murugesan\n",
      "(7, 2)\n",
      "s_murugesan  pass\n",
      "For name:  j_camacho\n",
      "(139, 2)\n",
      "j_camacho  pass\n",
      "For name:  b_moreno\n",
      "(8, 2)\n",
      "b_moreno  pass\n",
      "For name:  j_andersen\n",
      "(129, 2)\n",
      "j_andersen  pass\n",
      "For name:  j_bell\n",
      "(34, 2)\n",
      "j_bell  pass\n",
      "For name:  m_bull\n",
      "(5, 2)\n",
      "m_bull  pass\n",
      "For name:  s_gandhi\n",
      "(9, 2)\n",
      "s_gandhi  pass\n",
      "For name:  c_yang\n",
      "(514, 2)\n",
      "c_yang  pass\n",
      "For name:  s_paul\n",
      "(52, 2)\n",
      "s_paul  pass\n",
      "For name:  l_roberts\n",
      "(363, 2)\n",
      "Total sample size before apply threshold:  363\n",
      "Counter({'0000-0003-4270-253X': 206, '0000-0001-7885-8574': 120, '0000-0002-1455-5248': 18, '0000-0003-0085-9213': 14, '0000-0003-3892-2900': 3, '0000-0002-0329-8389': 2})\n",
      "Total author before apply threshoid:  6\n",
      "Total author after apply threshoid:  2\n",
      "Total sample size after apply threshold:  326\n",
      "Total missing sample:  0\n",
      "(326, 101)\n",
      "Total missing sample:  69\n",
      "(326, 101)\n",
      "Papers with no citation:  [1, 4, 9, 11, 13, 19, 21, 26, 32, 42, 45, 46, 48, 64, 66, 73, 76, 78, 89, 93, 97, 101, 102, 104, 109, 116, 120, 125, 128, 132, 133, 139, 140, 143, 144, 154, 159, 171, 173, 174, 179, 180, 190, 193, 198, 202, 207, 216, 219, 227, 234, 239, 243, 246, 252, 269, 271, 273, 274, 277, 278, 291, 292, 301, 302, 312, 313, 317, 324]\n",
      "Total paper with citation:  257\n",
      "For name:  s_keating\n",
      "(54, 2)\n",
      "s_keating  pass\n",
      "For name:  a_bennett\n",
      "(56, 2)\n",
      "a_bennett  pass\n",
      "For name:  a_aggarwal\n",
      "(22, 2)\n",
      "a_aggarwal  pass\n",
      "For name:  i_moura\n",
      "(203, 2)\n",
      "i_moura  pass\n",
      "For name:  d_teixeira\n",
      "(27, 2)\n",
      "d_teixeira  pass\n",
      "For name:  c_klein\n",
      "(106, 2)\n",
      "c_klein  pass\n",
      "For name:  m_andersson\n",
      "(152, 2)\n",
      "m_andersson  pass\n",
      "For name:  h_shi\n",
      "(21, 2)\n",
      "h_shi  pass\n",
      "For name:  d_howard\n",
      "(79, 2)\n",
      "d_howard  pass\n",
      "For name:  j_thomsen\n",
      "(28, 2)\n",
      "j_thomsen  pass\n",
      "For name:  v_gupta\n",
      "(238, 2)\n",
      "v_gupta  pass\n",
      "For name:  j_manning\n",
      "(16, 2)\n",
      "j_manning  pass\n",
      "For name:  r_wood\n",
      "(97, 2)\n",
      "r_wood  pass\n",
      "For name:  y_ding\n",
      "(106, 2)\n",
      "y_ding  pass\n",
      "For name:  j_rasmussen\n",
      "(33, 2)\n",
      "j_rasmussen  pass\n",
      "For name:  n_lee\n",
      "(108, 2)\n",
      "n_lee  pass\n",
      "For name:  a_oliveira\n",
      "(302, 2)\n",
      "a_oliveira  pass\n",
      "For name:  h_yin\n",
      "(130, 2)\n",
      "h_yin  pass\n",
      "For name:  k_brown\n",
      "(231, 2)\n",
      "k_brown  pass\n",
      "For name:  s_hong\n",
      "(383, 2)\n",
      "s_hong  pass\n",
      "For name:  l_zhou\n",
      "(49, 2)\n",
      "l_zhou  pass\n",
      "For name:  h_jiang\n",
      "(135, 2)\n",
      "h_jiang  pass\n",
      "For name:  a_lewis\n",
      "(98, 2)\n",
      "a_lewis  pass\n",
      "For name:  c_meyer\n",
      "(136, 2)\n",
      "c_meyer  pass\n",
      "For name:  a_islam\n",
      "(18, 2)\n",
      "a_islam  pass\n",
      "For name:  k_fujita\n",
      "(42, 2)\n",
      "k_fujita  pass\n",
      "For name:  a_khan\n",
      "(226, 2)\n",
      "a_khan  pass\n",
      "For name:  a_kim\n",
      "(25, 2)\n",
      "a_kim  pass\n",
      "For name:  m_martinez\n",
      "(69, 2)\n",
      "m_martinez  pass\n",
      "For name:  m_aslam\n",
      "(55, 2)\n",
      "m_aslam  pass\n",
      "For name:  j_wolf\n",
      "(65, 2)\n",
      "j_wolf  pass\n",
      "For name:  s_agrawal\n",
      "(30, 2)\n",
      "s_agrawal  pass\n",
      "For name:  a_othman\n",
      "(40, 2)\n",
      "a_othman  pass\n",
      "For name:  k_evans\n",
      "(29, 2)\n",
      "k_evans  pass\n",
      "For name:  k_yoo\n",
      "(10, 2)\n",
      "k_yoo  pass\n",
      "For name:  d_turner\n",
      "(71, 2)\n",
      "d_turner  pass\n",
      "For name:  j_king\n",
      "(75, 2)\n",
      "j_king  pass\n",
      "For name:  b_shen\n",
      "(36, 2)\n",
      "b_shen  pass\n",
      "For name:  s_mishra\n",
      "(116, 2)\n",
      "s_mishra  pass\n",
      "For name:  c_o'connor\n",
      "(10, 2)\n",
      "c_o'connor  pass\n",
      "For name:  e_svensson\n",
      "(87, 2)\n",
      "e_svensson  pass\n",
      "For name:  o_ahmed\n",
      "(48, 2)\n",
      "o_ahmed  pass\n",
      "For name:  t_shimada\n",
      "(144, 2)\n",
      "t_shimada  pass\n",
      "For name:  a_watts\n",
      "(25, 2)\n",
      "a_watts  pass\n",
      "For name:  b_oliveira\n",
      "(60, 2)\n",
      "b_oliveira  pass\n",
      "For name:  t_ito\n",
      "(69, 2)\n",
      "t_ito  pass\n",
      "For name:  t_jackson\n",
      "(47, 2)\n",
      "t_jackson  pass\n",
      "For name:  m_romero\n",
      "(29, 2)\n",
      "m_romero  pass\n",
      "For name:  j_west\n",
      "(198, 2)\n",
      "j_west  pass\n",
      "For name:  c_guo\n",
      "(6, 2)\n",
      "c_guo  pass\n",
      "For name:  m_hansen\n",
      "(252, 2)\n",
      "m_hansen  pass\n",
      "For name:  x_qian\n",
      "(20, 2)\n",
      "x_qian  pass\n",
      "For name:  m_wagner\n",
      "(314, 2)\n",
      "m_wagner  pass\n",
      "For name:  d_campos\n",
      "(49, 2)\n",
      "d_campos  pass\n",
      "For name:  r_clark\n",
      "(152, 2)\n",
      "r_clark  pass\n",
      "For name:  b_zhou\n",
      "(20, 2)\n",
      "b_zhou  pass\n",
      "For name:  x_yan\n",
      "(111, 2)\n",
      "x_yan  pass\n",
      "For name:  x_li\n",
      "(867, 2)\n",
      "x_li  pass\n",
      "For name:  j_burton\n",
      "(46, 2)\n",
      "j_burton  pass\n",
      "For name:  x_feng\n",
      "(102, 2)\n",
      "x_feng  pass\n",
      "For name:  w_hussein\n",
      "(33, 2)\n",
      "w_hussein  pass\n",
      "For name:  c_santos\n",
      "(293, 2)\n",
      "c_santos  pass\n",
      "For name:  j_figueroa\n",
      "(59, 2)\n",
      "j_figueroa  pass\n",
      "For name:  w_cui\n",
      "(14, 2)\n",
      "w_cui  pass\n",
      "For name:  d_moreira\n",
      "(26, 2)\n",
      "d_moreira  pass\n",
      "For name:  m_graham\n",
      "(64, 2)\n",
      "m_graham  pass\n",
      "For name:  g_dias\n",
      "(9, 2)\n",
      "g_dias  pass\n",
      "For name:  h_yoshida\n",
      "(72, 2)\n",
      "h_yoshida  pass\n",
      "For name:  m_branco\n",
      "(56, 2)\n",
      "m_branco  pass\n",
      "For name:  k_chong\n",
      "(39, 2)\n",
      "k_chong  pass\n",
      "For name:  j_kumar\n",
      "(16, 2)\n",
      "j_kumar  pass\n",
      "For name:  a_shenoy\n",
      "(33, 2)\n",
      "a_shenoy  pass\n",
      "For name:  h_yang\n",
      "(417, 2)\n",
      "h_yang  pass\n",
      "For name:  m_magnusson\n",
      "(67, 2)\n",
      "m_magnusson  pass\n",
      "For name:  m_foster\n",
      "(103, 2)\n",
      "m_foster  pass\n",
      "For name:  j_lynch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 2)\n",
      "j_lynch  pass\n",
      "For name:  j_boyle\n",
      "(14, 2)\n",
      "j_boyle  pass\n",
      "For name:  r_turner\n",
      "(147, 2)\n",
      "r_turner  pass\n",
      "For name:  s_brooks\n",
      "(58, 2)\n",
      "s_brooks  pass\n",
      "For name:  p_moreira\n",
      "(217, 2)\n",
      "p_moreira  pass\n",
      "For name:  s_mukhopadhyay\n",
      "(119, 2)\n",
      "s_mukhopadhyay  pass\n",
      "For name:  a_hudson\n",
      "(129, 2)\n",
      "a_hudson  pass\n",
      "For name:  d_thomas\n",
      "(62, 2)\n",
      "d_thomas  pass\n",
      "For name:  w_smith\n",
      "(61, 2)\n",
      "w_smith  pass\n",
      "For name:  l_martin\n",
      "(253, 2)\n",
      "l_martin  pass\n",
      "For name:  c_garcia\n",
      "(106, 2)\n",
      "c_garcia  pass\n",
      "For name:  g_huang\n",
      "(160, 2)\n",
      "g_huang  pass\n",
      "For name:  j_huber\n",
      "(96, 2)\n",
      "j_huber  pass\n",
      "For name:  j_qin\n",
      "(96, 2)\n",
      "j_qin  pass\n",
      "For name:  t_ho\n",
      "(83, 2)\n",
      "t_ho  pass\n",
      "For name:  c_keller\n",
      "(15, 2)\n",
      "c_keller  pass\n",
      "For name:  m_maia\n",
      "(99, 2)\n",
      "m_maia  pass\n",
      "For name:  p_bates\n",
      "(154, 2)\n",
      "p_bates  pass\n",
      "For name:  s_chow\n",
      "(29, 2)\n",
      "s_chow  pass\n",
      "For name:  m_simon\n",
      "(66, 2)\n",
      "m_simon  pass\n",
      "For name:  s_kar\n",
      "(36, 2)\n",
      "s_kar  pass\n",
      "For name:  d_vlachos\n",
      "(101, 2)\n",
      "d_vlachos  pass\n",
      "For name:  e_law\n",
      "(12, 2)\n",
      "e_law  pass\n",
      "For name:  m_ribeiro\n",
      "(134, 2)\n",
      "m_ribeiro  pass\n",
      "For name:  r_king\n",
      "(69, 2)\n",
      "r_king  pass\n",
      "For name:  o_nielsen\n",
      "(212, 2)\n",
      "o_nielsen  pass\n",
      "For name:  j_moreno\n",
      "(138, 2)\n",
      "j_moreno  pass\n",
      "For name:  f_yu\n",
      "(78, 2)\n",
      "f_yu  pass\n",
      "For name:  f_esposito\n",
      "(342, 2)\n",
      "f_esposito  pass\n",
      "For name:  p_miranda\n",
      "(69, 2)\n",
      "p_miranda  pass\n",
      "For name:  s_yang\n",
      "(611, 2)\n",
      "s_yang  pass\n",
      "For name:  d_huang\n",
      "(38, 2)\n",
      "d_huang  pass\n",
      "For name:  h_kuo\n",
      "(144, 2)\n",
      "h_kuo  pass\n",
      "For name:  a_santoro\n",
      "(189, 2)\n",
      "a_santoro  pass\n",
      "For name:  q_lu\n",
      "(35, 2)\n",
      "q_lu  pass\n",
      "For name:  s_kumar\n",
      "(419, 2)\n",
      "s_kumar  pass\n",
      "For name:  s_rocha\n",
      "(139, 2)\n",
      "s_rocha  pass\n",
      "For name:  t_han\n",
      "(53, 2)\n",
      "t_han  pass\n",
      "For name:  m_sandberg\n",
      "(59, 2)\n",
      "m_sandberg  pass\n",
      "For name:  j_marshall\n",
      "(80, 2)\n",
      "j_marshall  pass\n",
      "For name:  f_bianchi\n",
      "(131, 2)\n",
      "f_bianchi  pass\n",
      "For name:  c_liu\n",
      "(681, 2)\n",
      "c_liu  pass\n",
      "For name:  d_sanders\n",
      "(16, 2)\n",
      "d_sanders  pass\n",
      "For name:  r_brito\n",
      "(51, 2)\n",
      "r_brito  pass\n",
      "For name:  w_chang\n",
      "(91, 2)\n",
      "w_chang  pass\n",
      "For name:  a_murray\n",
      "(76, 2)\n",
      "a_murray  pass\n",
      "For name:  b_cao\n",
      "(58, 2)\n",
      "b_cao  pass\n",
      "For name:  k_sohn\n",
      "(31, 2)\n",
      "k_sohn  pass\n",
      "For name:  m_bennett\n",
      "(208, 2)\n",
      "m_bennett  pass\n",
      "For name:  a_sharma\n",
      "(223, 2)\n",
      "a_sharma  pass\n",
      "For name:  z_wei\n",
      "(54, 2)\n",
      "z_wei  pass\n",
      "For name:  x_gu\n",
      "(61, 2)\n",
      "x_gu  pass\n",
      "For name:  l_yang\n",
      "(193, 2)\n",
      "l_yang  pass\n",
      "For name:  h_hassan\n",
      "(22, 2)\n",
      "h_hassan  pass\n",
      "For name:  f_chen\n",
      "(40, 2)\n",
      "f_chen  pass\n",
      "For name:  g_rossi\n",
      "(245, 2)\n",
      "g_rossi  pass\n",
      "For name:  s_patil\n",
      "(65, 2)\n",
      "s_patil  pass\n",
      "For name:  m_kelly\n",
      "(97, 2)\n",
      "m_kelly  pass\n",
      "For name:  m_cheung\n",
      "(13, 2)\n",
      "m_cheung  pass\n",
      "For name:  j_weaver\n",
      "(7, 2)\n",
      "j_weaver  pass\n",
      "For name:  c_chien\n",
      "(157, 2)\n",
      "c_chien  pass\n",
      "For name:  s_yun\n",
      "(102, 2)\n",
      "s_yun  pass\n",
      "For name:  s_jung\n",
      "(76, 2)\n",
      "s_jung  pass\n",
      "For name:  e_gomes\n",
      "(40, 2)\n",
      "e_gomes  pass\n",
      "For name:  t_yamaguchi\n",
      "(62, 2)\n",
      "t_yamaguchi  pass\n",
      "For name:  p_oliveira\n",
      "(358, 2)\n",
      "p_oliveira  pass\n",
      "For name:  r_torres\n",
      "(40, 2)\n",
      "r_torres  pass\n",
      "For name:  a_esteves\n",
      "(63, 2)\n",
      "a_esteves  pass\n",
      "For name:  l_stevens\n",
      "(77, 2)\n",
      "l_stevens  pass\n",
      "For name:  a_chang\n",
      "(178, 2)\n",
      "a_chang  pass\n",
      "For name:  l_song\n",
      "(58, 2)\n",
      "l_song  pass\n",
      "For name:  j_delgado\n",
      "(123, 2)\n",
      "j_delgado  pass\n",
      "For name:  p_jensen\n",
      "(319, 2)\n",
      "p_jensen  pass\n",
      "For name:  t_allen\n",
      "(48, 2)\n",
      "t_allen  pass\n",
      "For name:  j_sullivan\n",
      "(79, 2)\n",
      "j_sullivan  pass\n",
      "For name:  s_rogers\n",
      "(224, 2)\n",
      "s_rogers  pass\n",
      "For name:  h_yoon\n",
      "(72, 2)\n",
      "h_yoon  pass\n",
      "For name:  a_young\n",
      "(442, 2)\n",
      "a_young  pass\n",
      "For name:  m_richardson\n",
      "(175, 2)\n",
      "m_richardson  pass\n",
      "For name:  c_ryan\n",
      "(159, 2)\n",
      "c_ryan  pass\n",
      "For name:  l_jensen\n",
      "(275, 2)\n",
      "l_jensen  pass\n",
      "For name:  h_ferreira\n",
      "(135, 2)\n",
      "h_ferreira  pass\n",
      "For name:  a_mahmoud\n",
      "(48, 2)\n",
      "a_mahmoud  pass\n",
      "For name:  y_liao\n",
      "(115, 2)\n",
      "y_liao  pass\n",
      "For name:  m_svensson\n",
      "(142, 2)\n",
      "m_svensson  pass\n",
      "For name:  p_tsai\n",
      "(73, 2)\n",
      "p_tsai  pass\n",
      "For name:  r_berry\n",
      "(85, 2)\n",
      "r_berry  pass\n",
      "For name:  j_kwok\n",
      "(100, 2)\n",
      "j_kwok  pass\n",
      "For name:  m_schneider\n",
      "(367, 2)\n",
      "m_schneider  pass\n",
      "For name:  k_wood\n",
      "(24, 2)\n",
      "k_wood  pass\n",
      "For name:  c_viegas\n",
      "(67, 2)\n",
      "c_viegas  pass\n",
      "For name:  r_d'souza\n",
      "(83, 2)\n",
      "r_d'souza  pass\n",
      "For name:  s_shim\n",
      "(41, 2)\n",
      "s_shim  pass\n",
      "For name:  j_herrero\n",
      "(105, 2)\n",
      "j_herrero  pass\n",
      "For name:  m_acosta\n",
      "(47, 2)\n",
      "m_acosta  pass\n",
      "For name:  a_chan\n",
      "(249, 2)\n",
      "a_chan  pass\n",
      "For name:  p_kelly\n",
      "(55, 2)\n",
      "p_kelly  pass\n",
      "For name:  j_weiner\n",
      "(61, 2)\n",
      "j_weiner  pass\n",
      "For name:  b_yu\n",
      "(24, 2)\n",
      "b_yu  pass\n",
      "For name:  s_lucas\n",
      "(96, 2)\n",
      "s_lucas  pass\n",
      "For name:  e_davis\n",
      "(21, 2)\n",
      "e_davis  pass\n",
      "For name:  z_yu\n",
      "(135, 2)\n",
      "z_yu  pass\n",
      "For name:  c_pan\n",
      "(161, 2)\n",
      "c_pan  pass\n",
      "For name:  x_cao\n",
      "(74, 2)\n",
      "x_cao  pass\n",
      "For name:  j_yoo\n",
      "(112, 2)\n",
      "j_yoo  pass\n",
      "For name:  l_wong\n",
      "(131, 2)\n",
      "l_wong  pass\n",
      "For name:  h_chen\n",
      "(986, 2)\n",
      "h_chen  pass\n",
      "For name:  c_huang\n",
      "(425, 2)\n",
      "c_huang  pass\n",
      "For name:  s_chong\n",
      "(40, 2)\n",
      "s_chong  pass\n",
      "For name:  z_wu\n",
      "(221, 2)\n",
      "z_wu  pass\n",
      "For name:  m_swamy\n",
      "(134, 2)\n",
      "m_swamy  pass\n",
      "For name:  k_nomura\n",
      "(38, 2)\n",
      "k_nomura  pass\n",
      "For name:  m_wu\n",
      "(658, 2)\n",
      "Total sample size before apply threshold:  658\n",
      "Counter({'0000-0002-1940-6428': 219, '0000-0002-7074-8087': 194, '0000-0002-1674-443X': 56, '0000-0003-3327-828X': 42, '0000-0001-6587-7055': 33, '0000-0002-8811-9203': 29, '0000-0002-7509-1643': 22, '0000-0003-2045-9372': 13, '0000-0002-9161-7940': 11, '0000-0003-3712-1554': 10, '0000-0001-7672-9357': 6, '0000-0003-2113-0245': 5, '0000-0001-6847-7065': 5, '0000-0003-0977-3600': 4, '0000-0003-1372-4764': 2, '0000-0003-1734-7994': 2, '0000-0002-3269-1681': 2, '0000-0002-0183-0490': 1, '0000-0001-6646-050X': 1, '0000-0002-6646-951X': 1})\n",
      "Total author before apply threshoid:  20\n",
      "Total author after apply threshoid:  2\n",
      "Total sample size after apply threshold:  413\n",
      "Total missing sample:  0\n",
      "(413, 101)\n",
      "Total missing sample:  19\n",
      "(413, 101)\n",
      "Papers with no citation:  [22, 38, 50, 52, 60, 77, 98, 108, 195, 198, 208, 228, 237, 287, 301, 312, 381, 391, 395]\n",
      "Total paper with citation:  394\n",
      "For name:  e_lee\n",
      "(300, 2)\n",
      "e_lee  pass\n",
      "For name:  j_weber\n",
      "(146, 2)\n",
      "j_weber  pass\n",
      "For name:  c_fox\n",
      "(102, 2)\n",
      "c_fox  pass\n",
      "For name:  s_thompson\n",
      "(45, 2)\n",
      "s_thompson  pass\n",
      "For name:  b_choi\n",
      "(24, 2)\n",
      "b_choi  pass\n",
      "For name:  j_schwartz\n",
      "(51, 2)\n",
      "j_schwartz  pass\n",
      "For name:  a_brooks\n",
      "(185, 2)\n",
      "a_brooks  pass\n",
      "For name:  l_rocha\n",
      "(81, 2)\n",
      "l_rocha  pass\n",
      "For name:  s_fleming\n",
      "(35, 2)\n",
      "s_fleming  pass\n",
      "For name:  w_tsai\n",
      "(113, 2)\n",
      "w_tsai  pass\n",
      "For name:  m_rodriguez\n",
      "(214, 2)\n",
      "m_rodriguez  pass\n",
      "For name:  r_miranda\n",
      "(81, 2)\n",
      "r_miranda  pass\n",
      "For name:  j_richardson\n",
      "(84, 2)\n",
      "j_richardson  pass\n",
      "For name:  a_chin\n",
      "(73, 2)\n",
      "a_chin  pass\n",
      "For name:  h_madsen\n",
      "(8, 2)\n",
      "h_madsen  pass\n",
      "For name:  m_ferguson\n",
      "(168, 2)\n",
      "m_ferguson  pass\n",
      "For name:  s_mitra\n",
      "(48, 2)\n",
      "s_mitra  pass\n",
      "For name:  v_pinto\n",
      "(48, 2)\n",
      "v_pinto  pass\n",
      "For name:  m_field\n",
      "(126, 2)\n",
      "m_field  pass\n",
      "For name:  c_jones\n",
      "(354, 2)\n",
      "c_jones  pass\n",
      "For name:  k_hong\n",
      "(127, 2)\n",
      "k_hong  pass\n",
      "For name:  t_williams\n",
      "(190, 2)\n",
      "t_williams  pass\n",
      "For name:  j_xavier\n",
      "(22, 2)\n",
      "j_xavier  pass\n",
      "For name:  b_bhushan\n",
      "(187, 2)\n",
      "b_bhushan  pass\n",
      "For name:  r_ellis\n",
      "(176, 2)\n",
      "r_ellis  pass\n",
      "For name:  v_saini\n",
      "(18, 2)\n",
      "v_saini  pass\n",
      "For name:  a_ellis\n",
      "(168, 2)\n",
      "a_ellis  pass\n",
      "For name:  f_reis\n",
      "(222, 2)\n",
      "f_reis  pass\n",
      "For name:  j_gray\n",
      "(112, 2)\n",
      "j_gray  pass\n",
      "For name:  r_hughes\n",
      "(57, 2)\n",
      "r_hughes  pass\n",
      "For name:  a_green\n",
      "(169, 2)\n",
      "a_green  pass\n",
      "For name:  c_reis\n",
      "(77, 2)\n",
      "c_reis  pass\n",
      "For name:  f_scott\n",
      "(26, 2)\n",
      "f_scott  pass\n",
      "For name:  l_han\n",
      "(20, 2)\n",
      "l_han  pass\n",
      "For name:  c_martins\n",
      "(121, 2)\n",
      "c_martins  pass\n",
      "For name:  r_schneider\n",
      "(158, 2)\n",
      "r_schneider  pass\n",
      "For name:  j_regan\n",
      "(27, 2)\n",
      "j_regan  pass\n",
      "For name:  s_brennan\n",
      "(31, 2)\n",
      "s_brennan  pass\n",
      "For name:  v_patel\n",
      "(27, 2)\n",
      "v_patel  pass\n",
      "For name:  d_johnston\n",
      "(29, 2)\n",
      "d_johnston  pass\n",
      "For name:  r_gupta\n",
      "(188, 2)\n",
      "r_gupta  pass\n",
      "For name:  s_reddy\n",
      "(51, 2)\n",
      "s_reddy  pass\n",
      "For name:  y_yao\n",
      "(58, 2)\n",
      "y_yao  pass\n",
      "For name:  a_huang\n",
      "(58, 2)\n",
      "a_huang  pass\n",
      "For name:  d_ghosh\n",
      "(23, 2)\n",
      "d_ghosh  pass\n",
      "For name:  r_morgan\n",
      "(15, 2)\n",
      "r_morgan  pass\n",
      "For name:  q_li\n",
      "(227, 2)\n",
      "q_li  pass\n",
      "For name:  w_wang\n",
      "(765, 2)\n",
      "Total sample size before apply threshold:  765\n",
      "Counter({'0000-0001-9033-0158': 194, '0000-0002-1430-1360': 101, '0000-0001-9093-412X': 39, '0000-0003-0509-2605': 30, '0000-0001-5983-3937': 29, '0000-0002-5369-5446': 28, '0000-0003-4287-1704': 27, '0000-0003-4053-5088': 24, '0000-0001-6022-1567': 21, '0000-0002-4309-9077': 19, '0000-0002-9852-1589': 19, '0000-0003-3987-9270': 16, '0000-0002-1935-6301': 15, '0000-0001-9208-7569': 14, '0000-0003-4163-3173': 14, '0000-0002-5257-7675': 13, '0000-0002-6652-5964': 13, '0000-0002-2269-1952': 13, '0000-0002-4628-1755': 12, '0000-0002-5943-4589': 11, '0000-0003-1319-1988': 10, '0000-0003-3007-1750': 9, '0000-0002-3780-5158': 8, '0000-0002-1083-6720': 7, '0000-0002-7762-7560': 6, '0000-0001-6587-8859': 5, '0000-0002-8330-9913': 5, '0000-0001-7496-4548': 5, '0000-0001-9568-3876': 4, '0000-0002-1260-2098': 4, '0000-0001-8947-4867': 4, '0000-0002-6154-7750': 4, '0000-0001-6109-1645': 4, '0000-0003-1567-2371': 4, '0000-0003-1788-2727': 4, '0000-0003-3941-4860': 4, '0000-0002-8814-525X': 3, '0000-0002-9303-516X': 3, '0000-0002-9865-6811': 2, '0000-0002-3245-9049': 2, '0000-0001-9223-6472': 2, '0000-0001-6818-7711': 2, '0000-0002-2468-5222': 2, '0000-0003-1930-4891': 1, '0000-0002-3116-5954': 1, '0000-0002-1225-4011': 1, '0000-0003-4712-3692': 1, '0000-0001-7511-3497': 1, '0000-0002-3709-021X': 1, '0000-0002-2914-1638': 1, '0000-0003-4426-019X': 1, '0000-0002-6983-2548': 1, '0000-0002-5812-6744': 1})\n",
      "Total author before apply threshoid:  53\n",
      "Total author after apply threshoid:  2\n",
      "Total sample size after apply threshold:  295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing sample:  0\n",
      "(295, 101)\n",
      "Total missing sample:  7\n",
      "(295, 101)\n",
      "Papers with no citation:  [13, 80, 109, 140, 208, 209, 221]\n",
      "Total paper with citation:  288\n",
      "For name:  r_ross\n",
      "(374, 2)\n",
      "r_ross  pass\n",
      "For name:  k_yamamoto\n",
      "(106, 2)\n",
      "k_yamamoto  pass\n",
      "For name:  j_silva\n",
      "(268, 2)\n",
      "j_silva  pass\n",
      "For name:  m_pellegrini\n",
      "(64, 2)\n",
      "m_pellegrini  pass\n",
      "For name:  s_kwon\n",
      "(51, 2)\n",
      "s_kwon  pass\n",
      "For name:  m_correa\n",
      "(72, 2)\n",
      "m_correa  pass\n",
      "For name:  a_pal\n",
      "(14, 2)\n",
      "a_pal  pass\n",
      "For name:  v_costa\n",
      "(141, 2)\n",
      "v_costa  pass\n",
      "For name:  j_allen\n",
      "(111, 2)\n",
      "j_allen  pass\n",
      "For name:  y_dong\n",
      "(76, 2)\n",
      "y_dong  pass\n",
      "For name:  m_fitzgerald\n",
      "(133, 2)\n",
      "m_fitzgerald  pass\n",
      "For name:  m_ferreira\n",
      "(253, 2)\n",
      "m_ferreira  pass\n",
      "For name:  m_roberts\n",
      "(320, 2)\n",
      "m_roberts  pass\n",
      "For name:  y_lim\n",
      "(76, 2)\n",
      "y_lim  pass\n",
      "For name:  g_miller\n",
      "(76, 2)\n",
      "g_miller  pass\n",
      "For name:  x_kong\n",
      "(69, 2)\n",
      "x_kong  pass\n",
      "For name:  w_cao\n",
      "(126, 2)\n",
      "w_cao  pass\n",
      "For name:  c_ma\n",
      "(126, 2)\n",
      "c_ma  pass\n",
      "For name:  j_chin\n",
      "(27, 2)\n",
      "j_chin  pass\n",
      "For name:  h_kwon\n",
      "(35, 2)\n",
      "h_kwon  pass\n",
      "For name:  s_gao\n",
      "(31, 2)\n",
      "s_gao  pass\n",
      "For name:  f_tian\n",
      "(17, 2)\n",
      "f_tian  pass\n",
      "For name:  f_martins\n",
      "(65, 2)\n",
      "f_martins  pass\n",
      "For name:  s_wolf\n",
      "(363, 2)\n",
      "Total sample size before apply threshold:  363\n",
      "Counter({'0000-0003-2972-3440': 173, '0000-0002-7467-7028': 102, '0000-0002-5337-5063': 46, '0000-0003-0832-6315': 15, '0000-0002-3747-8097': 12, '0000-0003-1752-6175': 9, '0000-0003-3921-6629': 3, '0000-0001-7717-6993': 2, '0000-0002-6748-3911': 1})\n",
      "Total author before apply threshoid:  9\n",
      "Total author after apply threshoid:  2\n",
      "Total sample size after apply threshold:  275\n",
      "Total missing sample:  0\n",
      "(275, 101)\n",
      "Total missing sample:  32\n",
      "(275, 101)\n",
      "Papers with no citation:  [1, 2, 15, 29, 59, 72, 75, 85, 90, 109, 119, 126, 133, 140, 149, 168, 177, 179, 182, 205, 206, 208, 217, 221, 235, 237, 248, 251, 252, 258, 260, 273]\n",
      "Total paper with citation:  243\n",
      "For name:  m_goldman\n",
      "(81, 2)\n",
      "m_goldman  pass\n",
      "For name:  d_tang\n",
      "(89, 2)\n",
      "d_tang  pass\n",
      "For name:  m_adams\n",
      "(190, 2)\n",
      "m_adams  pass\n",
      "For name:  t_singh\n",
      "(52, 2)\n",
      "t_singh  pass\n",
      "For name:  m_thompson\n",
      "(150, 2)\n",
      "m_thompson  pass\n",
      "For name:  s_garcia\n",
      "(20, 2)\n",
      "s_garcia  pass\n",
      "For name:  e_wang\n",
      "(155, 2)\n",
      "e_wang  pass\n",
      "For name:  c_scott\n",
      "(162, 2)\n",
      "c_scott  pass\n",
      "For name:  m_mukherjee\n",
      "(16, 2)\n",
      "m_mukherjee  pass\n",
      "For name:  j_schroeder\n",
      "(150, 2)\n",
      "j_schroeder  pass\n",
      "For name:  a_mayer\n",
      "(17, 2)\n",
      "a_mayer  pass\n",
      "For name:  e_wright\n",
      "(28, 2)\n",
      "e_wright  pass\n",
      "For name:  c_moreno\n",
      "(136, 2)\n",
      "c_moreno  pass\n",
      "For name:  a_moura\n",
      "(36, 2)\n",
      "a_moura  pass\n",
      "For name:  j_lopez\n",
      "(122, 2)\n",
      "j_lopez  pass\n",
      "For name:  a_logan\n",
      "(26, 2)\n",
      "a_logan  pass\n",
      "For name:  l_williams\n",
      "(42, 2)\n",
      "l_williams  pass\n",
      "For name:  h_young\n",
      "(109, 2)\n",
      "h_young  pass\n",
      "For name:  a_vincent\n",
      "(79, 2)\n",
      "a_vincent  pass\n",
      "For name:  a_monteiro\n",
      "(132, 2)\n",
      "a_monteiro  pass\n",
      "For name:  d_park\n",
      "(156, 2)\n",
      "d_park  pass\n",
      "For name:  d_gao\n",
      "(23, 2)\n",
      "d_gao  pass\n",
      "For name:  d_quinn\n",
      "(145, 2)\n",
      "d_quinn  pass\n",
      "For name:  n_dias\n",
      "(17, 2)\n",
      "n_dias  pass\n",
      "For name:  k_fisher\n",
      "(24, 2)\n",
      "k_fisher  pass\n",
      "For name:  m_schubert\n",
      "(84, 2)\n",
      "m_schubert  pass\n",
      "For name:  j_peters\n",
      "(154, 2)\n",
      "j_peters  pass\n",
      "For name:  e_zimmermann\n",
      "(57, 2)\n",
      "e_zimmermann  pass\n",
      "For name:  c_zhang\n",
      "(321, 2)\n",
      "c_zhang  pass\n",
      "For name:  h_shin\n",
      "(114, 2)\n",
      "h_shin  pass\n",
      "For name:  r_reis\n",
      "(615, 2)\n",
      "Total sample size before apply threshold:  615\n",
      "Counter({'0000-0002-4295-6129': 423, '0000-0002-9639-7940': 113, '0000-0002-9872-9865': 27, '0000-0001-9689-4085': 21, '0000-0002-0681-4721': 10, '0000-0003-0328-1840': 7, '0000-0003-0937-8045': 7, '0000-0003-3746-6894': 4, '0000-0002-6618-2412': 2, '0000-0002-6935-3459': 1})\n",
      "Total author before apply threshoid:  10\n",
      "Total author after apply threshoid:  2\n",
      "Total sample size after apply threshold:  536\n",
      "Total missing sample:  0\n",
      "(536, 101)\n",
      "Total missing sample:  36\n",
      "(536, 101)\n",
      "Papers with no citation:  [20, 48, 76, 98, 122, 196, 201, 204, 207, 220, 224, 234, 241, 243, 292, 310, 314, 339, 358, 359, 363, 382, 410, 413, 419, 434, 438, 443, 454, 462, 481, 505, 514, 525, 533, 534]\n",
      "Total paper with citation:  500\n",
      "For name:  z_ren\n",
      "(31, 2)\n",
      "z_ren  pass\n",
      "For name:  m_kumar\n",
      "(104, 2)\n",
      "m_kumar  pass\n",
      "For name:  j_wong\n",
      "(183, 2)\n",
      "j_wong  pass\n",
      "For name:  s_turner\n",
      "(101, 2)\n",
      "s_turner  pass\n",
      "For name:  y_yuan\n",
      "(67, 2)\n",
      "y_yuan  pass\n",
      "For name:  l_liu\n",
      "(267, 2)\n",
      "l_liu  pass\n",
      "For name:  a_fonseca\n",
      "(91, 2)\n",
      "a_fonseca  pass\n",
      "For name:  r_francis\n",
      "(13, 2)\n",
      "r_francis  pass\n",
      "For name:  l_castro\n",
      "(74, 2)\n",
      "l_castro  pass\n",
      "For name:  k_zhou\n",
      "(14, 2)\n",
      "k_zhou  pass\n",
      "For name:  m_macdonald\n",
      "(52, 2)\n",
      "m_macdonald  pass\n",
      "For name:  h_guan\n",
      "(44, 2)\n",
      "h_guan  pass\n",
      "For name:  t_miller\n",
      "(165, 2)\n",
      "t_miller  pass\n",
      "For name:  m_kang\n",
      "(131, 2)\n",
      "m_kang  pass\n",
      "For name:  z_shi\n",
      "(180, 2)\n",
      "z_shi  pass\n",
      "For name:  t_johnson\n",
      "(293, 2)\n",
      "t_johnson  pass\n",
      "For name:  m_ferretti\n",
      "(43, 2)\n",
      "m_ferretti  pass\n",
      "For name:  b_peng\n",
      "(25, 2)\n",
      "b_peng  pass\n",
      "For name:  m_fernandes\n",
      "(118, 2)\n",
      "m_fernandes  pass\n",
      "For name:  l_cui\n",
      "(25, 2)\n",
      "l_cui  pass\n",
      "For name:  s_monteiro\n",
      "(50, 2)\n",
      "s_monteiro  pass\n",
      "For name:  m_hsieh\n",
      "(35, 2)\n",
      "m_hsieh  pass\n",
      "For name:  c_nelson\n",
      "(26, 2)\n",
      "c_nelson  pass\n",
      "For name:  j_barnett\n",
      "(23, 2)\n",
      "j_barnett  pass\n",
      "For name:  j_tian\n",
      "(22, 2)\n",
      "j_tian  pass\n",
      "For name:  f_costa\n",
      "(52, 2)\n",
      "f_costa  pass\n",
      "For name:  a_mccarthy\n",
      "(88, 2)\n",
      "a_mccarthy  pass\n",
      "For name:  y_cheng\n",
      "(177, 2)\n",
      "y_cheng  pass\n",
      "For name:  i_hwang\n",
      "(84, 2)\n",
      "i_hwang  pass\n",
      "For name:  y_liu\n",
      "(965, 2)\n",
      "y_liu  pass\n",
      "For name:  m_engel\n",
      "(100, 2)\n",
      "m_engel  pass\n",
      "For name:  w_shi\n",
      "(148, 2)\n",
      "w_shi  pass\n",
      "For name:  d_matthews\n",
      "(57, 2)\n",
      "d_matthews  pass\n",
      "For name:  j_christensen\n",
      "(203, 2)\n",
      "j_christensen  pass\n",
      "For name:  j_sampaio\n",
      "(117, 2)\n",
      "j_sampaio  pass\n",
      "For name:  j_dias\n",
      "(31, 2)\n",
      "j_dias  pass\n",
      "For name:  p_nunes\n",
      "(36, 2)\n",
      "p_nunes  pass\n",
      "For name:  c_bauer\n",
      "(7, 2)\n",
      "c_bauer  pass\n",
      "For name:  r_patel\n",
      "(182, 2)\n",
      "r_patel  pass\n",
      "For name:  a_das\n",
      "(74, 2)\n",
      "a_das  pass\n",
      "For name:  c_becker\n",
      "(110, 2)\n",
      "c_becker  pass\n",
      "For name:  k_zhu\n",
      "(6, 2)\n",
      "k_zhu  pass\n",
      "For name:  a_machado\n",
      "(150, 2)\n",
      "a_machado  pass\n",
      "For name:  j_alexander\n",
      "(31, 2)\n",
      "j_alexander  pass\n",
      "For name:  j_schneider\n",
      "(40, 2)\n",
      "j_schneider  pass\n",
      "For name:  g_russo\n",
      "(58, 2)\n",
      "g_russo  pass\n",
      "For name:  j_carvalho\n",
      "(136, 2)\n",
      "j_carvalho  pass\n",
      "For name:  y_nishikawa\n",
      "(21, 2)\n",
      "y_nishikawa  pass\n",
      "For name:  j_ward\n",
      "(22, 2)\n",
      "j_ward  pass\n",
      "For name:  m_singh\n",
      "(133, 2)\n",
      "m_singh  pass\n",
      "For name:  a_bhattacharyya\n",
      "(14, 2)\n",
      "a_bhattacharyya  pass\n",
      "For name:  e_morris\n",
      "(40, 2)\n",
      "e_morris  pass\n",
      "For name:  m_lewis\n",
      "(177, 2)\n",
      "m_lewis  pass\n",
      "For name:  v_fernandes\n",
      "(55, 2)\n",
      "v_fernandes  pass\n",
      "For name:  m_pinheiro\n",
      "(54, 2)\n",
      "m_pinheiro  pass\n",
      "For name:  j_petersen\n",
      "(41, 2)\n",
      "j_petersen  pass\n",
      "For name:  k_shimizu\n",
      "(103, 2)\n",
      "k_shimizu  pass\n",
      "For name:  p_shaw\n",
      "(57, 2)\n",
      "p_shaw  pass\n",
      "For name:  g_coppola\n",
      "(142, 2)\n",
      "g_coppola  pass\n",
      "For name:  a_sinclair\n",
      "(109, 2)\n",
      "a_sinclair  pass\n",
      "For name:  y_pan\n",
      "(46, 2)\n",
      "y_pan  pass\n",
      "For name:  m_ramos\n",
      "(251, 2)\n",
      "m_ramos  pass\n",
      "For name:  j_tsai\n",
      "(153, 2)\n",
      "j_tsai  pass\n",
      "For name:  f_dai\n",
      "(34, 2)\n",
      "f_dai  pass\n",
      "For name:  t_martin\n",
      "(83, 2)\n",
      "t_martin  pass\n",
      "For name:  t_o'brien\n",
      "(262, 2)\n",
      "t_o'brien  pass\n",
      "For name:  s_may\n",
      "(115, 2)\n",
      "s_may  pass\n",
      "For name:  z_cai\n",
      "(244, 2)\n",
      "z_cai  pass\n",
      "For name:  a_pereira\n",
      "(205, 2)\n",
      "a_pereira  pass\n",
      "For name:  d_patel\n",
      "(33, 2)\n",
      "d_patel  pass\n",
      "For name:  a_james\n",
      "(154, 2)\n",
      "a_james  pass\n",
      "For name:  c_cao\n",
      "(74, 2)\n",
      "c_cao  pass\n",
      "For name:  c_brown\n",
      "(384, 2)\n",
      "c_brown  pass\n",
      "For name:  y_liang\n",
      "(30, 2)\n",
      "y_liang  pass\n",
      "For name:  y_fan\n",
      "(50, 2)\n",
      "y_fan  pass\n",
      "For name:  j_simon\n",
      "(93, 2)\n",
      "j_simon  pass\n",
      "For name:  m_jeong\n",
      "(41, 2)\n",
      "m_jeong  pass\n",
      "For name:  j_barrett\n",
      "(130, 2)\n",
      "j_barrett  pass\n",
      "For name:  d_elliott\n",
      "(216, 2)\n",
      "d_elliott  pass\n",
      "For name:  p_antunes\n",
      "(41, 2)\n",
      "p_antunes  pass\n",
      "For name:  x_yuan\n",
      "(71, 2)\n",
      "x_yuan  pass\n",
      "For name:  t_kim\n",
      "(568, 2)\n",
      "t_kim  pass\n",
      "For name:  a_cruz\n",
      "(80, 2)\n",
      "a_cruz  pass\n",
      "For name:  a_mora\n",
      "(84, 2)\n",
      "a_mora  pass\n",
      "For name:  j_walker\n",
      "(253, 2)\n",
      "j_walker  pass\n",
      "For name:  j_alves\n",
      "(53, 2)\n",
      "j_alves  pass\n",
      "For name:  j_seo\n",
      "(146, 2)\n",
      "j_seo  pass\n",
      "For name:  y_tang\n",
      "(66, 2)\n",
      "y_tang  pass\n",
      "For name:  a_norman\n",
      "(28, 2)\n",
      "a_norman  pass\n",
      "For name:  s_tanaka\n",
      "(80, 2)\n",
      "s_tanaka  pass\n",
      "For name:  c_wen\n",
      "(36, 2)\n",
      "c_wen  pass\n",
      "For name:  c_myers\n",
      "(100, 2)\n",
      "c_myers  pass\n",
      "For name:  v_santos\n",
      "(30, 2)\n",
      "v_santos  pass\n",
      "For name:  j_brown\n",
      "(290, 2)\n",
      "j_brown  pass\n",
      "For name:  b_pandey\n",
      "(42, 2)\n",
      "b_pandey  pass\n",
      "For name:  d_morgan\n",
      "(86, 2)\n",
      "d_morgan  pass\n",
      "For name:  r_smith\n",
      "(789, 2)\n",
      "r_smith  pass\n",
      "For name:  a_guerrero\n",
      "(57, 2)\n",
      "a_guerrero  pass\n",
      "For name:  a_grant\n",
      "(45, 2)\n",
      "a_grant  pass\n",
      "For name:  v_kumar\n",
      "(98, 2)\n",
      "v_kumar  pass\n",
      "For name:  p_shah\n",
      "(84, 2)\n",
      "p_shah  pass\n",
      "For name:  t_yu\n",
      "(134, 2)\n",
      "t_yu  pass\n",
      "For name:  r_singh\n",
      "(197, 2)\n",
      "r_singh  pass\n",
      "For name:  c_baker\n",
      "(112, 2)\n",
      "c_baker  pass\n",
      "For name:  a_cattaneo\n",
      "(196, 2)\n",
      "a_cattaneo  pass\n",
      "For name:  a_ferrari\n",
      "(114, 2)\n",
      "a_ferrari  pass\n",
      "For name:  a_murphy\n",
      "(178, 2)\n",
      "a_murphy  pass\n",
      "For name:  f_hong\n",
      "(41, 2)\n",
      "f_hong  pass\n",
      "For name:  m_ferrari\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 2)\n",
      "m_ferrari  pass\n",
      "For name:  j_paredes\n",
      "(68, 2)\n",
      "j_paredes  pass\n",
      "For name:  z_zhao\n",
      "(186, 2)\n",
      "z_zhao  pass\n",
      "For name:  j_cao\n",
      "(39, 2)\n",
      "j_cao  pass\n",
      "For name:  d_kuo\n",
      "(34, 2)\n",
      "d_kuo  pass\n",
      "For name:  a_andersen\n",
      "(18, 2)\n",
      "a_andersen  pass\n",
      "For name:  m_longo\n",
      "(44, 2)\n",
      "m_longo  pass\n",
      "For name:  h_chiang\n",
      "(44, 2)\n",
      "h_chiang  pass\n",
      "For name:  m_o'brien\n",
      "(34, 2)\n",
      "m_o'brien  pass\n",
      "For name:  s_ray\n",
      "(123, 2)\n",
      "s_ray  pass\n",
      "For name:  a_cheng\n",
      "(636, 2)\n",
      "Total sample size before apply threshold:  636\n",
      "Counter({'0000-0002-9152-6512': 265, '0000-0003-3152-116X': 180, '0000-0003-2345-6951': 71, '0000-0002-1182-7375': 38, '0000-0001-7897-4751': 29, '0000-0003-3862-2967': 25, '0000-0003-2729-606X': 22, '0000-0001-5137-000X': 2, '0000-0002-0977-0381': 2, '0000-0001-5196-3307': 1, '0000-0002-8166-0806': 1})\n",
      "Total author before apply threshoid:  11\n",
      "Total author after apply threshoid:  2\n",
      "Total sample size after apply threshold:  445\n",
      "Total missing sample:  0\n",
      "(445, 101)\n",
      "Total missing sample:  20\n",
      "(445, 101)\n",
      "Papers with no citation:  [6, 58, 90, 103, 114, 125, 141, 182, 186, 265, 303, 310, 343, 357, 369, 390, 408, 409, 419, 441]\n",
      "Total paper with citation:  425\n",
      "For name:  j_savage\n",
      "(17, 2)\n",
      "j_savage  pass\n",
      "For name:  p_matthews\n",
      "(329, 2)\n",
      "p_matthews  pass\n",
      "For name:  i_carvalho\n",
      "(39, 2)\n",
      "i_carvalho  pass\n",
      "For name:  j_parsons\n",
      "(255, 2)\n",
      "j_parsons  pass\n",
      "For name:  s_oliveira\n",
      "(143, 2)\n",
      "s_oliveira  pass\n",
      "For name:  h_kang\n",
      "(47, 2)\n",
      "h_kang  pass\n",
      "For name:  s_vogt\n",
      "(93, 2)\n",
      "s_vogt  pass\n",
      "For name:  d_garcia\n",
      "(60, 2)\n",
      "d_garcia  pass\n",
      "For name:  w_xie\n",
      "(115, 2)\n",
      "w_xie  pass\n",
      "For name:  m_cruz\n",
      "(141, 2)\n",
      "m_cruz  pass\n",
      "For name:  w_xu\n",
      "(126, 2)\n",
      "w_xu  pass\n",
      "For name:  k_roy\n",
      "(131, 2)\n",
      "k_roy  pass\n",
      "For name:  b_white\n",
      "(47, 2)\n",
      "b_white  pass\n",
      "For name:  p_graham\n",
      "(89, 2)\n",
      "p_graham  pass\n",
      "For name:  d_rubin\n",
      "(43, 2)\n",
      "d_rubin  pass\n",
      "For name:  b_ryan\n",
      "(31, 2)\n",
      "b_ryan  pass\n",
      "For name:  j_kim\n",
      "(2116, 2)\n",
      "Total sample size before apply threshold:  2116\n",
      "Counter({'0000-0003-1835-9436': 200, '0000-0003-3477-1172': 146, '0000-0003-1232-5307': 124, '0000-0001-6537-0350': 78, '0000-0003-0934-3344': 73, '0000-0001-7964-106X': 56, '0000-0003-2337-6935': 52, '0000-0003-2068-7287': 51, '0000-0002-3573-638X': 46, '0000-0003-4085-293X': 41, '0000-0002-6349-6950': 41, '0000-0002-6931-8581': 38, '0000-0002-4171-3803': 38, '0000-0003-0373-5080': 36, '0000-0002-1299-4300': 36, '0000-0002-8383-8524': 33, '0000-0002-0087-1151': 32, '0000-0002-3500-7494': 32, '0000-0002-4687-6732': 31, '0000-0001-5979-5774': 30, '0000-0001-9660-6303': 29, '0000-0002-1903-8354': 28, '0000-0002-5390-8763': 27, '0000-0003-0767-1918': 26, '0000-0002-4747-9763': 25, '0000-0003-0103-7457': 24, '0000-0003-4035-0438': 23, '0000-0003-2841-147X': 23, '0000-0003-0693-1415': 23, '0000-0002-3566-3379': 19, '0000-0003-4978-1867': 18, '0000-0002-9570-4216': 18, '0000-0001-5080-7097': 17, '0000-0002-1672-5730': 17, '0000-0002-9159-0733': 16, '0000-0001-8208-8568': 16, '0000-0002-5329-6605': 16, '0000-0003-0578-0635': 16, '0000-0001-5204-3369': 16, '0000-0002-3729-8774': 15, '0000-0002-6152-2924': 15, '0000-0001-6417-864X': 15, '0000-0001-6426-9074': 15, '0000-0002-0195-1460': 14, '0000-0001-5951-8013': 14, '0000-0002-8218-0062': 13, '0000-0003-1519-3274': 12, '0000-0001-9881-2784': 12, '0000-0003-0530-3425': 12, '0000-0002-1376-9498': 12, '0000-0001-5096-4068': 12, '0000-0003-4217-3228': 11, '0000-0003-4438-1872': 11, '0000-0001-9840-4780': 11, '0000-0001-7649-4244': 11, '0000-0001-7842-2172': 10, '0000-0001-9595-2765': 10, '0000-0003-4157-9365': 10, '0000-0003-4802-010X': 9, '0000-0001-6188-7571': 9, '0000-0002-0484-9189': 8, '0000-0003-0448-1684': 8, '0000-0002-8580-8134': 8, '0000-0002-0359-2887': 8, '0000-0002-7040-7397': 8, '0000-0001-6603-6768': 8, '0000-0002-7419-021X': 7, '0000-0002-4490-3610': 7, '0000-0001-7819-2784': 7, '0000-0002-3849-649X': 6, '0000-0001-8984-2914': 6, '0000-0002-6575-452X': 6, '0000-0003-0462-6521': 5, '0000-0002-2713-1006': 5, '0000-0002-1810-5383': 5, '0000-0002-0066-534X': 4, '0000-0002-1076-1095': 4, '0000-0003-0340-4169': 4, '0000-0002-8321-026X': 4, '0000-0001-7340-2770': 4, '0000-0001-5228-4939': 4, '0000-0001-6210-4540': 4, '0000-0003-1222-0054': 3, '0000-0002-7425-1828': 3, '0000-0003-1522-9038': 3, '0000-0001-7409-6306': 3, '0000-0002-5810-1512': 3, '0000-0002-3502-7604': 3, '0000-0001-8087-7977': 3, '0000-0001-9302-0040': 3, '0000-0002-3010-1641': 3, '0000-0001-6201-9602': 3, '0000-0003-3172-3212': 3, '0000-0002-3512-5837': 2, '0000-0003-3889-2289': 2, '0000-0002-2124-0818': 2, '0000-0002-5678-2019': 2, '0000-0001-7353-9259': 2, '0000-0001-5235-2612': 2, '0000-0003-4074-877X': 2, '0000-0002-3984-0686': 2, '0000-0002-2679-8802': 2, '0000-0002-9423-438X': 2, '0000-0002-8908-0902': 2, '0000-0001-6746-7447': 2, '0000-0001-5794-975X': 2, '0000-0001-5402-7725': 1, '0000-0002-1273-6096': 1, '0000-0002-3531-489X': 1, '0000-0002-5886-8545': 1, '0000-0003-1834-4867': 1, '0000-0001-8641-7904': 1, '0000-0002-7918-1072': 1, '0000-0001-8371-2852': 1, '0000-0001-7176-409X': 1, '0000-0002-5409-2743': 1, '0000-0001-8616-1654': 1, '0000-0001-6886-2449': 1, '0000-0002-5201-9841': 1, '0000-0002-4966-1980': 1, '0000-0002-0947-876X': 1, '0000-0001-5104-4634': 1, '0000-0002-8663-798X': 1, '0000-0001-7565-068X': 1, '0000-0003-3530-9342': 1, '0000-0003-4907-4716': 1, '0000-0002-7689-6822': 1, '0000-0001-8986-8436': 1, '0000-0002-6944-473X': 1, '0000-0002-8416-3872': 1, '0000-0001-5086-0277': 1, '0000-0002-1384-6799': 1, '0000-0003-0812-6663': 1, '0000-0002-2156-9875': 1, '0000-0002-1094-3761': 1, '0000-0001-7282-0559': 1, '0000-0003-4677-0513': 1, '0000-0002-1418-3309': 1, '0000-0002-3365-8007': 1, '0000-0002-6143-8810': 1, '0000-0003-2479-0548': 1, '0000-0002-2556-7404': 1, '0000-0001-5494-4582': 1, '0000-0002-1764-1045': 1, '0000-0002-0872-4906': 1, '0000-0002-1368-6684': 1, '0000-0002-8237-3956': 1, '0000-0003-4856-6305': 1, '0000-0002-3423-5401': 1, '0000-0002-6204-5170': 1, '0000-0003-3155-0569': 1, '0000-0002-0341-7085': 1, '0000-0002-2938-3995': 1, '0000-0001-6600-9647': 1, '0000-0003-4184-363X': 1, '0000-0002-9011-4209': 1, '0000-0003-0461-6438': 1, '0000-0002-5065-5916': 1, '0000-0001-9078-6892': 1, '0000-0003-2304-6549': 1, '0000-0003-4491-0308': 1, '0000-0001-5182-0242': 1, '0000-0002-0708-9242': 1, '0000-0002-1690-9396': 1, '0000-0002-0824-8532': 1, '0000-0001-9661-5015': 1, '0000-0001-9061-3350': 1, '0000-0002-6214-3889': 1, '0000-0002-4478-6127': 1})\n",
      "Total author before apply threshoid:  169\n",
      "Total author after apply threshoid:  3\n",
      "Total sample size after apply threshold:  470\n",
      "Total missing sample:  0\n",
      "(470, 101)\n",
      "Total missing sample:  22\n",
      "(470, 101)\n",
      "Papers with no citation:  [38, 39, 53, 60, 79, 103, 111, 117, 121, 232, 246, 286, 321, 377, 393, 403, 411, 413, 421, 434, 451, 452]\n",
      "Total paper with citation:  448\n",
      "j_kim is multi-class case, ignored\n",
      "For name:  a_duarte\n",
      "(373, 2)\n",
      "a_duarte  pass\n",
      "For name:  a_correia\n",
      "(136, 2)\n",
      "a_correia  pass\n",
      "For name:  a_reynolds\n",
      "(40, 2)\n",
      "a_reynolds  pass\n",
      "For name:  g_qin\n",
      "(15, 2)\n",
      "g_qin  pass\n",
      "For name:  m_tang\n",
      "(86, 2)\n",
      "m_tang  pass\n",
      "For name:  a_baranov\n",
      "(42, 2)\n",
      "a_baranov  pass\n",
      "For name:  r_gray\n",
      "(162, 2)\n",
      "r_gray  pass\n",
      "For name:  r_nunes\n",
      "(46, 2)\n",
      "r_nunes  pass\n",
      "For name:  s_huang\n",
      "(441, 2)\n",
      "s_huang  pass\n",
      "For name:  c_reid\n",
      "(60, 2)\n",
      "c_reid  pass\n",
      "For name:  h_lu\n",
      "(108, 2)\n",
      "h_lu  pass\n",
      "For name:  j_cordeiro\n",
      "(30, 2)\n",
      "j_cordeiro  pass\n",
      "For name:  c_yu\n",
      "(335, 2)\n",
      "c_yu  pass\n",
      "For name:  d_simpson\n",
      "(27, 2)\n",
      "d_simpson  pass\n",
      "For name:  c_pereira\n",
      "(258, 2)\n",
      "c_pereira  pass\n",
      "For name:  h_wang\n",
      "(848, 2)\n",
      "h_wang  pass\n",
      "For name:  a_tan\n",
      "(200, 2)\n",
      "a_tan  pass\n",
      "For name:  m_aguilar\n",
      "(108, 2)\n",
      "m_aguilar  pass\n",
      "For name:  a_bianchi\n",
      "(73, 2)\n",
      "a_bianchi  pass\n",
      "For name:  p_rossi\n",
      "(200, 2)\n",
      "p_rossi  pass\n",
      "For name:  y_yang\n",
      "(665, 2)\n",
      "y_yang  pass\n",
      "For name:  s_hsieh\n",
      "(166, 2)\n",
      "s_hsieh  pass\n",
      "For name:  c_baptista\n",
      "(19, 2)\n",
      "c_baptista  pass\n",
      "For name:  d_kavanagh\n",
      "(178, 2)\n",
      "d_kavanagh  pass\n",
      "For name:  l_wang\n",
      "(828, 2)\n",
      "l_wang  pass\n",
      "For name:  m_pinho\n",
      "(97, 2)\n",
      "m_pinho  pass\n",
      "For name:  m_bergman\n",
      "(36, 2)\n",
      "m_bergman  pass\n",
      "For name:  j_castro\n",
      "(39, 2)\n",
      "j_castro  pass\n",
      "For name:  n_hall\n",
      "(115, 2)\n",
      "n_hall  pass\n",
      "For name:  d_schneider\n",
      "(93, 2)\n",
      "d_schneider  pass\n",
      "For name:  n_kumar\n",
      "(156, 2)\n",
      "n_kumar  pass\n",
      "For name:  i_martins\n",
      "(54, 2)\n",
      "i_martins  pass\n",
      "For name:  j_qiu\n",
      "(58, 2)\n",
      "j_qiu  pass\n",
      "For name:  m_antunes\n",
      "(27, 2)\n",
      "m_antunes  pass\n",
      "For name:  m_andersen\n",
      "(399, 2)\n",
      "m_andersen  pass\n",
      "For name:  l_xiao\n",
      "(302, 2)\n",
      "l_xiao  pass\n",
      "For name:  m_hartmann\n",
      "(88, 2)\n",
      "m_hartmann  pass\n",
      "For name:  k_nielsen\n",
      "(194, 2)\n",
      "k_nielsen  pass\n",
      "For name:  m_sousa\n",
      "(211, 2)\n",
      "m_sousa  pass\n",
      "For name:  a_coelho\n",
      "(128, 2)\n",
      "a_coelho  pass\n",
      "For name:  r_sanz\n",
      "(42, 2)\n",
      "r_sanz  pass\n",
      "For name:  m_ferrara\n",
      "(103, 2)\n",
      "m_ferrara  pass\n",
      "For name:  c_hui\n",
      "(44, 2)\n",
      "c_hui  pass\n",
      "For name:  l_bruno\n",
      "(42, 2)\n",
      "l_bruno  pass\n",
      "For name:  s_nielsen\n",
      "(290, 2)\n",
      "s_nielsen  pass\n",
      "For name:  b_russell\n",
      "(84, 2)\n",
      "b_russell  pass\n",
      "For name:  y_wu\n",
      "(612, 2)\n",
      "y_wu  pass\n",
      "For name:  j_soto\n",
      "(64, 2)\n",
      "j_soto  pass\n",
      "For name:  r_mckay\n",
      "(53, 2)\n",
      "r_mckay  pass\n",
      "For name:  d_sharma\n",
      "(60, 2)\n",
      "d_sharma  pass\n",
      "For name:  a_wilson\n",
      "(252, 2)\n",
      "a_wilson  pass\n",
      "For name:  f_marini\n",
      "(65, 2)\n",
      "f_marini  pass\n",
      "For name:  h_tsai\n",
      "(93, 2)\n",
      "h_tsai  pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For name:  s_o'brien\n",
      "(32, 2)\n",
      "s_o'brien  pass\n",
      "For name:  c_webb\n",
      "(35, 2)\n",
      "c_webb  pass\n",
      "For name:  c_adams\n",
      "(69, 2)\n",
      "c_adams  pass\n",
      "For name:  c_peng\n",
      "(103, 2)\n",
      "c_peng  pass\n",
      "For name:  k_kobayashi\n",
      "(35, 2)\n",
      "k_kobayashi  pass\n",
      "For name:  s_larsen\n",
      "(68, 2)\n",
      "s_larsen  pass\n",
      "Total number of selected group: 18\n",
      "Total number of selected binary group: 0\n",
      "Total number of selected muti-class group: 18\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "v1: pv_dbow  v2: n2v threshold 100  Done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "# loop through all files in directory add name to name list\n",
    "fileDir = \"../Data/\"+Dataset+\"/canopies_labeled/\"\n",
    "listfiles = os.listdir(fileDir)\n",
    "\n",
    "init_labeled_size = 10\n",
    "\n",
    "co_lr_diff_embedding_final_result = collections.defaultdict(list)\n",
    "\n",
    "#---------------- load different embedding combination ---------------#\n",
    "for v1_emb, v2_emb in zip(pp_text, pp_citation):\n",
    "    # read embeddings\n",
    "    print(\"Load text embedding: \", v1_emb)\n",
    "    viewone_text_emb = com_func.read_text_embedding(emb_type=v1_emb, training_size = \"140k\")\n",
    "    print(\"Load citation embedding: \", v2_emb)\n",
    "    viewtwo_citation_embedding = com_func.read_citation_embedding_sorted(emb_type = v2_emb, labeled_only = True)\n",
    "    # print(viewone_text_emb[0])\n",
    "    # print(viewtwo_citation_embedding[0])\n",
    "    \n",
    "    threshold_change_all_method_f1s = collections.defaultdict(list)\n",
    "    # -------------- different threshold (step by 10) -----------------------#\n",
    "    for step_threshold in range(threshold_lower, threshold_upper, 10):\n",
    "        plot_save_path = \"../../plot/3_co_train_detail_plots/threshold=\"+str(step_threshold)+\"/temp_cs=0.5/(Fixed class ratio)10RPFA_V1=\"+v1_emb+\"_V2=\"+v2_emb+\"/\"\n",
    "        #plot_save_path = None\n",
    "        threshold_change_all_method_f1s[\"threshold\"].append(step_threshold)\n",
    "        # collect statistic to output\n",
    "        statistic_detail = collections.defaultdict(list)\n",
    "        total_selected_group = 0\n",
    "        selected_binary_case_group = 0\n",
    "\n",
    "        # ------- different name group in all name group --------------------#\n",
    "        for file in listfiles:\n",
    "            # group name\n",
    "            temp = file.split(\"_\")\n",
    "            name = str(temp[1]+\"_\"+temp[-1])\n",
    "            print(\"For name: \",name)\n",
    "            # read label (pid : author ORCID) from file\n",
    "            data = com_func.read_pid_aid(fileDir+file)\n",
    "            labeled_mask = data[\"authorID\"] != \"-1\"\n",
    "            labeled_data = data[labeled_mask]\n",
    "            print(labeled_data.shape)\n",
    "            # ---------------- collect all labeled sample -------------------- #\n",
    "            # ---------------- if use all samples as negative --------------- #\n",
    "            all_labeled_samples = labeled_data[\"paperID\"].tolist()\n",
    "            authorCounter = com_func.select_productive_groups(labeled_data=labeled_data, \n",
    "                                                              threshold=threshold_select_name_group)\n",
    "            ''' \n",
    "            Case 1: no author under this name have written more than threshold number of papers, dataset not used\n",
    "            Case 2: only one author under this name written more than threshold number of papers, dataset not used\n",
    "            Case 3: 2 or more author under this name written more than threshold number of papers, dataset used\n",
    "            '''\n",
    "            if(len(authorCounter)==0) or (len(authorCounter)==1):\n",
    "                print(name,\" pass\")\n",
    "            else:\n",
    "                total_selected_group+= 1\n",
    "                '''\n",
    "                Case 1: All papers under name group is used. We include authors written less than threshold of paper,\n",
    "                        and treat it as negative class. This will be OVR.(Not used)\n",
    "                Case 2: All papers under name group is used. We include authors written less than threshold of paper,\n",
    "                        and perform muti-class classification (Not used)\n",
    "                Case 3: Only Include author with more than threshold number of paper and perform muti-class classification (Not used)\n",
    "                Case 5: Only Include author with more than threshold number of paper and perform OVR(used)\n",
    "                Case 4: Only Include author with more than threshold number of paper and only select binary case(used)\n",
    "                '''\n",
    "                if apply_threshold_to_name_group_samples == True:\n",
    "                    # ---------- only use sample pass threshold ------- #\n",
    "                    #-------- only select authors in name group are very productive (more than threshold)---------#\n",
    "                    labeled_data, author_list, _= com_func.only_select_productive_authors(labeled_data, step_threshold)\n",
    "                    # ----------------- if use filtered samples as negative  --------- #\n",
    "                    filtered_all_labeled_samples = labeled_data[\"paperID\"].tolist()\n",
    "                else:\n",
    "                    # ----------- use all sample in name group --------- #\n",
    "                    author_list = com_func.productive_authors_list(labeled_data, step_threshold)\n",
    "                    print(name, \" name group sample size: \",labeled_data.shape)\n",
    "                # -------------- extract all samples for name group -------------- #\n",
    "                # for each name group\n",
    "                # read in labeled data\n",
    "                labeled_viewone_text = com_func.extract_sorted_embedding(viewone_text_emb, labeled_data[\"paperID\"])\n",
    "                print(labeled_viewone_text.shape)\n",
    "                labeled_viewtwo_citation = com_func.extract_sorted_embedding(viewtwo_citation_embedding, labeled_data[\"paperID\"])\n",
    "                print(labeled_viewtwo_citation.shape)\n",
    "                # ---------------- shuffle the data ----------------- #\n",
    "                labeled_data = labeled_data.sample(frac=1).reset_index(drop=True)\n",
    "                ''' Alignment and deal with missing data rows: \n",
    "                Case 1: fill missing data with 0, using labeled_viewtwo_citation.fillna(0, inplace=True)\n",
    "                Case 2: fill missing data with average of other data\n",
    "                Case 3: drop the sample contains missing data\n",
    "                '''\n",
    "                labeled_viewone_text = pd.merge(labeled_data, labeled_viewone_text, left_on=\"paperID\", right_on = [0], how = \"left\")\n",
    "                labeled_viewtwo_citation = pd.merge(labeled_data, labeled_viewtwo_citation, left_on=\"paperID\", right_on = [0], how = \"left\")\n",
    "                rows_with_nan = [index for index, row in labeled_viewtwo_citation.iterrows() if row.isnull().any()]\n",
    "                print(\"Papers with no citation: \", rows_with_nan)\n",
    "                print(\"Total paper with citation: \", len(labeled_viewone_text)-len(rows_with_nan))\n",
    "                labeled_viewtwo_citation.fillna(0, inplace=True)\n",
    "                #rows_with_nan = [index for index, row in labeled_viewtwo_citation.iterrows() if row.isnull().any()]\n",
    "                #print(rows_with_nan)\n",
    "                #labeled_viewone_text = labeled_viewone_text.drop(rows_with_nan).reset_index(drop=True)\n",
    "                #labeled_viewtwo_citation = labeled_viewtwo_citation.drop(rows_with_nan).reset_index(drop=True)\n",
    "                unique_labels = labeled_viewone_text.authorID.unique()\n",
    "                map_dict = {}\n",
    "                for idx, unique_label in enumerate(unique_labels):\n",
    "                    map_dict[unique_label] = name+\"_\"+str(idx+1)\n",
    "                true_label = labeled_viewone_text[\"authorID\"].replace(map_dict)\n",
    "                \n",
    "                '''\n",
    "                only work on binary case, ignored multi-class case\n",
    "                We need to check whether the name group only contain binary case or not\n",
    "                '''\n",
    "                if len(author_list)==2:\n",
    "                    #if name in [\"p_robinson\",\"r_reis\", \"a_silva\",\"w_lee\"]:\n",
    "                    #   print(name, \" Pass for error checking\")\n",
    "                    #    continue\n",
    "                    selected_binary_case_group +=1\n",
    "                    print(name + \" is binary case\")\n",
    "                    viewone_text_final = labeled_viewone_text.drop([\"paperID\", \"authorID\", 0], axis=1)\n",
    "                    viewtwo_citation_final = labeled_viewtwo_citation.drop([\"paperID\", \"authorID\", 0], axis=1)\n",
    "                    print(viewone_text_final.shape)\n",
    "                    print(viewtwo_citation_final.shape)\n",
    "                    ''' Apply different algorithm:\n",
    "                    Part 1: Basic supervised algorithm \n",
    "                    Part 2: Basic co-training algorithm \n",
    "                    Part 3: 2 clf co-training \n",
    "                    Part 4: Improved co-training algorithm (Self-proposed with all different improvement)\n",
    "                    '''\n",
    "                    # -------------------- part 1 ------------------------- #\n",
    "                    LR_clf = LogisticRegression(solver= \"liblinear\")\n",
    "                    SVM_clf = SVC(gamma=\"auto\", kernel='linear')\n",
    "                    # -------------------- part 2 ------------------------- #\n",
    "                    initial_cotrain_parameters = {\"k\":30}\n",
    "                    co_LR_clf = Co_training_clf(clf1=LogisticRegression(solver= \"liblinear\"),**initial_cotrain_parameters)\n",
    "                    ''' For co-training with SVM\n",
    "                    Case 1: Using Scikit-learn where we set probability=True\n",
    "                    Case 1 details: Scikit-learn uses LibSVM internally, and this in turn uses Platt scaling\n",
    "                    Platt scaling requires first training the SVM as usual, then optimizing parameter \n",
    "                    vectors A and B such that: P(y|X) = 1 / (1 + exp(A * f(X) + B))\n",
    "                    where f(X) is the signed distance of a sample from the hyperplane\n",
    "                    (scikit-learn's decision_function method).\n",
    "                    Case 2: Using decision_function to get signed distance of sample from the hyperplane, calculate proba\n",
    "                    Case 2 details: use sigmoid (binary)/ softmax (muti-class) for calculate probability based on \n",
    "                    signed distance of sample from the hyperplane. Then follow same step as case 1. The result is relatively\n",
    "                    bad\n",
    "                    '''\n",
    "                    # ----------- Case 1 Using Scikit-learn where we set probability=True ------ #\n",
    "                    co_SVM_clf = Co_training_clf(clf1=SVC(gamma=\"auto\", kernel='linear',probability=True),**initial_cotrain_parameters)\n",
    "                    # ------- Case 2 Using decision_function get distance, calculate proba------ #\n",
    "                    #co_svm_clf = Co_training_clf(clf1=SVC(gamma=\"auto\", kernel='linear'),**initial_cotrain_parameters)\n",
    "                    #co_train_clfs = [(co_svm_clf,\"co_train_SVM\")]\n",
    "                    # -------------------- part 3 ------------------------- #\n",
    "                    # two different clf with basic co-training\n",
    "                    co_LR_SVM_clf = Co_training_clf(clf1 = LogisticRegression(solver= \"liblinear\"),\n",
    "                                                    clf2 = SVC(gamma=\"auto\", kernel='linear',probability=True),\n",
    "                                                    **initial_cotrain_parameters)\n",
    "                    # -------------------- part 4 ------------------------- #\n",
    "                    improved_cotrain_parameters = {\"view_num\":2,\"k\":30}\n",
    "                    improved_co_LR = Improved_co_training_clf(clf = [LogisticRegression(solver= \"liblinear\")], \n",
    "                                                              **improved_cotrain_parameters)\n",
    "                    # -------------------- train together ----------------- #\n",
    "                    baseline_clfs = [(LR_clf,\"LR\"),(SVM_clf,\"SVM\")]\n",
    "                    final_co_train_clfs = [(co_LR_clf,\"co_LR\"), (co_SVM_clf,\"co_SVM\"),\n",
    "                                           (co_LR_SVM_clf, \"co_LR_SVM\"), (improved_co_LR, \"Improved_co_LR\")]\n",
    "                    #baseline_clfs = [(LR_clf,\"LR\")]\n",
    "                    #final_co_train_clfs = [(co_LR_clf,\"co_LR\"),(improved_co_LR, \"Improved_co_LR\")]\n",
    "                    final_f1_score, cv_per_fold_status= k_fold_cv_all_algorithm(dv1=viewone_text_final,\n",
    "                                                                                dv2=viewtwo_citation_final,\n",
    "                                                                                label=true_label,\n",
    "                                                                                init_labeled_size=init_labeled_size,\n",
    "                                                                                muti_view_clf=final_co_train_clfs,\n",
    "                                                                                combined_clf=baseline_clfs,\n",
    "                                                                                num_fold=5,\n",
    "                                                                                dataset_name=name,\n",
    "                                                                                plot_save_path=plot_save_path)\n",
    "                    \n",
    "                    statistic_detail['Name'].append(name)\n",
    "                    statistic_detail['Total_sample_size'].append(len(true_label))\n",
    "                    statistic_detail['Train_size'].append(cv_per_fold_status[0][\"train_size\"])\n",
    "                    statistic_detail['Test_size'].append(cv_per_fold_status[0][\"test_size\"])\n",
    "                    statistic_detail[\"All_fold_details\"].append(cv_per_fold_status)\n",
    "                    if len(final_co_train_clfs)!=0:\n",
    "                        statistic_detail['Unlabel_size'].append(cv_per_fold_status[0][\"unlabel_size\"])\n",
    "                        statistic_detail['Validation_size'].append(cv_per_fold_status[0][\"validation_size\"])\n",
    "                        for clf, clf_name in final_co_train_clfs:\n",
    "                            statistic_detail[clf_name+'_all_iterations'].append(int(cv_per_fold_status[0][clf_name+\"_total_iteration\"]))\n",
    "                            statistic_detail[clf_name+'_total_self_labeled'].append(int(cv_per_fold_status[0][clf_name+\"_total_self_labeled\"]))\n",
    "                            statistic_detail[clf_name+\"_select_iteration\"].append(int(cv_per_fold_status[0][clf_name+\"_select_iteration\"]))\n",
    "                            statistic_detail[clf_name+\"_k_iter_self_labeled\"].append(int(cv_per_fold_status[0][clf_name+\"_iter_k_self_labeled\"]))\n",
    "\n",
    "                    for clf_name, clf_f1 in final_f1_score:\n",
    "                        statistic_detail[clf_name+\"_f1\"].append(clf_f1)\n",
    "                        \n",
    "                else:\n",
    "                    print(name+ \" is multi-class case, ignored\")\n",
    "                    \n",
    "        # print(statistic_detail)\n",
    "        print(\"Total number of selected group:\",total_selected_group)\n",
    "        print(\"Total number of selected binary group:\",selected_binary_case_group)\n",
    "        print(\"Total number of selected muti-class group:\",(total_selected_group-selected_binary_case_group))\n",
    "        # write evaluation result to excel\n",
    "        output = pd.DataFrame(statistic_detail)\n",
    "        print(output)\n",
    "        #savePath = \"../../result/\"+Dataset+\"/3_co_train_sample=140k/temp_cs=0.5/\"\n",
    "        #filename = \"(init_labeled_size=\"+str(init_labeled_size)+\")(Average of 100 run)(Fixed class ratio) V1=\"+v1_emb+\"_V2=\"+v2_emb+\"_threshold=\"+str(step_threshold)+\".csv\"\n",
    "        #com_func.write_csv_df(savePath, filename, output)\n",
    "        print(\"v1:\",v1_emb,\" v2:\",v2_emb, \"threshold\",step_threshold,\" Done\")\n",
    "        \n",
    "        '''Save result with respect to threshold change'''\n",
    "        threshold_change_all_method_f1s['Name'].append(statistic_detail['Name'])\n",
    "        threshold_change_all_method_f1s['All_details'].append(statistic_detail[\"All_fold_details\"])\n",
    "        for col in output.columns: \n",
    "            if \"f1\" in col:\n",
    "                threshold_change_all_method_f1s[col].append(statistic_detail[col])\n",
    "\n",
    "    co_lr_diff_embedding_final_result[\"v1:\"+v1_emb+\" v2:\"+v2_emb].append(threshold_change_all_method_f1s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T03:16:13.545120Z",
     "start_time": "2020-08-13T03:14:50.342Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots: 1. All method per fold variance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T05:22:10.677723Z",
     "start_time": "2020-08-03T05:22:00.233Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_author_details = co_lr_diff_embedding_final_result['v1:pv_dbow v2:n2v'][0]['All_details'][0]\n",
    "\n",
    "cotrain_all_per_fold_result= {}\n",
    "\n",
    "for author in all_author_details:\n",
    "    per_fold_f1 = collections.defaultdict(list)\n",
    "    for per_fold_details in author:\n",
    "        #print(per_fold_details)\n",
    "        for key in per_fold_details:\n",
    "            if key in ['co_LR f1','co_SVM f1','co_LR_SVM f1','Improved_co_LR f1','LR-LB f1','SVM-LB f1','LR-UB f1','SVM-UB f1']:\n",
    "                per_fold_f1[key].append(per_fold_details[key])\n",
    "    cotrain_all_per_fold_result[author[0]['author']]=(per_fold_f1)\n",
    "#print(cotrain_all_per_fold_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T05:22:10.679891Z",
     "start_time": "2020-08-03T05:22:00.274Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name_list = list(cotrain_all_per_fold_result.keys())\n",
    "method_list = []\n",
    "f1_mean = collections.defaultdict(list)\n",
    "f1_min = collections.defaultdict(list)\n",
    "f1_max = collections.defaultdict(list)\n",
    "for author, author_result in cotrain_all_per_fold_result.items():\n",
    "    #print(author)\n",
    "    for method, method_result in author_result.items():\n",
    "        if method.replace(' f1','') not in method_list:\n",
    "            method_list.append(method.replace(' f1',''))\n",
    "        f1_mean[method.replace(' f1','')].append(np.mean(method_result))\n",
    "        f1_min[method.replace(' f1','')].append(np.min(method_result))\n",
    "        f1_max[method.replace(' f1','')].append(np.max(method_result))\n",
    "        #print(method_result, \" mean: \", np.mean(method_result), \" min: \",np.min(method_result),\" max: \", np.max(method_result))\n",
    "print(name_list)\n",
    "print(method_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T05:22:10.699137Z",
     "start_time": "2020-08-03T05:22:00.319Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = name_list\n",
    "random_color = np.random.rand(len(method_list),3)\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "for idx, method in enumerate(method_list):\n",
    "    y = f1_mean[method]\n",
    "    y_min = f1_min[method]\n",
    "    y_max = f1_max[method]\n",
    "    \n",
    "    plt.plot(x, y, 'k', color = random_color[idx], label=method)\n",
    "    plt.fill_between(x, y_min, y_max, alpha=0.5, edgecolor = random_color[idx], facecolor=random_color[idx])\n",
    "    plt.xticks(rotation=45, horizontalalignment='center')\n",
    "\n",
    "legend = plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=2)\n",
    "plt.xlabel('Datasets')\n",
    "plt.ylabel('F1 score')\n",
    "#plt.savefig(plot_save_path+\"/all_method_result_variance.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T05:22:10.709119Z",
     "start_time": "2020-08-03T05:22:00.362Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = name_list\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "\n",
    "for idx, method in enumerate(method_list):\n",
    "    y = f1_mean[method]\n",
    "    y_min_error = [a - b for a, b in zip(f1_mean[method], f1_min[method])]\n",
    "    y_max_error = [a - b for a, b in zip(f1_max[method], f1_mean[method])]\n",
    "    y_error = [y_min_error,y_max_error]\n",
    "\n",
    "    (_, caps, _) = ax.errorbar(x, y,yerr=y_error, fmt='-o',capsize=10, label=method)\n",
    "    for cap in caps:\n",
    "        cap.set_markeredgewidth(1)\n",
    "    plt.xticks(rotation=45, horizontalalignment='center')\n",
    "\n",
    "legend = plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=2)\n",
    "plt.xlabel('Datasets')\n",
    "plt.ylabel('F1 score')\n",
    "#plt.savefig(plot_save_path+\"/all_method_result_variance.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Plot: Each method per fold variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T05:22:10.719119Z",
     "start_time": "2020-08-03T05:22:00.557Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for idx, method in enumerate(method_list):\n",
    "    plt.figure(figsize=(15,10))\n",
    "    x = name_list\n",
    "    y = f1_mean[method]\n",
    "    y_min = f1_min[method]\n",
    "    y_max = f1_max[method]\n",
    "    \n",
    "    plt.plot(x, y, 'k', color = random_color[idx],marker='o', label=method)\n",
    "    plt.fill_between(x, y_min, y_max, alpha=0.5, edgecolor = random_color[idx], facecolor=random_color[idx])\n",
    "    plt.xticks(rotation=45, horizontalalignment='center')\n",
    "    legend = plt.legend()\n",
    "    plt.xlabel('Datasets')\n",
    "    plt.ylabel('F1 score')\n",
    "    #plt.savefig(plot_save_path+\"/\"+method+\"_result_variance.png\", dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T05:22:10.729121Z",
     "start_time": "2020-08-03T05:22:00.699Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for idx, method in enumerate(method_list):\n",
    "    fig, ax = plt.subplots(figsize=(15,10))\n",
    "    x = name_list\n",
    "    y = f1_mean[method]\n",
    "    y_min_error = [a - b for a, b in zip(f1_mean[method], f1_min[method])]\n",
    "    y_max_error = [a - b for a, b in zip(f1_max[method], f1_mean[method])]\n",
    "    y_error = [y_min_error,y_max_error]\n",
    "\n",
    "    (_, caps, _) = ax.errorbar(x, y,yerr=y_error, fmt='-o',capsize=10, label=method)\n",
    "    for cap in caps:\n",
    "        cap.set_markeredgewidth(1)\n",
    "    plt.xticks(rotation=45, horizontalalignment='center')\n",
    "    legend = plt.legend()\n",
    "    plt.xlabel('Datasets')\n",
    "    plt.ylabel('F1 score')\n",
    "    #plt.savefig(plot_save_path+\"/\"+method+\"_result_variance.png\", dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-08T01:28:55.195963Z",
     "start_time": "2020-07-08T01:28:03.583Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#         # --------------- plot overall result f1 variance --------------- #\n",
    "#         all_per_fold_f1_score_variance_plot = pd.DataFrame(all_per_fold_f1_score_variance)\n",
    "#         ax = sns.boxplot(x=\"author\", y=\"f1\", data=all_per_fold_f1_score_variance_plot)\n",
    "#         ax = sns.swarmplot(x=\"author\", y=\"f1\", data=all_per_fold_f1_score_variance_plot, color=\".25\")\n",
    "#         plt.savefig(plot_save_path+\"all_result_variance.png\", dpi=300)\n",
    "#         # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test code parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "curr_iter_proba = [[0.99,0.01],[0.99,0.01],[0.99,0.01],[0.99,0.01],[0.99,0.01],\n",
    "                   [0.01,0.99],[0.01,0.99],[0.01,0.99],[0.01,0.99],[0.01,0.99]]\n",
    "\n",
    "\n",
    "# if one class have lots of high confident samples while other don't, change class ratio\n",
    "class_ratio = []\n",
    "for class_idx in range(2):\n",
    "    high_confident = [proba for proba in curr_iter_proba if proba[class_idx]>0.95]\n",
    "    high_confident_count = max(len(high_confident), 1)\n",
    "    class_ratio.append(high_confident_count)\n",
    "print(class_ratio)\n",
    "for class_idx in range(2):\n",
    "    class_new_sample_size = class_ratio[class_idx]\n",
    "    if class_new_sample_size>5:\n",
    "        class_new_sample_size=5\n",
    "    print(class_new_sample_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
