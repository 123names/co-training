{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T13:21:09.523967Z",
     "start_time": "2019-01-31T13:21:09.475331Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "\n",
    "# warnings.filterwarnings('error')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import com_func\n",
    "\n",
    "#----- threshold for selecting set of name group -----------#\n",
    "threshold_select_name_group = 100\n",
    "#----- threshold for selecting min sample in name group ----#\n",
    "threshold_lower = 50\n",
    "threshold_upper = 60\n",
    "\n",
    "apply_threshold_to_sample = True\n",
    "\n",
    "pp_textual = [\"lsa\", \"pv_dm\", \"pv_dbow\"]\n",
    "pp_citation = \"n2v\"\n",
    "\n",
    "Dataset = \"pubmed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-30T21:37:59.416317Z",
     "start_time": "2019-01-30T21:37:59.387109Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_file(infile):\n",
    "    AllRecords_original = []\n",
    "    with open(infile, 'r', encoding = 'utf8') as f:\n",
    "        for line in f:\n",
    "            read_data = line.split(\"\\t\")\n",
    "            # get ride of bad formated lines\n",
    "            if(len(read_data)==13 or len(read_data)==12):\n",
    "                paper_detail = {\"paperID\": read_data[0], \"authorID\":read_data[1]}\n",
    "                AllRecords_original.append(paper_detail)\n",
    "            else:\n",
    "                print(len(read_data))\n",
    "        f.close()\n",
    "    return pd.DataFrame(AllRecords_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T13:21:14.981663Z",
     "start_time": "2019-01-31T13:21:14.915990Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_embedding(all_embedding, wanted_pid_list):\n",
    "    extracted_emb = []\n",
    "    wanted_pid_list = wanted_pid_list.values.tolist()\n",
    "    wanted_pid_list = [int(x) for x in wanted_pid_list]\n",
    "    wanted_pid_list = list(sorted(set(wanted_pid_list)))\n",
    "    total_missing_sample = 0\n",
    "    # only if embedding exist\n",
    "    if len(all_embedding)>0:\n",
    "        # loop through wanted pid list to keep input order\n",
    "        for embedding in all_embedding:\n",
    "            if(len(wanted_pid_list)==0):\n",
    "                break\n",
    "            while (wanted_pid_list[0]<=int(embedding[0])):\n",
    "                if wanted_pid_list[0]==int(embedding[0]):\n",
    "                    extracted_emb.append(embedding)\n",
    "                    wanted_pid_list.remove(int(embedding[0]))\n",
    "                elif (wanted_pid_list[0]<int(embedding[0])):\n",
    "                    total_missing_sample+=1\n",
    "                    # ------------------------ fill it up with 0's -------------------------- #\n",
    "                    fill_na = [wanted_pid_list[0]]\n",
    "                    temp = [0] * (len(all_embedding[0])-1)\n",
    "                    final_filled_zero_emb = fill_na+temp\n",
    "                    extracted_emb.append(final_filled_zero_emb)\n",
    "                    # ----- or do nothing and remove those missing samples from dataset ----- #\n",
    "                    # remove paper that not in all dataset\n",
    "                    wanted_pid_list.remove(wanted_pid_list[0])\n",
    "                if len(wanted_pid_list)==0:\n",
    "                    break\n",
    "    print(\"Total missing sample: \", total_missing_sample)\n",
    "    extracted_emb = pd.DataFrame(extracted_emb)\n",
    "    return extracted_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T13:21:15.756181Z",
     "start_time": "2019-01-31T13:21:15.720624Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# collect unlabeled vectors\n",
    "def extract_unlabeled_embedding(allembedding, unlabeled_pid):\n",
    "    unlabeled_pid = [int(x) for x in unlabeled_pid]\n",
    "    unlabeled_pid = list(sorted(set(unlabeled_pid)))\n",
    "    wanted_embedding = []\n",
    "    for embedding in allembedding:\n",
    "        if(len(unlabeled_pid)==0):\n",
    "            break\n",
    "        while (unlabeled_pid[0]<=int(embedding[0])):\n",
    "            if unlabeled_pid[0]==int(embedding[0]):\n",
    "                wanted_embedding.append(embedding)\n",
    "                unlabeled_pid.remove(int(embedding[0]))\n",
    "            elif (unlabeled_pid[0]<int(embedding[0])):\n",
    "                # remove paper that not in all dataset\n",
    "                unlabeled_pid.remove(unlabeled_pid[0])\n",
    "            if len(unlabeled_pid)==0:\n",
    "                break\n",
    "    unlabeled_data = pd.DataFrame(wanted_embedding)\n",
    "    unlabeled_data['label'] = \"-1\"\n",
    "    unlabeled_data = unlabeled_data.rename(columns={0: 'paperID'})\n",
    "    return unlabeled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T13:21:16.240689Z",
     "start_time": "2019-01-31T13:21:16.218167Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some of the record doesn't have citation links, therefore we will have to remove those papers from train and test set\n",
    "# synchronize data wrt pid\n",
    "def synchro_views(labeled_dv1, labeled_dv2, unlabeled_data1, unlabeled_data2):\n",
    "    noCitationPids_labeled = set(labeled_dv1[0])-set(labeled_dv2[0])\n",
    "    print(\"labeled no citation link: \", len(noCitationPids_labeled))\n",
    "    noCitationPids_unlabeled = set(unlabeled_data1['paperID'])-set(unlabeled_data2['paperID'])\n",
    "    print(\"Unlabeled no citation link size: \", len(noCitationPids_unlabeled))\n",
    "    # process unlabeled data\n",
    "    unlabeled_dv1 = unlabeled_data1[~unlabeled_data1['paperID'].isin(noCitationPids_unlabeled)].reset_index(drop=True)\n",
    "    unlabeled_dv2 = unlabeled_data2\n",
    "    # process labeled data\n",
    "    labeled_dv1_final = labeled_dv1[~labeled_dv1[0].isin(noCitationPids_labeled)].reset_index(drop=True)\n",
    "    labeled_dv2_final = labeled_dv2\n",
    "    # since our input data are sorted, all data are in order with pid\n",
    "    return labeled_dv1_final, labeled_dv2_final, unlabeled_dv1, unlabeled_dv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T13:21:16.679468Z",
     "start_time": "2019-01-31T13:21:16.673805Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove author(positive sample) from other(negative sample)\n",
    "import random\n",
    "def extractNegativeSample(positiveSample, allSample):\n",
    "    negativeSample = [x for x in allSample if x not in positiveSample]\n",
    "    return negativeSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T15:11:29.031329Z",
     "start_time": "2019-01-31T15:11:23.540449Z"
    },
    "code_folding": [
     9,
     33,
     47,
     80,
     127,
     221,
     228,
     257
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create co training classifier\n",
    "class Co_training_clf(object):\n",
    "    \n",
    "    import copy\n",
    "    \n",
    "    def __init__(self, clf1, clf2=None, p=1, n=1, k=30, u = 75):\n",
    "        \n",
    "        self.clf1 = clf1\n",
    "        # assume co_training on one classifier\n",
    "        if clf2 == None:\n",
    "            self.clf2 = self.copy.copy(clf1)\n",
    "        else:\n",
    "            self.clf2 = clf2\n",
    "        # take p example from most confidently positive labels to example\n",
    "        self.p = p\n",
    "        # take n example from most confidently negative label to example\n",
    "        self.n = n\n",
    "        # number of iteration\n",
    "        self.k = k\n",
    "        # size of pool of unlabeled samples\n",
    "        self.u = u\n",
    "        # index of positive labeled samples\n",
    "        self.new_labeled_pos = []\n",
    "        # index of negative labeled samples\n",
    "        self.new_labeled_neg = []\n",
    "        # when fit co-train, we collect f1 on test samples wrt each iteration\n",
    "        self.f1_on_test_dv1 = []\n",
    "        self.f1_on_test_dv2 = []\n",
    "\n",
    "    def init_L_U_U_prime(self, labels):\n",
    "        # index of the samples that are initially labeled\n",
    "        L = [i for i, label_i in enumerate(labels) if label_i != -1]\n",
    "        # index of unlabeled samples\n",
    "        U = [i for i, label_i in enumerate(labels) if label_i == -1]\n",
    "        print(\"Initial L size: \", len(L))\n",
    "        print(\"Initial U size: \", len(U))\n",
    "        # random drawing sample from U\n",
    "        random.shuffle(U)\n",
    "        U_prime = U[-min(len(U), self.u):]\n",
    "        # remove the samples in U_prime from U\n",
    "        U = U[:-len(U_prime)]\n",
    "        return L, U, U_prime\n",
    "\n",
    "    def label_p_n_samples(self, proba, rank):\n",
    "        U_prime_size = len(proba)\n",
    "        print(U_prime_size)\n",
    "        p, n = [], []\n",
    "        for label, conf_measure in enumerate(rank):\n",
    "            # 0 positive sample\n",
    "            if label==0:\n",
    "                index = 0\n",
    "                while(len(p) < self.p):\n",
    "                    max_conf_sample_index = conf_measure[index]\n",
    "                    # ---- if positive predict proba is more than 50% ------- #\n",
    "                    if (proba[max_conf_sample_index][label] > 0.5):\n",
    "                        print('P: ', max_conf_sample_index, \" : \", proba[max_conf_sample_index])\n",
    "                        p.append(max_conf_sample_index)\n",
    "                    index +=1\n",
    "                    if (index>=U_prime_size):\n",
    "                        break\n",
    "            # 1 negative sample\n",
    "            elif label == 1:\n",
    "                index = 0\n",
    "                while(len(n) < self.n):\n",
    "                    max_conf_sample_index = conf_measure[index]\n",
    "                    # ---- if negative predict proba is more than 50% ------- #\n",
    "                    if (proba[max_conf_sample_index][label] > 0.5):\n",
    "                        print('N: ', max_conf_sample_index, \" : \", proba[max_conf_sample_index])\n",
    "                        n.append(max_conf_sample_index)\n",
    "                    index +=1\n",
    "                    if (index>=U_prime_size):\n",
    "                        break\n",
    "            else:\n",
    "                print(\"Class label error\")\n",
    "        return p, n\n",
    "\n",
    "    def get_self_labeled_sample(self):\n",
    "        '''\n",
    "        return:\n",
    "            self-labeled new positive, self-labeled new negative (Index), all self_labeled_sample_size\n",
    "        '''\n",
    "        all_new_data = self.new_labeled_pos+self.new_labeled_neg\n",
    "        \n",
    "        return self.new_labeled_pos, self.new_labeled_neg, len(all_new_data)\n",
    "\n",
    "    def plot_co_training_process(self, iterCount, data, label, plotSavingPath, name):\n",
    "        if not os.path.exists(plotSavingPath):\n",
    "            os.makedirs(plotSavingPath)\n",
    "        # apply PCA on input data\n",
    "        pca = PCA(n_components=2)\n",
    "        pca_transformed = pca.fit_transform(X=data)\n",
    "        pca_one = pca_transformed[:,0]\n",
    "        pca_two = pca_transformed[:,1]\n",
    "        # plot the result\n",
    "        fig, ax = plt.subplots(figsize=(9,7))\n",
    "        for author in np.unique(label):\n",
    "            ix = np.where(label == author)\n",
    "            ax.scatter(pca_one[ix], pca_two[ix], cmap='viridis', label = author, s = 50)\n",
    "        ax.legend()\n",
    "        plt.title('Co-training iteration: '+ str(iterCount))\n",
    "        plt.xlabel(\"PCA one\")\n",
    "        plt.ylabel(\"PCA two\")\n",
    "        plt.savefig((plotSavingPath+name+\"_PCA_i-\"+str(iterCount)+\".png\").encode('utf-8'), dpi=100)\n",
    "        plt.close()\n",
    "        # plt.show()\n",
    "        \n",
    "\n",
    "    def fit(self, dataView1, dataView2, labels, dv1_test, dv2_test, label_test, plot_save_name=None, plot_save_path=None):\n",
    "        \n",
    "        labels = np.asarray(labels, dtype='int32')\n",
    "        print(\"P value: \", self.p, \" N value: \", self.n)\n",
    "        \n",
    "        L, U, U_prime = self.init_L_U_U_prime(labels)\n",
    "        \n",
    "        iterCount = 0\n",
    "        #loop until we have assigned labels to every sample in U and U_prime or we hit our iteration break condition\n",
    "        while iterCount < self.k and U_prime:\n",
    "#             print(\"step\",iterCount, \" L: \",L)\n",
    "#             print(\"step\",iterCount, \" U_prime: \",U_prime)\n",
    "            iter_train_d1 = dataView1.iloc[L]\n",
    "            iter_train_d2= dataView2.iloc[L]\n",
    "            iter_train_label = labels[L]\n",
    "            # ----------- plot the co-training process -------------- #\n",
    "            if plot_save_name != None:\n",
    "                # ----- save pca reduced plot for dv1 ------ #\n",
    "                plot_save_dv1_name = plot_save_name+\"_dv1\"\n",
    "                self.plot_co_training_process(iterCount, iter_train_d1, iter_train_label, plot_save_path, plot_save_dv1_name)\n",
    "                # ----- dv2 -------- #\n",
    "                plot_save_dv2_name = plot_save_name+\"_dv2\"\n",
    "                self.plot_co_training_process(iterCount, iter_train_d2, iter_train_label, plot_save_path, plot_save_dv2_name)\n",
    "            self.clf1.fit(iter_train_d1, iter_train_label)\n",
    "            self.clf2.fit(iter_train_d2, iter_train_label)\n",
    "            # --------- test error on test data --------------------- #\n",
    "            # make prediction on test data\n",
    "            y1 = self.clf1.predict(dv1_test)\n",
    "            y2 = self.clf2.predict(dv2_test)\n",
    "            # f1 score on each iteration\n",
    "            f1_dv1 = f1_score(label_test, y1, average='macro')\n",
    "            f1_dv2 = f1_score(label_test, y2, average='macro')\n",
    "            # collect f1 for current iteration\n",
    "            self.f1_on_test_dv1.append(f1_dv1)\n",
    "            self.f1_on_test_dv2.append(f1_dv2)\n",
    "            # ---------- get U_prime sample to be label at ---------- #\n",
    "            iter_labeling_d1 = dataView1.iloc[U_prime]\n",
    "            iter_labeling_d2 = dataView2.iloc[U_prime]\n",
    "            # rank class probabilities for unlabeled sample for it's confidence measure\n",
    "            dv1_proba = self.clf1.predict_proba(iter_labeling_d1)\n",
    "            dv2_proba = self.clf2.predict_proba(iter_labeling_d2)\n",
    "            dv1_proba_rank = []\n",
    "            dv2_proba_rank = []\n",
    "            # proba1_rank[i] is label i's confidence measure\n",
    "            for class_proba in dv1_proba.T:\n",
    "                dv1_proba_rank.append((-class_proba).argsort())\n",
    "            for class_proba in dv2_proba.T:\n",
    "                dv2_proba_rank.append((-class_proba).argsort())\n",
    "#             print(dv1_proba)\n",
    "#             print(dv1_proba_rank)\n",
    "#             print(dv2_proba)\n",
    "#             print(dv2_proba_rank)\n",
    "            # h1 classifier\n",
    "            p1,n1 = self.label_p_n_samples(dv1_proba, dv1_proba_rank)\n",
    "            # h2 classifier\n",
    "            p2,n2 = self.label_p_n_samples(dv2_proba, dv2_proba_rank)\n",
    "            roundP = set(p1+p2)\n",
    "            roundN = set(n1+n2)\n",
    "            print(\"P: \", len(roundP), \" N: \", len(roundN))\n",
    "            print(roundP, roundN)\n",
    "            # auto label the samples and remove it from U_prime\n",
    "            auto_labeled_pos = [U_prime[x] for x in roundP]\n",
    "            auto_labeled_neg = [U_prime[x] for x in roundN]\n",
    "            auto_labeled_samples = auto_labeled_pos+auto_labeled_neg\n",
    "            # ---------- collect index of auto_labeled_samples ------------ #\n",
    "            self.new_labeled_pos.extend(auto_labeled_pos)\n",
    "            self.new_labeled_neg.extend(auto_labeled_neg)\n",
    "            \n",
    "            labels[auto_labeled_pos] = 0\n",
    "            labels[auto_labeled_neg] = 1\n",
    "            # extend the labeled sample\n",
    "            L.extend(auto_labeled_pos)\n",
    "            L.extend(auto_labeled_neg)\n",
    "            # remove the labeled sample from U_prime\n",
    "            U_prime = [x for x in U_prime if x not in auto_labeled_samples]\n",
    "            #print(U_prime)\n",
    "            # randomly choice 2p+2n examples from u to replenish u_prime\n",
    "            replenishItem = U[-(2*self.p+2*self.n):]\n",
    "            U_prime.extend(replenishItem)\n",
    "            U = U[:-len(replenishItem)]\n",
    "            iterCount +=1\n",
    "        print(\"Total Labeled number: \", len(L), \" Still unlabeled number: \", len(U_prime))\n",
    "        # final train\n",
    "        newtrain_d1 = dataView1.iloc[L]\n",
    "        newtrain_d2 = dataView2.iloc[L]\n",
    "        self.clf1.fit(newtrain_d1, labels[L])\n",
    "        self.clf2.fit(newtrain_d2, labels[L])\n",
    "        # ------ save f1 vs number of iteration plot ------- #\n",
    "        if plot_save_name != None:\n",
    "            co_train_text_based = self.f1_on_test_dv1[1:]\n",
    "            co_train_citation_based = self.f1_on_test_dv2[1:]\n",
    "            co_training_step = np.arange(1,iterCount)\n",
    "            default_text_based = [self.f1_on_test_dv1[0]] * iterCount\n",
    "            default_citation_based = [self.f1_on_test_dv2[0]] * iterCount\n",
    "            default_step = np.arange(0,iterCount)\n",
    "\n",
    "            fig = plt.figure()\n",
    "            ax = plt.axes()\n",
    "            plt.plot(default_step, default_text_based, linestyle='dashed', label=\"Text based default\")\n",
    "            plt.plot(default_step, default_citation_based, linestyle='dashdot', label=\"Citation based default\")\n",
    "            plt.plot(co_training_step, co_train_text_based, linestyle='solid', marker = \"*\", label=\"Text based\")\n",
    "            plt.plot(co_training_step, co_train_citation_based, linestyle='dotted', marker = \"+\", label=\"Citation based\")\n",
    "            ax.autoscale_view()\n",
    "            plt.legend()\n",
    "            plt.xlabel('Co-Training Iterations')\n",
    "            plt.ylabel('F1 score')\n",
    "            plt.savefig((plot_save_path+plot_save_name+\"_diff_iter_f1.png\"), dpi=300)\n",
    "            plt.show()\n",
    "            plt.close(\"all\")\n",
    "    \n",
    "    def supports_proba(self, clf, x):\n",
    "        try:\n",
    "            clf.predict_proba([x])\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def predict(self, dataView1, dataView2):\n",
    "        y1 = self.clf1.predict(dataView1)\n",
    "        y2 = self.clf2.predict(dataView2)\n",
    "        proba_supported = self.supports_proba(self.clf1, dataView1.iloc[0]) and self.supports_proba(self.clf2, dataView2.iloc[0])\n",
    "        #fill pred with -1 so we can identify the samples in which sample classifiers failed to agree\n",
    "        y_pred = np.asarray([-1] * dataView1.shape[0])\n",
    "        for i, (y1_i, y2_i) in enumerate(zip(y1, y2)):\n",
    "            # if both agree on label\n",
    "            if y1_i == y2_i:\n",
    "                y_pred[i] = y1_i\n",
    "            # if disagree on label, times probability together, choice the class have higher probabilities\n",
    "            elif proba_supported:\n",
    "                y1_probas = self.clf1.predict_proba([dataView1.iloc[i]])[0]\n",
    "                y2_probas = self.clf2.predict_proba([dataView2.iloc[i]])[0]\n",
    "                print(\"y1 disagree on\",i, \" Proba: \",y1_probas)\n",
    "                print(\"y2 not aggreed on \",i, \"Proba: \", y2_probas)\n",
    "                prod_y_probas = [proba_y1 * proba_y2 for (proba_y1, proba_y2) in zip(y1_probas, y2_probas)]\n",
    "                print(\"product probas:\",prod_y_probas)\n",
    "                y_pred[i] = prod_y_probas.index(max(prod_y_probas))\n",
    "                print(\"result\",y_pred[i])\n",
    "            else:\n",
    "                #the classifiers disagree and don't support probability, so we guess\n",
    "                warnings.warn(\"classifiers disagree with label, result may not accurate\")\n",
    "                print(\"sample at: \", i, \" c1: \", y1_i, \" c2: \", y2_i)\n",
    "                y_pred[i] = random.randint(0, 1)\n",
    "        #check if predict works\n",
    "        assert not (-1 in y_pred)\n",
    "        return y_pred\n",
    "\n",
    "    def predict_proba(self, dataView1, dataView2):\n",
    "        # the predicted probabilities is simply a product (*) of probabilities given from each classifier trained\n",
    "        y1_probas = self.clf1.predict_proba(dataView1)\n",
    "        y2_probas = self.clf2.predict_proba(dataView2)\n",
    "        \n",
    "        proba = (y1_probas*y2_probas)\n",
    "        return proba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T15:11:29.285568Z",
     "start_time": "2019-01-31T15:11:29.033989Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_score, recall_score,f1_score,accuracy_score\n",
    "# cross validation\n",
    "def k_fold_cv_co_train_binary(dataview1, dataview2, unlabeled_dv1, unlabeled_dv2, label, clf, k=10, plot_save_name=None, plot_emb_type = None):\n",
    "    kf = StratifiedKFold(n_splits=k)\n",
    "    allTrueLabel = []\n",
    "    allPredLabel = []\n",
    "    per_fold_train_size = []\n",
    "    per_fold_generated_train_size = []\n",
    "    per_fold_test_size = []\n",
    "    fold = 0\n",
    "    \n",
    "    for train_index, test_index in kf.split(dataview1, label):\n",
    "        fold +=1\n",
    "        # print(\"TRAIN:\", train_index, \" \\n TEST:\", test_index)\n",
    "        # ---------------split train and test -------------------- #\n",
    "        dv1_train, dv1_test = dataview1.iloc[train_index], dataview1.iloc[test_index]\n",
    "        dv2_train, dv2_test = dataview2.iloc[train_index], dataview2.iloc[test_index]\n",
    "        _, label_test = label.iloc[train_index], label.iloc[test_index]\n",
    "        # -------------- add unlabeled to train ------------------ #\n",
    "        final_dv1 = pd.concat([dv1_train,unlabeled_dv1], ignore_index=True)\n",
    "        final_dv2 = pd.concat([dv2_train,unlabeled_dv2], ignore_index=True)\n",
    "        # ----------------extract label for training ---------------- #\n",
    "        label_train = final_dv1[\"label\"]\n",
    "        final_dv1.drop([\"label\", \"paperID\"], axis=1, inplace = True)\n",
    "        final_dv2.drop([\"label\", \"paperID\"], axis=1, inplace = True)\n",
    "        # ----------------- extract data for test ------------------------ #\n",
    "        dv1_test = dv1_test.drop([\"label\", \"paperID\"], axis=1)\n",
    "        dv2_test = dv2_test.drop([\"label\", \"paperID\"], axis=1)\n",
    "        # -------------- train binary co-training ------------------- #\n",
    "        plot_save_path = \"./co_train_detail_plots/\"+plot_emb_type+\"/fold\"+str(fold)+\"/\"\n",
    "        clf.fit(final_dv1, final_dv2, label_train, dv1_test, dv2_test, label_test, plot_save_name, plot_save_path)\n",
    "        # -------------- get self-labeled sample index -------------- #\n",
    "        self_labeled_pos_index, self_labeled_neg_index, self_labeled_sample_size = clf.get_self_labeled_sample()\n",
    "        self_labeled_pos_sample = final_dv1.iloc[self_labeled_pos_index]\n",
    "        self_labeled_neg_sample = final_dv1.iloc[self_labeled_neg_index]\n",
    "        print(\"Self labeled pos size: \", len(self_labeled_pos_index))\n",
    "        print(\"Self labeled neg size: \", len(self_labeled_neg_index))\n",
    "        # get predicted label\n",
    "        co_lr_label_predict = clf.predict(dv1_test, dv2_test)\n",
    "        allTrueLabel.extend(label_test[\"label\"].values.tolist())\n",
    "        allPredLabel.extend(co_lr_label_predict)\n",
    "        \n",
    "        per_fold_train_size.append(dv1_train.shape[0])\n",
    "        per_fold_generated_train_size.append(self_labeled_sample_size)\n",
    "        per_fold_test_size.append(dv1_test.shape[0])\n",
    "        break\n",
    "        # print(allTrueLabel)\n",
    "        # print(allPredLabel)\n",
    "        # print(\"True positive: {tp}, False positive: {fp}, False negative: {fn}, True negative: {tn}\"\n",
    "        # .format(tp=round_tp, fp=round_fp, fn=round_fn, tn=round_tn))\n",
    "\n",
    "    accuracy = accuracy_score(allTrueLabel, allPredLabel)\n",
    "    f1 = f1_score(allTrueLabel, allPredLabel,average='macro')\n",
    "    \n",
    "    print(metrics.classification_report(allTrueLabel, allPredLabel))\n",
    "    print(metrics.confusion_matrix(allTrueLabel, allPredLabel).ravel())\n",
    "    print(per_fold_train_size)\n",
    "    print(per_fold_generated_train_size)\n",
    "    print(per_fold_test_size)\n",
    "    \n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T15:12:16.608610Z",
     "start_time": "2019-01-31T15:11:29.288613Z"
    },
    "code_folding": [
     173,
     175
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For name:  j_read\n",
      "(136, 2)\n",
      "j_read  pass\n",
      "For name:  f_esteves\n",
      "(34, 2)\n",
      "f_esteves  pass\n",
      "For name:  c_miller\n",
      "(252, 2)\n",
      "c_miller  pass\n",
      "For name:  r_jha\n",
      "(11, 2)\n",
      "r_jha  pass\n",
      "For name:  a_lowe\n",
      "(102, 2)\n",
      "a_lowe  pass\n",
      "For name:  a_vega\n",
      "(20, 2)\n",
      "a_vega  pass\n",
      "For name:  k_smith\n",
      "(338, 2)\n",
      "k_smith  pass\n",
      "For name:  j_gordon\n",
      "(19, 2)\n",
      "j_gordon  pass\n",
      "For name:  s_liao\n",
      "(104, 2)\n",
      "s_liao  pass\n",
      "For name:  j_qian\n",
      "(17, 2)\n",
      "j_qian  pass\n",
      "For name:  s_bernardi\n",
      "(91, 2)\n",
      "s_bernardi  pass\n",
      "For name:  t_hill\n",
      "(15, 2)\n",
      "t_hill  pass\n",
      "For name:  s_schindler\n",
      "(51, 2)\n",
      "s_schindler  pass\n",
      "For name:  j_williams\n",
      "(625, 2)\n",
      "j_williams  pass\n",
      "For name:  s_jacobson\n",
      "(28, 2)\n",
      "s_jacobson  pass\n",
      "For name:  e_andrade\n",
      "(17, 2)\n",
      "e_andrade  pass\n",
      "For name:  t_santos\n",
      "(45, 2)\n",
      "t_santos  pass\n",
      "For name:  k_kim\n",
      "(1111, 2)\n",
      "Total sample size before apply threshold:  1111\n",
      "Counter({'0000-0002-6929-5359': 211, '0000-0001-9498-284X': 154, '0000-0002-5878-8895': 139, '0000-0002-1864-3392': 92, '0000-0002-7045-8004': 57, '0000-0001-7896-6751': 57, '0000-0002-7991-9428': 55, '0000-0002-4010-1063': 45, '0000-0002-2186-3484': 28, '0000-0002-4899-1929': 25, '0000-0003-0487-4242': 24, '0000-0002-3642-1486': 22, '0000-0001-9965-3535': 17, '0000-0002-4168-757X': 17, '0000-0001-6525-3744': 14, '0000-0002-3897-0278': 14, '0000-0002-1181-5112': 12, '0000-0003-1447-9385': 11, '0000-0002-7305-8786': 11, '0000-0002-2655-7806': 10, '0000-0003-3466-5353': 9, '0000-0002-7359-663X': 8, '0000-0003-4600-8668': 6, '0000-0002-1382-7088': 5, '0000-0002-9505-4882': 5, '0000-0003-3667-9900': 4, '0000-0001-9714-6038': 4, '0000-0002-4760-0228': 3, '0000-0003-4188-7915': 3, '0000-0001-9454-0427': 3, '0000-0002-0333-6808': 3, '0000-0003-2134-4964': 3, '0000-0002-6658-047X': 3, '0000-0003-1273-379X': 3, '0000-0002-7047-3183': 3, '0000-0002-1814-9546': 3, '0000-0003-4812-6297': 2, '0000-0001-6597-578X': 2, '0000-0002-5285-9138': 2, '0000-0002-6796-7844': 2, '0000-0002-1130-8698': 2, '0000-0001-8518-8150': 2, '0000-0002-7103-924X': 2, '0000-0002-5407-0202': 1, '0000-0001-6220-8411': 1, '0000-0002-7440-6703': 1, '0000-0002-1603-7559': 1, '0000-0003-0257-1707': 1, '0000-0001-8532-6517': 1, '0000-0001-6626-316X': 1, '0000-0002-3246-9861': 1, '0000-0002-7207-4389': 1, '0000-0001-9682-9654': 1, '0000-0002-0196-3832': 1, '0000-0001-8063-6081': 1, '0000-0003-2037-3333': 1, '0000-0001-8872-6751': 1})\n",
      "Total author before apply threshoid:  57\n",
      "Total author after apply threshoid:  7\n",
      "Total sample size after apply threshold:  765\n",
      "Total missing sample:  0\n",
      "(765, 101)\n",
      "Total missing sample:  74\n",
      "(765, 101)\n",
      "Labeled:  765  :  765\n",
      "(28524, 102)\n",
      "(23077, 102)\n",
      "Unlabeled:  28524  :  23077\n",
      "Unlabeled no citation link size:  5447\n",
      "(765, 103)\n",
      "(765, 103)\n",
      "(23077, 102)\n",
      "(23077, 102)\n",
      "k_kim_0  :  0000-0001-9498-284X\n",
      "P value:  1  N value:  1\n",
      "Initial L size:  687\n",
      "Initial U size:  23077\n",
      "75\n",
      "P:  60  :  [0.9565 0.0435]\n",
      "N:  56  :  [0. 1.]\n",
      "75\n",
      "P:  41  :  [0.8805 0.1195]\n",
      "N:  56  :  [0.0016 0.9984]\n",
      "P:  2  N:  1\n",
      "{41, 60} {56}\n",
      "76\n",
      "P:  52  :  [0.9244 0.0756]\n",
      "N:  6  :  [0. 1.]\n",
      "76\n",
      "P:  45  :  [0.8452 0.1548]\n",
      "N:  15  :  [0.0018 0.9982]\n",
      "P:  2  N:  2\n",
      "{52, 45} {6, 15}\n",
      "76\n",
      "P:  72  :  [0.9564 0.0436]\n",
      "N:  30  :  [0.0001 0.9999]\n",
      "76\n",
      "P:  73  :  [0.8652 0.1348]\n",
      "N:  43  :  [0.0021 0.9979]\n",
      "P:  2  N:  2\n",
      "{72, 73} {43, 30}\n",
      "76\n",
      "P:  40  :  [0.9259 0.0741]\n",
      "N:  0  :  [0.0001 0.9999]\n",
      "76\n",
      "P:  11  :  [0.7555 0.2445]\n",
      "N:  3  :  [0.0054 0.9946]\n",
      "P:  2  N:  2\n",
      "{40, 11} {0, 3}\n",
      "76\n",
      "P:  32  :  [0.8865 0.1135]\n",
      "N:  10  :  [0.0004 0.9996]\n",
      "76\n",
      "P:  57  :  [0.7594 0.2406]\n",
      "N:  73  :  [0.0052 0.9948]\n",
      "P:  2  N:  2\n",
      "{32, 57} {73, 10}\n",
      "76\n",
      "P:  75  :  [0.9912 0.0088]\n",
      "N:  53  :  [0.0005 0.9995]\n",
      "76\n",
      "P:  75  :  [0.8455 0.1545]\n",
      "N:  29  :  [0.0058 0.9942]\n",
      "P:  1  N:  2\n",
      "{75} {29, 53}\n",
      "77\n",
      "P:  75  :  [0.948 0.052]\n",
      "N:  31  :  [0.0006 0.9994]\n",
      "77\n",
      "P:  64  :  [0.7498 0.2502]\n",
      "N:  63  :  [0.0113 0.9887]\n",
      "P:  2  N:  2\n",
      "{64, 75} {63, 31}\n",
      "77\n",
      "P:  75  :  [0.9587 0.0413]\n",
      "N:  14  :  [0.0009 0.9991]\n",
      "77\n",
      "P:  70  :  [0.7274 0.2726]\n",
      "N:  43  :  [0.0121 0.9879]\n",
      "P:  2  N:  2\n",
      "{75, 70} {43, 14}\n",
      "77\n",
      "P:  16  :  [0.8695 0.1305]\n",
      "N:  76  :  [0.0002 0.9998]\n",
      "77\n",
      "P:  5  :  [0.6182 0.3818]\n",
      "N:  23  :  [0.0118 0.9882]\n",
      "P:  2  N:  2\n",
      "{16, 5} {76, 23}\n",
      "77\n",
      "P:  70  :  [0.8945 0.1055]\n",
      "N:  76  :  [0. 1.]\n",
      "77\n",
      "P:  55  :  [0.6518 0.3482]\n",
      "N:  10  :  [0.0143 0.9857]\n",
      "P:  2  N:  2\n",
      "{70, 55} {10, 76}\n",
      "77\n",
      "P:  1  :  [0.7957 0.2043]\n",
      "N:  69  :  [0.0015 0.9985]\n",
      "77\n",
      "P:  74  :  [0.6919 0.3081]\n",
      "N:  23  :  [0.014 0.986]\n",
      "P:  2  N:  2\n",
      "{1, 74} {69, 23}\n",
      "77\n",
      "P:  75  :  [0.981 0.019]\n",
      "N:  23  :  [0.0022 0.9978]\n",
      "77\n",
      "P:  75  :  [0.7716 0.2284]\n",
      "N:  76  :  [0.0001 0.9999]\n",
      "P:  1  N:  2\n",
      "{75} {76, 23}\n",
      "78\n",
      "P:  72  :  [0.9225 0.0775]\n",
      "N:  75  :  [0.001 0.999]\n",
      "78\n",
      "P:  74  :  [0.9761 0.0239]\n",
      "N:  75  :  [0.0133 0.9867]\n",
      "P:  2  N:  1\n",
      "{72, 74} {75}\n",
      "79\n",
      "P:  73  :  [0.8898 0.1102]\n",
      "N:  74  :  [0.0018 0.9982]\n",
      "79\n",
      "P:  73  :  [0.9027 0.0973]\n",
      "N:  40  :  [0.0157 0.9843]\n",
      "P:  1  N:  2\n",
      "{73} {40, 74}\n",
      "80\n",
      "P:  20  :  [0.7097 0.2903]\n",
      "N:  77  :  [0.0002 0.9998]\n",
      "80\n",
      "P:  26  :  [0.6712 0.3288]\n",
      "N:  67  :  [0.0138 0.9862]\n",
      "P:  2  N:  2\n",
      "{26, 20} {67, 77}\n",
      "80\n",
      "P:  61  :  [0.6919 0.3081]\n",
      "N:  76  :  [0.0013 0.9987]\n",
      "80\n",
      "P:  74  :  [0.6325 0.3675]\n",
      "N:  42  :  [0.0182 0.9818]\n",
      "P:  2  N:  2\n",
      "{74, 61} {42, 76}\n",
      "80\n",
      "P:  39  :  [0.6625 0.3375]\n",
      "N:  79  :  [0.0002 0.9998]\n",
      "80\n",
      "P:  54  :  [0.6345 0.3655]\n",
      "N:  77  :  [0.0115 0.9885]\n",
      "P:  2  N:  2\n",
      "{54, 39} {77, 79}\n",
      "80\n",
      "P:  76  :  [0.9938 0.0062]\n",
      "N:  77  :  [0.0003 0.9997]\n",
      "80\n",
      "P:  62  :  [0.6443 0.3557]\n",
      "N:  12  :  [0.019 0.981]\n",
      "P:  2  N:  2\n",
      "{76, 62} {12, 77}\n",
      "80\n",
      "P:  23  :  [0.7878 0.2122]\n",
      "N:  76  :  [0.0002 0.9998]\n",
      "80\n",
      "P:  33  :  [0.5641 0.4359]\n",
      "N:  9  :  [0.0213 0.9787]\n",
      "P:  2  N:  2\n",
      "{33, 23} {9, 76}\n",
      "80\n",
      "P:  74  :  [0.7343 0.2657]\n",
      "N:  72  :  [0.0006 0.9994]\n",
      "80\n",
      "P:  72  :  [0.5688 0.4312]\n",
      "N:  78  :  [0.0095 0.9905]\n",
      "P:  2  N:  2\n",
      "{72, 74} {72, 78}\n",
      "81\n",
      "P:  35  :  [0.6785 0.3215]\n",
      "N:  77  :  [0.0005 0.9995]\n",
      "81\n",
      "P:  79  :  [0.7731 0.2269]\n",
      "N:  37  :  [0.0231 0.9769]\n",
      "P:  2  N:  2\n",
      "{35, 79} {37, 77}\n",
      "81\n",
      "P:  79  :  [0.9206 0.0794]\n",
      "N:  0  :  [0.0019 0.9981]\n",
      "81\n",
      "P:  80  :  [0.7752 0.2248]\n",
      "N:  78  :  [0.023 0.977]\n",
      "P:  2  N:  2\n",
      "{80, 79} {0, 78}\n",
      "81\n",
      "P:  47  :  [0.657 0.343]\n",
      "N:  66  :  [0.0022 0.9978]\n",
      "81\n",
      "P:  80  :  [0.9495 0.0505]\n",
      "N:  78  :  [0.0169 0.9831]\n",
      "P:  2  N:  2\n",
      "{80, 47} {66, 78}\n",
      "81\n",
      "P:  40  :  [0.5228 0.4772]\n",
      "N:  78  :  [0.0003 0.9997]\n",
      "81\n",
      "P:  44  :  [0.6822 0.3178]\n",
      "N:  79  :  [0.0069 0.9931]\n",
      "P:  2  N:  2\n",
      "{40, 44} {78, 79}\n",
      "81\n",
      "P:  77  :  [0.9379 0.0621]\n",
      "N:  56  :  [0.0035 0.9965]\n",
      "81\n",
      "P:  7  :  [0.6645 0.3355]\n",
      "N:  37  :  [0.0267 0.9733]\n",
      "P:  2  N:  2\n",
      "{77, 7} {56, 37}\n",
      "81\n",
      "P:  29  :  [0.513 0.487]\n",
      "N:  50  :  [0.004 0.996]\n",
      "81\n",
      "P:  44  :  [0.6211 0.3789]\n",
      "N:  79  :  [0.0035 0.9965]\n",
      "P:  2  N:  2\n",
      "{44, 29} {50, 79}\n",
      "81\n",
      "P:  77  :  [0.7409 0.2591]\n",
      "N:  62  :  [0.0036 0.9964]\n",
      "81\n",
      "P:  80  :  [0.9108 0.0892]\n",
      "N:  47  :  [0.0296 0.9704]\n",
      "P:  2  N:  2\n",
      "{80, 77} {62, 47}\n",
      "81\n",
      "P:  79  :  [0.9555 0.0445]\n",
      "N:  44  :  [0.0061 0.9939]\n",
      "81\n",
      "P:  77  :  [0.6244 0.3756]\n",
      "N:  80  :  [0.0306 0.9694]\n",
      "P:  2  N:  2\n",
      "{77, 79} {80, 44}\n",
      "81\n",
      "P:  74  :  [0.7065 0.2935]\n",
      "N:  80  :  [0.0028 0.9972]\n",
      "81\n",
      "P:  63  :  [0.6074 0.3926]\n",
      "N:  80  :  [0.0145 0.9855]\n",
      "P:  2  N:  1\n",
      "{74, 63} {80}\n",
      "82\n",
      "P:  79  :  [0.8257 0.1743]\n",
      "N:  21  :  [0.0058 0.9942]\n",
      "82\n",
      "P:  79  :  [0.903 0.097]\n",
      "N:  18  :  [0.0314 0.9686]\n",
      "P:  1  N:  2\n",
      "{79} {18, 21}\n",
      "Total Labeled number:  800  Still unlabeled number:  83\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7ffdc019e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self labeled pos size:  56\n",
      "Self labeled neg size:  57\n",
      "y1 disagree on 1  Proba:  [0.8039 0.1961]\n",
      "y2 not aggreed on  1 Proba:  [0.2927 0.7073]\n",
      "product probas: [0.23529799695058198, 0.13867251028724928]\n",
      "result 0\n",
      "y1 disagree on 15  Proba:  [0.2512 0.7488]\n",
      "y2 not aggreed on  15 Proba:  [0.7012 0.2988]\n",
      "product probas: [0.17617416392299304, 0.2237287061955979]\n",
      "result 1\n",
      "y1 disagree on 23  Proba:  [0.6481 0.3519]\n",
      "y2 not aggreed on  23 Proba:  [0.0041 0.9959]\n",
      "product probas: [0.002665511178843867, 0.3504262502151168]\n",
      "result 1\n",
      "y1 disagree on 41  Proba:  [0.7186 0.2814]\n",
      "y2 not aggreed on  41 Proba:  [0.2927 0.7073]\n",
      "product probas: [0.2103280722902467, 0.19901778595139857]\n",
      "result 0\n",
      "y1 disagree on 61  Proba:  [0.6457 0.3543]\n",
      "y2 not aggreed on  61 Proba:  [0.1691 0.8309]\n",
      "product probas: [0.10920330449089982, 0.29437190884191505]\n",
      "result 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.81      0.87        16\n",
      "           1       0.95      0.98      0.97        62\n",
      "\n",
      "   micro avg       0.95      0.95      0.95        78\n",
      "   macro avg       0.94      0.90      0.92        78\n",
      "weighted avg       0.95      0.95      0.95        78\n",
      "\n",
      "[13  3  1 61]\n",
      "[687]\n",
      "[113]\n",
      "[78]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "# loop through all files in directory add name to name list\n",
    "fileDir = \"../../Data/\"+Dataset+\"/canopies/\"\n",
    "listfiles = os.listdir(fileDir)\n",
    "\n",
    "co_lr_diff_embedding_result = []\n",
    "\n",
    "# ------------ view two citation is fix, so move out to save time ------- #\n",
    "# # read viewtwo embedding\n",
    "# print(\"Load citation embedding: \", pp_citation)\n",
    "# viewtwo_citation_embedding = com_func.read_all_citation_embedding_sorted(emb_type = pp_citation)\n",
    "\n",
    "#---------------- load different embeddings for view one ---------------#\n",
    "for select_emb in pp_textual:\n",
    "#     print(\"Load textual embedding: \", select_emb)\n",
    "#     # read viewone embeddings\n",
    "#     viewone_textual_emb = com_func.read_all_textual_embedding_sorted(emb_type=select_emb, training_size = \"3m\")\n",
    "    \n",
    "#     print(viewone_textual_emb[0])\n",
    "#     print(viewtwo_citation_embedding[0])\n",
    "    \n",
    "    threshold_change_all_co_lr_f1s = []\n",
    "    threshold_change = []\n",
    "    \n",
    "    # -------------- different threshold (step by 10) -----------------------#\n",
    "    for step_threshold in range(threshold_lower, threshold_upper, 10):\n",
    "        threshold_change.append(step_threshold)\n",
    "        # collect statistic to output\n",
    "        allname, positive_sample_size, negative_sample_size  = ([] for i in range(3))\n",
    "        all_labeled_count, unlabeled_count = ([] for i in range(2))\n",
    "\n",
    "        all_co_LR_accuracy, all_co_LR_f1 = ([] for i in range(2))\n",
    "\n",
    "        total_selected_group = 0\n",
    "\n",
    "        # ------- different name group in all name group --------------------#\n",
    "        for file in listfiles:\n",
    "            # group name\n",
    "            temp = file.split(\"_\")\n",
    "            name = temp[1]+\"_\"+temp[-1]\n",
    "            print(\"For name: \",name)\n",
    "            # read pid and aid from file\n",
    "            data = read_file(fileDir+file)\n",
    "            labeled_mask = data[\"authorID\"] != \"-1\"\n",
    "            labeled_data = data[labeled_mask]\n",
    "            unlabeled_mask = data[\"authorID\"] == \"-1\"\n",
    "            ublabeled_data = data[unlabeled_mask]\n",
    "            unlabeled_pid = ublabeled_data[\"paperID\"].tolist()\n",
    "            print(labeled_data.shape)\n",
    "            # ---------------- collect all labeled sample -------------------- #\n",
    "            # ---------------- if use all samples as negative --------------- #\n",
    "            all_labeled_samples = labeled_data[\"paperID\"].tolist()\n",
    "            authorCounter = com_func.select_productive_groups(labeled_data, threshold_select_name_group)\n",
    "            # if only have one class or no class pass the threshold, not applicable\n",
    "            if(len(authorCounter)==0) or (len(authorCounter)==1):\n",
    "                print(name,\" pass\")\n",
    "            else:\n",
    "                total_selected_group+= 1\n",
    "                # --------------for each name group---------------- #\n",
    "                if apply_threshold_to_sample == True:\n",
    "                    # ---------- only use sample pass threshold ------- #\n",
    "                    #-------- only select authors in name group are very productive (more than threshold)---------#\n",
    "                    labeled_data, author_list, _= com_func.only_select_productive_authors(labeled_data, step_threshold)\n",
    "                    # ----------------- if use filtered samples as negative  --------- #\n",
    "                    filtered_all_labeled_samples = labeled_data[\"paperID\"].tolist()\n",
    "                else:\n",
    "                    # ----------- use all sample in name group --------- #\n",
    "                    author_list = com_func.productive_authors_list(labeled_data, step_threshold)\n",
    "                    print(name, \" name group sample size: \",labeled_data.shape)\n",
    "                    \n",
    "                # -------------- extract all samples for name group -------------- #\n",
    "                # for each name group\n",
    "                # read in labeled data\n",
    "                labeled_viewone_textual = extract_embedding(viewone_textual_emb, labeled_data[\"paperID\"])\n",
    "                print(labeled_viewone_textual.shape)\n",
    "                labeled_viewtwo_citation = extract_embedding(viewtwo_citation_embedding, labeled_data[\"paperID\"])\n",
    "                print(labeled_viewtwo_citation.shape)\n",
    "                print(\"Labeled: \",len(labeled_viewone_textual), \" : \", len(labeled_viewtwo_citation))\n",
    "\n",
    "                # read in unlabeled data\n",
    "                unlabeled_viewone_textual = extract_unlabeled_embedding(viewone_textual_emb, unlabeled_pid)\n",
    "                print(unlabeled_viewone_textual.shape)\n",
    "                unlabeled_viewtwo_citation = extract_unlabeled_embedding(viewtwo_citation_embedding, unlabeled_pid)\n",
    "                print(unlabeled_viewtwo_citation.shape)\n",
    "                print(\"Unlabeled: \",len(unlabeled_viewone_textual), \" : \", len(unlabeled_viewtwo_citation))\n",
    "                \n",
    "                # remove samples that have no citation link from ublabeled data\n",
    "                noCitationPids_unlabeled = set(unlabeled_viewone_textual['paperID'])-set(unlabeled_viewtwo_citation['paperID'])\n",
    "                print(\"Unlabeled no citation link size: \", len(noCitationPids_unlabeled))\n",
    "                # process unlabeled data\n",
    "                unlabeled_dv1 = unlabeled_viewone_textual[~unlabeled_viewone_textual['paperID'].isin(noCitationPids_unlabeled)].reset_index(drop=True)\n",
    "                unlabeled_dv2 = unlabeled_viewtwo_citation\n",
    "                \n",
    "                # ---------------- shuffle the data ----------------- #\n",
    "                labeled_data = labeled_data.sample(frac=1).reset_index(drop=True)\n",
    "                # ------------------ alignment ---------------------- #\n",
    "                labeled_viewone_textual = pd.merge(labeled_data, labeled_viewone_textual, left_on=\"paperID\", right_on = [0], how = \"left\")\n",
    "                labeled_viewtwo_citation = pd.merge(labeled_data, labeled_viewtwo_citation, left_on=\"paperID\", right_on = [0], how = \"left\")\n",
    "                labeled_viewtwo_citation.fillna(0, inplace=True)\n",
    "                \n",
    "                print(labeled_viewone_textual.shape)\n",
    "                print(labeled_viewtwo_citation.shape)\n",
    "                print(unlabeled_dv1.shape)\n",
    "                print(unlabeled_dv2.shape)\n",
    "                counter = 0\n",
    "                # loop through each author\n",
    "                for author in author_list:\n",
    "                    all_labeled_count.append(len(labeled_data))\n",
    "                    unlabeled_count.append(len(unlabeled_dv1))\n",
    "                    author_name = name+'_'+str(counter)\n",
    "                    allname.append(author_name)\n",
    "                    print(author_name, \" : \", author)\n",
    "                    mask = labeled_data[\"authorID\"] == author\n",
    "                    temp = labeled_data[mask]\n",
    "                    positive_sample_pid = temp[\"paperID\"].tolist()\n",
    "                    negative_sample_pid = extractNegativeSample(positive_sample_pid, filtered_all_labeled_samples)\n",
    "                    \n",
    "                    # save number of positive and negative samples\n",
    "                    positive_sample_size.append(len(positive_sample_pid))\n",
    "                    negative_sample_size.append(len(negative_sample_pid))\n",
    "                    \n",
    "                    # ----------------- generate binary label ------------------ #\n",
    "                    # form positive and negative (negative class come from similar name group)\n",
    "                    all_authors = []\n",
    "                    all_authors.append(positive_sample_pid)\n",
    "                    all_authors.append(negative_sample_pid)\n",
    "                    appended_data = []\n",
    "                    for label, pid in enumerate(all_authors):\n",
    "                        # create df save one author data \n",
    "                        authordf = pd.DataFrame({\"paperID\":pid})\n",
    "                        authordf['label'] = label\n",
    "                        appended_data.append(authordf)\n",
    "                    label_pid = pd.concat(appended_data, axis=0,ignore_index=True)\n",
    "                    # ----------- alignment of label with input data ------------ #\n",
    "                    label_pid = pd.merge(labeled_viewone_textual[\"paperID\"].to_frame(), label_pid, on = \"paperID\")\n",
    "                    #------------- process data for k-fold cv ------------------- #\n",
    "                    # throw away some column for labeled data\n",
    "                    labeled_dv1 = labeled_viewone_textual.drop([\"authorID\", 0], axis=1)\n",
    "                    labeled_dv2 = labeled_viewtwo_citation.drop([\"authorID\", 0], axis=1)\n",
    "                    # merge label into data\n",
    "                    labeled_dv1 = pd.merge(labeled_dv1, label_pid, on = \"paperID\")\n",
    "                    labeled_dv2 = pd.merge(labeled_dv2, label_pid, on = \"paperID\")\n",
    "                    label = label_pid.drop([\"paperID\"], axis=1)\n",
    "                    # ----------- check the final inputs------------------ #\n",
    "#                     print(labeled_dv1.head())\n",
    "#                     print(unlabeled_dv1.head())\n",
    "                    # ------------ fit co-training model with k-fold ------------------------ #\n",
    "                    co_logistic_clf = Co_training_clf(clf1=LogisticRegression(solver= \"liblinear\"),p=1,n=1, k=30)\n",
    "                    co_lr_accuracy, co_lr_f1 = k_fold_cv_co_train_binary(labeled_dv1, labeled_dv2, \n",
    "                                                                         unlabeled_dv1, unlabeled_dv2,\n",
    "                                                                         label, co_logistic_clf, 10, author_name, select_emb)\n",
    "                    \n",
    "                    all_co_LR_accuracy.append(co_lr_accuracy)\n",
    "                    all_co_LR_f1.append(co_lr_f1)\n",
    "                    counter+=1\n",
    "                    break\n",
    "                break\n",
    "                    \n",
    "#         # write evaluation result to excel\n",
    "#         output = pd.DataFrame({'Author Name':allname, \"positive sample size\":positive_sample_size,\"negative sample size\":negative_sample_size, \n",
    "#                                \"labeled sample size\": all_labeled_count, \"unlabeled sample size\": unlabeled_count, \n",
    "#                                \"co_logisticRegression Accuracy\":all_co_LR_accuracy, \"co_logisticRegression F1\": all_co_LR_f1})\n",
    "#         savePath = \"../../result/\"+Dataset+\"/co_train_binary/\"\n",
    "#         filename = \"(Global emb sample 3m) viewone_textual=\"+select_emb+\"_viewtwo_citation=\"+pp_citation+\"_threshold=\"+str(step_threshold)+\"_namegroupcount=\"+str(total_selected_group)+\".csv\"\n",
    "#         com_func.write_csv_df(savePath, filename, output)\n",
    "#         print(\"Done\")\n",
    "        \n",
    "#         threshold_change_all_co_lr_f1s.append(all_co_LR_f1)\n",
    "        \n",
    "#     co_lr_diff_embedding_result.append(threshold_change_all_co_lr_f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T06:32:57.910412Z",
     "start_time": "2019-01-31T06:32:57.885629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean \n",
    "\n",
    "print(threshold_change_all_co_lr_f1s)\n",
    "print(co_lr_diff_embedding_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-23T03:52:46.851Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %whos\n",
    "del viewtwo_citation_embedding\n",
    "del viewone_textual_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
