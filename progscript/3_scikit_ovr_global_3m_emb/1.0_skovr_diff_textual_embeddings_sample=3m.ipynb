{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T18:09:18.999089Z",
     "start_time": "2019-01-15T18:09:16.902999Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import com_func\n",
    "\n",
    "# parameters\n",
    "#----- threshold for selecting set of name group -----------#\n",
    "threshold_select_name_group = 100\n",
    "#----- threshold for selecting min sample in name group ----#\n",
    "threshold_lower = 100\n",
    "threshold_upper = 110\n",
    "\n",
    "pp_textual = [\"lsa\", \"pv_dm\", \"pv_dbow\"]\n",
    "\n",
    "Dataset = \"pubmed\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T18:09:22.150933Z",
     "start_time": "2019-01-15T18:09:22.123272Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_labeled_file(infile):\n",
    "    LabeledRecords_original = []\n",
    "    with open(infile, 'r', encoding = 'utf8') as f:\n",
    "        for line in f:\n",
    "            read_data = line.split(\"\\t\")\n",
    "            # get ride of bad formated lines\n",
    "            if(len(read_data)==13 or len(read_data)==12):\n",
    "                paper_detail = {\"paperID\": read_data[0], \"authorID\":read_data[1], \n",
    "                                \"co-author\": read_data[5], \"venue_id\": read_data[7]}\n",
    "                LabeledRecords_original.append(paper_detail)\n",
    "            else:\n",
    "                print(len(read_data))\n",
    "        f.close()\n",
    "    return pd.DataFrame(LabeledRecords_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T18:09:22.558099Z",
     "start_time": "2019-01-15T18:09:22.535085Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_embedding(all_embedding, all_embedding_pid, wanted_pid_list):\n",
    "    extracted_emb = []\n",
    "    wanted_pid_list = wanted_pid_list.values.tolist()\n",
    "    # only if embedding exist\n",
    "    if len(all_embedding)>0:\n",
    "        # loop through wanted pid list to keep input order\n",
    "        for wanted_pid in wanted_pid_list:\n",
    "            # if wanted paper in all pretrained embeddings\n",
    "            if wanted_pid in all_embedding_pid:\n",
    "                emb_idx = all_embedding_pid.index(wanted_pid)\n",
    "                extracted_emb.append(all_embedding[emb_idx])\n",
    "            # if wanted paper not in all pretrained embeddings, fill missing sample with 0's\n",
    "            else:\n",
    "                print(\"Missing Sample: \", wanted_pid)\n",
    "                temp = [0] * len(all_embedding[0])\n",
    "                extracted_emb.append(temp)\n",
    "                \n",
    "    extracted_emb = pd.DataFrame(extracted_emb)\n",
    "    return extracted_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T18:09:22.861508Z",
     "start_time": "2019-01-15T18:09:22.823988Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_score, recall_score,f1_score,accuracy_score\n",
    "\n",
    "def normal_group_predict(X_train, y_train, X_test, y_test, clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    # get predicted label\n",
    "    label_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test,label_pred)\n",
    "    f1 = f1_score(y_test, label_pred,average='macro')\n",
    "    \n",
    "    print(metrics.classification_report(y_test, label_pred))\n",
    "    print(metrics.confusion_matrix(y_test, label_pred).ravel())\n",
    "    \n",
    "    # accumulate statistic for entire model f1\n",
    "    cnf_matrix = confusion_matrix(y_test, label_pred)\n",
    "    TP = np.diag(cnf_matrix)\n",
    "    FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)  \n",
    "    FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "    TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "#     print(cnf_matrix)\n",
    "#     print(\"TP: \",TP, \"TN: \",TN, \"FP: \",FP,\"FN: \",FN)\n",
    "    return accuracy, f1, TP.sum(), TN.sum(), FP.sum(), FN.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T18:09:23.342841Z",
     "start_time": "2019-01-15T18:09:23.256854Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_score, recall_score,f1_score,accuracy_score\n",
    "# cross validation\n",
    "def k_fold_cv(data, label, clf, k=10):\n",
    "    kf = StratifiedKFold(n_splits=k, shuffle=False)\n",
    "    allTrueLabel = []\n",
    "    allPredLabel = []\n",
    "    for train_index, test_index in kf.split(data, label):\n",
    "        # print(\"TRAIN:\", train_index, \" \\n TEST:\", test_index)\n",
    "        # split train and test\n",
    "        data_train, data_test = data[train_index], data[test_index]\n",
    "        label_train, label_test = label[train_index], label[test_index]\n",
    "        # fit data to clf\n",
    "        clf.fit(data_train, label_train)\n",
    "        # get predicted label\n",
    "        label_pred = clf.predict(data_test)\n",
    "        allTrueLabel.extend(label_test)\n",
    "        allPredLabel.extend(label_pred)\n",
    "        # print(\"True positive: {tp}, False positive: {fp}, False negative: {fn}, True negative: {tn}\"\n",
    "        # .format(tp=round_tp, fp=round_fp, fn=round_fn, tn=round_tn))\n",
    "\n",
    "    accuracy = accuracy_score(allTrueLabel, allPredLabel)\n",
    "    f1 = f1_score(allTrueLabel, allPredLabel,average='macro')\n",
    "    \n",
    "    print(metrics.classification_report(allTrueLabel, allPredLabel))\n",
    "    print(metrics.confusion_matrix(allTrueLabel, allPredLabel).ravel())\n",
    "    \n",
    "    # accumulate statistic for entire model f1\n",
    "    cnf_matrix = confusion_matrix(allTrueLabel, allPredLabel)\n",
    "    TP = np.diag(cnf_matrix)\n",
    "    FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)  \n",
    "    FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "    TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "#     print(cnf_matrix)\n",
    "#     print(\"TP: \",TP, \"TN: \",TN, \"FP: \",FP,\"FN: \",FN)\n",
    "\n",
    "    return accuracy, f1, TP.sum(), TN.sum(), FP.sum(), FN.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T18:09:39.166954Z",
     "start_time": "2019-01-15T18:09:23.927253Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load textual embedding:  pv_dm\n",
      "Total textual vector records: 135796\n",
      "Vector dimension:  100\n",
      "For name:  j_read\n",
      "j_read  pass\n",
      "For name:  f_esteves\n",
      "f_esteves  pass\n",
      "For name:  c_miller\n",
      "c_miller  pass\n",
      "For name:  r_jha\n",
      "r_jha  pass\n",
      "For name:  a_lowe\n",
      "a_lowe  pass\n",
      "For name:  a_vega\n",
      "a_vega  pass\n",
      "For name:  k_smith\n",
      "k_smith  pass\n",
      "For name:  j_gordon\n",
      "j_gordon  pass\n",
      "For name:  s_liao\n",
      "s_liao  pass\n",
      "For name:  j_qian\n",
      "j_qian  pass\n",
      "For name:  s_bernardi\n",
      "s_bernardi  pass\n",
      "For name:  t_hill\n",
      "t_hill  pass\n",
      "For name:  s_schindler\n",
      "s_schindler  pass\n",
      "For name:  j_williams\n",
      "j_williams  pass\n",
      "For name:  s_jacobson\n",
      "s_jacobson  pass\n",
      "For name:  e_andrade\n",
      "e_andrade  pass\n",
      "For name:  t_santos\n",
      "t_santos  pass\n",
      "For name:  k_kim\n",
      "Total sample size before apply threshold:  1111\n",
      "Counter({'0000-0002-6929-5359': 211, '0000-0001-9498-284X': 154, '0000-0002-5878-8895': 139, '0000-0002-1864-3392': 92, '0000-0002-7045-8004': 57, '0000-0001-7896-6751': 57, '0000-0002-7991-9428': 55, '0000-0002-4010-1063': 45, '0000-0002-2186-3484': 28, '0000-0002-4899-1929': 25, '0000-0003-0487-4242': 24, '0000-0002-3642-1486': 22, '0000-0001-9965-3535': 17, '0000-0002-4168-757X': 17, '0000-0001-6525-3744': 14, '0000-0002-3897-0278': 14, '0000-0002-1181-5112': 12, '0000-0003-1447-9385': 11, '0000-0002-7305-8786': 11, '0000-0002-2655-7806': 10, '0000-0003-3466-5353': 9, '0000-0002-7359-663X': 8, '0000-0003-4600-8668': 6, '0000-0002-1382-7088': 5, '0000-0002-9505-4882': 5, '0000-0003-3667-9900': 4, '0000-0001-9714-6038': 4, '0000-0002-4760-0228': 3, '0000-0003-4188-7915': 3, '0000-0001-9454-0427': 3, '0000-0002-0333-6808': 3, '0000-0003-2134-4964': 3, '0000-0002-6658-047X': 3, '0000-0003-1273-379X': 3, '0000-0002-7047-3183': 3, '0000-0002-1814-9546': 3, '0000-0003-4812-6297': 2, '0000-0001-6597-578X': 2, '0000-0002-5285-9138': 2, '0000-0002-6796-7844': 2, '0000-0002-1130-8698': 2, '0000-0001-8518-8150': 2, '0000-0002-7103-924X': 2, '0000-0002-5407-0202': 1, '0000-0001-6220-8411': 1, '0000-0002-7440-6703': 1, '0000-0002-1603-7559': 1, '0000-0003-0257-1707': 1, '0000-0001-8532-6517': 1, '0000-0001-6626-316X': 1, '0000-0002-3246-9861': 1, '0000-0002-7207-4389': 1, '0000-0001-9682-9654': 1, '0000-0002-0196-3832': 1, '0000-0001-8063-6081': 1, '0000-0003-2037-3333': 1, '0000-0001-8872-6751': 1})\n",
      "Total author before apply threshoid:  57\n",
      "Total author after apply threshoid:  3\n",
      "Total sample size after apply threshold:  504\n",
      "(504, 100)\n",
      "1\n",
      "(504, 100)\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "0000-0001-9498-284X       0.95      0.95      0.95       154\n",
      "0000-0002-5878-8895       0.96      0.92      0.94       139\n",
      "0000-0002-6929-5359       0.97      1.00      0.99       211\n",
      "\n",
      "          micro avg       0.96      0.96      0.96       504\n",
      "          macro avg       0.96      0.96      0.96       504\n",
      "       weighted avg       0.96      0.96      0.96       504\n",
      "\n",
      "[146   5   3   8 128   3   0   0 211]\n",
      "LR Accuracy:  0.9623015873015873\n",
      "LR F1:  0.9584032423504661\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "0000-0001-9498-284X       0.93      0.92      0.93       154\n",
      "0000-0002-5878-8895       0.93      0.91      0.92       139\n",
      "0000-0002-6929-5359       0.97      0.99      0.98       211\n",
      "\n",
      "          micro avg       0.95      0.95      0.95       504\n",
      "          macro avg       0.95      0.94      0.94       504\n",
      "       weighted avg       0.95      0.95      0.95       504\n",
      "\n",
      "[142   8   4   9 127   3   1   1 209]\n",
      "svc Accuracy:  0.9484126984126984\n",
      "svc F1:  0.9435545518091324\n",
      "For name:  d_ricci\n",
      "d_ricci  pass\n",
      "For name:  s_cameron\n",
      "s_cameron  pass\n",
      "For name:  t_wright\n",
      "t_wright  pass\n",
      "For name:  r_cunha\n",
      "r_cunha  pass\n",
      "For name:  s_fuchs\n",
      "s_fuchs  pass\n",
      "For name:  m_nawaz\n",
      "m_nawaz  pass\n",
      "For name:  k_harris\n",
      "k_harris  pass\n",
      "For name:  r_daniel\n",
      "r_daniel  pass\n",
      "For name:  k_xu\n",
      "k_xu  pass\n",
      "For name:  s_antunes\n",
      "s_antunes  pass\n",
      "For name:  k_cho\n",
      "k_cho  pass\n",
      "For name:  j_sanderson\n",
      "j_sanderson  pass\n",
      "For name:  s_uddin\n",
      "s_uddin  pass\n",
      "For name:  a_batista\n",
      "a_batista  pass\n",
      "For name:  h_pereira\n",
      "h_pereira  pass\n",
      "For name:  a_patel\n",
      "a_patel  pass\n",
      "For name:  r_graham\n",
      "r_graham  pass\n",
      "For name:  a_nilsson\n",
      "a_nilsson  pass\n",
      "For name:  m_soto\n",
      "m_soto  pass\n",
      "For name:  g_guidi\n",
      "g_guidi  pass\n",
      "For name:  e_andersson\n",
      "e_andersson  pass\n",
      "For name:  s_reid\n",
      "s_reid  pass\n",
      "For name:  a_maleki\n",
      "a_maleki  pass\n",
      "For name:  j_moon\n",
      "j_moon  pass\n",
      "For name:  t_abe\n",
      "t_abe  pass\n",
      "For name:  x_fu\n",
      "x_fu  pass\n",
      "For name:  f_ortega\n",
      "f_ortega  pass\n",
      "For name:  r_morris\n",
      "r_morris  pass\n",
      "For name:  w_fang\n",
      "w_fang  pass\n",
      "For name:  m_amaral\n",
      "m_amaral  pass\n",
      "For name:  h_song\n",
      "h_song  pass\n",
      "For name:  h_dai\n",
      "h_dai  pass\n",
      "For name:  y_nakajima\n",
      "y_nakajima  pass\n",
      "For name:  t_warner\n",
      "t_warner  pass\n",
      "For name:  s_saha\n",
      "s_saha  pass\n",
      "For name:  j_fernandez\n",
      "j_fernandez  pass\n",
      "For name:  m_pan\n",
      "m_pan  pass\n",
      "For name:  a_simon\n",
      "a_simon  pass\n",
      "For name:  r_freitas\n",
      "r_freitas  pass\n",
      "For name:  c_yun\n",
      "c_yun  pass\n",
      "For name:  j_huang\n",
      "j_huang  pass\n",
      "For name:  p_santos\n",
      "p_santos  pass\n",
      "For name:  n_young\n",
      "n_young  pass\n",
      "For name:  d_ross\n",
      "d_ross  pass\n",
      "For name:  q_wang\n",
      "q_wang  pass\n",
      "For name:  c_cardoso\n",
      "c_cardoso  pass\n",
      "For name:  j_matthews\n",
      "j_matthews  pass\n",
      "For name:  g_lee\n",
      "g_lee  pass\n",
      "For name:  m_salem\n",
      "m_salem  pass\n",
      "For name:  h_lai\n",
      "h_lai  pass\n",
      "For name:  r_harris\n",
      "r_harris  pass\n",
      "For name:  c_vaughan\n",
      "c_vaughan  pass\n",
      "For name:  e_thompson\n",
      "e_thompson  pass\n",
      "For name:  r_gomes\n",
      "r_gomes  pass\n",
      "For name:  r_bennett\n",
      "r_bennett  pass\n",
      "For name:  m_collins\n",
      "m_collins  pass\n",
      "For name:  m_cowley\n",
      "m_cowley  pass\n",
      "For name:  p_teixeira\n",
      "p_teixeira  pass\n",
      "For name:  c_cox\n",
      "c_cox  pass\n",
      "For name:  s_hsu\n",
      "s_hsu  pass\n",
      "For name:  f_williams\n",
      "f_williams  pass\n",
      "For name:  d_parsons\n",
      "d_parsons  pass\n",
      "For name:  a_choudhury\n",
      "a_choudhury  pass\n",
      "For name:  c_richter\n",
      "c_richter  pass\n",
      "For name:  m_hossain\n",
      "m_hossain  pass\n",
      "For name:  v_alves\n",
      "v_alves  pass\n",
      "For name:  j_becker\n",
      "j_becker  pass\n",
      "For name:  m_soares\n",
      "m_soares  pass\n",
      "For name:  j_yi\n",
      "j_yi  pass\n",
      "For name:  s_khan\n",
      "s_khan  pass\n",
      "For name:  a_rao\n",
      "a_rao  pass\n",
      "For name:  d_cameron\n",
      "d_cameron  pass\n",
      "For name:  c_morgan\n",
      "c_morgan  pass\n",
      "For name:  h_cui\n",
      "h_cui  pass\n",
      "For name:  p_zhang\n",
      "p_zhang  pass\n",
      "For name:  j_fernandes\n",
      "j_fernandes  pass\n",
      "For name:  a_jain\n",
      "a_jain  pass\n",
      "For name:  d_zhang\n",
      "d_zhang  pass\n",
      "For name:  b_huang\n",
      "b_huang  pass\n",
      "For name:  m_chong\n",
      "m_chong  pass\n",
      "For name:  m_cerqueira\n",
      "m_cerqueira  pass\n",
      "For name:  p_yang\n",
      "p_yang  pass\n",
      "For name:  j_marques\n",
      "j_marques  pass\n",
      "For name:  n_ali\n",
      "n_ali  pass\n",
      "For name:  h_ng\n",
      "h_ng  pass\n",
      "For name:  m_viana\n",
      "m_viana  pass\n",
      "For name:  t_inoue\n",
      "t_inoue  pass\n",
      "For name:  b_meyer\n",
      "b_meyer  pass\n",
      "For name:  c_liao\n",
      "c_liao  pass\n",
      "For name:  k_wheeler\n",
      "k_wheeler  pass\n",
      "For name:  m_rizzo\n",
      "m_rizzo  pass\n",
      "For name:  y_shi\n",
      "y_shi  pass\n",
      "For name:  c_luo\n",
      "c_luo  pass\n",
      "For name:  j_arthur\n",
      "j_arthur  pass\n",
      "For name:  m_ansari\n",
      "m_ansari  pass\n",
      "For name:  g_anderson\n",
      "g_anderson  pass\n",
      "For name:  m_hidalgo\n",
      "m_hidalgo  pass\n",
      "For name:  k_jacobsen\n",
      "k_jacobsen  pass\n",
      "For name:  s_kelly\n",
      "s_kelly  pass\n",
      "For name:  s_james\n",
      "s_james  pass\n",
      "For name:  p_persson\n",
      "p_persson  pass\n",
      "For name:  y_tanaka\n",
      "y_tanaka  pass\n",
      "For name:  c_gao\n",
      "c_gao  pass\n",
      "For name:  w_jung\n",
      "w_jung  pass\n",
      "For name:  s_lewis\n",
      "s_lewis  pass\n",
      "For name:  w_han\n",
      "w_han  pass\n",
      "For name:  m_shah\n",
      "m_shah  pass\n",
      "For name:  c_arango\n",
      "c_arango  pass\n",
      "For name:  r_young\n",
      "r_young  pass\n",
      "For name:  r_coleman\n",
      "r_coleman  pass\n",
      "For name:  b_kang\n",
      "b_kang  pass\n",
      "For name:  s_carter\n",
      "s_carter  pass\n",
      "For name:  c_thomas\n",
      "c_thomas  pass\n",
      "For name:  m_gutierrez\n",
      "m_gutierrez  pass\n",
      "For name:  s_moon\n",
      "s_moon  pass\n",
      "For name:  r_pereira\n",
      "r_pereira  pass\n",
      "For name:  a_nielsen\n",
      "a_nielsen  pass\n",
      "For name:  j_conde\n",
      "j_conde  pass\n",
      "For name:  k_wright\n",
      "k_wright  pass\n",
      "For name:  m_parker\n",
      "m_parker  pass\n",
      "For name:  h_huang\n",
      "h_huang  pass\n",
      "For name:  j_terry\n",
      "j_terry  pass\n",
      "For name:  y_xu\n",
      "y_xu  pass\n",
      "For name:  a_melo\n",
      "a_melo  pass\n",
      "For name:  r_doyle\n",
      "r_doyle  pass\n",
      "For name:  m_bernardo\n",
      "m_bernardo  pass\n",
      "For name:  j_soares\n",
      "j_soares  pass\n",
      "For name:  j_richard\n",
      "j_richard  pass\n",
      "For name:  p_robinson\n",
      "Total sample size before apply threshold:  275\n",
      "Counter({'0000-0002-7878-0313': 133, '0000-0002-0736-9199': 119, '0000-0002-3156-3418': 19, '0000-0002-0577-3147': 4})\n",
      "Total author before apply threshoid:  4\n",
      "Total author after apply threshoid:  2\n",
      "Total sample size after apply threshold:  252\n",
      "(252, 100)\n",
      "1\n",
      "(252, 100)\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "0000-0002-0736-9199       0.93      0.93      0.93       119\n",
      "0000-0002-7878-0313       0.94      0.93      0.94       133\n",
      "\n",
      "          micro avg       0.93      0.93      0.93       252\n",
      "          macro avg       0.93      0.93      0.93       252\n",
      "       weighted avg       0.93      0.93      0.93       252\n",
      "\n",
      "[111   8   9 124]\n",
      "LR Accuracy:  0.9325396825396826\n",
      "LR F1:  0.9323596747454015\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "0000-0002-0736-9199       0.91      0.89      0.90       119\n",
      "0000-0002-7878-0313       0.90      0.92      0.91       133\n",
      "\n",
      "          micro avg       0.91      0.91      0.91       252\n",
      "          macro avg       0.91      0.91      0.91       252\n",
      "       weighted avg       0.91      0.91      0.91       252\n",
      "\n",
      "[106  13  10 123]\n",
      "svc Accuracy:  0.9087301587301587\n",
      "svc F1:  0.9083129004192043\n",
      "For name:  c_zou\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_zou  pass\n",
      "For name:  s_rana\n",
      "s_rana  pass\n",
      "For name:  a_nunes\n",
      "a_nunes  pass\n",
      "For name:  s_jeong\n",
      "s_jeong  pass\n",
      "For name:  b_olsen\n",
      "b_olsen  pass\n",
      "For name:  m_reilly\n",
      "m_reilly  pass\n",
      "For name:  d_nguyen\n",
      "d_nguyen  pass\n",
      "For name:  r_santos\n",
      "r_santos  pass\n",
      "For name:  f_ferreira\n",
      "f_ferreira  pass\n",
      "For name:  y_ng\n",
      "y_ng  pass\n",
      "For name:  j_madsen\n",
      "j_madsen  pass\n",
      "For name:  d_collins\n",
      "d_collins  pass\n",
      "For name:  l_davies\n",
      "l_davies  pass\n",
      "For name:  m_mora\n",
      "m_mora  pass\n",
      "For name:  a_fontana\n",
      "a_fontana  pass\n",
      "For name:  r_chen\n",
      "r_chen  pass\n",
      "For name:  s_krause\n",
      "s_krause  pass\n",
      "For name:  t_smith\n",
      "Total sample size before apply threshold:  603\n",
      "Counter({'0000-0002-3650-9381': 154, '0000-0003-1673-2954': 113, '0000-0002-2120-2766': 85, '0000-0002-6279-9685': 84, '0000-0003-3528-6793': 65, '0000-0003-4453-9713': 32, '0000-0002-5197-5030': 26, '0000-0002-3945-630X': 10, '0000-0001-7894-6814': 9, '0000-0002-5750-0706': 6, '0000-0002-5495-8906': 4, '0000-0003-3762-6253': 4, '0000-0002-0479-4261': 3, '0000-0003-2389-461X': 2, '0000-0001-6272-8871': 2, '0000-0001-7683-2653': 1, '0000-0002-2104-2264': 1, '0000-0001-9068-4642': 1, '0000-0002-1881-2766': 1})\n",
      "Total author before apply threshoid:  19\n",
      "Total author after apply threshoid:  2\n",
      "Total sample size after apply threshold:  267\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e1e178bcb6cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;31m# select feature wanted to fit to clustering/classification algorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0;31m# data part, textual information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0mdata_part_textual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_textual_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_textual_emb_pid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_part_textual\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0mpart_collection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_part_textual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-b7f6942e9c58>\u001b[0m in \u001b[0;36mextract_embedding\u001b[0;34m(all_embedding, all_embedding_pid, wanted_pid_list)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mwanted_pid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwanted_pid_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;31m# if wanted paper in all pretrained embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mwanted_pid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_embedding_pid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                 \u001b[0memb_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_embedding_pid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwanted_pid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mextracted_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_embedding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0memb_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load the file\n",
    "import io\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from statistics import mean \n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "fileDir = \"../../Data/\"+Dataset+\"/canopies_labeled/\"\n",
    "listfiles = os.listdir(fileDir)\n",
    "\n",
    "lr_diff_embedding_result = []\n",
    "svm_diff_embedding_result = []\n",
    "\n",
    "# ----------------------- different ciataion embedding ----------------------#\n",
    "for select_emb in pp_textual:\n",
    "    \n",
    "    # read pretrained embeddings\n",
    "    print(\"Load textual embedding: \", select_emb)\n",
    "    all_textual_embedding, all_textual_emb_pid = com_func.read_textual_embedding(emb_type=select_emb, training_size = \"3m\")\n",
    "    \n",
    "    threshold_change_all_lr_f1s = []\n",
    "    threshold_change_all_svm_f1s = []\n",
    "    threshold_change = []\n",
    "    \n",
    "    # -------------- different threshold (step by 10) -----------------------#\n",
    "    for step_threshold in range(threshold_lower, threshold_upper, 10):\n",
    "        threshold_change.append(step_threshold)\n",
    "        # collect statistic to output\n",
    "        allname, num_class, per_class_count, all_sample_count = ([] for i in range(4))\n",
    "        \n",
    "        all_mnb_accuracy, all_mnb_f1, all_LR_accuracy = ([] for i in range(3))\n",
    "        all_LR_f1, all_svcLinear_accuracy, all_svcLinear_f1 = ([] for i in range(3))\n",
    "    \n",
    "        total_selected_group = 0\n",
    "\n",
    "        # ------- different name group in all name group --------------------#\n",
    "        for file in listfiles:\n",
    "            # group name\n",
    "            temp = file.split(\"_\")\n",
    "            name = temp[1]+\"_\"+temp[-1]\n",
    "            print(\"For name: \",name)\n",
    "            # read needed content in labeled file\n",
    "            labeled_data = read_labeled_file(fileDir+file)\n",
    "            #----------- select name group contain productive author------------------------------------#\n",
    "            #----------- (contain pair of author write more than 100 papers) ---------------------------#\n",
    "            # count number of paper each author write based on author ID\n",
    "            authorCounter = com_func.select_productive_groups(labeled_data, threshold_select_name_group)\n",
    "            # if only have one class or no class pass the threshold, not applicable\n",
    "            if(len(authorCounter)==0) or (len(authorCounter)==1):\n",
    "                print(name, \" pass\")\n",
    "            else:\n",
    "                total_selected_group+= 1\n",
    "                #--------select authors in name group are very productive (more than threshold)---------#\n",
    "                labeled_data, author_list, paperCounter= com_func.only_select_productive_authors(labeled_data, step_threshold)\n",
    "                all_sample_count.append(len(labeled_data))\n",
    "                allname.append(name)\n",
    "                num_class.append(len(paperCounter))\n",
    "                per_class_count.append(paperCounter)\n",
    "                #------------ extract paper representation -------------------------------------------#\n",
    "                # shuffle the data\n",
    "                labeled_data = labeled_data.sample(frac=1).reset_index(drop=True)\n",
    "                # extract true label and pid\n",
    "                label = labeled_data[\"authorID\"]\n",
    "                pid = labeled_data[\"paperID\"]\n",
    "                # list of different data field\n",
    "                part_collection = []\n",
    "                # select feature wanted to fit to clustering/classification algorithm\n",
    "                # data part, textual information\n",
    "                data_part_textual = extract_embedding(all_textual_embedding, all_textual_emb_pid, pid)\n",
    "                print(data_part_textual.shape)\n",
    "                part_collection.append(data_part_textual)\n",
    "                # merge different part of data data together by concatenate it all together\n",
    "                combinedata = com_func.merge_data_parts(part_collection)\n",
    "                print(combinedata.shape)\n",
    "                                # -------------- using converted feature vector to train classifier-------------------#\n",
    "                if select_emb == \"tf\":\n",
    "                    # using multinomial naive bayes\n",
    "                    clf = MultinomialNB()\n",
    "                    mnbaccuracy, mnbmarcof1, tp, tn, fp, fn = k_fold_cv(combinedata, label, clf, k=10)\n",
    "                    print(\"MNB Accuracy: \",mnbaccuracy)\n",
    "                    print(\"MNB F1: \", mnbmarcof1)\n",
    "                    all_mnb_accuracy.append(mnbaccuracy)\n",
    "                    all_mnb_f1.append(mnbmarcof1)\n",
    "                # using logistic regression\n",
    "                clf = LogisticRegression(multi_class='ovr')\n",
    "                LRaccuracy, LRmarcof1, tp, tn, fp, fn = k_fold_cv(combinedata, label, clf, k=10)\n",
    "                print(\"LR Accuracy: \",LRaccuracy)\n",
    "                print(\"LR F1: \", LRmarcof1)\n",
    "                all_LR_accuracy.append(LRaccuracy)\n",
    "                all_LR_f1.append(LRmarcof1)\n",
    "                # using SVM with linear kernal\n",
    "                clf = SVC(decision_function_shape='ovr', kernel='linear')\n",
    "                svcaccuracy, svcmarcof1, tp, tn, fp, fn = k_fold_cv(combinedata, label, clf, k=10)\n",
    "                print(\"svc Accuracy: \",svcaccuracy)\n",
    "                print(\"svc F1: \", svcmarcof1)\n",
    "                all_svcLinear_accuracy.append(svcaccuracy)\n",
    "                all_svcLinear_f1.append(svcmarcof1)\n",
    "            \n",
    "        if select_emb == \"tf\":\n",
    "            # write evaluation result to excel\n",
    "            output = pd.DataFrame({'Name Group':allname,\"Class number\":num_class,\n",
    "                                   \"Per class size\":per_class_count, \"Total samples\":all_sample_count,\n",
    "                                   \"MNB Accuracy\":all_mnb_accuracy, \"MNB macro F1\": all_mnb_f1, \n",
    "                                   \"svc(linear) accuracy\":all_svcLinear_accuracy, \"svc(linear) macro f1\": all_svcLinear_f1, \n",
    "                                   \"logistic regression accuracy\":all_LR_accuracy, \"logistic regression macro f1\": all_LR_f1})\n",
    "        else:\n",
    "            # write evaluation result to excel\n",
    "            output = pd.DataFrame({'Name Group':allname, \"Class number\":num_class,\n",
    "                                   \"Per class size\":per_class_count, \"Total samples\":all_sample_count,\n",
    "                                   \"svc(linear) accuracy\":all_svcLinear_accuracy, \"svc(linear) macro f1\": all_svcLinear_f1, \n",
    "                                   \"logistic regression accuracy\":all_LR_accuracy, \"logistic regression macro f1\": all_LR_f1})\n",
    "\n",
    "        savePath = \"../../result/\"+Dataset+\"/OCEN_global_emb_sample=3m/\"\n",
    "        filename = \"(Global emb sample 3m) textual=\"+select_emb+\"_threshold=\"+str(step_threshold)+\"_namegroupcount=\"+str(total_selected_group)+\".csv\"\n",
    "        com_func.write_csv_df(savePath, filename, output)\n",
    "        print(\"Done\")\n",
    "        \n",
    "        threshold_change_all_lr_f1s.append(all_svcLinear_f1)\n",
    "        threshold_change_all_svm_f1s.append(all_LR_f1)\n",
    "    \n",
    "    lr_diff_embedding_result.append(threshold_change_all_lr_f1s)\n",
    "    svm_diff_embedding_result.append(threshold_change_all_svm_f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T05:13:02.749773Z",
     "start_time": "2019-01-13T05:13:02.742337Z"
    }
   },
   "outputs": [],
   "source": [
    "# ----------- plot f1 score w.r.t each name group on different embedding -------------- #\n",
    "print(pp_textual)\n",
    "print(allname)\n",
    "# 3d, d1 diff emb, d2 diff threshold, d3 result for different author\n",
    "print(lr_diff_embedding_result)\n",
    "print(svm_diff_embedding_result)\n",
    "print(threshold_change)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T05:13:03.416112Z",
     "start_time": "2019-01-13T05:13:03.397451Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------- plot f1 score w.r.t each name group on different embedding -------------- #\n",
    "# -------------- extract result for plot --------------------- #\n",
    "lr_per_author = []\n",
    "lr_lsa_per_author_result = lr_diff_embedding_result[0][0]\n",
    "lr_pv_dm_per_author_result = lr_diff_embedding_result[1][0]\n",
    "lr_pv_dbow_per_author_result = lr_diff_embedding_result[2][0]\n",
    "lr_per_author.append(lr_lsa_per_author_result)\n",
    "lr_per_author.append(lr_pv_dm_per_author_result)\n",
    "lr_per_author.append(lr_pv_dbow_per_author_result)\n",
    "\n",
    "svm_per_author = []\n",
    "svm_lsa_per_author_result = lr_diff_embedding_result[0][0]\n",
    "svm_pv_dm_per_author_result = svm_diff_embedding_result[1][0]\n",
    "svm_pv_dbow_per_author_result = svm_diff_embedding_result[2][0]\n",
    "svm_per_author.append(svm_lsa_per_author_result)\n",
    "svm_per_author.append(svm_pv_dm_per_author_result)\n",
    "svm_per_author.append(svm_pv_dbow_per_author_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T05:15:09.200228Z",
     "start_time": "2019-01-13T05:15:08.203410Z"
    }
   },
   "outputs": [],
   "source": [
    "# ----------- plot f1 score w.r.t each name group on different embedding -------------- #\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "#--------------   logistic regression --------------------------#\n",
    "# process result into np array\n",
    "logistic_regression_result = np.array(lr_per_author)\n",
    "name_group = np.array(allname)\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "for emb_type, result in zip(pp_textual, logistic_regression_result):\n",
    "    plt.xticks(range(len(result)), name_group)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "    plt.plot(result, label=emb_type)\n",
    "ax.autoscale_view()\n",
    "plt.legend()\n",
    "plt.title('F1 for different embedding method in logistic regression')\n",
    "plt.xlabel('Name group')\n",
    "plt.ylabel('marco f1 score')\n",
    "# plt.savefig('diff_textual_embedding_sample=3m_clf=logistic regression_threshold=100.eps', format='eps', dpi=300)\n",
    "\n",
    "#--------------   svm  -------------- --------------------------#\n",
    "# process result into np array\n",
    "svm_result = np.array(svm_per_author)\n",
    "name_group = np.array(allname)\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "for emb_type, result in zip(pp_textual, svm_result):\n",
    "    plt.xticks(range(len(result)), name_group)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "    plt.plot(result, label=emb_type)\n",
    "ax.autoscale_view()\n",
    "plt.legend()\n",
    "plt.title('F1 for different embedding method in svm')\n",
    "plt.xlabel('Name group')\n",
    "plt.ylabel('marco f1 score')\n",
    "# plt.savefig('diff_textual_embedding_sample=3m_clf=svm_threshold=100.eps', format='eps', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T02:26:21.530720Z",
     "start_time": "2019-01-11T02:26:21.512883Z"
    }
   },
   "outputs": [],
   "source": [
    "# ----------- plot f1 score change w.r.t threshold on different embedding ------------- #\n",
    "print(pp_textual)\n",
    "print(lr_diff_embedding_result)\n",
    "print(svm_diff_embedding_result)\n",
    "print(threshold_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T05:17:47.652455Z",
     "start_time": "2019-01-11T05:17:45.886193Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "threshold_change = np.array(threshold_change)\n",
    "#--------------   logistic regression --------------------------#\n",
    "# process result into np array\n",
    "logistic_regression_result = np.array(lr_diff_embedding_result)\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "for emb_type, result in zip(pp_textual, logistic_regression_result):\n",
    "    plt.plot(threshold_change, result, label=emb_type)\n",
    "plt.legend()\n",
    "plt.title('Average f1 for different embedding in logistic regression')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('marco f1 score')\n",
    "\n",
    "# plt.savefig('diff_textual_emb_smaple=3m_clf=logistic regression.eps', format='eps', dpi=300)\n",
    "\n",
    "\n",
    "# -------------------- svm -------------------------------------#\n",
    "svm_result = np.array(svm_diff_embedding_result)\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "for emb_type, result in zip(pp_textual, svm_result):\n",
    "    plt.plot(threshold_change, result, label=emb_type)\n",
    "plt.legend()\n",
    "plt.title('Average f1 for different embedding in SVM')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('marco f1 score')\n",
    "\n",
    "# plt.savefig('diff_textual_emb_smaple=3m_clf=svm.eps', format='eps', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T22:22:12.719980Z",
     "start_time": "2018-12-16T22:22:12.706986Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"textual =\",pp_textual,\"_citation =\",citation_emb)\n",
    "print(\"svc: \", svcF1)\n",
    "print(\"lr:\", lrF1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T23:10:05.635253Z",
     "start_time": "2018-12-13T23:10:05.605356Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# accuracy\n",
    "from statistics import mean \n",
    "cleaned_svcLinear_accuracy = [x for x in all_svcLinear_accuracy if isinstance(x, float)]\n",
    "cleaned_lr_accuracy = [x for x in all_LR_accuracy if isinstance(x, float)]\n",
    "print(len(cleaned_svcLinear_accuracy))\n",
    "print(len(cleaned_lr_accuracy))\n",
    "print(mean(cleaned_svcLinear_accuracy))\n",
    "print(mean(cleaned_lr_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T23:10:06.108636Z",
     "start_time": "2018-12-13T23:10:06.084585Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# f1\n",
    "from statistics import mean \n",
    "# remove string from result\n",
    "cleaned_svcLinear_f1 = [x for x in all_svcLinear_f1 if isinstance(x, float)]\n",
    "cleaned_lr_f1 = [x for x in all_LR_f1 if isinstance(x, float)]\n",
    "print(len(cleaned_svcLinear_f1))\n",
    "print(len(cleaned_lr_f1))\n",
    "print(mean(cleaned_svcLinear_f1))\n",
    "print(mean(cleaned_lr_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
