{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T00:01:14.146108Z",
     "start_time": "2019-01-23T00:01:08.845750Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import com_func\n",
    "\n",
    "#----- threshold for selecting set of name group -----------#\n",
    "threshold_select_name_group = 100\n",
    "#----- threshold for selecting min sample in name group ----#\n",
    "threshold_lower = 100\n",
    "threshold_upper = 110\n",
    "\n",
    "apply_threshold_to_sample = True\n",
    "\n",
    "pp_textual = [\"lsa\", \"pv_dm\", \"pv_dbow\"]\n",
    "pp_citation = \"n2v\"\n",
    "\n",
    "Dataset = \"pubmed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T19:48:48.699954Z",
     "start_time": "2019-01-20T19:48:48.673825Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_file(infile):\n",
    "    AllRecords_original = []\n",
    "    with open(infile, 'r', encoding = 'utf8') as f:\n",
    "        for line in f:\n",
    "            read_data = line.split(\"\\t\")\n",
    "            # get ride of bad formated lines\n",
    "            if(len(read_data)==13 or len(read_data)==12):\n",
    "                paper_detail = {\"paperID\": read_data[0], \"authorID\":read_data[1]}\n",
    "                AllRecords_original.append(paper_detail)\n",
    "            else:\n",
    "                print(len(read_data))\n",
    "        f.close()\n",
    "    return pd.DataFrame(AllRecords_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T19:48:49.378771Z",
     "start_time": "2019-01-20T19:48:49.313011Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_embedding(all_embedding, wanted_pid_list):\n",
    "    extracted_emb = []\n",
    "    wanted_pid_list = wanted_pid_list.values.tolist()\n",
    "    wanted_pid_list = [int(x) for x in wanted_pid_list]\n",
    "    wanted_pid_list = list(sorted(set(wanted_pid_list)))\n",
    "    total_missing_sample = 0\n",
    "    # only if embedding exist\n",
    "    if len(all_embedding)>0:\n",
    "        # loop through wanted pid list to keep input order\n",
    "        for embedding in all_embedding:\n",
    "            if(len(wanted_pid_list)==0):\n",
    "                break\n",
    "            while (wanted_pid_list[0]<=int(embedding[0])):\n",
    "                if wanted_pid_list[0]==int(embedding[0]):\n",
    "                    extracted_emb.append(embedding)\n",
    "                    wanted_pid_list.remove(int(embedding[0]))\n",
    "                elif (wanted_pid_list[0]<int(embedding[0])):\n",
    "                    total_missing_sample+=1\n",
    "                    # ------------------------ fill it up with 0's -------------------------- #\n",
    "                    fill_na = [wanted_pid_list[0]]\n",
    "                    temp = [0] * (len(all_embedding[0])-1)\n",
    "                    final_filled_zero_emb = fill_na+temp\n",
    "                    extracted_emb.append(final_filled_zero_emb)\n",
    "                    # ----- or do nothing and remove those missing samples from dataset ----- #\n",
    "                    # remove paper that not in all dataset\n",
    "                    wanted_pid_list.remove(wanted_pid_list[0])\n",
    "                if len(wanted_pid_list)==0:\n",
    "                    break\n",
    "    print(\"Total missing sample: \", total_missing_sample)\n",
    "    extracted_emb = pd.DataFrame(extracted_emb)\n",
    "    return extracted_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T19:48:49.817538Z",
     "start_time": "2019-01-20T19:48:49.781700Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# collect unlabeled vectors\n",
    "def extract_unlabeled_embedding(allembedding, unlabeled_pid):\n",
    "    unlabeled_pid = [int(x) for x in unlabeled_pid]\n",
    "    unlabeled_pid = list(sorted(set(unlabeled_pid)))\n",
    "    wanted_embedding = []\n",
    "    for embedding in allembedding:\n",
    "        if(len(unlabeled_pid)==0):\n",
    "            break\n",
    "        while (unlabeled_pid[0]<=int(embedding[0])):\n",
    "            if unlabeled_pid[0]==int(embedding[0]):\n",
    "                wanted_embedding.append(embedding)\n",
    "                unlabeled_pid.remove(int(embedding[0]))\n",
    "            elif (unlabeled_pid[0]<int(embedding[0])):\n",
    "                # remove paper that not in all dataset\n",
    "                unlabeled_pid.remove(unlabeled_pid[0])\n",
    "            if len(unlabeled_pid)==0:\n",
    "                break\n",
    "    unlabeled_data = pd.DataFrame(wanted_embedding)\n",
    "    unlabeled_data['label'] = \"-1\"\n",
    "    unlabeled_data = unlabeled_data.rename(columns={0: 'paperID'})\n",
    "    return unlabeled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T19:48:50.349419Z",
     "start_time": "2019-01-20T19:48:50.327871Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some of the record doesn't have citation links, therefore we will have to remove those papers from train and test set\n",
    "# synchronize data wrt pid\n",
    "def synchro_views(labeled_dv1, labeled_dv2, unlabeled_data1, unlabeled_data2):\n",
    "    noCitationPids_labeled = set(labeled_dv1[0])-set(labeled_dv2[0])\n",
    "    print(\"labeled no citation link: \", len(noCitationPids_labeled))\n",
    "    noCitationPids_unlabeled = set(unlabeled_data1['paperID'])-set(unlabeled_data2['paperID'])\n",
    "    print(\"Unlabeled no citation link size: \", len(noCitationPids_unlabeled))\n",
    "    # process unlabeled data\n",
    "    unlabeled_dv1 = unlabeled_data1[~unlabeled_data1['paperID'].isin(noCitationPids_unlabeled)].reset_index(drop=True)\n",
    "    unlabeled_dv2 = unlabeled_data2\n",
    "    # process labeled data\n",
    "    labeled_dv1_final = labeled_dv1[~labeled_dv1[0].isin(noCitationPids_labeled)].reset_index(drop=True)\n",
    "    labeled_dv2_final = labeled_dv2\n",
    "    # since our input data are sorted, all data are in order with pid\n",
    "    return labeled_dv1_final, labeled_dv2_final, unlabeled_dv1, unlabeled_dv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T19:48:50.872921Z",
     "start_time": "2019-01-20T19:48:50.867545Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove author(positive sample) from other(negative sample)\n",
    "import random\n",
    "def extractNegativeSample(positiveSample, allSample):\n",
    "    negativeSample = [x for x in allSample if x not in positiveSample]\n",
    "    return negativeSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T19:48:52.635688Z",
     "start_time": "2019-01-20T19:48:52.508940Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_score, recall_score,f1_score,accuracy_score\n",
    "# cross validation\n",
    "def k_fold_cv_co_train_binary(dataview1, dataview2, unlabeled_dv1, unlabeled_dv2, label, clf, k=10):\n",
    "    kf = StratifiedKFold(n_splits=k)\n",
    "    allTrueLabel = []\n",
    "    allPredLabel = []\n",
    "    for train_index, test_index in kf.split(dataview1, label):\n",
    "        # print(\"TRAIN:\", train_index, \" \\n TEST:\", test_index)\n",
    "        # ---------------split train and test -------------------- #\n",
    "        dv1_train, dv1_test = dataview1.iloc[train_index], dataview1.iloc[test_index]\n",
    "        dv2_train, dv2_test = dataview2.iloc[train_index], dataview2.iloc[test_index]\n",
    "        _, label_test = label.iloc[train_index], label.iloc[test_index]\n",
    "        # -------------- add unlabeled to train ------------------ #\n",
    "        final_dv1 = pd.concat([dv1_train,unlabeled_dv1], ignore_index=True)\n",
    "        final_dv2 = pd.concat([dv2_train,unlabeled_dv2], ignore_index=True)\n",
    "        # ----------------extract label for training ---------------- #\n",
    "        label_train = final_dv1[\"label\"]\n",
    "        final_dv1.drop([\"label\", \"paperID\"], axis=1, inplace = True)\n",
    "        final_dv2.drop([\"label\", \"paperID\"], axis=1, inplace = True)\n",
    "        # -------------- train binary co-training ------------------- #\n",
    "        clf.fit(final_dv1, final_dv2, label_train)\n",
    "        \n",
    "        dv1_test.drop([\"label\", \"paperID\"], axis=1, inplace = True)\n",
    "        dv2_test.drop([\"label\", \"paperID\"], axis=1, inplace = True)\n",
    "        # get predicted label\n",
    "        co_lr_label_predict = clf.predict(dv1_test, dv2_test)\n",
    "        allTrueLabel.extend(label_test[\"label\"].values.tolist())\n",
    "        allPredLabel.extend(co_lr_label_predict)\n",
    "        # print(allTrueLabel)\n",
    "        # print(allPredLabel)\n",
    "        # print(\"True positive: {tp}, False positive: {fp}, False negative: {fn}, True negative: {tn}\"\n",
    "        # .format(tp=round_tp, fp=round_fp, fn=round_fn, tn=round_tn))\n",
    "        break\n",
    "\n",
    "    accuracy = accuracy_score(allTrueLabel, allPredLabel)\n",
    "    f1 = f1_score(allTrueLabel, allPredLabel,average='macro')\n",
    "    \n",
    "    print(metrics.classification_report(allTrueLabel, allPredLabel))\n",
    "    print(metrics.confusion_matrix(allTrueLabel, allPredLabel).ravel())\n",
    "    \n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T01:18:52.683135Z",
     "start_time": "2019-01-21T01:18:50.092739Z"
    },
    "code_folding": [
     7,
     63,
     71,
     77
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "# create co training classifier\n",
    "class Co_training_clf(object):\n",
    "    \n",
    "    import copy\n",
    "    \n",
    "    def __init__(self, clf1, clf2=None, max_k=30, p=1, n=1, u = 75):\n",
    "        \n",
    "        self.clf1 = clf1\n",
    "        # assume co_training on one classifier\n",
    "        if clf2 == None:\n",
    "            self.clf2 = self.copy.copy(clf1)\n",
    "        else:\n",
    "            self.clf2 = clf2\n",
    "        # number of iteration\n",
    "        self.max_k = max_k\n",
    "        # take p example from most confidently positive labels to example\n",
    "        self.p = p\n",
    "        # take n example from most confidently negative label to example\n",
    "        self.n = n\n",
    "        # size of pool of unlabeled samples\n",
    "        self.u = u\n",
    "        # debug\n",
    "        self.verbose = 0\n",
    "        \n",
    "        \n",
    "    def label_p_n_samples(self, dv1_proba, dv2_proba, rank):\n",
    "        '''\n",
    "        Only label it if consistent with result from two classifier\n",
    "        '''\n",
    "        U_prime_size = len(dv1_proba)\n",
    "        p, n = [], []\n",
    "        for label, conf_measure in enumerate(rank):\n",
    "            # 0 positive sample\n",
    "            if label==0:\n",
    "                index = 0\n",
    "                while(len(p) <self.p):\n",
    "                    max_conf_sample_index = conf_measure[index]\n",
    "                    dv1_result = np.argmax(dv1_proba[max_conf_sample_index])\n",
    "                    dv2_result = np.argmax(dv2_proba[max_conf_sample_index])\n",
    "                    positive_confidence = dv1_proba[max_conf_sample_index][0] * dv2_proba[max_conf_sample_index][0]\n",
    "                    if (dv1_result == dv2_result) and (positive_confidence>=0.5):\n",
    "                        if self.verbose:\n",
    "                            print(positive_confidence)\n",
    "                            print(dv1_proba[max_conf_sample_index])\n",
    "                            print(dv2_proba[max_conf_sample_index])\n",
    "                        p.append(max_conf_sample_index)\n",
    "                    index +=1\n",
    "                    if (index>=U_prime_size):\n",
    "                        break\n",
    "                    if self.verbose:\n",
    "                        print(\"positive idx \", index)\n",
    "                    \n",
    "            # 1 negative sample\n",
    "            elif label == 1:\n",
    "                index = 0\n",
    "                while(len(n) <self.n):\n",
    "                    max_conf_sample_index = conf_measure[index]\n",
    "                    dv1_result = np.argmax(dv1_proba[max_conf_sample_index])\n",
    "                    dv2_result = np.argmax(dv2_proba[max_conf_sample_index])\n",
    "                    negative_confidence = dv1_proba[max_conf_sample_index][1] * dv2_proba[max_conf_sample_index][1]\n",
    "                    if(dv1_result == dv2_result) and (negative_confidence>=0.5):\n",
    "                        if self.verbose:\n",
    "                            print(negative_confidence)\n",
    "                            print(dv1_proba[max_conf_sample_index])\n",
    "                            print(dv2_proba[max_conf_sample_index])\n",
    "                        n.append(max_conf_sample_index)\n",
    "                    index +=1\n",
    "                    if (index>=U_prime_size):\n",
    "                        break\n",
    "                    if self.verbose: \n",
    "                        print(\"negative idx \", index)\n",
    "            else:\n",
    "                print(\"Class label error\")\n",
    "        return p, n\n",
    "    \n",
    "    def init_L_U_U_prime(self, labels):\n",
    "        # index of the samples that are initially labeled\n",
    "        L = [i for i, label_i in enumerate(labels) if label_i != -1]\n",
    "        # index of unlabeled samples\n",
    "        U = [i for i, label_i in enumerate(labels) if label_i == -1]\n",
    "        print(\"Initial L size: \", len(L))\n",
    "        print(\"Initial U size: \", len(U))\n",
    "        # random drawing sample from U\n",
    "        U_prime = random.sample(U, min(len(U), self.u))\n",
    "        # remove the samples in U_prime from U\n",
    "        U = [x for x in U if x not in U_prime]\n",
    "        return L, U, U_prime\n",
    "        \n",
    "        \n",
    "    def fit(self, dataView1, dataView2, labels):\n",
    "        \n",
    "        labels = np.asarray(labels, dtype='int32')\n",
    "        print(\"P: \", self.p, \" N: \", self.n)\n",
    "        L, U, U_prime = self.init_L_U_U_prime(labels)\n",
    "        print(\"U: \", len(U))\n",
    "        print(\"U_prime: \", len(U_prime))\n",
    "        #----------- auto estimate number of iteration should run -------- #\n",
    "        pos_sample_num = np.count_nonzero(labels==0)\n",
    "        neg_sample_num = np.count_nonzero(labels==1)\n",
    "        n_p_ratio = int(neg_sample_num/pos_sample_num)\n",
    "        print(pos_sample_num)\n",
    "        print(neg_sample_num)\n",
    "        print(n_p_ratio)\n",
    "        label_sample_size = len(L)\n",
    "        \n",
    "        iterCount = 0\n",
    "        #loop until we have assigned labels to every sample in U and U_prime or we hit our iteration break condition\n",
    "        while iterCount <= self.max_k and U_prime:\n",
    "            iterCount +=1\n",
    "            if self.verbose:\n",
    "                print(\"step\",iterCount, \" L: \",L)\n",
    "                print(\"step\",iterCount, \" U_prime: \",U_prime)\n",
    "            iter_train_d1 = dataView1.iloc[L]\n",
    "            iter_train_d2= dataView2.iloc[L]\n",
    "            iter_train_label = labels[L]\n",
    "            print(iter_train_label.shape)\n",
    "            self.clf1.fit(iter_train_d1, iter_train_label)\n",
    "            self.clf2.fit(iter_train_d2, iter_train_label)\n",
    "            \n",
    "            iter_labeling_d1 = dataView1.iloc[U_prime]\n",
    "            iter_labeling_d2 = dataView2.iloc[U_prime]\n",
    "            \n",
    "            # ---------- 1. rank class probabilities for unlabeled sample for it's confidence measure ---- #\n",
    "            dv1_proba = self.clf1.predict_proba(iter_labeling_d1)\n",
    "            dv2_proba = self.clf2.predict_proba(iter_labeling_d2)\n",
    "            dv1_proba_rank = []\n",
    "            dv2_proba_rank = []\n",
    "            # proba1[i] is label i's confidence measure, rank is index of sample\n",
    "            for class_proba in dv1_proba.T:\n",
    "                dv1_proba_rank.append((-class_proba).argsort())\n",
    "            for class_proba in dv2_proba.T:\n",
    "                dv2_proba_rank.append((-class_proba).argsort())\n",
    "            if self.verbose:\n",
    "                print(dv1_proba)\n",
    "                print(dv1_proba_rank)\n",
    "                print(dv2_proba)\n",
    "                print(dv2_proba_rank)\n",
    "            # ----2. use probability to have p, n new label samples (result must consistent with 2 classifier) --- #\n",
    "            #h1 classifier\n",
    "            p1,n1 = self.label_p_n_samples(dv1_proba, dv2_proba, dv1_proba_rank)\n",
    "            # h2 classifier\n",
    "            p2,n2 = self.label_p_n_samples(dv1_proba, dv2_proba, dv2_proba_rank)\n",
    "            finalP = set(p1+p2)\n",
    "            finalN = set(n1+n2)\n",
    "            print(\"Final p: \", len(finalP), \" Final n: \", len(finalN))\n",
    "                \n",
    "            # ------------ 4. if U_prime not produce new positive or negative sample, resample from U ------ #\n",
    "            if (len(finalP) ==0) or (len(finalN) ==0):\n",
    "                # random drawing sample from U\n",
    "                U_prime = random.sample(U, min(len(U), self.u))\n",
    "                # remove the samples in U_prime from U\n",
    "                U = [x for x in U if x not in U_prime]\n",
    "            # --------------------------- else add new sample to training set ----------------------------- #\n",
    "            else:\n",
    "                if self.verbose:\n",
    "                    print(\"P: \", finalP, \" N: \", finalN)\n",
    "                # auto label the samples and remove it from U_prime\n",
    "                auto_labeled_pos = [U_prime[x] for x in finalP]\n",
    "                auto_labeled_neg = [U_prime[x] for x in finalN]\n",
    "                auto_labeled_samples = auto_labeled_pos+auto_labeled_neg\n",
    "                labels[auto_labeled_pos] = 0\n",
    "                labels[auto_labeled_neg] = 1\n",
    "                # extend the labeled sample\n",
    "                L.extend(auto_labeled_pos)\n",
    "                L.extend(auto_labeled_neg)\n",
    "                # remove the labeled sample from U_prime\n",
    "                U_prime = [x for x in U_prime if x not in auto_labeled_samples]\n",
    "                if self.verbose:\n",
    "                    print(U_prime)\n",
    "                # randomly choice 2p+2n examples from u to replenish u_prime\n",
    "                replenishItem = U[-(2*self.p+2*self.n):]\n",
    "                U_prime.extend(replenishItem)\n",
    "                U = U[:-len(replenishItem)]\n",
    "                print(\"U: \", len(U))\n",
    "                print(\"U_prime: \", len(U_prime))\n",
    "        print(\"Total Labeled number: \", len(L), \" Still unlabeled number: \", len(U_prime))\n",
    "        # final train\n",
    "        newtrain_d1 = dataView1.iloc[L]\n",
    "        newtrain_d2 = dataView2.iloc[L]\n",
    "        self.clf1.fit(newtrain_d1, labels[L])\n",
    "        self.clf2.fit(newtrain_d2, labels[L])\n",
    "    \n",
    "    def supports_proba(self, clf, x):\n",
    "        try:\n",
    "            clf.predict_proba([x])\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "        \n",
    "    def predict(self, dataView1, dataView2):\n",
    "        y1 = self.clf1.predict(dataView1)\n",
    "        y2 = self.clf2.predict(dataView2)\n",
    "        proba_supported = self.supports_proba(self.clf1, dataView1.iloc[0]) and self.supports_proba(self.clf2, dataView2.iloc[0])\n",
    "        #fill pred with -1 so we can identify the samples in which sample classifiers failed to agree\n",
    "        y_pred = np.asarray([-1] * dataView1.shape[0])\n",
    "        for i, (y1_i, y2_i) in enumerate(zip(y1, y2)):\n",
    "            # if both agree on label\n",
    "            if y1_i == y2_i:\n",
    "                y_pred[i] = y1_i\n",
    "            # if disagree on label, times probability together, choice the class have higher probabilities\n",
    "            elif proba_supported:\n",
    "                y1_probas = self.clf1.predict_proba([dataView1.iloc[i]])[0]\n",
    "                y2_probas = self.clf2.predict_proba([dataView2.iloc[i]])[0]\n",
    "                print(\"y1 disagree on\",i, \" Proba: \",y1_probas)\n",
    "                print(\"y2 not aggreed on \",i, \"Proba: \", y2_probas)\n",
    "                prod_y_probas = [proba_y1 * proba_y2 for (proba_y1, proba_y2) in zip(y1_probas, y2_probas)]\n",
    "                print(\"product probas:\",prod_y_probas)\n",
    "                y_pred[i] = prod_y_probas.index(max(prod_y_probas))\n",
    "                print(\"result\",y_pred[i])\n",
    "            else:\n",
    "                #the classifiers disagree and don't support probability, so we guess\n",
    "                warnings.warn(\"classifiers disagree with label, result may not accurate\")\n",
    "                print(\"sample at: \", i, \" c1: \", y1_i, \" c2: \", y2_i)\n",
    "                y_pred[i] = random.randint(0, 1)\n",
    "        #check if predict works\n",
    "        assert not (-1 in y_pred)\n",
    "        return y_pred\n",
    "    \n",
    "    def predict_proba(self, dataView1, dataView2):\n",
    "        # the predicted probabilities is simply a product (*) of probabilities given from each classifier trained\n",
    "        y1_probas = self.clf1.predict_proba(dataView1)\n",
    "        y2_probas = self.clf2.predict_proba(dataView2)\n",
    "        \n",
    "        proba = (y1_probas*y2_probas)\n",
    "        return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T01:20:47.062490Z",
     "start_time": "2019-01-21T01:18:52.685795Z"
    },
    "code_folding": [
     32
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "# loop through all files in directory add name to name list\n",
    "fileDir = \"../../Data/\"+Dataset+\"/canopies/\"\n",
    "listfiles = os.listdir(fileDir)\n",
    "\n",
    "co_lr_diff_embedding_result = []\n",
    "\n",
    "# # ------------ view two citation is fix, so move out to save time ------- #\n",
    "# # read viewtwo embedding\n",
    "# print(\"Load citation embedding: \", pp_citation)\n",
    "# viewtwo_citation_embedding = com_func.read_all_citation_embedding_sorted(emb_type = pp_citation)\n",
    "\n",
    "#---------------- load different embeddings for view one ---------------#\n",
    "for select_emb in pp_textual:\n",
    "    print(\"Load textual embedding: \", select_emb)\n",
    "#     # read viewone embeddings\n",
    "#     viewone_textual_emb = com_func.read_all_textual_embedding_sorted(emb_type=select_emb, training_size = \"3m\")\n",
    "    \n",
    "#     print(viewone_textual_emb[0])\n",
    "#     print(viewtwo_citation_embedding[0])\n",
    "    \n",
    "    threshold_change_all_co_lr_f1s = []\n",
    "    threshold_change = []\n",
    "    \n",
    "    # -------------- different threshold (step by 10) -----------------------#\n",
    "    for step_threshold in range(threshold_lower, threshold_upper, 10):\n",
    "        threshold_change.append(step_threshold)\n",
    "        # collect statistic to output\n",
    "        allname, positive_sample_size, negative_sample_size  = ([] for i in range(3))\n",
    "        all_labeled_count, unlabeled_count = ([] for i in range(2))\n",
    "\n",
    "        all_co_LR_accuracy, all_co_LR_f1 = ([] for i in range(2))\n",
    "\n",
    "        total_selected_group = 0\n",
    "\n",
    "        # ------- different name group in all name group --------------------#\n",
    "        for file in listfiles:\n",
    "            # group name\n",
    "            temp = file.split(\"_\")\n",
    "            name = temp[1]+\"_\"+temp[-1]\n",
    "            print(\"For name: \",name)\n",
    "            # read pid and aid from file\n",
    "            data = read_file(fileDir+file)\n",
    "            labeled_mask = data[\"authorID\"] != \"-1\"\n",
    "            labeled_data = data[labeled_mask]\n",
    "            unlabeled_mask = data[\"authorID\"] == \"-1\"\n",
    "            ublabeled_data = data[unlabeled_mask]\n",
    "            unlabeled_pid = ublabeled_data[\"paperID\"].tolist()\n",
    "            print(labeled_data.shape)\n",
    "            # ---------------- collect all labeled sample -------------------- #\n",
    "            # ---------------- if use all samples as negative --------------- #\n",
    "            all_labeled_samples = labeled_data[\"paperID\"].tolist()\n",
    "            authorCounter = com_func.select_productive_groups(labeled_data, threshold_select_name_group)\n",
    "            # if only have one class or no class pass the threshold, not applicable\n",
    "            if(len(authorCounter)==0) or (len(authorCounter)==1):\n",
    "                print(name,\" pass\")\n",
    "            else:\n",
    "                total_selected_group+= 1\n",
    "                # --------------for each name group---------------- #\n",
    "                if apply_threshold_to_sample == True:\n",
    "                    # ---------- only use sample pass threshold ------- #\n",
    "                    #-------- only select authors in name group are very productive (more than threshold)---------#\n",
    "                    labeled_data, author_list, _= com_func.only_select_productive_authors(labeled_data, step_threshold)\n",
    "                    # ----------------- if use filtered samples as negative  --------- #\n",
    "                    filtered_all_labeled_samples = labeled_data[\"paperID\"].tolist()\n",
    "                else:\n",
    "                    # ----------- use all sample in name group --------- #\n",
    "                    author_list = com_func.productive_authors_list(labeled_data, step_threshold)\n",
    "                    print(name, \" name group sample size: \",labeled_data.shape)\n",
    "                    \n",
    "                # -------------- extract all samples for name group -------------- #\n",
    "                # for each name group\n",
    "                # read in labeled data\n",
    "                labeled_viewone_textual = extract_embedding(viewone_textual_emb, labeled_data[\"paperID\"])\n",
    "                print(labeled_viewone_textual.shape)\n",
    "                labeled_viewtwo_citation = extract_embedding(viewtwo_citation_embedding, labeled_data[\"paperID\"])\n",
    "                print(labeled_viewtwo_citation.shape)\n",
    "                print(\"Labeled: \",len(labeled_viewone_textual), \" : \", len(labeled_viewtwo_citation))\n",
    "\n",
    "                # read in unlabeled data\n",
    "                unlabeled_viewone_textual = extract_unlabeled_embedding(viewone_textual_emb, unlabeled_pid)\n",
    "                print(unlabeled_viewone_textual.shape)\n",
    "                unlabeled_viewtwo_citation = extract_unlabeled_embedding(viewtwo_citation_embedding, unlabeled_pid)\n",
    "                print(unlabeled_viewtwo_citation.shape)\n",
    "                print(\"Unlabeled: \",len(unlabeled_viewone_textual), \" : \", len(unlabeled_viewtwo_citation))\n",
    "                \n",
    "                # remove samples that have no citation link from ublabeled data\n",
    "                noCitationPids_unlabeled = set(unlabeled_viewone_textual['paperID'])-set(unlabeled_viewtwo_citation['paperID'])\n",
    "                print(\"Unlabeled no citation link size: \", len(noCitationPids_unlabeled))\n",
    "                # process unlabeled data\n",
    "                unlabeled_dv1 = unlabeled_viewone_textual[~unlabeled_viewone_textual['paperID'].isin(noCitationPids_unlabeled)].reset_index(drop=True)\n",
    "                unlabeled_dv2 = unlabeled_viewtwo_citation\n",
    "                \n",
    "                # ---------------- shuffle the data ----------------- #\n",
    "                labeled_data = labeled_data.sample(frac=1).reset_index(drop=True)\n",
    "                # ------------------ alignment ---------------------- #\n",
    "                labeled_viewone_textual = pd.merge(labeled_data, labeled_viewone_textual, left_on=\"paperID\", right_on = [0], how = \"left\")\n",
    "                labeled_viewtwo_citation = pd.merge(labeled_data, labeled_viewtwo_citation, left_on=\"paperID\", right_on = [0], how = \"left\")\n",
    "                labeled_viewtwo_citation.fillna(0, inplace=True)\n",
    "                \n",
    "                print(labeled_viewone_textual.shape)\n",
    "                print(labeled_viewtwo_citation.shape)\n",
    "                print(unlabeled_dv1.shape)\n",
    "                print(unlabeled_dv2.shape)\n",
    "                counter = 0\n",
    "                # loop through each author\n",
    "                for author in author_list:\n",
    "                    all_labeled_count.append(len(labeled_data))\n",
    "                    unlabeled_count.append(len(unlabeled_dv1))\n",
    "                    author_name = name+'_'+str(counter)\n",
    "                    allname.append(author_name)\n",
    "                    print(author_name, \" : \", author)\n",
    "                    mask = labeled_data[\"authorID\"] == author\n",
    "                    temp = labeled_data[mask]\n",
    "                    positive_sample_pid = temp[\"paperID\"].tolist()\n",
    "                    negative_sample_pid = extractNegativeSample(positive_sample_pid, filtered_all_labeled_samples)\n",
    "                    \n",
    "                    # save number of positive and negative samples\n",
    "                    positive_sample_size.append(len(positive_sample_pid))\n",
    "                    negative_sample_size.append(len(negative_sample_pid))\n",
    "                    \n",
    "                    # ----------------- generate binary label ------------------ #\n",
    "                    # form positive and negative (negative class come from similar name group)\n",
    "                    all_authors = []\n",
    "                    all_authors.append(positive_sample_pid)\n",
    "                    all_authors.append(negative_sample_pid)\n",
    "                    appended_data = []\n",
    "                    for label, pid in enumerate(all_authors):\n",
    "                        # create df save one author data \n",
    "                        authordf = pd.DataFrame({\"paperID\":pid})\n",
    "                        authordf['label'] = label\n",
    "                        appended_data.append(authordf)\n",
    "                    label_pid = pd.concat(appended_data, axis=0,ignore_index=True)\n",
    "                    # ----------- alignment of label with input data ------------ #\n",
    "                    label_pid = pd.merge(labeled_viewone_textual[\"paperID\"].to_frame(), label_pid, on = \"paperID\")\n",
    "                    #------------- process data for k-fold cv ------------------- #\n",
    "                    # throw away some column for labeled data\n",
    "                    labeled_dv1 = labeled_viewone_textual.drop([\"authorID\", 0], axis=1)\n",
    "                    labeled_dv2 = labeled_viewtwo_citation.drop([\"authorID\", 0], axis=1)\n",
    "                    # merge label into data\n",
    "                    labeled_dv1 = pd.merge(labeled_dv1, label_pid, on = \"paperID\")\n",
    "                    labeled_dv2 = pd.merge(labeled_dv2, label_pid, on = \"paperID\")\n",
    "                    label = label_pid.drop([\"paperID\"], axis=1)\n",
    "                    # ----------- check the final inputs------------------ #\n",
    "#                     print(labeled_dv1.head())\n",
    "#                     print(unlabeled_dv1.head())\n",
    "                    # ------------ fit co-training model with k-fold ------------------------ #\n",
    "                    co_logistic_clf = Co_training_clf(clf1=LogisticRegression(),p=1,n=1)\n",
    "                    co_lr_accuracy, co_lr_f1 = k_fold_cv_co_train_binary(labeled_dv1, labeled_dv2, unlabeled_dv1, unlabeled_dv2,\n",
    "                                                                  label, co_logistic_clf, 10)\n",
    "                    \n",
    "                    all_co_LR_accuracy.append(co_lr_accuracy)\n",
    "                    all_co_LR_f1.append(co_lr_f1)\n",
    "                    counter+=1\n",
    "                    \n",
    "                    break\n",
    "                break\n",
    "                \n",
    "#         # write evaluation result to excel\n",
    "#         output = pd.DataFrame({'Author Name':allname, \"positive sample size\":positive_sample_size,\"negative sample size\":negative_sample_size, \n",
    "#                                \"labeled sample size\": all_labeled_count, \"unlabeled sample size\": unlabeled_count, \n",
    "#                                \"co_logisticRegression Accuracy\":all_co_LR_accuracy, \"co_logisticRegression F1\": all_co_LR_f1})\n",
    "#         savePath = \"../../result/\"+Dataset+\"/co_train_binary_advanced/\"\n",
    "#         filename = \"(Global emb sample 3m) viewone_textual=\"+select_emb+\"_viewtwo_citation=\"+pp_citation+\"_threshold=\"+str(step_threshold)+\"_namegroupcount=\"+str(total_selected_group)+\".csv\"\n",
    "#         com_func.write_csv_df(savePath, filename, output)\n",
    "#         print(\"Done\")\n",
    "        \n",
    "#         threshold_change_all_co_lr_f1s.append(all_co_LR_f1)\n",
    "        \n",
    "#     co_lr_diff_embedding_result.append(threshold_change_all_co_lr_f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T21:38:24.086059Z",
     "start_time": "2019-01-20T21:38:24.081807Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(positive_sample_pid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-18T17:21:21.818Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from statistics import mean \n",
    "\n",
    "print(threshold_change_all_co_lr_f1s)\n",
    "print(co_lr_diff_embedding_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %whos\n",
    "del v1_all_features\n",
    "del v2_all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
