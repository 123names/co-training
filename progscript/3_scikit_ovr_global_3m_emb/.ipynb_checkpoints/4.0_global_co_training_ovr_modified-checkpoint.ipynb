{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T17:27:34.612924Z",
     "start_time": "2019-01-18T17:27:26.141778Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import com_func\n",
    "# parameters\n",
    "#----- threshold for selecting set of name group -----------#\n",
    "threshold_select_name_group = 100\n",
    "#----- threshold for selecting min sample in name group ----#\n",
    "threshold_lower = 10\n",
    "threshold_upper = 50\n",
    "\n",
    "pp_textual = [\"pv_dbow\"]\n",
    "# pp_textual = [\"lsa\", \"pv_dm\", \"pv_dbow\"]\n",
    "pp_citation = \"n2v\"\n",
    "\n",
    "Dataset = \"pubmed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T17:27:36.156326Z",
     "start_time": "2019-01-18T17:27:36.058288Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gensim\n",
    "# read trained rec to rec textual graph\n",
    "def read_textual_embedding(Dataset = \"pubmed\", emb_type = \"off\"):\n",
    "    textual_emb = []\n",
    "    while True:\n",
    "        if emb_type == \"lsa\":\n",
    "            modelSaveDir = \"../../Data/\"+Dataset+\"/models/lsa/textual_sample=3m/\"\n",
    "            with open(modelSaveDir+'lsa_Matrix.pickle', \"rb\") as input_file:\n",
    "                vec = pickle.load(input_file)\n",
    "            with open(modelSaveDir+'feature_pid.pickle', \"rb\") as input_file:\n",
    "                allPaperid = pickle.load(input_file)\n",
    "            allPaperid = np.array(allPaperid)\n",
    "            textual_emb = np.column_stack((allPaperid,vec))\n",
    "            break\n",
    "        elif emb_type == \"pv_dm\":\n",
    "            loadDir = \"../../Data/\"+Dataset+\"/vectors/d2v/textual_sample=3m/Doc2Vec(dbow,d100,n5,mc3,s0.001,t24).txt\"\n",
    "            with open(loadDir, 'r', encoding = 'utf8') as f:\n",
    "                for line in f:\n",
    "                    read_data = line.split(\" \")\n",
    "                    paper_Vectors = read_data\n",
    "                    textual_emb.append(paper_Vectors)\n",
    "            f.close()\n",
    "            break\n",
    "        elif emb_type == \"pv_dbow\":\n",
    "            loadDir = \"../../Data/\"+Dataset+\"/vectors/d2v/textual_sample=3m/Doc2Vec(dmm,d100,n5,w5,mc3,s0.001,t24).txt\"\n",
    "            with open(loadDir, 'r', encoding = 'utf8') as f:\n",
    "                for line in f:\n",
    "                    read_data = line.split(\" \")\n",
    "                    paper_Vectors = read_data\n",
    "                    textual_emb.append(paper_Vectors)\n",
    "            f.close()\n",
    "            break\n",
    "        elif emb_type == \"off\":\n",
    "            break\n",
    "        else:\n",
    "            print(\"Embedding type not available, selecting default setting\")\n",
    "            emb_type=\"off\"\n",
    "    print(\"Total textual vector records:\",len(textual_emb))\n",
    "    print(\"Vector dimension: \", len(textual_emb[0]))\n",
    "    return textual_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T17:27:36.736091Z",
     "start_time": "2019-01-18T17:27:36.706957Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read trained rec to rec node2vec citation graph\n",
    "def read_citation_embedding(Dataset = \"pubmed\", emb_type = \"off\"):\n",
    "    citation_emb = []\n",
    "    while True:\n",
    "        if emb_type == \"n2v\":\n",
    "            citation_emb_dir = \"../../Data/\"+Dataset+\"/vectors/\"+emb_type+\"/n2v.txt\"\n",
    "            with open(citation_emb_dir, 'r', encoding = 'utf8') as f:\n",
    "                for line in f:\n",
    "                    read_data = line.split(\" \")\n",
    "                    if(len(read_data)==101):\n",
    "                        citation_emb.append(read_data)\n",
    "            f.close()\n",
    "            break\n",
    "        elif emb_type == \"off\":\n",
    "            break\n",
    "        else:\n",
    "            print(\"Embedding type not available, selecting default setting\")\n",
    "            emb_type=\"off\"\n",
    "    print(\"Total citation vector records:\",len(citation_emb))\n",
    "    print(\"Vector dimension: \", len(citation_emb[0]))\n",
    "    return citation_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T17:27:37.246594Z",
     "start_time": "2019-01-18T17:27:37.203735Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_embedding(all_embedding, wanted_pid_list):\n",
    "    extracted_emb = []\n",
    "    wanted_pid_list = wanted_pid_list.values.tolist()\n",
    "    wanted_pid_list = [int(x) for x in wanted_pid_list]\n",
    "    wanted_pid_list = list(sorted(set(wanted_pid_list)))\n",
    "    total_missing_sample = 0\n",
    "    # only if embedding exist\n",
    "    if len(all_embedding)>0:\n",
    "        # loop through wanted pid list to keep input order\n",
    "        for embedding in all_embedding:\n",
    "            if(len(wanted_pid_list)==0):\n",
    "                break\n",
    "            while (wanted_pid_list[0]<=int(embedding[0])):\n",
    "                if wanted_pid_list[0]==int(embedding[0]):\n",
    "                    extracted_emb.append(embedding)\n",
    "                    wanted_pid_list.remove(int(embedding[0]))\n",
    "                elif (wanted_pid_list[0]<int(embedding[0])):\n",
    "                    total_missing_sample+=1\n",
    "                    # remove paper that not in all dataset\n",
    "                    wanted_pid_list.remove(wanted_pid_list[0])\n",
    "                if len(wanted_pid_list)==0:\n",
    "                    break\n",
    "    print(\"Total missing sample: \", total_missing_sample)\n",
    "    extracted_emb = pd.DataFrame(extracted_emb)\n",
    "    return extracted_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T17:27:37.661639Z",
     "start_time": "2019-01-18T17:27:37.626577Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# collect unlabeled vectors\n",
    "def extract_unlabeled_embedding(allembedding, unlabeled_pid):\n",
    "    unlabeled_pid = [int(x) for x in unlabeled_pid]\n",
    "    unlabeled_pid = list(sorted(set(unlabeled_pid)))\n",
    "    wanted_embedding = []\n",
    "    for embedding in allembedding:\n",
    "        if(len(unlabeled_pid)==0):\n",
    "            break\n",
    "        while (unlabeled_pid[0]<=int(embedding[0])):\n",
    "            if unlabeled_pid[0]==int(embedding[0]):\n",
    "                wanted_embedding.append(embedding)\n",
    "                unlabeled_pid.remove(int(embedding[0]))\n",
    "            elif (unlabeled_pid[0]<int(embedding[0])):\n",
    "                # remove paper that not in all dataset\n",
    "                unlabeled_pid.remove(unlabeled_pid[0])\n",
    "            if len(unlabeled_pid)==0:\n",
    "                break\n",
    "    unlabeled_data = pd.DataFrame(wanted_embedding)\n",
    "    unlabeled_data['authorID'] = \"-1\"\n",
    "    unlabeled_data = unlabeled_data.rename(columns={0: 'paperID'})\n",
    "    return unlabeled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T17:27:38.033654Z",
     "start_time": "2019-01-18T17:27:38.016211Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_file(infile):\n",
    "    AllRecords_original = []\n",
    "    with open(infile, 'r', encoding = 'utf8') as f:\n",
    "        for line in f:\n",
    "            read_data = line.split(\"\\t\")\n",
    "            # get ride of bad formated lines\n",
    "            if(len(read_data)==13 or len(read_data)==12):\n",
    "                paper_detail = {\"paperID\": read_data[0], \"authorID\":read_data[1]}\n",
    "                AllRecords_original.append(paper_detail)\n",
    "            else:\n",
    "                print(len(read_data))\n",
    "        f.close()\n",
    "    return pd.DataFrame(AllRecords_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T17:27:38.491175Z",
     "start_time": "2019-01-18T17:27:38.485774Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove author(positive sample) from other(negative sample)\n",
    "import random\n",
    "def extractNegativeSample(positiveSample, allSample):\n",
    "    negativeSample = [x for x in allSample if x not in positiveSample]\n",
    "    return negativeSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T17:27:38.956207Z",
     "start_time": "2019-01-18T17:27:38.933893Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some of the record doesn't have citation links, therefore we will have to remove those papers from train and test set\n",
    "# synchronize data wrt pid\n",
    "def synchro_views(labeled_dv1, labeled_dv2, unlabeled_data1, unlabeled_data2):\n",
    "    noCitationPids_labeled = set(labeled_dv1[0])-set(labeled_dv2[0])\n",
    "    print(\"labeled no citation link: \", len(noCitationPids_labeled))\n",
    "    noCitationPids_unlabeled = set(unlabeled_data1[\"paperID\"])-set(unlabeled_data2[\"paperID\"])\n",
    "    print(\"Unlabeled no citation link size: \", len(noCitationPids_unlabeled))\n",
    "    # process unlabeled data\n",
    "    unlabeled_dv1 = unlabeled_data1[~unlabeled_data1[\"paperID\"].isin(noCitationPids_unlabeled)].reset_index(drop=True)\n",
    "    unlabeled_dv2 = unlabeled_data2\n",
    "    # process labeled data\n",
    "    labeled_dv1_final = labeled_dv1[~labeled_dv1[0].isin(noCitationPids_labeled)].reset_index(drop=True)\n",
    "    labeled_dv2_final = labeled_dv2.reset_index(drop=True)\n",
    "    # since our input data are sorted, all data are in order with pid\n",
    "    return labeled_dv1_final, labeled_dv2_final, unlabeled_dv1, unlabeled_dv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T17:27:41.355546Z",
     "start_time": "2019-01-18T17:27:39.748333Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "# create co training classifier\n",
    "class Co_training_clf(object):\n",
    "    \n",
    "    import copy\n",
    "    \n",
    "    def __init__(self, clf1, clf2=None, p=1, n=1, k=30, u = 75):\n",
    "        \n",
    "        self.clf1 = clf1\n",
    "        # assume co_training on one classifier\n",
    "        if clf2 == None:\n",
    "            self.clf2 = self.copy.copy(clf1)\n",
    "        else:\n",
    "            self.clf2 = clf2\n",
    "        # take p example from most confidently positive labels to example\n",
    "        self.p = p\n",
    "        # take n example from most confidently negative label to example\n",
    "        self.n = n\n",
    "        # number of iteration\n",
    "        self.k = k\n",
    "        # size of pool of unlabeled samples\n",
    "        self.u = u\n",
    "        \n",
    "    def label_p_n_samples(self, rank):\n",
    "        p, n = [], []\n",
    "        for label, conf_measure in enumerate(rank):\n",
    "            # 0 positive sample\n",
    "            if label==0:\n",
    "                index = 0\n",
    "                while(len(p) < self.p):\n",
    "                    p.append(conf_measure[index])\n",
    "                    index +=1\n",
    "            # 1 negative sample\n",
    "            elif label == 1:\n",
    "                index = 0\n",
    "                while(len(n) < self.n):\n",
    "                    n.append(conf_measure[index])\n",
    "                    index +=1\n",
    "            else:\n",
    "                print(\"Class label error\")\n",
    "        return p, n\n",
    "        \n",
    "    def fit(self, dataView1, dataView2, labels):\n",
    "        \n",
    "        labels = np.asarray(labels, dtype='int32')\n",
    "        print(\"P: \", self.p, \" N: \", self.n)\n",
    "        assert(self.p > 0 and self.n > 0 and self.k > 0 and self.u > 0)\n",
    "        \n",
    "        # index of the samples that are initially labeled\n",
    "        L = [i for i, label_i in enumerate(labels) if label_i != -1]\n",
    "        # index of unlabeled samples\n",
    "        U = [i for i, label_i in enumerate(labels) if label_i == -1]\n",
    "        print(\"Initial L size: \", len(L))\n",
    "        print(\"Initial U size: \", len(U))\n",
    "        # random drawing sample from U\n",
    "        random.shuffle(U)\n",
    "        U_prime = U[-min(len(U), self.u):]\n",
    "        # remove the samples in U_prime from U\n",
    "        U = U[:-len(U_prime)]\n",
    "        iterCount = 0\n",
    "        #loop until we have assigned labels to every sample in U and U_prime or we hit our iteration break condition\n",
    "        while iterCount <= self.k and U_prime:\n",
    "            iterCount +=1\n",
    "#             print(\"step\",iterCount, \" L: \",L)\n",
    "#             print(\"step\",iterCount, \" U_prime: \",U_prime)\n",
    "            iter_train_d1 = dataView1.iloc[L]\n",
    "            iter_train_d2= dataView2.iloc[L]\n",
    "            iter_train_label = labels[L]\n",
    "#             print(iter_train_label.shape)\n",
    "            self.clf1.fit(iter_train_d1, iter_train_label.ravel())\n",
    "            self.clf2.fit(iter_train_d2, iter_train_label.ravel())\n",
    "            \n",
    "            iter_labeling_d1 = dataView1.iloc[U_prime]\n",
    "            iter_labeling_d2 = dataView2.iloc[U_prime]\n",
    "            # rank class probabilities for unlabeled sample for it's confidence measure\n",
    "            dv1_proba = self.clf1.predict_proba(iter_labeling_d1)\n",
    "            dv2_proba = self.clf2.predict_proba(iter_labeling_d2)\n",
    "            # make prediction on data\n",
    "#             y1 = self.clf1.predict(iter_labeling_d1)\n",
    "#             y2 = self.clf2.predict(iter_labeling_d2)\n",
    "#             print(\"dataviewone prediction on unlabeled: \",y1)\n",
    "#             print(\"dataviewtwo prediction on unlabeled: \",y2)\n",
    "            dv1_proba_rank = []\n",
    "            dv2_proba_rank = []\n",
    "            # proba1_rank[i] is label i's confidence measure\n",
    "            for class_proba in dv1_proba.T:\n",
    "                dv1_proba_rank.append((-class_proba).argsort())\n",
    "            for class_proba in dv2_proba.T:\n",
    "                dv2_proba_rank.append((-class_proba).argsort())\n",
    "#             print(dv1_proba)\n",
    "#             print(dv1_proba_rank)\n",
    "#             print(dv2_proba)\n",
    "#             print(dv2_proba_rank)\n",
    "            # h1 classifier\n",
    "            p1,n1 = self.label_p_n_samples(dv1_proba_rank)\n",
    "            # h2 classifier\n",
    "            p2,n2 = self.label_p_n_samples(dv2_proba_rank)\n",
    "            finalP = set(p1+p2)\n",
    "            finalN = set(n1+n2)\n",
    "#             print(\"P: \", finalP, \" N: \", finalN)\n",
    "            # auto label the samples and remove it from U_prime\n",
    "            auto_labeled_pos = [U_prime[x] for x in finalP]\n",
    "            auto_labeled_neg = [U_prime[x] for x in finalN]\n",
    "            auto_labeled_samples = auto_labeled_pos+auto_labeled_neg\n",
    "            labels[auto_labeled_pos] = 0\n",
    "            labels[auto_labeled_neg] = 1\n",
    "            # extend the labeled sample\n",
    "            L.extend(auto_labeled_pos)\n",
    "            L.extend(auto_labeled_neg)\n",
    "            # remove the labeled sample from U_prime\n",
    "            U_prime = [x for x in U_prime if x not in auto_labeled_samples]\n",
    "            #print(U_prime)\n",
    "            # randomly choice 2p+2n examples from u to replenish u_prime\n",
    "            replenishItem = U[-(2*self.p+2*self.n):]\n",
    "            U_prime.extend(replenishItem)\n",
    "            U = U[:-len(replenishItem)]\n",
    "        print(\"Total Labeled number: \", len(L), \" Still unlabeled number: \", len(U_prime))\n",
    "        # final train\n",
    "        newtrain_d1 = dataView1.iloc[L]\n",
    "        newtrain_d2 = dataView2.iloc[L]\n",
    "        self.clf1.fit(newtrain_d1, labels[L])\n",
    "        self.clf2.fit(newtrain_d2, labels[L])\n",
    "    \n",
    "    def supports_proba(self, clf, x):\n",
    "        try:\n",
    "            clf.predict_proba([x])\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "        \n",
    "    def predict(self, dataView1, dataView2):\n",
    "        y1 = self.clf1.predict(dataView1)\n",
    "        y2 = self.clf2.predict(dataView2)\n",
    "        proba_supported = self.supports_proba(self.clf1, dataView1.iloc[0]) and self.supports_proba(self.clf2, dataView2.iloc[0])\n",
    "        #fill pred with -1 so we can identify the samples in which sample classifiers failed to agree\n",
    "        y_pred = np.asarray([-1] * dataView1.shape[0])\n",
    "        for i, (y1_i, y2_i) in enumerate(zip(y1, y2)):\n",
    "            # if both agree on label\n",
    "            if y1_i == y2_i:\n",
    "                y_pred[i] = y1_i\n",
    "            # if disagree on label, times probability together, choice the class have higher probabilities\n",
    "            elif proba_supported:\n",
    "                y1_probas = self.clf1.predict_proba([dataView1.iloc[i]])[0]\n",
    "                y2_probas = self.clf2.predict_proba([dataView2.iloc[i]])[0]\n",
    "                print(\"y1 disagree on\",i, \" Proba: \",y1_probas)\n",
    "                print(\"y2 not aggreed on \",i, \"Proba: \", y2_probas)\n",
    "                prod_y_probas = [proba_y1 * proba_y2 for (proba_y1, proba_y2) in zip(y1_probas, y2_probas)]\n",
    "                print(\"product probas:\",prod_y_probas)\n",
    "                y_pred[i] = prod_y_probas.index(max(prod_y_probas))\n",
    "                print(\"result\",y_pred[i])\n",
    "            else:\n",
    "                #the classifiers disagree and don't support probability, so we guess\n",
    "                warnings.warn(\"classifiers disagree with label, result may not accurate\")\n",
    "                print(\"sample at: \", i, \" c1: \", y1_i, \" c2: \", y2_i)\n",
    "                y_pred[i] = random.randint(0, 1)\n",
    "        #check if predict works\n",
    "        assert not (-1 in y_pred)\n",
    "        return y_pred\n",
    "    \n",
    "    def predict_proba(self, dataView1, dataView2):\n",
    "        # the predicted probabilities is simply a product of probabilities given from each classifier trained\n",
    "        y1_probas = self.clf1.predict_proba(dataView1)\n",
    "        y2_probas = self.clf2.predict_proba(dataView2)\n",
    "        \n",
    "        proba = (y1_probas*y2_probas)\n",
    "        return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T17:27:41.778159Z",
     "start_time": "2019-01-18T17:27:41.357500Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "# self defined one vs rest\n",
    "class co_train_one_vs_rest:\n",
    "    \n",
    "    def __init__ (self):\n",
    "        self.binary_clf = []\n",
    "\n",
    "    def fit_one_vs_rest(self, all_train_dv1, all_train_dv2, clf):\n",
    "        # ----------- binary statistic collection --------------#\n",
    "        self.positive_sample_size = []\n",
    "        self.negative_sample_size = []\n",
    "        # --------- split labeled/unlabeled from all data ----------------#\n",
    "        labeled_mask = all_train_dv1[\"authorID\"] != \"-1\"\n",
    "        labeled_processed_dv1 = all_train_dv1[labeled_mask]\n",
    "        labeled_processed_dv2 = all_train_dv2[labeled_mask]\n",
    "        all_labeled_sample_pid = labeled_processed_dv1[\"paperID\"].tolist()\n",
    "        \n",
    "        unlabeled_mask = all_train_dv1[\"authorID\"] == \"-1\"\n",
    "        unlabeled_dv1 = all_train_dv1[unlabeled_mask]\n",
    "        unlabeled_dv2 = all_train_dv2[unlabeled_mask]\n",
    "        print(\"labled_samples: \", labeled_processed_dv1.shape)\n",
    "        print(\"unlabled_samples: \", unlabeled_dv1.shape)\n",
    "        # ---------------- generate binary labels -------------------- #\n",
    "\n",
    "        self.classes = np.unique(all_train_dv1[\"authorID\"]).tolist()\n",
    "        # check for \"RARE_VALUE\" special marker that only used for train binary classifier, not creating new class\n",
    "        for author in self.classes:\n",
    "            if author ==\"-1\":\n",
    "                pass\n",
    "            else:\n",
    "                print(\"Binary clf: \", author)\n",
    "                mask = labeled_processed_dv1[\"authorID\"] == author\n",
    "                temp = labeled_processed_dv1[mask]\n",
    "                positive_sample_pid = temp[\"paperID\"].tolist()\n",
    "                negative_sample_pid = extractNegativeSample(positive_sample_pid, all_labeled_sample_pid)\n",
    "                # append to statistic collection\n",
    "                self.positive_sample_size.append(len(positive_sample_pid))\n",
    "                self.negative_sample_size.append(len(negative_sample_pid))\n",
    "                # form positive and negative (negative class come from similar name group)\n",
    "                all_authors = []\n",
    "                all_authors.append(positive_sample_pid)\n",
    "                all_authors.append(negative_sample_pid)\n",
    "                appended_data = []\n",
    "                for label, pid in enumerate(all_authors):\n",
    "                    # create df save one author data \n",
    "                    authordf = pd.DataFrame({\"paperID\":pid})\n",
    "                    authordf['label'] = label\n",
    "                    appended_data.append(authordf)\n",
    "                processed_data = pd.concat(appended_data, axis=0,ignore_index=True)\n",
    "                \n",
    "                # alignment \n",
    "                processed_data = pd.merge(labeled_processed_dv1[\"paperID\"].to_frame(), processed_data, on=\"paperID\")\n",
    "#                 print(processed_data)\n",
    "                # -------------construct binary labeleds dataset ----------------#\n",
    "                dv1_with_binary_label = pd.merge(labeled_processed_dv1, processed_data, on=\"paperID\", how = 'outer')\n",
    "                dv2_with_binary_label = pd.merge(labeled_processed_dv2, processed_data, on=\"paperID\", how = 'outer')\n",
    "                dv1_with_binary_label = dv1_with_binary_label.drop([\"authorID\"], axis=1).reset_index(drop=True)\n",
    "                dv2_with_binary_label = dv2_with_binary_label.drop([\"authorID\"], axis=1).reset_index(drop=True)\n",
    "                # ------------- add unlabeled data to form final dataset ---------#\n",
    "                unlabeled_dv1 = unlabeled_dv1.rename(columns={\"authorID\": 'label'})\n",
    "                unlabeled_dv2 = unlabeled_dv2.rename(columns={\"authorID\": 'label'})\n",
    "                final_dv1 = pd.concat([dv1_with_binary_label,unlabeled_dv1], ignore_index=True)\n",
    "                final_dv2 = pd.concat([dv2_with_binary_label,unlabeled_dv2], ignore_index=True)\n",
    "                # ---------------------- final data ------------------------------#\n",
    "                label = final_dv1[[\"label\"]]\n",
    "                pid = final_dv1[[\"paperID\"]]\n",
    "                \n",
    "                final_dv1.drop([\"paperID\", \"label\"], axis=1, inplace = True)\n",
    "                final_dv2.drop([\"paperID\", \"label\"], axis=1, inplace = True)\n",
    "#                 print(label)\n",
    "                print(final_dv1.shape)\n",
    "                print(final_dv2.shape)\n",
    "#                 LRaccuracy, LRmarcof1 = k_fold_cv(train_data, label, clf, k=10)\n",
    "#                 print(\"LR Accuracy: \",LRaccuracy)\n",
    "#                 print(\"LR F1: \", LRmarcof1)\n",
    "\n",
    "                # using converted feature vector to train classifier\n",
    "                traing_clf = copy.deepcopy(clf)\n",
    "                traing_clf.fit(final_dv1, final_dv2, label)\n",
    "                self.binary_clf.append(traing_clf)\n",
    "        print(self.classes)\n",
    "        print(self.positive_sample_size)\n",
    "        print(self.negative_sample_size)\n",
    "        self.classes.remove('-1')\n",
    "        return self\n",
    "        \n",
    "    def predict(self, dataviewone, dataviewtwo):\n",
    "        author_proba = pd.DataFrame()\n",
    "        for author, author_clf in zip(self.classes, self.binary_clf):\n",
    "#             print(author_clf.predict_proba(dataviewone, dataviewtwo))\n",
    "            # only look at probability of 0 (belone to that author)\n",
    "            author_proba[author] = author_clf.predict_proba(dataviewone, dataviewtwo)[:,0]\n",
    "        # for author less than threshold number of samples\n",
    "        self.predict_proba = author_proba\n",
    "        labels = author_proba.idxmax(axis=1).values\n",
    "        return labels\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T17:27:41.862843Z",
     "start_time": "2019-01-18T17:27:41.780487Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_score, recall_score,f1_score,accuracy_score\n",
    "# cross validation\n",
    "def k_fold_cv_co_train_ovr(dataview1, dataview2, unlabeled_dv1, unlabeled_dv2, label, clf, k=10):\n",
    "    kf = StratifiedKFold(n_splits=k)\n",
    "    allTrueLabel = []\n",
    "    allPredLabel = []\n",
    "    for train_index, test_index in kf.split(dataview1, label):\n",
    "        # print(\"TRAIN:\", train_index, \" \\n TEST:\", test_index)\n",
    "        # ---------------split train and test -------------------- #\n",
    "        dv1_train, dv1_test = dataview1.iloc[train_index], dataview1.iloc[test_index]\n",
    "        dv2_train, dv2_test = dataview2.iloc[train_index], dataview2.iloc[test_index]\n",
    "        label_train, label_test = label[train_index], label[test_index]\n",
    "        # -------------- add unlabeled to train ------------------ #\n",
    "        final_dv1 = pd.concat([dv1_train,unlabeled_dv1], ignore_index=True)\n",
    "        final_dv2 = pd.concat([dv2_train,unlabeled_dv2], ignore_index=True)\n",
    "        # -------------- train ovr co-training ------------------- #\n",
    "        ovr_clf = co_train_one_vs_rest().fit_one_vs_rest(final_dv1, final_dv2, clf)\n",
    "        \n",
    "        dv1_test.drop([\"authorID\", \"paperID\"], axis=1, inplace = True)\n",
    "        dv2_test.drop([\"authorID\", \"paperID\"], axis=1, inplace = True)\n",
    "        # get predicted label\n",
    "        co_lr_label_predict = ovr_clf.predict(dv1_test, dv2_test)\n",
    "        allTrueLabel.extend(label_test)\n",
    "        allPredLabel.extend(co_lr_label_predict)\n",
    "        # print(\"True positive: {tp}, False positive: {fp}, False negative: {fn}, True negative: {tn}\"\n",
    "        # .format(tp=round_tp, fp=round_fp, fn=round_fn, tn=round_tn))\n",
    "\n",
    "    accuracy = accuracy_score(allTrueLabel, allPredLabel)\n",
    "    f1 = f1_score(allTrueLabel, allPredLabel,average='macro')\n",
    "    \n",
    "    print(metrics.classification_report(allTrueLabel, allPredLabel))\n",
    "    print(metrics.confusion_matrix(allTrueLabel, allPredLabel).ravel())\n",
    "    \n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-18T17:27:42.460Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load citation embedding:  n2v\n"
     ]
    }
   ],
   "source": [
    "# load the file\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from statistics import mean \n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "# loop through all files in directory add name to name list\n",
    "fileDir = \"../../Data/\"+Dataset+\"/canopies/\"\n",
    "listfiles = os.listdir(fileDir)\n",
    "\n",
    "co_lr_diff_embedding_result = []\n",
    "\n",
    "# ------------ view two citation is fix, so move out to save time ------- #\n",
    "# read viewtwo embedding\n",
    "print(\"Load citation embedding: \", pp_citation)\n",
    "viewtwo_citation_embedding = com_func.read_all_citation_embedding_sorted(emb_type = pp_citation)\n",
    "\n",
    "for select_emb in pp_textual:\n",
    "    #---------------- load embeddings for different view ---------------#\n",
    "    print(\"Load textual embedding: \", select_emb)\n",
    "    # read viewone embeddings\n",
    "    viewone_textual_emb = com_func.read_all_textual_embedding_sorted(emb_type=select_emb, training_size = \"3m\")\n",
    "\n",
    "    threshold_change_all_co_lr_f1s = []\n",
    "    threshold_change = []\n",
    "    \n",
    "    # -------------- different threshold (step by 10) -----------------------#\n",
    "    for step_threshold in range(threshold_lower, threshold_upper, 10):\n",
    "        threshold_change.append(step_threshold)\n",
    "        # collect statistic to output\n",
    "        allname, num_class, per_class_count, all_labeled_count, selected_labeled_count, unlabeled_count = ([] for i in range(6))\n",
    "\n",
    "        all_co_LR_accuracy, all_co_LR_f1, = ([] for i in range(2))\n",
    "\n",
    "        total_selected_group = 0\n",
    "\n",
    "        # ------- different name group in all name group --------------------#\n",
    "        for file in listfiles:\n",
    "            # group name\n",
    "            temp = file.split(\"_\")\n",
    "            name = temp[1]+\"_\"+temp[-1]\n",
    "            print(\"For name: \",name)\n",
    "            # read pid and aid from file\n",
    "            data = read_file(fileDir+file)\n",
    "            labeled_mask = data[\"authorID\"] != \"-1\"\n",
    "            labeled_data = data[labeled_mask]\n",
    "            unlabeled_mask = data[\"authorID\"] == \"-1\"\n",
    "            ublabeled_data = data[unlabeled_mask]\n",
    "            unlabeled_pid = ublabeled_data[\"paperID\"]\n",
    "            print(len(unlabeled_pid))\n",
    "            print(labeled_data.shape)\n",
    "            #----------- select name group contain productive author------------------------------------#\n",
    "            #----------- (contain pair of author write more than 100 papers) ---------------------------#\n",
    "            # count number of paper each author write based on author ID\n",
    "            authorCounter = com_func.select_productive_groups(labeled_data, threshold_select_name_group)\n",
    "            # if only have one class or no class pass the threshold, not applicable\n",
    "            if(len(authorCounter)==0) or (len(authorCounter)==1):\n",
    "                print(name, \" pass\")\n",
    "            else:\n",
    "                total_selected_group+= 1\n",
    "                labeled_data, author_list, paperCounter= com_func.only_select_productive_authors(labeled_data, step_threshold)\n",
    "                allname.append(name)\n",
    "                all_labeled_count.append(len(labeled_data))\n",
    "                num_class.append(len(paperCounter))\n",
    "                per_class_count.append(paperCounter)\n",
    "\n",
    "                # -------------- extract all samples for name group -------------- #\n",
    "                # for each name group\n",
    "                # read in labeled data\n",
    "                labeled_viewone_textual = extract_embedding(viewone_textual_emb, labeled_data[\"paperID\"])\n",
    "                print(labeled_viewone_textual.shape)\n",
    "                labeled_viewtwo_citation = extract_embedding(viewtwo_citation_embedding, labeled_data[\"paperID\"])\n",
    "                print(labeled_viewtwo_citation.shape)\n",
    "                print(\"Labeled: \",len(labeled_viewone_textual), \" : \", len(labeled_viewtwo_citation))\n",
    "\n",
    "                # read in unlabeled data\n",
    "                unlabeled_viewone_textual = extract_unlabeled_embedding(viewone_textual_emb, unlabeled_pid)\n",
    "                print(unlabeled_viewone_textual.shape)\n",
    "                unlabeled_viewtwo_citation = extract_unlabeled_embedding(viewtwo_citation_embedding, unlabeled_pid)\n",
    "                print(unlabeled_viewtwo_citation.shape)\n",
    "                print(\"Unlabeled: \",len(unlabeled_viewone_textual), \" : \", len(unlabeled_viewtwo_citation))\n",
    "\n",
    "                # synchronize different view based on pid\n",
    "                sorted_dv1, sorted_dv2, unlabeled_dv1, unlabeled_dv2= synchro_views(labeled_viewone_textual, labeled_viewtwo_citation,\n",
    "                                                                                    unlabeled_viewone_textual, unlabeled_viewtwo_citation)\n",
    "                print(sorted_dv1.shape)\n",
    "                print(sorted_dv2.shape)\n",
    "                print(unlabeled_dv1.shape)\n",
    "                print(unlabeled_dv2.shape)\n",
    "                unlabeled_count.append(unlabeled_dv1.shape[0])\n",
    "                selected_labeled_count.append(sorted_dv1.shape[0])\n",
    "                labeled_data = labeled_data[labeled_data.paperID.isin(sorted_dv1[0])]\n",
    "                # ---------------------- alignment -------------------------------------------------#\n",
    "                # shuffle the data\n",
    "                labeled_data = labeled_data.sample(frac=1).reset_index(drop=True)\n",
    "                # alignment \n",
    "                sorted_dv1 = pd.merge(labeled_data, sorted_dv1, left_on=\"paperID\", right_on = [0], how = \"outer\")\n",
    "                sorted_dv2 = pd.merge(labeled_data, sorted_dv2, left_on=\"paperID\", right_on = [0], how = \"outer\")\n",
    "                sorted_dv1 = sorted_dv1.drop([0], axis=1)\n",
    "                sorted_dv2 = sorted_dv2.drop([0], axis=1)\n",
    "#               # ------------------- train test split ------------------------------ --------------#\n",
    "#                 # ------------------- train test split 1:9 ratio -----------------------------------#\n",
    "#                 dv1_train, dv1_test, dv_y_train, dv1_y_test = train_test_split(sorted_dv1, labeled_data[\"authorID\"], \n",
    "#                                                                     test_size=0.1, stratify = labeled_data[\"authorID\"])\n",
    "#                 # get index of train and test\n",
    "#                 train_index = dv_y_train.index.tolist()\n",
    "#                 test_index = dv1_y_test.index.tolist()\n",
    "#                 dv2_train, dv2_test = sorted_dv2.iloc[train_index], sorted_dv2.iloc[test_index]\n",
    "#                 # ----------------------add ublabeled data to labeled to form final train set---------#\n",
    "#                 # rename authorID as label\n",
    "#                 print(\"labeled size: \", sorted_dv1.shape)\n",
    "#                 print(\"unlabeled size: \", unlabeled_dv1.shape)\n",
    "#                 print(dv1_train.head())\n",
    "#                 final_dv1 = pd.concat([dv1_train,unlabeled_dv1], ignore_index=True)\n",
    "#                 final_dv2 = pd.concat([dv2_train,unlabeled_dv2], ignore_index=True)\n",
    "#                 print(final_dv1.head())\n",
    "#                 print(final_dv1.shape)\n",
    "#                 # get pid and labels for true labels\n",
    "#                 test_true_label =labeled_data[\"authorID\"].iloc[test_index]\n",
    "#                 dv1_test.drop([\"authorID\", \"paperID\"], axis=1, inplace = True)\n",
    "#                 dv2_test.drop([\"authorID\", \"paperID\"], axis=1, inplace = True)\n",
    "#                 # ----------------------------------- ovr co-training --------------------------------#\n",
    "#                 # co-training with logistic regression\n",
    "#                 co_logistic_clf = Co_training_clf(clf1=LogisticRegression(),p=1,n=1)\n",
    "#                 co_lr_clf_ovr = co_train_one_vs_rest().fit_one_vs_rest(final_dv1, final_dv2, co_logistic_clf)\n",
    "#                 co_lr_label_predict = co_lr_clf_ovr.predict(dv1_test, dv2_test)\n",
    "#                 co_lr_accuracy = accuracy_score(test_true_label, co_lr_label_predict)\n",
    "#                 co_lr_f1 = f1_score(test_true_label, co_lr_label_predict,average='macro')\n",
    "#                 print(\"lr macro f1: \",co_lr_f1)\n",
    "#                 all_co_LR_accuracy.append(co_lr_accuracy)\n",
    "#                 all_co_LR_f1.append(co_lr_f1)\n",
    "                # ---------------------- 10 fold cv ------------------------------------------------- #\n",
    "                co_logistic_clf = Co_training_clf(clf1=LogisticRegression(),p=1,n=1)\n",
    "                co_lr_accuracy, co_lr_f1 = k_fold_cv_co_train_ovr(sorted_dv1, sorted_dv2, unlabeled_dv1, unlabeled_dv2,\n",
    "                                                                  labeled_data[\"authorID\"], co_logistic_clf, 10)\n",
    "                print(\"lr macro f1: \",co_lr_f1)\n",
    "                all_co_LR_accuracy.append(co_lr_accuracy)\n",
    "                all_co_LR_f1.append(co_lr_f1)\n",
    "                \n",
    "\n",
    "        # write evaluation result to excel\n",
    "        output = pd.DataFrame({'Name Group':allname,\"Class number\":num_class, \"Per class size\":per_class_count, \n",
    "                               \"Total labeled samples\":all_labeled_count, \"Total unlabeled samples\":unlabeled_count, \n",
    "                               \"selected labeled samples\": selected_labeled_count, \n",
    "                               \"co-train with lr accuracy\":all_co_LR_accuracy, \"co-train with lr f1\": all_co_LR_f1})\n",
    "\n",
    "        savePath = \"../../result/\"+Dataset+\"/co_train_advanced/\"\n",
    "        filename = \"(Global emb sample 3m) viewone_textual=\"+select_emb+\"_viewtwo_citation=\"+pp_citation+\"_threshold=\"+str(step_threshold)+\"_namegroupcount=\"+str(total_selected_group)+\".csv\"\n",
    "        com_func.write_csv_df(savePath, filename, output)\n",
    "        print(\"Done\")\n",
    "        \n",
    "        threshold_change_all_co_lr_f1s.append(all_co_LR_f1)\n",
    "        \n",
    "    co_lr_diff_embedding_result.append(threshold_change_all_co_lr_f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-19T06:14:04.654315Z",
     "start_time": "2019-01-19T06:14:04.375370Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_co_LR_accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ea23ce673c16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_co_LR_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mco_lr_f1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_co_LR_accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "print(all_co_LR_accuracy)\n",
    "print(co_lr_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T09:01:48.381054Z",
     "start_time": "2019-01-14T09:01:48.375096Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------- plot f1 score w.r.t each name group on different embedding -------------- #\n",
    "print(pp_textual)\n",
    "print(allname)\n",
    "# 3d, d1 diff emb, d2 diff threshold, d3 result for different author\n",
    "print(co_lr_diff_embedding_result)\n",
    "print(all_co_LR_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T09:01:48.390815Z",
     "start_time": "2019-01-14T09:01:48.382837Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------- plot f1 score w.r.t each name group on different embedding -------------- #\n",
    "# -------------- extract result for plot --------------------- #\n",
    "colr_per_author = []\n",
    "colr_lsa_per_author_result = co_lr_diff_embedding_result[0][0]\n",
    "colr_pv_dm_per_author_result = co_lr_diff_embedding_result[1][0]\n",
    "colr_pv_dbow_per_author_result = co_lr_diff_embedding_result[2][0]\n",
    "colr_per_author.append(colr_lsa_per_author_result)\n",
    "colr_per_author.append(colr_pv_dm_per_author_result)\n",
    "colr_per_author.append(colr_pv_dbow_per_author_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T09:03:20.973708Z",
     "start_time": "2019-01-14T09:03:20.141682Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------- plot f1 score w.r.t each name group on different embedding -------------- #\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "#--------------   logistic regression --------------------------#\n",
    "# process result into np array\n",
    "co_logistic_regression_result = np.array(colr_per_author)\n",
    "name_group = np.array(allname)\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "for emb_type, result in zip(pp_textual, co_logistic_regression_result):\n",
    "    emb_type = \"Viewone: \"+emb_type+\" Viewtwo: n2v)\"\n",
    "    plt.xticks(range(len(result)), name_group)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "    plt.plot(result, label=emb_type)\n",
    "ax.autoscale_view()\n",
    "plt.legend()\n",
    "plt.title('F1 for different embedding method for co-trained logistic regression')\n",
    "plt.xlabel('Name group')\n",
    "plt.ylabel('marco f1 score')\n",
    "# plt.savefig('diff_combined_embedding_sample=3m_clf=co_train_logistic_threshold=100.eps', format='eps', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T08:45:20.440022Z",
     "start_time": "2018-12-13T08:45:20.436671Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(listfiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T14:00:00.964643Z",
     "start_time": "2019-01-13T05:37:55.421554Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T14:00:02.559218Z",
     "start_time": "2019-01-13T14:00:00.967099Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
