{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T17:48:11.943728Z",
     "start_time": "2019-01-07T17:48:10.621621Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "import pickle\n",
    "import com_func\n",
    "\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "Dataset = \"pubmed\"\n",
    "\n",
    "cutoff = 3\n",
    "coauthor_emb_type = \"off\"\n",
    "venue_emb_type = \"off\"\n",
    "pp_textual_emb_type = \"pv_dbow\"\n",
    "citation_emb_type = \"n2v\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T17:48:15.207715Z",
     "start_time": "2019-01-07T17:48:15.160783Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dummy(doc):\n",
    "    return doc\n",
    "def read_labeled_file(infile):\n",
    "    LabeledRecords_original = []\n",
    "    with open(infile, 'r', encoding = 'utf8') as f:\n",
    "        for line in f:\n",
    "            read_data = line.split(\"\\t\")\n",
    "            # get ride of bad formated lines\n",
    "            if(len(read_data)==13 or len(read_data)==12):\n",
    "                paper_detail = {\"paperID\": read_data[0], \"authorID\":read_data[1], \n",
    "                                \"co-author\": read_data[5], \"venue_id\": read_data[7]}\n",
    "                LabeledRecords_original.append(paper_detail)\n",
    "            else:\n",
    "                print(len(read_data))\n",
    "        f.close()\n",
    "    return pd.DataFrame(LabeledRecords_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T17:50:58.632646Z",
     "start_time": "2019-01-07T17:50:58.487879Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read trained rec to rec textual graph\n",
    "def read_textual_embedding(Dataset = \"pubmed\", emb_type = \"off\"):\n",
    "    textual_emb = []\n",
    "    while True:\n",
    "        if emb_type == \"pv_dm\":\n",
    "            loadDir = \"../../Data/\"+Dataset+\"/vectors/d2v/textual_sample=3m/extracted_labeled_pv_dm.txt\"\n",
    "            with open(loadDir, 'r', encoding = 'utf8') as f:\n",
    "                for line in f:\n",
    "                    read_data = line.split(\" \")\n",
    "                    paper_Vectors = read_data\n",
    "                    textual_emb.append(paper_Vectors)\n",
    "            f.close()\n",
    "\n",
    "            print(\"Total textual vector records:\",len(textual_emb))\n",
    "            print(textual_emb[0])\n",
    "            break\n",
    "        elif emb_type == \"pv_dbow\":\n",
    "            loadDir = \"../../Data/\"+Dataset+\"/vectors/d2v/textual_sample=3m/extracted_labeled_pv_dbow.txt\"\n",
    "            with open(loadDir, 'r', encoding = 'utf8') as f:\n",
    "                for line in f:\n",
    "                    read_data = line.split(\" \")\n",
    "                    paper_Vectors = read_data\n",
    "                    textual_emb.append(paper_Vectors)\n",
    "            f.close()\n",
    "            \n",
    "            print(\"Total textual vector records:\",len(textual_emb))\n",
    "            print(textual_emb[0])\n",
    "            break\n",
    "        elif emb_type == \"off\":\n",
    "            break\n",
    "        else:\n",
    "            print(\"Embedding type not available, selecting default setting\")\n",
    "            emb_type=\"pv_dbow\"\n",
    "    return textual_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T17:50:58.888713Z",
     "start_time": "2019-01-07T17:50:58.818477Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read trained rec to rec node2vec citation graph\n",
    "def read_citation_embedding(Dataset = \"pubmed\", emb_type = \"off\"):\n",
    "    citation_emb = []\n",
    "    while True:\n",
    "        if emb_type == \"n2v\":\n",
    "            citation_emb_dir = \"../Data/\"+Dataset+\"/vectors/n2v/extracted_labeled_n2v.txt\"\n",
    "            with open(citation_emb_dir, 'r', encoding = 'utf8') as f:\n",
    "                for line in f:\n",
    "                    read_data = line.split(\" \")\n",
    "                    if(len(read_data)==101):\n",
    "                        paper_Vectors = read_data\n",
    "                        citation_emb.append(paper_Vectors)\n",
    "            f.close()\n",
    "            print(\"Total citation vector records:\",len(citation_emb))\n",
    "            print(citation_emb[:3])\n",
    "            break\n",
    "        elif emb_type == \"off\":\n",
    "            break\n",
    "        else:\n",
    "            print(\"Embedding type not available, selecting default setting\")\n",
    "            emb_type=\"off\"\n",
    "    return citation_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T17:48:21.257777Z",
     "start_time": "2019-01-07T17:48:21.202395Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_embedding(all_embedding, pid):\n",
    "    extracted_emb = []\n",
    "    wanted_pid = pid.values.tolist()\n",
    "    # only if embedding exist\n",
    "    if len(all_embedding)>0:\n",
    "        for paper_embedding in all_embedding:\n",
    "            if paper_embedding[0] in wanted_pid:\n",
    "                extracted_emb.append(paper_embedding)\n",
    "    \n",
    "    extracted_emb = pd.DataFrame(extracted_emb)\n",
    "    # only if embedding exist\n",
    "    if len(all_embedding)>0:\n",
    "        # reorder embedding with pid and fill empty record with 0\n",
    "        extracted_emb = pd.merge(pid.to_frame(), extracted_emb, left_on='paperID', right_on=0, how='outer')\n",
    "        # fill missing value with 0\n",
    "        extracted_emb.fillna(0, inplace = True)\n",
    "        # remove index\n",
    "        extracted_emb.drop(['paperID', 0], axis=1, inplace=True)\n",
    "    return extracted_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T17:48:22.507011Z",
     "start_time": "2019-01-07T17:48:22.479549Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LSA(cleaned_token, dim=100):\n",
    "    # Tf-idf Transformation\n",
    "    tfidf_vectorizer = TfidfVectorizer(use_idf=True, smooth_idf=True, sublinear_tf=True, tokenizer=dummy,\n",
    "                                               preprocessor=dummy, stop_words = None,min_df=cutoff)\n",
    "    tfidfMatrix = tfidf_vectorizer.fit_transform(cleaned_token).toarray()\n",
    "    # tf-idf + svd\n",
    "    svd = TruncatedSVD(n_components=dim)\n",
    "    final_lsa_Matrix = svd.fit_transform(tfidfMatrix)\n",
    "    print(svd.explained_variance_ratio_.sum())\n",
    "    return final_lsa_Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T17:48:27.296320Z",
     "start_time": "2019-01-07T17:48:27.257187Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# co-author relation to frequence count\n",
    "def co_author_to_vector(raw_co_author_data, emb_type=\"off\"):\n",
    "    while True:\n",
    "        if emb_type == \"occurs\":\n",
    "            co_author_vectorizer = CountVectorizer()\n",
    "            print(co_author_vectorizer)\n",
    "            result_vector = co_author_vectorizer.fit_transform(raw_co_author_data).toarray()\n",
    "            #print(co_author_vectorizer.get_feature_names())\n",
    "            #print(len(co_author_vectorizer.vocabulary_))\n",
    "            break\n",
    "        elif emb_type == \"off\":\n",
    "            result_vector = pd.DataFrame()\n",
    "            break\n",
    "        else:\n",
    "            print(\"Embedding type not available, selecting default setting\")\n",
    "            emb_type=\"off\"\n",
    "    return result_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T17:48:28.814155Z",
     "start_time": "2019-01-07T17:48:28.776562Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# venue relation with author\n",
    "def venue_to_vector(raw_venue_id, emb_type=\"off\"):\n",
    "    while True:\n",
    "        if emb_type == \"occurs\":\n",
    "            venue_count_vectorizer = CountVectorizer()\n",
    "            print(venue_count_vectorizer)\n",
    "            result_vector = venue_count_vectorizer.fit_transform(raw_venue_id).toarray()\n",
    "            #print(len(venue_count_vectorizer.vocabulary_))\n",
    "            break\n",
    "        elif emb_type == \"off\":\n",
    "            result_vector = pd.DataFrame()\n",
    "            break\n",
    "        else:\n",
    "            print(\"Embedding type not available, selecting default setting\")\n",
    "            emb_type=\"off\"\n",
    "    return result_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T17:48:30.304695Z",
     "start_time": "2019-01-07T17:48:30.199419Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pairwise_f1(true_label, pred_label):\n",
    "    # predictions that are positive, TP+FP\n",
    "    pred_pos = 0\n",
    "    # conditions that are positive, TP+FN\n",
    "    cond_pos = 0\n",
    "    # Pairs Correctly Predicted To SameAuthor, TP\n",
    "    tp = 0\n",
    "    for i in range(len(true_label)):\n",
    "        for j in range(i + 1, len(true_label)):\n",
    "            if pred_label[i] == pred_label[j]:\n",
    "                pred_pos +=1\n",
    "            if true_label[i] == true_label[j]:\n",
    "                cond_pos +=1\n",
    "            if (true_label[i] == true_label[j]) and (pred_label[i] == pred_label[j]):\n",
    "                tp +=1\n",
    "    print(\"tp: \", tp)\n",
    "    print(\"tp+fp: \", pred_pos)\n",
    "    print(\"tp+fn:\", cond_pos)\n",
    "    # calculate pairwise f1 score\n",
    "    if tp == 0:\n",
    "        pairwise_precision = 0\n",
    "        pairwise_recall = 0\n",
    "        pairwise_f1 = 0\n",
    "    else:\n",
    "        pairwise_precision = tp / pred_pos\n",
    "        pairwise_recall = tp / cond_pos\n",
    "        pairwise_f1 = (2 * pairwise_precision * pairwise_recall) / (pairwise_precision + pairwise_recall)\n",
    "        \n",
    "    return pairwise_f1, pairwise_precision, pairwise_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T17:51:10.690179Z",
     "start_time": "2019-01-07T17:51:05.222009Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total textual vector records: 135796\n",
      "['8077', '-0.14659140', '-0.16460477', '-0.50664663', '-0.17956261', '0.21054362', '0.26002276', '0.15514752', '0.13244890', '-0.27113414', '0.47227725', '0.07357255', '-0.08964530', '0.35950011', '0.37851566', '-0.04907404', '0.56523114', '-0.60256726', '-0.21556917', '-0.09287039', '-0.18874674', '0.59881312', '-0.32156968', '0.39462098', '0.35133442', '0.08628392', '-0.04479222', '0.25453219', '0.23234852', '-0.10687385', '-0.00707190', '-0.11578006', '0.06657255', '0.19292782', '0.09975667', '-0.04673584', '0.47342294', '0.50503510', '-0.13644342', '0.35020310', '0.27452260', '0.45986831', '0.72157681', '-0.08654509', '-0.36922029', '-0.28984016', '0.26503867', '-0.14659104', '0.19001262', '-0.24055083', '0.10608102', '-0.21904105', '-0.02745518', '-0.27935785', '0.67074525', '0.57324684', '0.16567072', '-0.12955795', '-0.73991919', '0.20633785', '-0.13949864', '0.07348444', '-0.45851952', '-0.28298637', '0.65005982', '-0.16004808', '-0.33634639', '0.71481329', '-0.08647151', '0.61378562', '0.84790415', '-0.40871361', '0.43062189', '0.01380008', '0.81693238', '0.22439685', '-0.32138148', '0.39291272', '0.71506411', '0.03570956', '0.02829047', '0.04145329', '-0.13109712', '0.35155740', '-0.21288362', '0.19800952', '0.07500107', '-0.23002854', '0.24881081', '-0.33990535', '0.12203706', '-0.18824810', '0.12419476', '-0.09093910', '0.54129761', '-0.43786141', '0.05272437', '0.42604834', '-0.60860509', '0.29351130', '0.69737893\\n']\n",
      "Total citation vector records: 124922\n",
      "[['8077', '0.074837', '0.437304', '0.157833', '0.179944', '-0.0696371', '-0.0925071', '-0.37209', '0.16441', '0.257381', '0.482553', '0.420752', '0.294299', '0.48322', '0.310536', '0.451489', '-0.0321524', '-0.266308', '-0.507235', '0.302519', '-0.192578', '-0.196128', '-0.716089', '0.118927', '0.130549', '0.0538411', '-0.36721', '0.320577', '0.107628', '0.437685', '0.261019', '-0.134182', '0.467584', '-0.433934', '-0.337566', '-0.112999', '0.131627', '0.185436', '-0.0716854', '0.222004', '-0.296244', '0.0662622', '0.209887', '-0.177259', '-0.202866', '0.206727', '-0.0535898', '-0.0832955', '0.00406953', '-0.13292', '-0.0853675', '-0.241761', '-0.327425', '-0.46692', '0.0485383', '0.00806723', '0.0284221', '0.115838', '-0.255672', '-0.770949', '0.0873891', '0.00681434', '0.0626846', '-0.0590345', '0.299776', '-0.173271', '-0.00270774', '-0.498401', '-0.222046', '0.321921', '0.0837049', '-0.0501312', '-0.284909', '0.274566', '0.0670506', '0.0773459', '0.24957', '-0.0768505', '0.0357878', '-0.197779', '-0.110859', '-0.0586628', '-0.371421', '-0.331327', '-0.184969', '0.347994', '-0.535585', '0.136484', '0.606065', '-0.34836', '-0.153024', '0.264854', '-0.347494', '0.0979302', '0.352819', '0.116963', '-0.428671', '-0.203673', '0.340799', '-0.153595', '0.333619\\n'], ['17755', '0.117789', '0.0381872', '0.140725', '0.0353051', '0.0743335', '-0.0230412', '-0.142969', '0.133352', '0.0249408', '0.174816', '0.0710962', '-0.0186644', '0.131927', '-0.0813431', '0.00454495', '0.0411643', '-0.211366', '-0.334071', '0.122605', '0.0906733', '-0.0945511', '-0.204131', '0.0216074', '0.050275', '-0.0138835', '-0.00201036', '0.181787', '0.0625838', '0.163003', '-0.060977', '-0.099711', '0.199833', '-0.257413', '-0.20593', '-0.148003', '-0.0193572', '0.127712', '-0.00496556', '0.203692', '-0.0632423', '-0.085105', '0.111502', '0.0563165', '-0.150758', '0.130987', '-0.24935', '-0.0698974', '-0.159101', '-0.0325247', '0.112569', '-0.0933885', '0.0523473', '-0.311551', '-0.0534642', '-0.0335219', '0.0381494', '0.0325196', '0.0200638', '-0.25958', '0.116856', '-0.00533538', '0.015437', '-0.122961', '0.0405297', '-0.0402108', '-0.0648276', '-0.280718', '-0.0610803', '0.235437', '-0.154469', '0.040941', '0.0660014', '0.0144551', '0.0697055', '0.0861819', '0.22364', '-0.108656', '-0.159732', '0.0263292', '0.0976405', '0.0357826', '-0.0407278', '-0.109561', '-0.0477482', '0.0600652', '-0.244403', '0.0708506', '0.206281', '-0.0660206', '-0.246335', '0.0559538', '-0.250907', '-0.0278802', '0.138755', '-0.0880744', '-0.158979', '-0.0278477', '0.176379', '-0.0994503', '0.134535\\n'], ['28220', '-0.168945', '0.0421443', '0.23792', '0.0796514', '0.19611', '0.0638499', '-0.513983', '0.205836', '0.190542', '0.415064', '0.377228', '0.0176192', '0.152356', '-0.114991', '0.357162', '-0.296283', '0.0681007', '-0.204847', '0.135245', '-0.00781574', '-0.12612', '-0.143662', '-0.0130079', '0.185709', '-0.0763587', '0.0492441', '0.054612', '0.0511256', '0.134606', '0.117456', '-0.231842', '0.0450055', '-0.302675', '-0.0223345', '-0.055917', '-0.261424', '0.105746', '0.104476', '0.183511', '0.0450612', '-0.373329', '0.144302', '0.148158', '0.0699681', '0.270498', '0.0455624', '-0.120792', '-0.172808', '-0.161062', '-0.000496539', '-0.128889', '-0.0161444', '-0.246482', '-0.0448989', '-0.0789056', '-0.15212', '0.111224', '-0.161332', '-0.259847', '0.00823854', '0.0212978', '-0.0829648', '-0.224326', '0.337475', '0.153362', '0.0231983', '-0.138663', '-0.102745', '0.195753', '-0.16937', '-0.350263', '0.134394', '0.228506', '0.375576', '0.115358', '0.228747', '-0.235889', '-0.278699', '-0.151732', '-0.0599872', '0.130166', '-0.0182416', '-0.0320656', '-0.245937', '0.20861', '0.173579', '0.0255497', '0.0826704', '-0.169168', '-0.154122', '0.0454759', '-0.219318', '0.0486723', '0.349938', '0.118215', '-0.281961', '0.0711244', '0.166005', '0.0144367', '0.393137\\n']]\n"
     ]
    }
   ],
   "source": [
    "# read pretrained embeddings\n",
    "all_textual_embedding = read_textual_embedding(emb_type = pp_textual_emb_type)\n",
    "all_citation_embedding = read_citation_embedding(emb_type = citation_emb_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T17:51:10.707370Z",
     "start_time": "2019-01-07T17:51:10.692281Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predefined cos_sim\n",
    "def self_pairwise_cosine(in_matrix):\n",
    "    \"\"\"Takes matrix and returns the cosine similarity between each pairs\n",
    "    \"\"\"\n",
    "    final_sim = []\n",
    "    for row in in_matrix:\n",
    "        dot_product = np.dot(in_matrix, row)\n",
    "        print(dot_product)\n",
    "        row_norm = np.linalg.norm(row)\n",
    "        matrix_norm = np.linalg.norm(in_matrix, axis = 1)\n",
    "        print(row_norm)\n",
    "        print(matrix_norm)\n",
    "        final_sim.append(dot_product / (row_norm * matrix_norm))\n",
    "    return final_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T18:08:24.017676Z",
     "start_time": "2019-01-07T18:08:21.492127Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For name:  j_read\n",
      "Counter({'0000-0002-5159-1192': 57, '0000-0002-9029-5185': 39, '0000-0002-9697-0962': 31, '0000-0002-4739-9245': 3, '0000-0003-0605-5259': 3, '0000-0003-4316-7006': 1, '0000-0002-0784-0091': 1, '0000-0002-3888-6631': 1})\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(136, 100)\n",
      "(136, 100)\n",
      "2\n",
      "(136, 200)\n",
      "[ 0  0  0  0  0 -1  0  0  0  0  0 -1  0  0  0 -1  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 -1 -1\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 -1  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 -1  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  2 -1  2  0  0  1]\n",
      "tp:  2583\n",
      "tp+fp:  7656\n",
      "tp+fn: 2808\n",
      "(0.4936926605504587, 0.33738244514106586, 0.9198717948717948)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# write evaluation result to excel\\noutput = pd.DataFrame({\\'Name Group\\':allname,\"Class number\":num_class,\"average term in sample\":average_sample_size,\\n                       \"per_class_size\":per_class_count,\"accuracy\":allaccuracy})\\nfilename = \"2005_name_cluster_\"+setting+\"_k_mean\"+\".csv\"\\noutput.to_csv(\"../result/\"+Dataset+\"/\"+filename, encoding=\\'utf-8\\',index=False)\\nprint(\"Done\")\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the file\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN \n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "setting = \"tf\"\n",
    "Dataset = \"pubmed\"\n",
    "threshold = 0\n",
    "\n",
    "fileDir = \"../Data/\"+Dataset+\"/canopies_labeled/\"\n",
    "listfiles = os.listdir(fileDir)\n",
    "\n",
    "# collect statistic to output\n",
    "allname = []\n",
    "num_class = []\n",
    "per_class_count = []\n",
    "allPf1 = []\n",
    "\n",
    "\n",
    "# read all file in labeled group\n",
    "for file in listfiles:\n",
    "    # get author group name\n",
    "    temp = file.split(\"_\")\n",
    "    name = temp[1]+\"_\"+temp[-1]\n",
    "    print(\"For name: \",name)\n",
    "    allname.append(name)\n",
    "    # read needed content in labeled file\n",
    "    labeled_data = read_labeled_file(fileDir+file)\n",
    "    # count number of paper each author write based on author ID\n",
    "    paperCounter = collections.Counter(labeled_data[\"authorID\"])\n",
    "    print(paperCounter)\n",
    "    temp =list(paperCounter.keys())\n",
    "    # extract true label and pid\n",
    "    true_label = labeled_data[\"authorID\"]\n",
    "    pid = labeled_data[\"paperID\"]\n",
    "    # list of different data field\n",
    "    part_collection = []\n",
    "    # select feature wanted to fit to clustering/classification algorithm\n",
    "    # data part 1, co-author matrix\n",
    "    data_part_1_co_author = co_author_to_vector(labeled_data[\"co-author\"], emb_type=coauthor_emb_type)\n",
    "    print(data_part_1_co_author.shape)\n",
    "    part_collection.append(data_part_1_co_author)\n",
    "    # data part 2, venue_id that author attendent\n",
    "    data_part_2_venue = venue_to_vector(labeled_data[\"venue_id\"], emb_type=venue_emb_type)\n",
    "    print(data_part_2_venue.shape)\n",
    "    part_collection.append(data_part_2_venue)\n",
    "    # data part 3, extract textual embedding\n",
    "    data_part_3_textual = extract_embedding(all_textual_embedding, pid)\n",
    "    print(data_part_3_textual.shape)\n",
    "    part_collection.append(data_part_3_textual)\n",
    "    # data part 4, read citation embedding \n",
    "    data_part_4_citation = extract_embedding(all_citation_embedding, pid)\n",
    "    print(data_part_4_citation.shape)\n",
    "    part_collection.append(data_part_4_citation)\n",
    "    # merge different part of data data together by concatenate it all together\n",
    "    # remove empty emb (when emb set off)\n",
    "    part_collection = [part for part in part_collection if len(part)!=0]\n",
    "    print(len(part_collection))\n",
    "    if len(part_collection)>1:\n",
    "        combinedata = np.concatenate(part_collection,axis=1)\n",
    "    elif len(part_collection)==1:\n",
    "        if isinstance(part_collection[0], pd.DataFrame):\n",
    "            combinedata = part_collection[0].values\n",
    "        else:\n",
    "            combinedata = part_collection[0]\n",
    "    else:\n",
    "        print(\"No data available\")\n",
    "        break\n",
    "    # convert type to float\n",
    "    print(combinedata.shape)\n",
    "    combinedata = combinedata.astype(np.float)\n",
    "    # scale the data right after they been generated\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit_transform(combinedata)\n",
    "    # using converted feature vector to form cluster\n",
    "    # assume k (# of authors) is unknown, using Density Based Clustering Algorithms\n",
    "    dbscan_clustering = DBSCAN(min_samples=2, metric = \"cosine\").fit(combinedata)\n",
    "    db_scan_pred_labels = dbscan_clustering.labels_\n",
    "    # prediction result\n",
    "    # pairwise f1\n",
    "    print(db_scan_pred_labels)\n",
    "    dbscan_pairwise_f1_score = pairwise_f1(true_label, db_scan_pred_labels)\n",
    "    print(dbscan_pairwise_f1_score)\n",
    "    break\n",
    "'''\n",
    "# write evaluation result to excel\n",
    "output = pd.DataFrame({'Name Group':allname,\"Class number\":num_class,\"average term in sample\":average_sample_size,\n",
    "                       \"per_class_size\":per_class_count,\"accuracy\":allaccuracy})\n",
    "filename = \"2005_name_cluster_\"+setting+\"_k_mean\"+\".csv\"\n",
    "output.to_csv(\"../result/\"+Dataset+\"/\"+filename, encoding='utf-8',index=False)\n",
    "print(\"Done\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
