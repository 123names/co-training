{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T21:40:32.809301Z",
     "start_time": "2020-08-20T21:40:31.762272Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scikit-learn version is 0.23.1.\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))\n",
    "\n",
    "# update a pip in the current Jupyter kernel\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T21:40:34.711378Z",
     "start_time": "2020-08-20T21:40:32.811366Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import com_func\n",
    "\n",
    "# parameters\n",
    "#----- threshold for selecting set of name group -----------#\n",
    "threshold_select_name_group = 100\n",
    "#----- threshold for selecting min sample in name group ----#\n",
    "threshold_min_class_sample_size = 100\n",
    "\n",
    "Dataset = \"pubmed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T21:40:34.724530Z",
     "start_time": "2020-08-20T21:40:34.713499Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# text embedding only\n",
    "pp_text_emb = [\"tf\", \"tf_idf\", \"lsa\", \"pv_dm\", \"pv_dbow\"]\n",
    "pp_citation_emb = [\"off\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T03:34:26.895991Z",
     "start_time": "2020-06-27T03:34:26.893325Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# citation embedding only\n",
    "pp_text_emb = [\"off\"]\n",
    "pp_citation_emb = [\"node2vec\", \"n2v\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T23:30:49.617766Z",
     "start_time": "2020-08-19T23:30:49.614327Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combined embedding\n",
    "pp_text_emb = [\"lsa\", \"pv_dm\", \"pv_dbow\"]\n",
    "pp_citation_emb = [\"n2v\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T21:40:35.875468Z",
     "start_time": "2020-08-20T21:40:35.871674Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tf']\n",
      "['off']\n"
     ]
    }
   ],
   "source": [
    "print(pp_text_emb)\n",
    "print(pp_citation_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T21:40:37.032556Z",
     "start_time": "2020-08-20T21:40:36.957915Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# method to evaluate cluster result\n",
    "# https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html\n",
    "def pairwise_f1(true_label, pred_label):\n",
    "    total_pair = len(true_label)*(len(true_label)-1)/2\n",
    "    # predictions that are positive, TP+FP\n",
    "    pred_pos = 0\n",
    "    # conditions that are positive, TP+FN\n",
    "    cond_pos = 0\n",
    "    # Pairs Correctly Predicted To SameAuthor, TP\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for i in range(len(true_label)):\n",
    "        for j in range(i + 1, len(true_label)):\n",
    "            if pred_label[i] == pred_label[j]:\n",
    "                pred_pos +=1\n",
    "            if true_label[i] == true_label[j]:\n",
    "                cond_pos +=1\n",
    "            if (true_label[i] == true_label[j]) and (pred_label[i] == pred_label[j]):\n",
    "                #print(true_label[i], \" \", i)\n",
    "                #print(pred_label[i])\n",
    "                tp +=1\n",
    "    fp = pred_pos-tp\n",
    "    fn = cond_pos-tp\n",
    "    tn = int(total_pair-tp-fp-fn)\n",
    "    print(\"tp: \", tp)\n",
    "    print(\"fp: \", fp)\n",
    "    print(\"fn: \", fn)\n",
    "    print(\"tn: \", tn)\n",
    "    print(\"tp+fp: \", pred_pos)\n",
    "    print(\"tp+fn:\", cond_pos)\n",
    "    # calculate pairwise f1 score\n",
    "    if tp == 0:\n",
    "        pairwise_precision = 0\n",
    "        pairwise_recall = 0\n",
    "        pairwise_f1 = 0\n",
    "    else:\n",
    "        pairwise_precision = tp / pred_pos\n",
    "        pairwise_recall = tp / cond_pos\n",
    "        pairwise_f1 = (2 * pairwise_precision * pairwise_recall) / (pairwise_precision + pairwise_recall)\n",
    "        \n",
    "    return pairwise_f1, pairwise_precision, pairwise_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-20T21:40:38.599Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load text embedding:  tf\n",
      "Load citation embedding:  off\n",
      "For name:  j_read\n",
      "j_read  pass\n",
      "For name:  f_esteves\n",
      "f_esteves  pass\n",
      "For name:  c_miller\n",
      "c_miller  pass\n",
      "For name:  r_jha\n",
      "r_jha  pass\n",
      "For name:  a_lowe\n",
      "a_lowe  pass\n",
      "For name:  a_vega\n",
      "a_vega  pass\n",
      "For name:  k_smith\n",
      "k_smith  pass\n",
      "For name:  j_gordon\n",
      "j_gordon  pass\n",
      "For name:  s_liao\n",
      "s_liao  pass\n",
      "For name:  j_qian\n",
      "j_qian  pass\n",
      "For name:  s_bernardi\n",
      "s_bernardi  pass\n",
      "For name:  t_hill\n",
      "t_hill  pass\n",
      "For name:  s_schindler\n",
      "s_schindler  pass\n",
      "For name:  j_williams\n",
      "j_williams  pass\n",
      "For name:  s_jacobson\n",
      "s_jacobson  pass\n",
      "For name:  e_andrade\n",
      "e_andrade  pass\n",
      "For name:  t_santos\n",
      "t_santos  pass\n",
      "For name:  k_kim\n",
      "Total sample size before apply threshold:  1111\n",
      "Counter({'0000-0002-6929-5359': 211, '0000-0001-9498-284X': 154, '0000-0002-5878-8895': 139, '0000-0002-1864-3392': 92, '0000-0002-7045-8004': 57, '0000-0001-7896-6751': 57, '0000-0002-7991-9428': 55, '0000-0002-4010-1063': 45, '0000-0002-2186-3484': 28, '0000-0002-4899-1929': 25, '0000-0003-0487-4242': 24, '0000-0002-3642-1486': 22, '0000-0001-9965-3535': 17, '0000-0002-4168-757X': 17, '0000-0001-6525-3744': 14, '0000-0002-3897-0278': 14, '0000-0002-1181-5112': 12, '0000-0003-1447-9385': 11, '0000-0002-7305-8786': 11, '0000-0002-2655-7806': 10, '0000-0003-3466-5353': 9, '0000-0002-7359-663X': 8, '0000-0003-4600-8668': 6, '0000-0002-1382-7088': 5, '0000-0002-9505-4882': 5, '0000-0003-3667-9900': 4, '0000-0001-9714-6038': 4, '0000-0002-4760-0228': 3, '0000-0003-4188-7915': 3, '0000-0001-9454-0427': 3, '0000-0002-0333-6808': 3, '0000-0003-2134-4964': 3, '0000-0002-6658-047X': 3, '0000-0003-1273-379X': 3, '0000-0002-7047-3183': 3, '0000-0002-1814-9546': 3, '0000-0003-4812-6297': 2, '0000-0001-6597-578X': 2, '0000-0002-5285-9138': 2, '0000-0002-6796-7844': 2, '0000-0002-1130-8698': 2, '0000-0001-8518-8150': 2, '0000-0002-7103-924X': 2, '0000-0002-5407-0202': 1, '0000-0001-6220-8411': 1, '0000-0002-7440-6703': 1, '0000-0002-1603-7559': 1, '0000-0003-0257-1707': 1, '0000-0001-8532-6517': 1, '0000-0001-6626-316X': 1, '0000-0002-3246-9861': 1, '0000-0002-7207-4389': 1, '0000-0001-9682-9654': 1, '0000-0002-0196-3832': 1, '0000-0001-8063-6081': 1, '0000-0003-2037-3333': 1, '0000-0001-8872-6751': 1})\n",
      "Total author before apply threshoid:  57\n",
      "['0000-0001-9498-284X', '0000-0002-6929-5359', '0000-0002-5878-8895']\n",
      "Total author after apply threshoid:  3\n",
      "Total sample size after apply threshold:  504\n",
      "Total missing sample:  0\n",
      "Text embedding shape:  (504, 50000)\n",
      "Total missing sample:  0\n",
      "Citation embedding shape:  (0, 0)\n",
      "Final feature (combined embedding) shape:  (504, 50000)\n",
      "Set no plot\n",
      "tp:  36606\n",
      "fp:  10474\n",
      "fn:  6921\n",
      "tn:  72755\n",
      "tp+fp:  47080\n",
      "tp+fn: 43527\n",
      "tp:  37408\n",
      "fp:  9477\n",
      "fn:  6119\n",
      "tn:  73752\n",
      "tp+fp:  46885\n",
      "tp+fn: 43527\n",
      "tp:  35774\n",
      "fp:  13037\n",
      "fn:  7753\n",
      "tn:  70192\n",
      "tp+fp:  48811\n",
      "tp+fn: 43527\n",
      "tp:  38431\n",
      "fp:  6705\n",
      "fn:  5096\n",
      "tn:  76524\n",
      "tp+fp:  45136\n",
      "tp+fn: 43527\n",
      "tp:  36316\n",
      "fp:  11571\n",
      "fn:  7211\n",
      "tn:  71658\n",
      "tp+fp:  47887\n",
      "tp+fn: 43527\n",
      "tp:  37596\n",
      "fp:  7785\n",
      "fn:  5931\n",
      "tn:  75444\n",
      "tp+fp:  45381\n",
      "tp+fn: 43527\n",
      "tp:  37544\n",
      "fp:  9167\n",
      "fn:  5983\n",
      "tn:  74062\n",
      "tp+fp:  46711\n",
      "tp+fn: 43527\n",
      "tp:  37642\n",
      "fp:  8945\n",
      "fn:  5885\n",
      "tn:  74284\n",
      "tp+fp:  46587\n",
      "tp+fn: 43527\n",
      "tp:  38084\n",
      "fp:  7241\n",
      "fn:  5443\n",
      "tn:  75988\n",
      "tp+fp:  45325\n",
      "tp+fn: 43527\n",
      "tp:  38916\n",
      "fp:  6591\n",
      "fn:  4611\n",
      "tn:  76638\n",
      "tp+fp:  45507\n",
      "tp+fn: 43527\n",
      "tp:  39815\n",
      "fp:  4793\n",
      "fn:  3712\n",
      "tn:  78436\n",
      "tp+fp:  44608\n",
      "tp+fn: 43527\n",
      "tp:  37727\n",
      "fp:  7700\n",
      "fn:  5800\n",
      "tn:  75529\n",
      "tp+fp:  45427\n",
      "tp+fn: 43527\n",
      "tp:  38731\n",
      "fp:  6236\n",
      "fn:  4796\n",
      "tn:  76993\n",
      "tp+fp:  44967\n",
      "tp+fn: 43527\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering, MeanShift, DBSCAN\n",
    "from sklearn.cluster import estimate_bandwidth\n",
    "from statistics import mean\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "# np.random.seed(1)\n",
    "\n",
    "fileDir = \"../Data/\"+Dataset+\"/canopies_labeled/\"\n",
    "listfiles = os.listdir(fileDir)\n",
    "\n",
    "# ----------------------- different textual embedding ----------------------#\n",
    "for text_emb in pp_text_emb:\n",
    "    print(\"Load text embedding: \", text_emb)\n",
    "    # read pretrained embeddings\n",
    "    if text_emb in [\"tf\", \"tf_idf\"]:\n",
    "        all_text_emb_pid, all_text_embedding = com_func.read_text_embedding(emb_type=text_emb, training_size=\"140k\")\n",
    "    elif text_emb != \"off\":\n",
    "        all_text_embedding = com_func.read_text_embedding(emb_type=text_emb, training_size=\"140k\")\n",
    "        all_text_emb_pid = [emb[0] for emb in all_text_embedding]\n",
    "        all_text_embedding = [emb[1:] for emb in all_text_embedding]\n",
    "    \n",
    "    for citation_emb in pp_citation_emb:\n",
    "        print(\"Load citation embedding: \", citation_emb)\n",
    "        all_citation_embedding = com_func.read_citation_embedding_sorted(emb_type = citation_emb)\n",
    "        all_citation_emb_pid = [emb[0] for emb in all_citation_embedding]\n",
    "        all_citation_embedding = [emb[1:] for emb in all_citation_embedding]\n",
    "        \n",
    "        # collect statistic to output\n",
    "        allname, num_class, per_class_count, selected_sample_size, orginal_sample_size= ([] for i in range(5))\n",
    "        all_kmean_Pf1, AHC_single_Pf1,AHC_complete_Pf1,AHC_average_Pf1,AHC_ward_Pf1 = ([] for i in range(5))\n",
    "\n",
    "        cluster_count = 0\n",
    "\n",
    "        # ------- different name group in all name group --------------------#\n",
    "        for file in listfiles:\n",
    "            # group name\n",
    "            temp = file.split(\"_\")\n",
    "            name = temp[1]+\"_\"+temp[-1]\n",
    "            print(\"For name: \",name)\n",
    "            # read needed content in labeled file\n",
    "            labeled_data = com_func.read_pid_aid(fileDir+file)\n",
    "            #----------- select name group contain productive author------------------------------------#\n",
    "            #----------- (contain pair of author write more than 100 papers) ---------------------------#\n",
    "            # count number of paper each author write based on author ID\n",
    "            authorCounter = collections.Counter(labeled_data[\"authorID\"])\n",
    "            # remove name group that do not contain pair of author write more than 100 papers\n",
    "            for k in list(authorCounter):\n",
    "                if authorCounter[k] < threshold_select_name_group:\n",
    "                    del authorCounter[k]\n",
    "            # if only have one class or no class pass the threshold, not applicable\n",
    "            if(len(authorCounter)==0) or (len(authorCounter)==1):\n",
    "                print(name, \" pass\")\n",
    "            else:\n",
    "                #plot_save_path = \"../../plot/140k_sample_threshold=\"+str(threshold_min_class_sample_size)+\"/\"+\"textemb=\"+text_emb+\"_citation_emb=\"+citation_emb+\"/\"\n",
    "                plot_save_path = None\n",
    "                orginal_sample_size.append(len(labeled_data))\n",
    "                #--------select authors in name group are very productive (more than threshold)---------#\n",
    "                print(\"Total sample size before apply threshold: \",len(labeled_data))\n",
    "                # count number of paper each author write based on author ID\n",
    "                paperCounter = collections.Counter(labeled_data[\"authorID\"])\n",
    "                print(paperCounter)\n",
    "                print(\"Total author before apply threshoid: \", len(paperCounter))\n",
    "                # collect per class statistic\n",
    "                for k in list(paperCounter):\n",
    "                    if paperCounter[k] < threshold_min_class_sample_size:\n",
    "                        del paperCounter[k]\n",
    "                temp =list(paperCounter.keys())\n",
    "                print(temp)\n",
    "                print(\"Total author after apply threshoid: \", len(temp))\n",
    "                # remove samples that are smaller than threshold\n",
    "                labeled_data = labeled_data[labeled_data.authorID.isin(temp)]\n",
    "                print(\"Total sample size after apply threshold: \",len(labeled_data))\n",
    "                cluster_count = len(paperCounter)\n",
    "                selected_sample_size.append(len(labeled_data))\n",
    "                allname.append(name)\n",
    "                num_class.append(cluster_count)\n",
    "                per_class_count.append(paperCounter)\n",
    "                #------------ extract paper representation -------------------------------------------#\n",
    "                # shuffle the data\n",
    "                labeled_data = labeled_data.sample(frac=1).reset_index(drop=True)\n",
    "                # extract true label and pid\n",
    "                label = labeled_data[\"authorID\"]\n",
    "                pid = labeled_data[\"paperID\"]\n",
    "                # list of different data field\n",
    "                part_collection = []\n",
    "                # select feature wanted to fit to clustering/classification algorithm\n",
    "                # data part, textual information\n",
    "                data_part_textual = com_func.extract_embedding(all_text_embedding, all_text_emb_pid, pid)\n",
    "                print(\"Text embedding shape: \", data_part_textual.shape)\n",
    "                part_collection.append(data_part_textual)\n",
    "                # data part, citation information\n",
    "                data_part_citation = com_func.extract_embedding(all_citation_embedding, all_citation_emb_pid, pid)\n",
    "                data_part_citation.fillna(0, inplace=True)\n",
    "                print(\"Citation embedding shape: \", data_part_citation.shape)\n",
    "                part_collection.append(data_part_citation)\n",
    "                # merge different part of data data together by concatenate it all together\n",
    "                # remove empty emb (when emb set off)\n",
    "                part_collection = [part for part in part_collection if len(part)!=0]\n",
    "                if len(part_collection)>1:\n",
    "                    combinedata = np.concatenate(part_collection,axis=1)\n",
    "                elif len(part_collection)==1:\n",
    "                    if isinstance(part_collection[0], pd.DataFrame):\n",
    "                        combinedata = part_collection[0].values\n",
    "                    else:\n",
    "                        combinedata = part_collection[0]\n",
    "                else:\n",
    "                    print(\"No data available\")\n",
    "                    break\n",
    "                print(\"Final feature (combined embedding) shape: \", combinedata.shape)\n",
    "                # -------------- plot true label with PCA --------------------------------------- #\n",
    "                com_func.visualizeWithPCA(combinedata, label, plotSavingPath = plot_save_path,\n",
    "                                          name = name, plot_title='Dataset '+name+' applied PCA with ground truth labels')\n",
    "                # -------------- using converted feature vector to form cluster------------------ #\n",
    "                # --------------- algorithm need to know number of cluster ---------------------- #\n",
    "                # ------------- 100 runs for non deterministic result to stable ----------------- #\n",
    "                non_deterministic_PF1_result = collections.defaultdict(list)\n",
    "                for i in range(100):\n",
    "                    # k-means baseline\n",
    "                    kmeans = KMeans(n_clusters=cluster_count, random_state=i).fit(combinedata)\n",
    "                    kmeans_pred = kmeans.predict(combinedata)\n",
    "                    k_means_pairwise_f1 = pairwise_f1(label, kmeans_pred)\n",
    "                    non_deterministic_PF1_result[\"k_mean\"].append(k_means_pairwise_f1[0])\n",
    "                    \n",
    "                print(non_deterministic_PF1_result)\n",
    "                \n",
    "                # euclidean distance with single linkage Agglomerative hierarchical clustering\n",
    "                euclidean_single_AHC = AgglomerativeClustering(n_clusters = cluster_count, linkage = \"single\", affinity = \"euclidean\")\n",
    "                euclidean_single_AHC_pred = euclidean_single_AHC.fit_predict(combinedata)\n",
    "                # euclidean distance complete linkage Agglomerative hierarchical clustering\n",
    "                euclidean_complete_AHC = AgglomerativeClustering(n_clusters = cluster_count, linkage = \"complete\", affinity = \"euclidean\")\n",
    "                euclidean_complete_AHC_pred = euclidean_complete_AHC.fit_predict(combinedata)\n",
    "                # euclidean distance average linkage Agglomerative hierarchical clustering\n",
    "                euclidean_average_AHC = AgglomerativeClustering(n_clusters = cluster_count, linkage = \"average\", affinity = \"euclidean\")\n",
    "                euclidean_average_AHC_pred = euclidean_average_AHC.fit_predict(combinedata)\n",
    "                # ward linkage Agglomerative hierarchical clustering\n",
    "                ward_AHC = AgglomerativeClustering(n_clusters = cluster_count, linkage = \"ward\")\n",
    "                ward_AHC_pred = ward_AHC.fit_predict(combinedata)\n",
    "                # Spectral Clustering\n",
    "                # SpectralCluster = SpectralClustering(n_clusters=cluster_count, affinity = \"nearest_neighbors\")\n",
    "                # SpectralCluster_pred = SpectralCluster.fit_predict(combinedata)\n",
    "                # --------------- algorithm auto estimate number of cluster --------------------- #\n",
    "                # mean shift\n",
    "                # estimate bandwidth for mean shift\n",
    "                # bandwidth = estimate_bandwidth(combinedata, quantile=0.3)\n",
    "                # print(\"bandwidth: \",bandwidth)\n",
    "                # MScluster = MeanShift(bandwidth=bandwidth)\n",
    "                # MSclustering_pred = MScluster.fit_predict(combinedata)\n",
    "                # MS_estimated_cluster_count = np.unique(MSclustering_pred)\n",
    "                # print(\"MS:\",MSclustering_pred)\n",
    "                # print(\"MS:\",MS_estimated_cluster_count)\n",
    "                # DBSCAN\n",
    "                # DBSCANcluster = DBSCAN()\n",
    "                # DBSCANcluster_pred = DBSCANcluster.fit_predict(combinedata)\n",
    "                # DBSCAN_estimated_cluster_count = np.unique(DBSCANcluster_pred)\n",
    "                # print(\"DBSCAN: \",DBSCANcluster_pred)\n",
    "                # print(\"DBSCAN: \",DBSCAN_estimated_cluster_count)\n",
    "                # -------------- evaluate clustering result using pairwise f1 ------------------- #\n",
    "                euclidean_single_AHC_pairwise_f1 = pairwise_f1(label, euclidean_single_AHC_pred)\n",
    "                euclidean_complete_AHC_pairwise_f1 = pairwise_f1(label, euclidean_complete_AHC_pred)\n",
    "                euclidean_average_AHC_pairwise_f1 = pairwise_f1(label, euclidean_average_AHC_pred)\n",
    "                ward_AHC_pairwise_f1 = pairwise_f1(label, ward_AHC_pred)\n",
    "                # SpectralCluster_pairwise_f1_score = pairwise_f1(label, SpectralCluster_pred)\n",
    "                # MSclustering_pairwise_f1_score = pairwise_f1(label, MSclustering_pred)\n",
    "                # DBSCANcluster_pairwise_f1_score = pairwise_f1(label, DBSCANcluster_pred)\n",
    "                print(\"Average k_mean pf1: \",non_deterministic_PF1_result[\"k_mean\"])\n",
    "                print(kmeans)\n",
    "                print(\"euclidean_single_AHC_pairwise_f1: \", euclidean_single_AHC_pairwise_f1)\n",
    "                print(euclidean_single_AHC)\n",
    "                print(\"euclidean_complete_AHC_pairwise_f1: \", euclidean_complete_AHC_pairwise_f1)\n",
    "                print(euclidean_complete_AHC)\n",
    "                print(\"euclidean_average_AHC_pairwise_f1: \", euclidean_average_AHC_pairwise_f1)\n",
    "                print(euclidean_average_AHC)\n",
    "                print(\"ward_AHC_pairwise_f1_score: \", ward_AHC_pairwise_f1)\n",
    "                print(ward_AHC)\n",
    "                # print(\"SpectralCluster_pairwise_f1_score: \",SpectralCluster_pairwise_f1_score)\n",
    "                # print(SpectralCluster)\n",
    "                # print(\"mean_shift_pf1: \", MSclustering_pairwise_f1_score)\n",
    "                # print(MScluster)\n",
    "                # print(\"DBSCANcluster_pairwise_f1_score: \", DBSCANcluster_pairwise_f1_score)\n",
    "                # print(DBSCANcluster)\n",
    "                \n",
    "                all_kmean_Pf1.append(mean(non_deterministic_PF1_result[\"k_mean\"]))\n",
    "                AHC_single_Pf1.append(euclidean_single_AHC_pairwise_f1[0])\n",
    "                AHC_complete_Pf1.append(euclidean_complete_AHC_pairwise_f1[0])\n",
    "                AHC_average_Pf1.append(euclidean_average_AHC_pairwise_f1[0])\n",
    "                AHC_ward_Pf1.append(ward_AHC_pairwise_f1[0])\n",
    "\n",
    "        # write evaluation result to excel\n",
    "        output = pd.DataFrame({'Name Group':allname, \"Class number\":num_class, \"Per class size\":per_class_count,\n",
    "                               \"Total selected sample size\":selected_sample_size,\"Orginal sample size\":orginal_sample_size,\n",
    "                               \"kmean_Pf1\":all_kmean_Pf1, \"AHC_single_Pf1\": AHC_single_Pf1,\n",
    "                               \"AHC_complete_Pf1\": AHC_complete_Pf1,\"AHC_average_Pf1\": AHC_average_Pf1,\n",
    "                               \"AHC_ward_Pf1\": AHC_ward_Pf1})\n",
    "\n",
    "        savePath = \"../../result/\"+Dataset+\"/1_Clustering_sample=140k/\"\n",
    "        filename = \"citation=\"+citation_emb+\"_textual=\"+text_emb+\"_threshold=\"+str(threshold_min_class_sample_size)+\".csv\"\n",
    "        com_func.write_csv_df(savePath, filename, output)\n",
    "        print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cosine similarity with single linkage Agglomerative hierarchical clustering\n",
    "cosine_single_AHC = AgglomerativeClustering(n_clusters = cluster_count, linkage = \"single\", affinity = \"cosine\")\n",
    "cosine_single_AHC_pred = cosine_single_AHC.fit_predict(combinedata)\n",
    "# cosine similarity complete linkage Agglomerative hierarchical clustering\n",
    "cosine_complete_AHC = AgglomerativeClustering(n_clusters = cluster_count, linkage = \"complete\", affinity = \"cosine\")\n",
    "cosine_complete_AHC_pred = cosine_complete_AHC.fit_predict(combinedata)\n",
    "# cosine similarity average linkage Agglomerative hierarchical clustering\n",
    "cosine_average_AHC = AgglomerativeClustering(n_clusters = cluster_count, linkage = \"average\", affinity = \"cosine\")\n",
    "cosine_average_AHC_pred = cosine_average_AHC.fit_predict(combinedata)\n",
    "\n",
    "cosine_single_AHC_pairwise_f1 = pairwise_f1(label, cosine_single_AHC_pred)\n",
    "cosine_complete_AHC_pairwise_f1 = pairwise_f1(label, cosine_complete_AHC_pred)\n",
    "cosine_average_AHC_pairwise_f1 = pairwise_f1(label, cosine_average_AHC_pred)\n",
    "\n",
    "print(\"cosine_single_AHC_pairwise_f1: \", cosine_single_AHC_pairwise_f1)\n",
    "print(cosine_single_AHC)\n",
    "print(\"cosine_complete_AHC_pairwise_f1: \", cosine_complete_AHC_pairwise_f1)\n",
    "print(cosine_complete_AHC)\n",
    "print(\"cosine_average_AHC_pairwise_f1: \", cosine_average_AHC_pairwise_f1)\n",
    "print(cosine_average_AHC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T08:25:01.342232Z",
     "start_time": "2020-01-08T08:25:00.918942Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# def create_cluster_partition(labels):\n",
    "#     \"\"\"\n",
    "#     Create a dictionary where the key is the row number and the value is the\n",
    "#     actual label.\n",
    "#     In this case, labels is an array where the position corresponds to the row\n",
    "#     number and the value is an integer indicating the label.\n",
    "#     \"\"\"\n",
    "#     labels_lookup = {}\n",
    "#     for idx, label in enumerate(labels):\n",
    "#         labels_lookup[idx] = label\n",
    "#     # --------- create cluster_partition from label ---------------#\n",
    "#     cluster_partition = []\n",
    "#     unique_labels = pd.unique(labels)\n",
    "#     for unique_label in unique_labels:\n",
    "#         cluster_partition.append([idx for idx, item in enumerate(labels) if item == unique_label])\n",
    "#     return labels_lookup, cluster_partition\n",
    "\n",
    "\n",
    "# # method to evaluate cluster result\n",
    "# def pairwise_f1_v2(true_label, pred_label):\n",
    "#     datasize = len(true_label)\n",
    "#     # --------- create cluster_partition from label ---------------#\n",
    "#     _, true_cluster_partition = create_cluster_partition(true_label)\n",
    "#     _, pred_cluster_partition = create_cluster_partition(pred_label)\n",
    "#     num_pred_cluster = len(pred_cluster_partition)\n",
    "#     purity = 0\n",
    "#     f1_measure_i = 0\n",
    "#     f1_measure = 0\n",
    "#     print(true_cluster_partition)\n",
    "#     print(pred_cluster_partition)\n",
    "    \n",
    "#     TP = 0\n",
    "#     TN = 0\n",
    "#     FP = 0\n",
    "#     FN = 0\n",
    "    \n",
    "#     # For each cluster in clustering\n",
    "#     for i, pred_cluster in enumerate(pred_cluster_partition):\n",
    "#         max_intersec_ground_index = 0\n",
    "#         max_intersec = len(set(true_cluster_partition[0]) & set(pred_cluster_partition[i]))\n",
    "#         print(max_intersec)\n",
    "#         # For each cluster in ground truth\n",
    "#         for j in range(1, len(true_cluster_partition)):\n",
    "#             # Get the max number of elements which belongs both to the pred_cluster_partition[i] and the true_cluster_partition[j]\n",
    "#             local_intersec = len(set(true_cluster_partition[j]) & set(pred_cluster_partition[i]))\n",
    "#             if (local_intersec > max_intersec):\n",
    "#                 max_intersec = local_intersec\n",
    "#                 max_intersec_ground_index = j\n",
    "        \n",
    "#         # Precision of pred_cluster_partition[i]\n",
    "#         precision_i = 1.0 * max_intersec / len(pred_cluster_partition[i])\n",
    "#         # Recall of pred_cluster_partition[i]\n",
    "#         recall_i = 1.0 * max_intersec / len(true_cluster_partition[max_intersec_ground_index])\n",
    "        \n",
    "#         # Purity\n",
    "#         purity += (1.0 * len(pred_cluster_partition[i]) * precision_i / datasize)\n",
    "#         # F-measure\n",
    "#         f1_measure_i = (2.0 * precision_i * recall_i) / (precision_i + recall_i)\n",
    "#         f1_measure += f1_measure_i\n",
    "        \n",
    "#     final_f1_measure = f1_measure/num_pred_cluster\n",
    "#     print(final_f1_measure)\n",
    "    \n",
    "#     return final_f1_measure\n",
    "\n",
    "# pairwise_f1_v2([2,2,2,2,1,2,0,0,1,0,2,0,1,1,2,1,2],[1,1,1,1,1,1,2,2,2,2,2,2,0,0,0,0,0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
