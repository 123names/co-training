{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T03:34:25.746336Z",
     "start_time": "2020-06-27T03:34:25.067011Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scikit-learn version is 0.23.1.\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))\n",
    "\n",
    "# update a pip in the current Jupyter kernel\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T03:34:26.878897Z",
     "start_time": "2020-06-27T03:34:25.750569Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import com_func\n",
    "\n",
    "# parameters\n",
    "#----- threshold for selecting set of name group -----------#\n",
    "threshold_select_name_group = 100\n",
    "#----- threshold for selecting min sample in name group ----#\n",
    "threshold_min_class_sample_size = 100\n",
    "\n",
    "Dataset = \"pubmed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T03:34:26.891515Z",
     "start_time": "2020-06-27T03:34:26.881012Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# text embedding only\n",
    "pp_text_emb = [\"tf\", \"tf_idf\", \"lsa\", \"pv_dm\", \"pv_dbow\"]\n",
    "pp_citation_emb = [\"off\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T03:34:26.895991Z",
     "start_time": "2020-06-27T03:34:26.893325Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# citation embedding only\n",
    "pp_text_emb = [\"off\"]\n",
    "pp_citation_emb = [\"node2vec\", \"n2v\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T03:34:26.900150Z",
     "start_time": "2020-06-27T03:34:26.897450Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combined embedding\n",
    "pp_text_emb = [\"lsa\", \"pv_dm\", \"pv_dbow\"]\n",
    "pp_citation_emb = [\"n2v\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T03:34:26.904899Z",
     "start_time": "2020-06-27T03:34:26.901622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lsa', 'pv_dm', 'pv_dbow']\n",
      "['n2v']\n"
     ]
    }
   ],
   "source": [
    "print(pp_text_emb)\n",
    "print(pp_citation_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T03:34:26.973545Z",
     "start_time": "2020-06-27T03:34:26.906450Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# method to evaluate cluster result\n",
    "# https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html\n",
    "def pairwise_f1(true_label, pred_label):\n",
    "    total_pair = len(true_label)*(len(true_label)-1)/2\n",
    "    # predictions that are positive, TP+FP\n",
    "    pred_pos = 0\n",
    "    # conditions that are positive, TP+FN\n",
    "    cond_pos = 0\n",
    "    # Pairs Correctly Predicted To SameAuthor, TP\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for i in range(len(true_label)):\n",
    "        for j in range(i + 1, len(true_label)):\n",
    "            if pred_label[i] == pred_label[j]:\n",
    "                pred_pos +=1\n",
    "            if true_label[i] == true_label[j]:\n",
    "                cond_pos +=1\n",
    "            if (true_label[i] == true_label[j]) and (pred_label[i] == pred_label[j]):\n",
    "                #print(true_label[i], \" \", i)\n",
    "                #print(pred_label[i])\n",
    "                tp +=1\n",
    "    fp = pred_pos-tp\n",
    "    fn = cond_pos-tp\n",
    "    tn = int(total_pair-tp-fp-fn)\n",
    "    print(\"tp: \", tp)\n",
    "    print(\"fp: \", fp)\n",
    "    print(\"fn: \", fn)\n",
    "    print(\"tn: \", tn)\n",
    "    print(\"tp+fp: \", pred_pos)\n",
    "    print(\"tp+fn:\", cond_pos)\n",
    "    # calculate pairwise f1 score\n",
    "    if tp == 0:\n",
    "        pairwise_precision = 0\n",
    "        pairwise_recall = 0\n",
    "        pairwise_f1 = 0\n",
    "    else:\n",
    "        pairwise_precision = tp / pred_pos\n",
    "        pairwise_recall = tp / cond_pos\n",
    "        pairwise_f1 = (2 * pairwise_precision * pairwise_recall) / (pairwise_precision + pairwise_recall)\n",
    "        \n",
    "    return pairwise_f1, pairwise_precision, pairwise_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T03:35:10.145446Z",
     "start_time": "2020-06-27T03:34:26.975528Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load text embedding:  lsa\n",
      "Total text vector records: 135796\n",
      "Vector dimension:  100\n",
      "Load citation embedding:  n2v\n",
      "Total citation vector records: 124922\n",
      "Vector dimension:  100\n",
      "For name:  j_read\n",
      "j_read  pass\n",
      "For name:  f_esteves\n",
      "f_esteves  pass\n",
      "For name:  c_miller\n",
      "c_miller  pass\n",
      "For name:  r_jha\n",
      "r_jha  pass\n",
      "For name:  a_lowe\n",
      "a_lowe  pass\n",
      "For name:  a_vega\n",
      "a_vega  pass\n",
      "For name:  k_smith\n",
      "k_smith  pass\n",
      "For name:  j_gordon\n",
      "j_gordon  pass\n",
      "For name:  s_liao\n",
      "s_liao  pass\n",
      "For name:  j_qian\n",
      "j_qian  pass\n",
      "For name:  s_bernardi\n",
      "s_bernardi  pass\n",
      "For name:  t_hill\n",
      "t_hill  pass\n",
      "For name:  s_schindler\n",
      "s_schindler  pass\n",
      "For name:  j_williams\n",
      "j_williams  pass\n",
      "For name:  s_jacobson\n",
      "s_jacobson  pass\n",
      "For name:  e_andrade\n",
      "e_andrade  pass\n",
      "For name:  t_santos\n",
      "t_santos  pass\n",
      "For name:  k_kim\n",
      "Total sample size before apply threshold:  1111\n",
      "Counter({'0000-0002-6929-5359': 211, '0000-0001-9498-284X': 154, '0000-0002-5878-8895': 139, '0000-0002-1864-3392': 92, '0000-0002-7045-8004': 57, '0000-0001-7896-6751': 57, '0000-0002-7991-9428': 55, '0000-0002-4010-1063': 45, '0000-0002-2186-3484': 28, '0000-0002-4899-1929': 25, '0000-0003-0487-4242': 24, '0000-0002-3642-1486': 22, '0000-0001-9965-3535': 17, '0000-0002-4168-757X': 17, '0000-0001-6525-3744': 14, '0000-0002-3897-0278': 14, '0000-0002-1181-5112': 12, '0000-0003-1447-9385': 11, '0000-0002-7305-8786': 11, '0000-0002-2655-7806': 10, '0000-0003-3466-5353': 9, '0000-0002-7359-663X': 8, '0000-0003-4600-8668': 6, '0000-0002-1382-7088': 5, '0000-0002-9505-4882': 5, '0000-0003-3667-9900': 4, '0000-0001-9714-6038': 4, '0000-0002-4760-0228': 3, '0000-0003-4188-7915': 3, '0000-0001-9454-0427': 3, '0000-0002-0333-6808': 3, '0000-0003-2134-4964': 3, '0000-0002-6658-047X': 3, '0000-0003-1273-379X': 3, '0000-0002-7047-3183': 3, '0000-0002-1814-9546': 3, '0000-0003-4812-6297': 2, '0000-0001-6597-578X': 2, '0000-0002-5285-9138': 2, '0000-0002-6796-7844': 2, '0000-0002-1130-8698': 2, '0000-0001-8518-8150': 2, '0000-0002-7103-924X': 2, '0000-0002-5407-0202': 1, '0000-0001-6220-8411': 1, '0000-0002-7440-6703': 1, '0000-0002-1603-7559': 1, '0000-0003-0257-1707': 1, '0000-0001-8532-6517': 1, '0000-0001-6626-316X': 1, '0000-0002-3246-9861': 1, '0000-0002-7207-4389': 1, '0000-0001-9682-9654': 1, '0000-0002-0196-3832': 1, '0000-0001-8063-6081': 1, '0000-0003-2037-3333': 1, '0000-0001-8872-6751': 1})\n",
      "Total author before apply threshoid:  57\n",
      "['0000-0001-9498-284X', '0000-0002-6929-5359', '0000-0002-5878-8895']\n",
      "Total author after apply threshoid:  3\n",
      "Total sample size after apply threshold:  504\n",
      "Total missing sample:  0\n",
      "Text embedding shape:  (504, 100)\n",
      "Missing Sample:  12919017\n",
      "Missing Sample:  17359037\n",
      "Missing Sample:  16999403\n",
      "Missing Sample:  12833599\n",
      "Missing Sample:  19358601\n",
      "Missing Sample:  11973332\n",
      "Missing Sample:  14769839\n",
      "Missing Sample:  15845571\n",
      "Missing Sample:  16606283\n",
      "Missing Sample:  19260493\n",
      "Missing Sample:  24511052\n",
      "Missing Sample:  23965436\n",
      "Missing Sample:  14623918\n",
      "Missing Sample:  20446751\n",
      "Missing Sample:  12495907\n",
      "Missing Sample:  24086948\n",
      "Missing Sample:  18578481\n",
      "Missing Sample:  20422867\n",
      "Missing Sample:  19618910\n",
      "Missing Sample:  22983732\n",
      "Missing Sample:  17571173\n",
      "Missing Sample:  17629037\n",
      "Missing Sample:  26614023\n",
      "Missing Sample:  16685393\n",
      "Missing Sample:  18052393\n",
      "Missing Sample:  18788720\n",
      "Missing Sample:  15787556\n",
      "Missing Sample:  18037618\n",
      "Missing Sample:  17564465\n",
      "Missing Sample:  17722888\n",
      "Missing Sample:  23214901\n",
      "Missing Sample:  23215215\n",
      "Missing Sample:  11559199\n",
      "Missing Sample:  12630031\n",
      "Missing Sample:  20218591\n",
      "Missing Sample:  15532789\n",
      "Missing Sample:  16204727\n",
      "Missing Sample:  16861879\n",
      "Missing Sample:  19471081\n",
      "Missing Sample:  12068151\n",
      "Missing Sample:  21661423\n",
      "Missing Sample:  17928753\n",
      "Missing Sample:  21828679\n",
      "Missing Sample:  16503553\n",
      "Missing Sample:  15227745\n",
      "Missing Sample:  11456562\n",
      "Missing Sample:  26329310\n",
      "Total missing sample:  47\n",
      "Citation embedding shape:  (504, 100)\n",
      "Final feature (combined embedding) shape:  (504, 200)\n",
      "Set no plot\n",
      "tp:  24581\n",
      "fp:  17795\n",
      "fn:  18946\n",
      "tn:  65434\n",
      "tp+fp:  42376\n",
      "tp+fn: 43527\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ee8fb4d5f63f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0;31m# -------------- evaluate clustering result using pairwise f1 ------------------- #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0mk_means_pairwise_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpairwise_f1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkmeans_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0meuclidean_single_AHC_pairwise_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpairwise_f1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meuclidean_single_AHC_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m                 \u001b[0meuclidean_complete_AHC_pairwise_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpairwise_f1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meuclidean_complete_AHC_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0meuclidean_average_AHC_pairwise_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpairwise_f1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meuclidean_average_AHC_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-e3dd8c8f9566>\u001b[0m in \u001b[0;36mpairwise_f1\u001b[0;34m(true_label, pred_label)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpred_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpred_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mpred_pos\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mtrue_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtrue_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                 \u001b[0mcond_pos\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrue_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtrue_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpred_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpred_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/intel/intelpython3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/intel/intelpython3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4402\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"getitem\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4403\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4404\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4405\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4406\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/intel/intelpython3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mdtype\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0mReturn\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0munderlying\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m         \"\"\"\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/intel/intelpython3/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mdtype\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1548\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_block\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/intel/intelpython3/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_block\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1518\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1520\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import io\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering, MeanShift, DBSCAN\n",
    "from sklearn.cluster import estimate_bandwidth\n",
    "from statistics import mean \n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "fileDir = \"../Data/\"+Dataset+\"/canopies_labeled/\"\n",
    "listfiles = os.listdir(fileDir)\n",
    "\n",
    "# ----------------------- different textual embedding ----------------------#\n",
    "for text_emb in pp_text_emb:\n",
    "    print(\"Load text embedding: \", text_emb)\n",
    "    # read pretrained embeddings\n",
    "    all_textual_embedding= com_func.read_text_embedding(emb_type=text_emb, training_size=\"140k\")\n",
    "    all_textual_emb_pid = [emb[0] for emb in all_textual_embedding]\n",
    "    all_textual_embedding = [emb[1:] for emb in all_textual_embedding]\n",
    "    \n",
    "    for citation_emb in pp_citation_emb:\n",
    "        print(\"Load citation embedding: \", citation_emb)\n",
    "        all_citation_embedding = com_func.read_citation_embedding_sorted(emb_type = citation_emb)\n",
    "        all_citation_emb_pid = [emb[0] for emb in all_citation_embedding]\n",
    "        all_citation_embedding = [emb[1:] for emb in all_citation_embedding]\n",
    "        \n",
    "        # collect statistic to output\n",
    "        allname, num_class, per_class_count, selected_sample_size, orginal_sample_size= ([] for i in range(5))\n",
    "        all_kmean_Pf1, AHC_single_Pf1,AHC_complete_Pf1,AHC_average_Pf1,AHC_ward_Pf1 = ([] for i in range(5))\n",
    "\n",
    "        cluster_count = 0\n",
    "\n",
    "        # ------- different name group in all name group --------------------#\n",
    "        for file in listfiles:\n",
    "            # group name\n",
    "            temp = file.split(\"_\")\n",
    "            name = temp[1]+\"_\"+temp[-1]\n",
    "            print(\"For name: \",name)\n",
    "            # read needed content in labeled file\n",
    "            labeled_data = com_func.read_pid_aid(fileDir+file)\n",
    "            #----------- select name group contain productive author------------------------------------#\n",
    "            #----------- (contain pair of author write more than 100 papers) ---------------------------#\n",
    "            # count number of paper each author write based on author ID\n",
    "            authorCounter = collections.Counter(labeled_data[\"authorID\"])\n",
    "            # remove name group that do not contain pair of author write more than 100 papers\n",
    "            for k in list(authorCounter):\n",
    "                if authorCounter[k] < threshold_select_name_group:\n",
    "                    del authorCounter[k]\n",
    "            # if only have one class or no class pass the threshold, not applicable\n",
    "            if(len(authorCounter)==0) or (len(authorCounter)==1):\n",
    "                print(name, \" pass\")\n",
    "            else:\n",
    "                #plot_save_path = \"../../plot/140k_sample_threshold=\"+str(threshold_min_class_sample_size)+\"/\"+\"textemb=\"+text_emb+\"_citation_emb=\"+citation_emb+\"/\"\n",
    "                plot_save_path = None\n",
    "                orginal_sample_size.append(len(labeled_data))\n",
    "                #--------select authors in name group are very productive (more than threshold)---------#\n",
    "                print(\"Total sample size before apply threshold: \",len(labeled_data))\n",
    "                # count number of paper each author write based on author ID\n",
    "                paperCounter = collections.Counter(labeled_data[\"authorID\"])\n",
    "                print(paperCounter)\n",
    "                print(\"Total author before apply threshoid: \", len(paperCounter))\n",
    "                # collect per class statistic\n",
    "                for k in list(paperCounter):\n",
    "                    if paperCounter[k] < threshold_min_class_sample_size:\n",
    "                        del paperCounter[k]\n",
    "                temp =list(paperCounter.keys())\n",
    "                print(temp)\n",
    "                print(\"Total author after apply threshoid: \", len(temp))\n",
    "                # remove samples that are smaller than threshold\n",
    "                labeled_data = labeled_data[labeled_data.authorID.isin(temp)]\n",
    "                print(\"Total sample size after apply threshold: \",len(labeled_data))\n",
    "                cluster_count = len(paperCounter)\n",
    "                selected_sample_size.append(len(labeled_data))\n",
    "                allname.append(name)\n",
    "                num_class.append(cluster_count)\n",
    "                per_class_count.append(paperCounter)\n",
    "                #------------ extract paper representation -------------------------------------------#\n",
    "                # shuffle the data\n",
    "                labeled_data = labeled_data.sample(frac=1).reset_index(drop=True)\n",
    "                # extract true label and pid\n",
    "                label = labeled_data[\"authorID\"]\n",
    "                pid = labeled_data[\"paperID\"]\n",
    "                # list of different data field\n",
    "                part_collection = []\n",
    "                # select feature wanted to fit to clustering/classification algorithm\n",
    "                # data part, textual information\n",
    "                data_part_textual = com_func.extract_embedding(all_textual_embedding, all_textual_emb_pid, pid)\n",
    "                print(\"Text embedding shape: \", data_part_textual.shape)\n",
    "                part_collection.append(data_part_textual)\n",
    "                # data part, citation information\n",
    "                data_part_citation = com_func.extract_embedding(all_citation_embedding, all_citation_emb_pid, pid)\n",
    "                data_part_citation.fillna(0, inplace=True)\n",
    "                print(\"Citation embedding shape: \", data_part_citation.shape)\n",
    "                part_collection.append(data_part_citation)\n",
    "                # merge different part of data data together by concatenate it all together\n",
    "                # remove empty emb (when emb set off)\n",
    "                part_collection = [part for part in part_collection if len(part)!=0]\n",
    "                if len(part_collection)>1:\n",
    "                    combinedata = np.concatenate(part_collection,axis=1)\n",
    "                elif len(part_collection)==1:\n",
    "                    if isinstance(part_collection[0], pd.DataFrame):\n",
    "                        combinedata = part_collection[0].values\n",
    "                    else:\n",
    "                        combinedata = part_collection[0]\n",
    "                else:\n",
    "                    print(\"No data available\")\n",
    "                    break\n",
    "                print(\"Final feature (combined embedding) shape: \", combinedata.shape)\n",
    "                # -------------- plot true label with PCA --------------------------------------- #\n",
    "                com_func.visualizeWithPCA(combinedata, label, plotSavingPath = plot_save_path,\n",
    "                                          name = name, plot_title='Dataset '+name+' applied PCA with ground truth labels')\n",
    "                # -------------- using converted feature vector to form cluster------------------ #\n",
    "                # --------------- algorithm need to know number of cluster ---------------------- #\n",
    "                \n",
    "                # k-means baseline\n",
    "                kmeans = KMeans(n_clusters=cluster_count).fit(combinedata)\n",
    "                kmeans_pred = kmeans.predict(combinedata)\n",
    "                # euclidean distance with single linkage Agglomerative hierarchical clustering\n",
    "                euclidean_single_AHC = AgglomerativeClustering(n_clusters = cluster_count, linkage = \"single\", affinity = \"euclidean\")\n",
    "                euclidean_single_AHC_pred = euclidean_single_AHC.fit_predict(combinedata)\n",
    "                # euclidean distance complete linkage Agglomerative hierarchical clustering\n",
    "                euclidean_complete_AHC = AgglomerativeClustering(n_clusters = cluster_count, linkage = \"complete\", affinity = \"euclidean\")\n",
    "                euclidean_complete_AHC_pred = euclidean_complete_AHC.fit_predict(combinedata)\n",
    "                # euclidean distance average linkage Agglomerative hierarchical clustering\n",
    "                euclidean_average_AHC = AgglomerativeClustering(n_clusters = cluster_count, linkage = \"average\", affinity = \"euclidean\")\n",
    "                euclidean_average_AHC_pred = euclidean_average_AHC.fit_predict(combinedata)\n",
    "                # ward linkage Agglomerative hierarchical clustering\n",
    "                ward_AHC = AgglomerativeClustering(n_clusters = cluster_count, linkage = \"ward\")\n",
    "                ward_AHC_pred = ward_AHC.fit_predict(combinedata)\n",
    "                # Spectral Clustering\n",
    "                # SpectralCluster = SpectralClustering(n_clusters=cluster_count, affinity = \"nearest_neighbors\")\n",
    "                # SpectralCluster_pred = SpectralCluster.fit_predict(combinedata)\n",
    "                # --------------- algorithm auto estimate number of cluster --------------------- #\n",
    "                # mean shift\n",
    "                # estimate bandwidth for mean shift\n",
    "                # bandwidth = estimate_bandwidth(combinedata, quantile=0.3)\n",
    "                # print(\"bandwidth: \",bandwidth)\n",
    "                # MScluster = MeanShift(bandwidth=bandwidth)\n",
    "                # MSclustering_pred = MScluster.fit_predict(combinedata)\n",
    "                # MS_estimated_cluster_count = np.unique(MSclustering_pred)\n",
    "                # print(\"MS:\",MSclustering_pred)\n",
    "                # print(\"MS:\",MS_estimated_cluster_count)\n",
    "                # DBSCAN\n",
    "                # DBSCANcluster = DBSCAN()\n",
    "                # DBSCANcluster_pred = DBSCANcluster.fit_predict(combinedata)\n",
    "                # DBSCAN_estimated_cluster_count = np.unique(DBSCANcluster_pred)\n",
    "                # print(\"DBSCAN: \",DBSCANcluster_pred)\n",
    "                # print(\"DBSCAN: \",DBSCAN_estimated_cluster_count)\n",
    "                # -------------- evaluate clustering result using pairwise f1 ------------------- #\n",
    "                k_means_pairwise_f1 = pairwise_f1(label, kmeans_pred)\n",
    "                euclidean_single_AHC_pairwise_f1 = pairwise_f1(label, euclidean_single_AHC_pred)\n",
    "                euclidean_complete_AHC_pairwise_f1 = pairwise_f1(label, euclidean_complete_AHC_pred)\n",
    "                euclidean_average_AHC_pairwise_f1 = pairwise_f1(label, euclidean_average_AHC_pred)\n",
    "                ward_AHC_pairwise_f1 = pairwise_f1(label, ward_AHC_pred)\n",
    "                # SpectralCluster_pairwise_f1_score = pairwise_f1(label, SpectralCluster_pred)\n",
    "                # MSclustering_pairwise_f1_score = pairwise_f1(label, MSclustering_pred)\n",
    "                # DBSCANcluster_pairwise_f1_score = pairwise_f1(label, DBSCANcluster_pred)\n",
    "                print(\"k_mean pf1: \",k_means_pairwise_f1)\n",
    "                print(kmeans)\n",
    "                print(\"euclidean_single_AHC_pairwise_f1: \", euclidean_single_AHC_pairwise_f1)\n",
    "                print(euclidean_single_AHC)\n",
    "                print(\"euclidean_complete_AHC_pairwise_f1: \", euclidean_complete_AHC_pairwise_f1)\n",
    "                print(euclidean_complete_AHC)\n",
    "                print(\"euclidean_average_AHC_pairwise_f1: \", euclidean_average_AHC_pairwise_f1)\n",
    "                print(euclidean_average_AHC)\n",
    "                print(\"ward_AHC_pairwise_f1_score: \", ward_AHC_pairwise_f1)\n",
    "                print(ward_AHC)\n",
    "                # print(\"SpectralCluster_pairwise_f1_score: \",SpectralCluster_pairwise_f1_score)\n",
    "                # print(SpectralCluster)\n",
    "                # print(\"mean_shift_pf1: \", MSclustering_pairwise_f1_score)\n",
    "                # print(MScluster)\n",
    "                # print(\"DBSCANcluster_pairwise_f1_score: \", DBSCANcluster_pairwise_f1_score)\n",
    "                # print(DBSCANcluster)\n",
    "                \n",
    "                all_kmean_Pf1.append(k_means_pairwise_f1[0])\n",
    "                AHC_single_Pf1.append(euclidean_single_AHC_pairwise_f1[0])\n",
    "                AHC_complete_Pf1.append(euclidean_complete_AHC_pairwise_f1[0])\n",
    "                AHC_average_Pf1.append(euclidean_average_AHC_pairwise_f1[0])\n",
    "                AHC_ward_Pf1.append(ward_AHC_pairwise_f1[0])\n",
    "\n",
    "        # write evaluation result to excel\n",
    "        output = pd.DataFrame({'Name Group':allname, \"Class number\":num_class, \"Per class size\":per_class_count,\n",
    "                               \"Total selected sample size\":selected_sample_size,\"Orginal sample size\":orginal_sample_size,\n",
    "                               \"kmean_Pf1\":all_kmean_Pf1, \"AHC_single_Pf1\": AHC_single_Pf1,\n",
    "                               \"AHC_complete_Pf1\": AHC_complete_Pf1,\"AHC_average_Pf1\": AHC_average_Pf1,\n",
    "                               \"AHC_ward_Pf1\": AHC_ward_Pf1})\n",
    "\n",
    "        #savePath = \"../../result/\"+Dataset+\"/1_Clustering_sample=140k/\"\n",
    "        #filename = \"citation=\"+citation_emb+\"_textual=\"+text_emb+\"_threshold=\"+str(threshold_min_class_sample_size)+\".csv\"\n",
    "        #com_func.write_csv_df(savePath, filename, output)\n",
    "        print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cosine similarity with single linkage Agglomerative hierarchical clustering\n",
    "cosine_single_AHC = AgglomerativeClustering(n_clusters = cluster_count, linkage = \"single\", affinity = \"cosine\")\n",
    "cosine_single_AHC_pred = cosine_single_AHC.fit_predict(combinedata)\n",
    "# cosine similarity complete linkage Agglomerative hierarchical clustering\n",
    "cosine_complete_AHC = AgglomerativeClustering(n_clusters = cluster_count, linkage = \"complete\", affinity = \"cosine\")\n",
    "cosine_complete_AHC_pred = cosine_complete_AHC.fit_predict(combinedata)\n",
    "# cosine similarity average linkage Agglomerative hierarchical clustering\n",
    "cosine_average_AHC = AgglomerativeClustering(n_clusters = cluster_count, linkage = \"average\", affinity = \"cosine\")\n",
    "cosine_average_AHC_pred = cosine_average_AHC.fit_predict(combinedata)\n",
    "\n",
    "cosine_single_AHC_pairwise_f1 = pairwise_f1(label, cosine_single_AHC_pred)\n",
    "cosine_complete_AHC_pairwise_f1 = pairwise_f1(label, cosine_complete_AHC_pred)\n",
    "cosine_average_AHC_pairwise_f1 = pairwise_f1(label, cosine_average_AHC_pred)\n",
    "\n",
    "print(\"cosine_single_AHC_pairwise_f1: \", cosine_single_AHC_pairwise_f1)\n",
    "print(cosine_single_AHC)\n",
    "print(\"cosine_complete_AHC_pairwise_f1: \", cosine_complete_AHC_pairwise_f1)\n",
    "print(cosine_complete_AHC)\n",
    "print(\"cosine_average_AHC_pairwise_f1: \", cosine_average_AHC_pairwise_f1)\n",
    "print(cosine_average_AHC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T08:25:01.342232Z",
     "start_time": "2020-01-08T08:25:00.918942Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# def create_cluster_partition(labels):\n",
    "#     \"\"\"\n",
    "#     Create a dictionary where the key is the row number and the value is the\n",
    "#     actual label.\n",
    "#     In this case, labels is an array where the position corresponds to the row\n",
    "#     number and the value is an integer indicating the label.\n",
    "#     \"\"\"\n",
    "#     labels_lookup = {}\n",
    "#     for idx, label in enumerate(labels):\n",
    "#         labels_lookup[idx] = label\n",
    "#     # --------- create cluster_partition from label ---------------#\n",
    "#     cluster_partition = []\n",
    "#     unique_labels = pd.unique(labels)\n",
    "#     for unique_label in unique_labels:\n",
    "#         cluster_partition.append([idx for idx, item in enumerate(labels) if item == unique_label])\n",
    "#     return labels_lookup, cluster_partition\n",
    "\n",
    "\n",
    "# # method to evaluate cluster result\n",
    "# def pairwise_f1_v2(true_label, pred_label):\n",
    "#     datasize = len(true_label)\n",
    "#     # --------- create cluster_partition from label ---------------#\n",
    "#     _, true_cluster_partition = create_cluster_partition(true_label)\n",
    "#     _, pred_cluster_partition = create_cluster_partition(pred_label)\n",
    "#     num_pred_cluster = len(pred_cluster_partition)\n",
    "#     purity = 0\n",
    "#     f1_measure_i = 0\n",
    "#     f1_measure = 0\n",
    "#     print(true_cluster_partition)\n",
    "#     print(pred_cluster_partition)\n",
    "    \n",
    "#     TP = 0\n",
    "#     TN = 0\n",
    "#     FP = 0\n",
    "#     FN = 0\n",
    "    \n",
    "#     # For each cluster in clustering\n",
    "#     for i, pred_cluster in enumerate(pred_cluster_partition):\n",
    "#         max_intersec_ground_index = 0\n",
    "#         max_intersec = len(set(true_cluster_partition[0]) & set(pred_cluster_partition[i]))\n",
    "#         print(max_intersec)\n",
    "#         # For each cluster in ground truth\n",
    "#         for j in range(1, len(true_cluster_partition)):\n",
    "#             # Get the max number of elements which belongs both to the pred_cluster_partition[i] and the true_cluster_partition[j]\n",
    "#             local_intersec = len(set(true_cluster_partition[j]) & set(pred_cluster_partition[i]))\n",
    "#             if (local_intersec > max_intersec):\n",
    "#                 max_intersec = local_intersec\n",
    "#                 max_intersec_ground_index = j\n",
    "        \n",
    "#         # Precision of pred_cluster_partition[i]\n",
    "#         precision_i = 1.0 * max_intersec / len(pred_cluster_partition[i])\n",
    "#         # Recall of pred_cluster_partition[i]\n",
    "#         recall_i = 1.0 * max_intersec / len(true_cluster_partition[max_intersec_ground_index])\n",
    "        \n",
    "#         # Purity\n",
    "#         purity += (1.0 * len(pred_cluster_partition[i]) * precision_i / datasize)\n",
    "#         # F-measure\n",
    "#         f1_measure_i = (2.0 * precision_i * recall_i) / (precision_i + recall_i)\n",
    "#         f1_measure += f1_measure_i\n",
    "        \n",
    "#     final_f1_measure = f1_measure/num_pred_cluster\n",
    "#     print(final_f1_measure)\n",
    "    \n",
    "#     return final_f1_measure\n",
    "\n",
    "# pairwise_f1_v2([2,2,2,2,1,2,0,0,1,0,2,0,1,1,2,1,2],[1,1,1,1,1,1,2,2,2,2,2,2,0,0,0,0,0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
