{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# binary logistic regression\n",
    "class Logistic_regression(object):\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    def __init__(self, n_iter=100, l_rate = 0.3):\n",
    "        self.l_rate = l_rate\n",
    "        self.n_iter=n_iter\n",
    "        \n",
    "    def fit(self, data, label):\n",
    "        self.trained_coef = self.stochastic_gradient_descent(data, label, self.l_rate, self.n_iter)\n",
    "        print(\"Final coefficient: \", self.trained_coef)\n",
    "        \n",
    "    def sigmoid(self,datapoint, coefficients):\n",
    "        bias =coefficients[0]\n",
    "        yhat = bias\n",
    "        # yhat = c1*x1 + c2*x2 + ... + cn*xn + b\n",
    "        # print(coefficients)\n",
    "        for i in range(len(datapoint)):\n",
    "            yhat += coefficients[i+1]*datapoint[i]\n",
    "        # sigmoid function\n",
    "        return 1 / (1 + self.np.exp(-yhat))\n",
    "    \n",
    "    \n",
    "    # estimate the coefficient values\n",
    "    def stochastic_gradient_descent(self, data, label, l_rate, n_iter):\n",
    "        # start with all 0 with coefficient +1 for b\n",
    "        coef = [0.0 for i in range(len(data[0])+1)]\n",
    "        #  run through the training data while updating the coefficients for each iteration\n",
    "        for cycle in range(n_iter):\n",
    "            sum_error = 0\n",
    "            for i in range(len(data)):\n",
    "                iter_result = self.sigmoid(data[i], coef)\n",
    "                # error = y-prediction\n",
    "                error = label[i] - iter_result\n",
    "                sum_error += error**2\n",
    "                coef[0] = coef[0] + l_rate * error * iter_result * (1.0 - iter_result)\n",
    "                for j in range(len(data[i])):\n",
    "                    coef[j + 1] = coef[j + 1] + l_rate * error * iter_result * (1.0 - iter_result) * data[i][j]\n",
    "            print('>epoch=%d, lrate=%.3f, error=%.3f' % (cycle, l_rate, sum_error))\n",
    "        return coef\n",
    "    \n",
    "    def predict(newdata):\n",
    "        result = []\n",
    "        for datapoint in newdata:\n",
    "            pointresult = sigmoid(datapoint, self.trained_coef)\n",
    "            if(pointresult>=0.5):\n",
    "                result.append(1)\n",
    "            else:\n",
    "                result.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=0, lrate=0.300, error=2.217\n",
      ">epoch=1, lrate=0.300, error=1.613\n",
      ">epoch=2, lrate=0.300, error=1.113\n",
      ">epoch=3, lrate=0.300, error=0.827\n",
      ">epoch=4, lrate=0.300, error=0.623\n",
      ">epoch=5, lrate=0.300, error=0.494\n",
      ">epoch=6, lrate=0.300, error=0.412\n",
      ">epoch=7, lrate=0.300, error=0.354\n",
      ">epoch=8, lrate=0.300, error=0.310\n",
      ">epoch=9, lrate=0.300, error=0.276\n",
      ">epoch=10, lrate=0.300, error=0.248\n",
      ">epoch=11, lrate=0.300, error=0.224\n",
      ">epoch=12, lrate=0.300, error=0.205\n",
      ">epoch=13, lrate=0.300, error=0.189\n",
      ">epoch=14, lrate=0.300, error=0.174\n",
      ">epoch=15, lrate=0.300, error=0.162\n",
      ">epoch=16, lrate=0.300, error=0.151\n",
      ">epoch=17, lrate=0.300, error=0.142\n",
      ">epoch=18, lrate=0.300, error=0.134\n",
      ">epoch=19, lrate=0.300, error=0.126\n",
      ">epoch=20, lrate=0.300, error=0.119\n",
      ">epoch=21, lrate=0.300, error=0.113\n",
      ">epoch=22, lrate=0.300, error=0.108\n",
      ">epoch=23, lrate=0.300, error=0.103\n",
      ">epoch=24, lrate=0.300, error=0.098\n",
      ">epoch=25, lrate=0.300, error=0.094\n",
      ">epoch=26, lrate=0.300, error=0.090\n",
      ">epoch=27, lrate=0.300, error=0.087\n",
      ">epoch=28, lrate=0.300, error=0.084\n",
      ">epoch=29, lrate=0.300, error=0.080\n",
      ">epoch=30, lrate=0.300, error=0.078\n",
      ">epoch=31, lrate=0.300, error=0.075\n",
      ">epoch=32, lrate=0.300, error=0.073\n",
      ">epoch=33, lrate=0.300, error=0.070\n",
      ">epoch=34, lrate=0.300, error=0.068\n",
      ">epoch=35, lrate=0.300, error=0.066\n",
      ">epoch=36, lrate=0.300, error=0.064\n",
      ">epoch=37, lrate=0.300, error=0.062\n",
      ">epoch=38, lrate=0.300, error=0.060\n",
      ">epoch=39, lrate=0.300, error=0.059\n",
      ">epoch=40, lrate=0.300, error=0.057\n",
      ">epoch=41, lrate=0.300, error=0.056\n",
      ">epoch=42, lrate=0.300, error=0.054\n",
      ">epoch=43, lrate=0.300, error=0.053\n",
      ">epoch=44, lrate=0.300, error=0.052\n",
      ">epoch=45, lrate=0.300, error=0.051\n",
      ">epoch=46, lrate=0.300, error=0.050\n",
      ">epoch=47, lrate=0.300, error=0.048\n",
      ">epoch=48, lrate=0.300, error=0.047\n",
      ">epoch=49, lrate=0.300, error=0.046\n",
      ">epoch=50, lrate=0.300, error=0.045\n",
      ">epoch=51, lrate=0.300, error=0.044\n",
      ">epoch=52, lrate=0.300, error=0.044\n",
      ">epoch=53, lrate=0.300, error=0.043\n",
      ">epoch=54, lrate=0.300, error=0.042\n",
      ">epoch=55, lrate=0.300, error=0.041\n",
      ">epoch=56, lrate=0.300, error=0.040\n",
      ">epoch=57, lrate=0.300, error=0.040\n",
      ">epoch=58, lrate=0.300, error=0.039\n",
      ">epoch=59, lrate=0.300, error=0.038\n",
      ">epoch=60, lrate=0.300, error=0.038\n",
      ">epoch=61, lrate=0.300, error=0.037\n",
      ">epoch=62, lrate=0.300, error=0.036\n",
      ">epoch=63, lrate=0.300, error=0.036\n",
      ">epoch=64, lrate=0.300, error=0.035\n",
      ">epoch=65, lrate=0.300, error=0.035\n",
      ">epoch=66, lrate=0.300, error=0.034\n",
      ">epoch=67, lrate=0.300, error=0.033\n",
      ">epoch=68, lrate=0.300, error=0.033\n",
      ">epoch=69, lrate=0.300, error=0.032\n",
      ">epoch=70, lrate=0.300, error=0.032\n",
      ">epoch=71, lrate=0.300, error=0.032\n",
      ">epoch=72, lrate=0.300, error=0.031\n",
      ">epoch=73, lrate=0.300, error=0.031\n",
      ">epoch=74, lrate=0.300, error=0.030\n",
      ">epoch=75, lrate=0.300, error=0.030\n",
      ">epoch=76, lrate=0.300, error=0.029\n",
      ">epoch=77, lrate=0.300, error=0.029\n",
      ">epoch=78, lrate=0.300, error=0.029\n",
      ">epoch=79, lrate=0.300, error=0.028\n",
      ">epoch=80, lrate=0.300, error=0.028\n",
      ">epoch=81, lrate=0.300, error=0.027\n",
      ">epoch=82, lrate=0.300, error=0.027\n",
      ">epoch=83, lrate=0.300, error=0.027\n",
      ">epoch=84, lrate=0.300, error=0.026\n",
      ">epoch=85, lrate=0.300, error=0.026\n",
      ">epoch=86, lrate=0.300, error=0.026\n",
      ">epoch=87, lrate=0.300, error=0.026\n",
      ">epoch=88, lrate=0.300, error=0.025\n",
      ">epoch=89, lrate=0.300, error=0.025\n",
      ">epoch=90, lrate=0.300, error=0.025\n",
      ">epoch=91, lrate=0.300, error=0.024\n",
      ">epoch=92, lrate=0.300, error=0.024\n",
      ">epoch=93, lrate=0.300, error=0.024\n",
      ">epoch=94, lrate=0.300, error=0.024\n",
      ">epoch=95, lrate=0.300, error=0.023\n",
      ">epoch=96, lrate=0.300, error=0.023\n",
      ">epoch=97, lrate=0.300, error=0.023\n",
      ">epoch=98, lrate=0.300, error=0.023\n",
      ">epoch=99, lrate=0.300, error=0.022\n",
      "Final coefficient:  [-0.8596443546618897, 1.5223825112460005, -2.218700210565016]\n"
     ]
    }
   ],
   "source": [
    "data = [[2.7810836, 2.550537003],\n",
    "[1.465489372,2.362125076],\n",
    "[3.396561688,4.400293529],\n",
    "[1.38807019,1.850220317],\n",
    "[3.06407232,3.005305973],\n",
    "[7.627531214,2.759262235],\n",
    "[5.332441248,2.088626775],\n",
    "[6.922596716,1.77106367],\n",
    "[8.675418651,-0.242068655],\n",
    "[7.673756466,3.508563011]]\n",
    "\n",
    "label = [0,0,0,0,0,1,1,1,1,1]\n",
    "\n",
    "lrclf = Logistic_regression()\n",
    "lrclf.fit(data, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.22044219] [[ 0.48520197 -0.64030689]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(fit_intercept=True, C = 1e15)\n",
    "clf.fit(data, label)\n",
    "\n",
    "print(clf.intercept_, clf.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
