{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vector records: 8602916\n",
      "['22516865', '0.0109272', '0.126011', '0.186979', '0.0496719', '0.0373553', '0.0458918', '-0.119893', '0.217118', '0.0524591', '0.237477', '0.191269', '-0.0277055', '0.0290957', '-0.0366833', '0.118964', '0.0654807', '-0.0335345', '-0.0900123', '0.128621', '0.0561669', '-0.087823', '-0.0882296', '0.0740289', '0.082104', '0.0269581', '-0.0346502', '0.0153376', '0.104666', '0.0908716', '-0.085694', '-0.111344', '0.0787209', '-0.17003', '-0.103366', '-0.0832094', '-0.210496', '0.153037', '-0.0342884', '0.0698413', '-0.0719641', '-0.0535707', '0.172399', '0.106226', '-0.0593672', '-0.0348048', '-0.0863189', '-0.0801566', '-0.0665761', '0.0673258', '0.0306541', '-0.0896316', '-0.00800971', '-0.174798', '-0.0252528', '0.0098563', '0.0230368', '0.0282268', '-0.0366493', '-0.131323', '0.0318188', '-0.00778704', '-0.0608064', '-0.0860078', '0.215632', '0.0209927', '-0.0953191', '-0.191736', '-0.0741615', '0.151972', '-0.0522046', '-0.11081', '0.134878', '0.090797', '0.0160238', '0.113017', '0.196071', '-0.0598695', '-0.181981', '-0.0217668', '0.165394', '0.0724198', '0.0967185', '-0.115696', '-0.104803', '0.231757', '-0.0871117', '-0.0397107', '0.137358', '-0.083055', '-0.226604', '0.0186784', '-0.0608683', '-0.0177875', '0.082095', '-0.00222851', '-0.137642', '-0.0848053', '0.165064', '-0.022831', '0.240943\\n']\n"
     ]
    }
   ],
   "source": [
    "# extract different view of data\n",
    "# view two, node2vec\n",
    "setting = \"n2v\"\n",
    "\n",
    "viewTwoFilesDir = \"../Data/vectors/\"+setting+\"/data=Meta-alg=N2V-l2=1.0-n2v_p=0.85-iteration=100-no_self_predict=1-idx=0.emb\"\n",
    "viewTwoVectors = []\n",
    "\n",
    "with open(viewTwoFilesDir, 'r', encoding = 'utf8') as f:\n",
    "    for line in f:\n",
    "        read_data = line.split(\" \")\n",
    "        if(len(read_data[0])<=8):\n",
    "            paper_Vectors = read_data\n",
    "            viewTwoVectors.append(paper_Vectors)\n",
    "f.close()\n",
    "viewTwoVectors = viewTwoVectors[1:]\n",
    "print(\"Total vector records:\",len(viewTwoVectors))\n",
    "print(viewTwoVectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chung-may yang.txt', 'chung-may yang0.txt', 'chung-may yang1.txt', 'david g lloyd.txt', 'david g lloyd0.txt', 'david g lloyd1.txt', 'jeong hwan kim.txt', 'jeong hwan kim0.txt', 'jeong hwan kim1.txt', 'kevin m. ryan.txt', 'kevin m. ryan0.txt', 'kevin m. ryan1.txt', 'lei wang.txt', 'lei wang0.txt', 'lei wang1.txt', 'michael wagner.txt', 'michael wagner0.txt', 'michael wagner1.txt']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "# collect data\n",
    "fileDir = \"../Data/filteredSameNameAuthor/filter=30/\"\n",
    "fileList = os.listdir(fileDir)\n",
    "fileList.sort()\n",
    "print(fileList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove author(positive sample) from other(negative sample)\n",
    "import random\n",
    "def extractNegativeSample(positiveSample, allSample):\n",
    "    negativeSample = [x for x in allSample if x not in positiveSample]\n",
    "    print(\"Total negative sample size:\", len(negativeSample))\n",
    "    return negativeSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# collect class vectors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def extractVectors(author_pids, NegativeSample_pid, allPaperVectors):\n",
    "    # extract class one vectors\n",
    "    author_features = []\n",
    "    for pid in author_pids:\n",
    "         for paper_Vectors in allPaperVectors:\n",
    "            if(paper_Vectors[0] == pid):\n",
    "                author_features.append(paper_Vectors)\n",
    "    print(\"Positive sample size: \", len(author_features))\n",
    "    classOne = pd.DataFrame(author_features)\n",
    "    classOne[\"label\"] = 0\n",
    "    # extract class two vectors\n",
    "    other_features = []\n",
    "    for pid in NegativeSample_pid:\n",
    "        for paper_Vectors in allPaperVectors:\n",
    "            if(paper_Vectors[0] == pid):\n",
    "                other_features.append(paper_Vectors)\n",
    "    print(\"Negative sample size: \", len(other_features))\n",
    "    classTwo = pd.DataFrame(other_features)\n",
    "    classTwo[\"label\"] = 1\n",
    "    return classOne, classTwo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combine data from different class get all data\n",
    "def combineClassesData(classOne,classTwo):\n",
    "    combinedData = pd.concat([classOne, classTwo])\n",
    "    combinedData = combinedData.sample(frac=1).reset_index(drop=True)\n",
    "    # take the paper id out\n",
    "    paperID = combinedData[0]\n",
    "    # split data and label\n",
    "    data = combinedData.drop([0,'label'], axis=1)\n",
    "    label = combinedData['label']\n",
    "    print(\"Total sample size and shape: \",data.shape)\n",
    "    return data, label, paperID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import (precision_score, recall_score,f1_score,accuracy_score)\n",
    "# cross validation\n",
    "def k_fold_cv(data, label, classifier, k=10):\n",
    "    kf = KFold(n_splits=k, shuffle=False)\n",
    "    allTrueLabel = []\n",
    "    allPredLabel = []\n",
    "    for counter,(train_index, test_index) in enumerate(kf.split(data)):\n",
    "        # print(\"TRAIN:\", train_index, \" \\n TEST:\", test_index)\n",
    "        # split train and test\n",
    "        data_train, data_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        label_train, test_true_label = label.iloc[train_index], label.iloc[test_index]\n",
    "        # fit data to svm\n",
    "        classifier.fit(data_train, label_train)\n",
    "        # get predicted label\n",
    "        label_pred = classifier.predict(data_test)\n",
    "        allTrueLabel.extend(test_true_label)\n",
    "        allPredLabel.extend(label_pred)\n",
    "#         # get predict proba\n",
    "#         proba = classifier.predict_proba(data_test)\n",
    "        # find out which sample cause the issue\n",
    "        print(\"Pred: \",label_pred)\n",
    "        print(\"True: \", test_true_label.values.tolist())\n",
    "        print(\"Mislabeled sample: \",end='')\n",
    "        for i in range(len(test_true_label)):\n",
    "            if(label_pred[i]!=test_true_label[test_index[i]]):\n",
    "                print(paperID[test_index[i]]+\",\",end='')\n",
    "        print()\n",
    "        # print(\"True positive: {tp}, False positive: {fp}, False negative: {fn}, True negative: {tn}\"\n",
    "        # .format(tp=round_tp, fp=round_fp, fn=round_fn, tn=round_tn))\n",
    "\n",
    "    accuracy = accuracy_score(allTrueLabel, allPredLabel)\n",
    "    f1 = f1_score(allTrueLabel, allPredLabel,average='binary')\n",
    "    precision = precision_score(allTrueLabel, allPredLabel)\n",
    "    recall = recall_score(allTrueLabel, allPredLabel)\n",
    "    tn,fp,fn,tp = metrics.confusion_matrix(allTrueLabel, allPredLabel).ravel()\n",
    "    \n",
    "    print(\"Classifier: \",classifier)\n",
    "    print(metrics.classification_report(allTrueLabel, allPredLabel))\n",
    "    print(metrics.confusion_matrix(allTrueLabel, allPredLabel).ravel())\n",
    "    print(\"Accuracy: \",accuracy)\n",
    "    print(\"F1: \", f1)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    \n",
    "    return accuracy, f1, precision, recall, tn, fp, fn, tp\n",
    "    # return ppv, npv, specificity, sensitivity, accuracy, f1proba = linear_svc.predict_proba(allDatas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['27321135', '27078635', '26996321', '26301538', '25464845', '24615769', '25010185', '24449339', '25044429', '23913257', '24019075', '23322720', '24290358', '24139041', '22554080', '21956564', '22662873', '21805546', '21732687', '21681861', '21751313', '21926996', '21545173', '20872394', '20385807', '19318213', '19764747', '19668857', '19165727', '18426210', '18556466', '17603477', '17406421', '16689635', '16956756', '15840835', '15599909', '15556995', '15378068', '12580587', '12779328', '12518054', '14576413', '12537491', '12515477', '11866580', '12154230', '12148987', '12244330', '12120285', '12203503', '11564556', '11313494']\n",
      "27321135\n"
     ]
    }
   ],
   "source": [
    "# hard code to read the file one by one\n",
    "# store the features for classification\n",
    "author_pids = []\n",
    "other_pids = []\n",
    "# author as positive sample, other as all samples\n",
    "with open(fileDir+\"lei wang0.txt\", 'r', encoding = 'utf8') as f:\n",
    "    for line in f:\n",
    "        author_pids.extend(line.strip().split(\" \"))\n",
    "\n",
    "with open(fileDir+\"lei wang.txt\", 'r', encoding = 'utf8') as f:\n",
    "    for line in f:\n",
    "        other_pids.extend(line.strip().split(\" \"))\n",
    "        \n",
    "print(author_pids)\n",
    "print(other_pids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total negative sample size: 64\n",
      "Choicen negative sample  64\n"
     ]
    }
   ],
   "source": [
    "# extract negative Sample\n",
    "NegativeSample_pid = extractNegativeSample(author_pids, other_pids)\n",
    "print(\"Choicen negative sample \", len(NegativeSample_pid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive sample size:  50\n",
      "Negative sample size:  62\n",
      "(50, 102)\n",
      "(62, 102)\n"
     ]
    }
   ],
   "source": [
    "classOne, classTwo = extractVectors(author_pids,NegativeSample_pid,viewTwoVectors)\n",
    "print(classOne.shape)\n",
    "print(classTwo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sample size and shape:  (112, 100)\n",
      "            1           2           3          4           5            6    \\\n",
      "0      0.312339   0.0694004    0.632244  0.0651315   -0.489079    -0.139653   \n",
      "1      0.522446  -0.0980524       0.454  0.0218141   -0.639211    -0.081809   \n",
      "2    -0.0280236    0.176684    0.226024   0.374915   -0.209283    0.0981133   \n",
      "3      0.368911    0.201126    0.354669   0.356884   -0.542716    -0.368921   \n",
      "4     0.0096327    0.268802  -0.0147734   0.228448   -0.415773     0.105026   \n",
      "5      0.319688    0.256781    0.594806   0.241398   -0.771016    -0.329713   \n",
      "6      0.313397    0.101994    0.766564   0.433176   -0.706534    -0.478016   \n",
      "7     -0.295066    0.865667    0.236162   0.776292   -0.341878     0.277269   \n",
      "8      0.351753  -0.0163507    0.638449   0.288624   -0.541191     -0.28179   \n",
      "9      0.753287   -0.045984     0.68357   0.123793   -0.814707   -0.0590562   \n",
      "10     0.246118     0.19095    0.597295   0.276298    -0.53077    -0.323592   \n",
      "11      0.31503    0.140736    0.585713   0.278136   -0.676563     -0.23801   \n",
      "12    -0.163522    0.318482    0.207462    0.18479  -0.0763948    0.0648987   \n",
      "13     0.269603    0.191811    0.581939   0.268486   -0.652594    -0.304835   \n",
      "14    -0.288678    0.494498    0.238794   0.510845   -0.291816     0.156443   \n",
      "15    -0.405993    0.647346     0.39106   0.535903   -0.185593     0.154747   \n",
      "16     0.311938    0.228645    0.656835   0.281773   -0.480813    -0.241889   \n",
      "17     0.559413  -0.0443178     0.57216  0.0889116    -0.71064   -0.0530504   \n",
      "18    -0.168199    0.368887    0.147333   0.205038   -0.059219     0.102242   \n",
      "19     0.150839    0.104688    0.221947  0.0316789   -0.184647   -0.0662634   \n",
      "20     0.271879     0.14819    0.494645   0.175101   -0.571436    -0.213807   \n",
      "21     0.380538    0.121263    0.403236  0.0747906    -0.45615   -0.0540553   \n",
      "22    -0.303536    0.409209    0.311986   0.416944   -0.330327      0.19027   \n",
      "23    -0.164827    0.634668   0.0748797   0.363047   -0.145805     0.197568   \n",
      "24    0.0581074    0.835439   0.0785903   0.176855  -0.0794319     0.138323   \n",
      "25      0.24974    0.202104    0.498603   0.171444   -0.534245    -0.242606   \n",
      "26     -0.33752    0.611304     0.23888   0.649153    -0.24953     0.107732   \n",
      "27   -0.0772599     0.27797    0.117334   0.202279  -0.0649256    0.0211089   \n",
      "28    -0.146872    0.470237    0.103579   0.232332   -0.119617      0.15337   \n",
      "29     0.522745  -0.0915727    0.507123   0.135584   -0.657864  -0.00121975   \n",
      "..          ...         ...         ...        ...         ...          ...   \n",
      "82     0.312728    0.190665     0.59542  0.0323999   -0.372395    -0.135364   \n",
      "83     0.243127    0.634686    0.131349   0.474256   -0.332964   -0.0177399   \n",
      "84     0.672465  0.00304484    0.662028   0.098792   -0.812073   -0.0118185   \n",
      "85    -0.164175     0.36764    0.175343   0.247146   -0.146802     0.201232   \n",
      "86     0.314803     0.28168    0.604127   0.187316   -0.743633    -0.294455   \n",
      "87     0.399545   0.0568531    0.442448  0.0610259   -0.472911   -0.0558553   \n",
      "88     0.289757    0.338289    0.548639   0.253462   -0.676908    -0.311642   \n",
      "89     0.395038    0.122562    0.602916   0.238234   -0.749467    -0.264394   \n",
      "90    0.0354726    0.225708    0.112191   0.102208  -0.0827659     0.038131   \n",
      "91     0.232653  -0.0650274    0.575221   0.292597   -0.516968    -0.316759   \n",
      "92     0.202634    0.179873     0.50592   0.177701   -0.604406      -0.2758   \n",
      "93    -0.268723    0.502001    0.182708   0.275456  -0.0860903     0.100006   \n",
      "94    -0.233474    0.631236      0.2635   0.348828   -0.185804     0.151844   \n",
      "95     0.161048   0.0663123    0.214245  0.0192469   -0.204692   -0.0466969   \n",
      "96    -0.313021    0.604947    0.240482   0.461964   -0.218999     0.187683   \n",
      "97   -0.0641966    0.618912   0.0519956   0.382548  -0.0347956     0.161275   \n",
      "98    0.0838658    0.324434    0.113183   0.540617   -0.392653    -0.015335   \n",
      "99    0.0619753   0.0668584    0.100911  0.0188681  -0.0828766   -0.0253528   \n",
      "100  -0.0461823    0.216589   0.0633003   0.148942   -0.190869     0.115764   \n",
      "101   -0.169625    0.475657    0.206544   0.343621   -0.195971     0.118006   \n",
      "102    0.641844  -0.0547819    0.540274  0.0975248   -0.774294   -0.0128214   \n",
      "103    0.275027    0.191443    0.576472    0.25662   -0.717758     -0.32199   \n",
      "104    0.271881   0.0674175    0.428241  0.0135489   -0.401198   -0.0956461   \n",
      "105    0.533946    0.173269    0.681988   0.252114   -0.823432    -0.265975   \n",
      "106   -0.186186    0.465774    0.148909   0.239315   -0.118423     0.147263   \n",
      "107    0.334531    0.284098    0.543554   0.298699   -0.701318    -0.273719   \n",
      "108    0.283528    0.198824    0.384791  0.0933389   -0.475758    -0.151273   \n",
      "109   -0.147395    0.367811    0.219397   0.109776    -0.10241    0.0759856   \n",
      "110    0.371152    0.235812    0.520056   0.560386   -0.449153   -0.0519646   \n",
      "111   -0.375941     0.78536     0.39813   0.690662   -0.321835     0.144543   \n",
      "\n",
      "            7           8          9          10      ...              91   \\\n",
      "0      0.271449    0.275208   0.440133   0.386883     ...        -0.232998   \n",
      "1      0.178278    0.589343   0.240088   0.598308     ...       0.00259757   \n",
      "2     -0.327967      0.4235   0.193675   0.331035     ...         0.453074   \n",
      "3     0.0720435    0.196758   0.193056   0.543636     ...        -0.413019   \n",
      "4     -0.208472    0.339649   0.444732   0.395659     ...         0.138065   \n",
      "5       0.27797    0.496686   0.565374   0.490746     ...        -0.339288   \n",
      "6      0.275936    0.393276   0.499787   0.529803     ...        -0.294081   \n",
      "7     -0.428631    0.707763   0.271453   0.728955     ...         0.526824   \n",
      "8      0.191374    0.175253   0.431326   0.407457     ...        -0.246667   \n",
      "9      0.325746    0.928046   0.495911   0.613335     ...       -0.0916507   \n",
      "10     0.306112    0.511238   0.577319   0.418727     ...        -0.382097   \n",
      "11     0.283728    0.530109   0.442016   0.476702     ...        -0.262749   \n",
      "12    -0.145346    0.328799   0.108927   0.350307     ...         0.263589   \n",
      "13     0.243602    0.478456   0.516182   0.474885     ...        -0.283128   \n",
      "14    -0.406399    0.636727   0.214388   0.566748     ...         0.512563   \n",
      "15    -0.419174    0.623332  0.0653898   0.624394     ...         0.441895   \n",
      "16     0.203518    0.298609   0.248037   0.469147     ...        -0.405432   \n",
      "17     0.227022    0.728049   0.378893   0.511179     ...        -0.106222   \n",
      "18    -0.227387    0.313498   0.123975   0.297415     ...         0.233888   \n",
      "19    0.0531772    0.226502   0.230827   0.151637     ...        -0.115152   \n",
      "20     0.242227    0.468513   0.423289    0.41363     ...        -0.241941   \n",
      "21     0.187971    0.647763   0.427061   0.285073     ...        -0.189408   \n",
      "22    -0.200317    0.589292   0.199492   0.494499     ...         0.379106   \n",
      "23    -0.348885    0.479493   0.259992   0.524013     ...         0.409018   \n",
      "24    -0.385826   0.0599622   0.299533   0.429204     ...         0.133947   \n",
      "25     0.249953    0.363006   0.322245   0.382681     ...        -0.260058   \n",
      "26    -0.448464    0.558285   0.153357   0.477384     ...          0.48227   \n",
      "27    -0.201929    0.172348   0.132778   0.242569     ...         0.148524   \n",
      "28    -0.224427    0.358815   0.219192   0.365243     ...         0.272712   \n",
      "29     0.282546    0.806081   0.275219   0.516413     ...       -0.0866574   \n",
      "..          ...         ...        ...        ...     ...              ...   \n",
      "82    0.0679768   0.0496593   0.447891   0.298514     ...        -0.331459   \n",
      "83    -0.275343    0.100647   0.102693  0.0929257     ...        -0.107042   \n",
      "84     0.273485    0.869755   0.543149   0.575279     ...       -0.0879795   \n",
      "85     -0.33804    0.268341   0.154807   0.293146     ...         0.182143   \n",
      "86     0.289845    0.444415     0.5217   0.504374     ...         -0.30351   \n",
      "87      0.15856    0.541353   0.299006   0.363012     ...        -0.123551   \n",
      "88     0.256781    0.441758   0.533754     0.4977     ...        -0.301587   \n",
      "89     0.225461    0.718032   0.683543   0.437553     ...        -0.257722   \n",
      "90    -0.285109    0.125651   0.310019   0.348967     ...         0.129707   \n",
      "91     0.202785    0.433489   0.225148   0.365035     ...         -0.24516   \n",
      "92     0.269876    0.465344    0.42818   0.432329     ...        -0.273284   \n",
      "93     -0.28548     0.42909   0.164957     0.3483     ...         0.281522   \n",
      "94    -0.297944    0.545327   0.155309   0.479813     ...         0.416419   \n",
      "95    0.0895269    0.175253   0.149228   0.167064     ...       -0.0848281   \n",
      "96    -0.378772    0.624446   0.227205   0.506805     ...         0.347276   \n",
      "97    -0.393812      0.4085   0.151988     0.5195     ...         0.274353   \n",
      "98   -0.0789786    0.110761   0.254529   0.227854     ...         0.192134   \n",
      "99   0.00993341    0.108474    0.10095  0.0931024     ...       -0.0313857   \n",
      "100   -0.164632    0.252673   0.294012   0.250657     ...          0.15645   \n",
      "101   -0.263449    0.417115  0.0921473   0.314097     ...         0.293141   \n",
      "102    0.250069    0.821837   0.388414    0.47078     ...        -0.101854   \n",
      "103    0.298219    0.582468   0.469521   0.465079     ...        -0.333775   \n",
      "104    0.169762    0.461117   0.324261   0.269871     ...         -0.23739   \n",
      "105    0.250868    0.758345   0.641653   0.571273     ...        -0.214234   \n",
      "106   -0.275407    0.362512   0.144578   0.405839     ...         0.303392   \n",
      "107    0.260235    0.497133   0.688107   0.425527     ...        -0.275501   \n",
      "108    0.150056    0.476321   0.452274   0.318361     ...        -0.166594   \n",
      "109   -0.271067    0.339237   0.077184   0.379321     ...         0.306278   \n",
      "110   -0.173039  -0.0354575  0.0276577   0.370908     ...         0.235917   \n",
      "111   -0.482485    0.697503   0.297357   0.566093     ...         0.478473   \n",
      "\n",
      "              92            93          94          95            96   \\\n",
      "0       0.0168305    -0.0267929    0.357839   0.0880081  -3.05611e-05   \n",
      "1       0.0455124     -0.240474    0.420337    0.266373      0.219309   \n",
      "2       -0.490021     0.0872543   0.0749438   -0.236625     -0.103441   \n",
      "3       -0.145226     0.0215382    0.598143   -0.146015     -0.122412   \n",
      "4       -0.043191     -0.351296   -0.295818   0.0192335     0.0418552   \n",
      "5      -0.0460694     0.0427252    0.481958   0.0741479     -0.147782   \n",
      "6       -0.271784      0.134152    0.510453  -0.0137048    -0.0830789   \n",
      "7       -0.513266     0.0961657   -0.044986   -0.596514     -0.197037   \n",
      "8      -0.0831599    -0.0201813    0.329965   -0.026984     0.0296564   \n",
      "9       0.0161586       -0.3104    0.431219    0.407393       0.23611   \n",
      "10      0.0315118      0.147156    0.380619   0.0829065     -0.207258   \n",
      "11      -0.019883    -0.0378516    0.346298    0.119645    -0.0208895   \n",
      "12      -0.309997     0.0636778    0.102713  -0.0915909     -0.132058   \n",
      "13     -0.0840776     0.0608904    0.436728   0.0823357     -0.120348   \n",
      "14      -0.601432     0.0611136   0.0203368    -0.24473     -0.121395   \n",
      "15       -0.54318    -0.0798286   0.0690983   -0.281093     -0.330491   \n",
      "16      -0.138373      0.121395     0.45737   0.0227481     -0.043881   \n",
      "17   -0.000822004     -0.211092     0.36315    0.325028      0.247772   \n",
      "18       -0.24151    0.00476694   0.0615975   -0.100786     -0.156284   \n",
      "19      0.0333237     0.0568493    0.202199   0.0295294     -0.101373   \n",
      "20    -0.00325837     0.0323864    0.356174    0.123685     -0.101365   \n",
      "21      0.0998088     0.0175071    0.304795    0.147288    -0.0713862   \n",
      "22      -0.317231    -0.0452076   -0.033796   -0.225385     -0.137725   \n",
      "23      -0.313676    0.00193323  -0.0618809   -0.226054     -0.202369   \n",
      "24      -0.268498      0.260863    0.160324   -0.163119     -0.390704   \n",
      "25    -0.00836294     0.0619372    0.418634   0.0273228      -0.13841   \n",
      "26      -0.551576     0.0646932  0.00402427   -0.416962     -0.359064   \n",
      "27      -0.151808    -0.0122205    0.019703    -0.13162    -0.0972844   \n",
      "28       -0.19251     0.0363168  -0.0518264   -0.162587     -0.137112   \n",
      "29      0.0894222     -0.230243    0.381413    0.273018      0.181238   \n",
      "..            ...           ...         ...         ...           ...   \n",
      "82       0.106888      0.078921    0.603234  -0.0481297    -0.0569198   \n",
      "83      -0.237596      0.244728    0.251409   -0.291233     -0.359686   \n",
      "84      0.0256187     -0.250142     0.48381    0.366013      0.262076   \n",
      "85      -0.283405     0.0809263    0.157442   -0.125571    -0.0360691   \n",
      "86     -0.0550876     0.0322247    0.456782   0.0612437    -0.0573233   \n",
      "87      0.0285348     -0.119304    0.342495     0.20859     0.0580076   \n",
      "88     -0.0884467      0.105583    0.461126  0.00333663     -0.192406   \n",
      "89     -0.0350038     0.0352401    0.442812     0.19274    -0.0485767   \n",
      "90      -0.159966     0.0302271    -0.13231   -0.239961    -0.0843822   \n",
      "91     -0.0540825     0.0928027    0.450685   0.0223864     -0.123205   \n",
      "92     -0.0177487     0.0584223    0.426197   0.0228468     -0.191015   \n",
      "93       -0.27446    -0.0146036   -0.042928   -0.162826     -0.191359   \n",
      "94      -0.456734     0.0618811   0.0350916   -0.213184     -0.169916   \n",
      "95    -0.00809923     0.0190533    0.203757   0.0502364    -0.0117532   \n",
      "96        -0.3739     -0.122488   0.0830626   -0.260888     -0.254273   \n",
      "97       -0.29275     -0.266043    -0.19043   -0.344536     -0.165049   \n",
      "98      -0.211468      0.188618   0.0825376   -0.381222     -0.140351   \n",
      "99     -0.0063395     0.0161524   0.0891781   0.0283317     -0.035954   \n",
      "100    -0.0365089     -0.175593  -0.0641837   0.0111839     0.0133306   \n",
      "101     -0.299663  -0.000352262    0.111459   -0.200727      -0.20134   \n",
      "102     0.0900417        -0.261    0.355473    0.401996      0.211468   \n",
      "103   -0.00112138      0.105272    0.479814   0.0996077     -0.176474   \n",
      "104     0.0315932      0.111911    0.366825   0.0798389     -0.217842   \n",
      "105    -0.0837374    -0.0959239    0.459789    0.291521     0.0445975   \n",
      "106     -0.304772   -0.00111796    0.104405   -0.123683     -0.177997   \n",
      "107    -0.0702282     0.0695916    0.357496   0.0848582    -0.0651537   \n",
      "108     0.0216237     0.0599315    0.379571    0.138176    -0.0430525   \n",
      "109     -0.393791    -0.0129975  -0.0303957   -0.414298     -0.173514   \n",
      "110     -0.577832      0.145634    0.134944   -0.489576     -0.313514   \n",
      "111     -0.691488      0.118466   0.0558462   -0.524613     -0.292481   \n",
      "\n",
      "             97            98           99           100  \n",
      "0      -0.613027    -0.0982263   -0.0211418   0.880915\\n  \n",
      "1      -0.484229     0.0510525   -0.0939273    1.06955\\n  \n",
      "2       0.021869       0.65237      -0.3274   0.330904\\n  \n",
      "3      -0.613207    -0.0249194   -0.0367114   0.741256\\n  \n",
      "4      0.0641092      0.382677    -0.155036   0.263225\\n  \n",
      "5      -0.855296     0.0180438   -0.0104085    1.03709\\n  \n",
      "6      -0.854111    -0.0438245    0.0128134    1.00346\\n  \n",
      "7      -0.147854      0.853417    -0.551408    0.30761\\n  \n",
      "8      -0.616023     -0.109043   -0.0228512   0.905939\\n  \n",
      "9      -0.736667      0.211675    -0.138014    1.50959\\n  \n",
      "10     -0.818643     -0.124045    0.0781989   0.899937\\n  \n",
      "11     -0.799415     0.0545882   0.00509591    1.03797\\n  \n",
      "12    -0.0260724      0.298951    -0.284925   0.215364\\n  \n",
      "13     -0.835498    -0.0649694    0.0301752    1.01049\\n  \n",
      "14    -0.0555027      0.740994    -0.478988   0.336509\\n  \n",
      "15    -0.0844486      0.814091    -0.521282   0.296305\\n  \n",
      "16     -0.802077      0.133388  -0.00618068   0.864565\\n  \n",
      "17     -0.627527      0.166952    -0.157043    1.28365\\n  \n",
      "18    -0.0784493      0.378781     -0.22949   0.188804\\n  \n",
      "19     -0.281938  -0.000948445  -0.00752736   0.410033\\n  \n",
      "20     -0.679031   -0.00470214    0.0283109   0.893348\\n  \n",
      "21      -0.59743     0.0498241   -0.0289507   0.920408\\n  \n",
      "22    -0.0882159      0.601742     -0.40436   0.235324\\n  \n",
      "23    -0.0654677      0.566535    -0.342865     0.2465\\n  \n",
      "24      0.164567      0.337609    -0.162164   0.149854\\n  \n",
      "25     -0.718093     0.0102275    0.0141819   0.808152\\n  \n",
      "26     0.0164295      0.712074    -0.592889   0.301122\\n  \n",
      "27    -0.0437343      0.220064    -0.150595   0.133747\\n  \n",
      "28    -0.0468721      0.409485    -0.246212   0.169843\\n  \n",
      "29     -0.672716      0.154765   -0.0858512    1.32978\\n  \n",
      "..           ...           ...          ...          ...  \n",
      "82     -0.612303     -0.300209   -0.0850534   0.852794\\n  \n",
      "83     0.0277338      0.275755    -0.188053   0.256145\\n  \n",
      "84     -0.738125      0.179991    -0.195676    1.52101\\n  \n",
      "85   -0.00787117      0.541063     -0.34268   0.210313\\n  \n",
      "86     -0.848731   -0.00336472   -0.0172639    1.01948\\n  \n",
      "87     -0.513936      0.111186   -0.0657027   0.952013\\n  \n",
      "88     -0.840525    0.00924494    0.0177955   0.903183\\n  \n",
      "89     -0.811147      0.068241   -0.0194985    1.20949\\n  \n",
      "90       0.18268      0.143153    -0.166238   0.135834\\n  \n",
      "91     -0.781483     0.0583875      0.10478    0.88277\\n  \n",
      "92     -0.731478     0.0168472  -0.00339236   0.845879\\n  \n",
      "93     -0.103154      0.559836    -0.238806   0.239757\\n  \n",
      "94    -0.0652941      0.618866    -0.398735   0.229726\\n  \n",
      "95     -0.254298     0.0238216   -0.0266202   0.375759\\n  \n",
      "96     -0.159474      0.734963    -0.402661   0.314099\\n  \n",
      "97    -0.0156516      0.598702    -0.349413   0.208115\\n  \n",
      "98    0.00227342      0.332439    -0.279885   0.214751\\n  \n",
      "99     -0.116539     0.0268465  -0.00785123    0.18383\\n  \n",
      "100    -0.106251      0.212685    -0.110578   0.199794\\n  \n",
      "101   -0.0942284      0.544853    -0.321062   0.231124\\n  \n",
      "102    -0.634978      0.163728      -0.1215    1.29971\\n  \n",
      "103    -0.843491      0.038952    0.0553164    1.03396\\n  \n",
      "104    -0.523535   -0.00611983    0.0484119   0.691849\\n  \n",
      "105    -0.886986     0.0842039    -0.092165    1.37829\\n  \n",
      "106   -0.0382976      0.470513    -0.294512   0.211104\\n  \n",
      "107    -0.795819    0.00630704    0.0246165    1.10563\\n  \n",
      "108     -0.63012     0.0752036   0.00855764   0.903024\\n  \n",
      "109      0.15706      0.375812    -0.305746   0.120079\\n  \n",
      "110   -0.0319042      0.506835    -0.225651  0.0682963\\n  \n",
      "111   -0.0830504      0.815367    -0.648427   0.319975\\n  \n",
      "\n",
      "[112 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "data, label, paperID = combineClassesData(classOne, classTwo)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred:  [1 1 0 1 0 1 1 0 1 1 1 1]\n",
      "True:  [1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1]\n",
      "Mislabeled sample: \n",
      "Pred:  [0 1 0 0 1 1 0 1 1 1 0 0]\n",
      "True:  [0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0]\n",
      "Mislabeled sample: \n",
      "Pred:  [0 1 0 0 0 1 0 1 0 0 0]\n",
      "True:  [0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0]\n",
      "Mislabeled sample: \n",
      "Pred:  [1 1 1 0 1 0 0 0 0 1 1]\n",
      "True:  [1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1]\n",
      "Mislabeled sample: \n",
      "Pred:  [1 0 0 1 1 1 0 0 0 1 0]\n",
      "True:  [1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0]\n",
      "Mislabeled sample: \n",
      "Pred:  [1 1 1 1 0 1 1 0 0 1 1]\n",
      "True:  [1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1]\n",
      "Mislabeled sample: \n",
      "Pred:  [0 1 1 0 0 0 1 1 1 0 1]\n",
      "True:  [0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Mislabeled sample: \n",
      "Pred:  [1 1 1 1 0 1 0 1 1 1 1]\n",
      "True:  [1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1]\n",
      "Mislabeled sample: \n",
      "Pred:  [0 1 1 0 0 1 0 0 0 1 0]\n",
      "True:  [0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0]\n",
      "Mislabeled sample: \n",
      "Pred:  [0 1 1 1 1 0 1 1 0 0 0]\n",
      "True:  [0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0]\n",
      "Mislabeled sample: \n",
      "Classifier:  SVC(C=1.0, cache_size=4000, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        50\n",
      "          1       1.00      1.00      1.00        62\n",
      "\n",
      "avg / total       1.00      1.00      1.00       112\n",
      "\n",
      "[50  0  0 62]\n",
      "Accuracy:  1.0\n",
      "F1:  1.0\n",
      "Precision:  1.0\n",
      "Recall:  1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "linear_svc = svm.SVC(kernel='linear', class_weight='balanced', probability=True,cache_size=4000)\n",
    "accuracy, f1, precision, recall, tn, fp, fn, tp= k_fold_cv(data, label, linear_svc,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
