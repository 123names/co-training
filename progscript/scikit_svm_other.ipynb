{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chung-may yang1.txt', 'wei lu0.txt', 'yong wang1.txt', 'david g lloyd0.txt', 'wei lu1.txt', 'feng liu1.txt', 'david g lloyd1.txt', 'jeong hwan kim1.txt', 'chung-may yang0.txt', 'michael wagner0.txt', 'feng liu0.txt', 'hao song1.txt', 'hao song0.txt', 'kevin m. ryan0.txt', 'michael wagner1.txt', 'lei wang0.txt', 'jeong hwan kim0.txt', 'yong wang0.txt', 'lei wang1.txt', 'kevin m. ryan1.txt']\n",
      "['22889921', '-0.244403', '-0.0406115', '-0.241506', '-0.0276834', '0.035334', '0.172173', '-0.138247', '0.0907196', '-0.0780261', '0.0947579', '-0.0556994', '-0.092753', '-0.0394446', '0.164712', '0.0333666', '-0.155414', '0.0652004', '0.0845404', '-0.00309468', '0.103676', '0.138019', '0.0117297', '-0.0868801', '0.107426', '0.0734262', '0.0697482', '0.0374372', '0.135657', '0.14281', '0.0381899', '-0.125376', '0.00246952', '0.0992286', '-0.0779264', '0.0226886', '0.101398', '-0.0621663', '-0.18564', '-0.0982089', '0.0417952', '-0.123849', '-0.130739', '-0.00359905', '-0.0186436', '-0.098999', '-0.106249', '-0.0986074', '0.0420454', '0.0238472', '0.0442347', '0.121202', '-0.0560053', '-0.0200107', '0.114606', '0.146604', '-0.0895633', '0.0377405', '-0.00726176', '0.0188683', '-0.00058019', '0.121014', '-0.111559', '-0.0385865', '-0.100968', '-0.0224971', '0.00968139', '0.0205474', '-0.0604898', '-0.031894', '-0.0451031', '-0.0940785', '-0.0416085', '0.143822', '-0.0665332', '0.0928908', '-0.00662681', '0.00795803', '-0.101569', '0.104647', '0.145576', '-0.0352994', '-0.103165', '0.0801801', '-0.168112', '0.0286628', '-0.0259185', '0.0372581', '-0.222029', '0.0157335', '-0.048336', '-0.0390467', '-0.0910969', '-0.0672932', '-0.109656', '-0.076457', '-0.0954903', '-0.259281', '-0.00554032', '-0.246469', '-0.037069']\n",
      "['17227418', '-0.283691', '-0.0520869', '-0.15645', '0.0145559', '-0.159625', '-0.00675375', '-0.286343', '0.0175413', '-0.147506', '-0.0132761', '0.0561262', '-0.0126178', '-0.0655109', '0.178183', '-0.0552813', '-0.0143455', '0.107264', '0.0727324', '-0.0552292', '-0.0611871', '-0.0230139', '0.0136285', '-0.0786064', '0.142322', '0.230457', '-0.0438618', '-0.0321012', '0.166714', '-0.0859686', '-0.0237617', '-0.0260153', '-0.160985', '0.0316591', '-0.035627', '-0.000620118', '0.0162651', '-0.101424', '0.0410527', '0.0427487', '-0.0755454', '-0.1284', '-0.0547064', '-0.163642', '-0.0126223', '-0.185049', '-0.0137889', '-0.090742', '0.0258367', '-0.0236044', '-0.108098', '0.0579393', '0.00200847', '-0.0120077', '0.0219213', '0.0749515', '-0.115425', '-0.0392143', '-0.0125522', '0.109647', '-0.13623', '0.0224949', '-0.0348083', '0.00977056', '-0.137744', '0.0590945', '0.015789', '-0.119823', '-0.0619651', '0.151344', '0.0221873', '0.0406295', '-0.14317', '0.0454265', '0.0533377', '-0.0280716', '-0.0520637', '-0.0398124', '-0.11057', '0.0662073', '0.0993887', '0.0583323', '-0.137481', '0.1334', '-0.0842153', '0.103631', '-0.0623457', '0.126746', '-0.00579984', '0.068408', '-0.145884', '-0.138238', '-0.0649029', '0.0722162', '-0.0604179', '-0.0507626', '0.0400668', '-0.10985', '-0.112654', '-0.266982', '-0.101606']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# collect data\n",
    "# ../Data/DataForClassification/d2v/\n",
    "fileDir = \"../Data/DataForClassification/p2v/\"\n",
    "fileList = os.listdir(fileDir)\n",
    "print(fileList)\n",
    "\n",
    "# # auto method that go through all the file in directory\n",
    "# for file in fileList:\n",
    "#     if not file.startswith('.'):\n",
    "#         if file.endswith(\".txt\"):\n",
    "#             file = file[:-4]\n",
    "\n",
    "# hard code to read the file one by one\n",
    "author0 = []\n",
    "author1 = []\n",
    "with open(fileDir+\"michael wagner0.txt\", 'r', encoding = 'utf8') as f:\n",
    "    for line in f:\n",
    "        author0.append(line.strip().split(\" \"))\n",
    "\n",
    "with open(fileDir+\"michael wagner1.txt\", 'r', encoding = 'utf8') as f:\n",
    "    for line in f:\n",
    "        author1.append(line.strip().split(\" \"))\n",
    "print(author0[0])\n",
    "print(author1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n",
      "141\n"
     ]
    }
   ],
   "source": [
    "# size of each class\n",
    "print(len(author0))\n",
    "print(len(author1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "101\n"
     ]
    }
   ],
   "source": [
    "# number of features (dimension) with it's paper id\n",
    "print(len(author0[0]))\n",
    "print(len(author1[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate ppv,npv,specificity,sensitivity, and accuracy\n",
    "def calculate_important_value(tp, tn, fp, fn, sample_length,f1):\n",
    "    # 1. Positive predicted value (PPV) or precision aka hit rate = True positive/ )True positive + False positive)\n",
    "    ppv = (tp / (tp + fp))\n",
    "    # 2. Negative predicted value (NPV) = True negative / (True negative + False negative)\n",
    "    npv = (tn / (tn + fn))\n",
    "    # 3. Specificity = (1 - False positive)\n",
    "    specificity = (tn / (tn + fp))\n",
    "    # 4. Sensitivity = True positive\n",
    "    sensitivity = (tp / (tp + fn))\n",
    "    # 5. Accuracy = (True positive + True negative) / Total number of sample\n",
    "    accuracy = (tp + tn) / sample_length\n",
    "    print('PPV: ', ppv, 'NPV: ', npv, 'Specificity: ', specificity, 'Sensitivity: ', sensitivity)\n",
    "    print('Accuracy: ', accuracy, 'F1: ', f1)\n",
    "    return ppv, npv, specificity, sensitivity, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0          1            2          3            4           5  \\\n",
      "0  22889921  -0.244403   -0.0406115  -0.241506   -0.0276834    0.035334   \n",
      "1  23585882  -0.283335   -0.0669752  -0.196079  -0.00280898    -0.15149   \n",
      "2  23604333  -0.241933  -0.00540449  -0.255078    -0.140854  -0.0892556   \n",
      "3  23137390  -0.213667   -0.0366257  -0.206121   -0.0531313   -0.139354   \n",
      "4  22913370  -0.212666   -0.0438302  -0.215563   0.00510708   -0.133076   \n",
      "\n",
      "          6           7          8           9  ...           92          93  \\\n",
      "0  0.172173   -0.138247  0.0907196  -0.0780261  ...   -0.0910969  -0.0672932   \n",
      "1  0.130044  -0.0502186  0.0763087  -0.0267538  ...   -0.0553114  -0.0596153   \n",
      "2  0.102551     -0.0739  0.0868043   -0.110511  ...    -0.135502  -0.0438898   \n",
      "3  0.133103  -0.0861887  0.0295243  -0.0718318  ...    0.0203134   -0.126046   \n",
      "4  0.134719    -0.15929    0.04278   -0.121276  ...    -0.069607  -0.0631248   \n",
      "\n",
      "            94          95           96         97           98         99  \\\n",
      "0    -0.109656   -0.076457   -0.0954903  -0.259281  -0.00554032  -0.246469   \n",
      "1    -0.112089  -0.0625242  -0.00766423  -0.131741   -0.0132118  -0.248491   \n",
      "2   -0.0817898  -0.0623349   -0.0153093  -0.108956   -0.0106793  -0.287399   \n",
      "3  0.000854915  -0.0303831    -0.140949  -0.221603   -0.0742249  -0.173642   \n",
      "4   -0.0393302  -0.0641497   -0.0444151  -0.225594    0.0283743   -0.13393   \n",
      "\n",
      "           100 label  \n",
      "0    -0.037069     0  \n",
      "1  -0.00784513     0  \n",
      "2    0.0384781     0  \n",
      "3     0.019295     0  \n",
      "4    0.0818025     0  \n",
      "\n",
      "[5 rows x 102 columns]\n"
     ]
    }
   ],
   "source": [
    "# reconstract data so that we can feed it to svm\n",
    "import pandas as pd\n",
    "classOne = pd.DataFrame(author0)\n",
    "classOne[\"label\"] = 0\n",
    "#print(classOne[:2:])\n",
    "classTwo = pd.DataFrame(author1)\n",
    "classTwo[\"label\"] = 1\n",
    "#print(classTwo[:2:])\n",
    "# combine data from different class get all data\n",
    "combinedData = pd.concat([classOne, classTwo])\n",
    "print(combinedData[:5])\n",
    "combinedData = combinedData.sample(frac=1).reset_index(drop=True)\n",
    "# take the paper id out\n",
    "paperID = combinedData[0]\n",
    "# split data and label\n",
    "data = combinedData.drop([0,'label'], axis=1)\n",
    "label = combinedData['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cross validation\n",
    "def k_fold_cv(data, label, classifier, clfname):\n",
    "    kf = KFold(n_splits=10, shuffle=False)\n",
    "    # create lists to collect statistic\n",
    "    tp = []\n",
    "    fp = []\n",
    "    tn = []\n",
    "    fn = []\n",
    "    roundf1 = []\n",
    "    for train_index, test_index in kf.split(data):\n",
    "        # print(\"TRAIN:\", train_index, \" \\n TEST:\", test_index)\n",
    "        # split train and test\n",
    "        data_train, data_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        label_train, test_true_label = label.iloc[train_index], label.iloc[test_index]\n",
    "        # fit data to svm\n",
    "        classifier.fit(data_train, label_train)\n",
    "        # get predicted label\n",
    "        label_pred = classifier.predict(data_test)\n",
    "        # find out which sample cause the issue\n",
    "        print(\"Pred: \",label_pred)\n",
    "        print(\"True: \", test_true_label.values.tolist())\n",
    "        print(\"Mislabeled sample: \",end='')\n",
    "        for i in range(len(test_true_label)):\n",
    "            if(label_pred[i]!=test_true_label[test_index[i]]):\n",
    "                print(paperID[test_index[i]]+\",\",end='')\n",
    "        print()\n",
    "        # find round confusion matrix\n",
    "        try:\n",
    "            round_tn, round_fp, round_fn, round_tp = metrics.confusion_matrix(test_true_label, label_pred).ravel()\n",
    "        except ValueError:\n",
    "            round_tn, round_fp, round_fn, round_tp = metrics.confusion_matrix(test_true_label, label_pred,labels=[0,1]).ravel()\n",
    "        # add data data to array\n",
    "        tp.append(round_tp)\n",
    "        fp.append(round_fp)\n",
    "        fn.append(round_fn)\n",
    "        tn.append(round_tn)\n",
    "        roundf1.append(f1_score(test_true_label, label_pred,average='micro'))\n",
    "        # print(\"True positive: {tp}, False positive: {fp}, False negative: {fn}, True negative: {tn}\"\n",
    "        # .format(tp=round_tp, fp=round_fp, fn=round_fn, tn=round_tn))\n",
    "\n",
    "    print(\"Classifier: {name}\\nTrue positive: {tp}, False positive: {fp}, False negative: {fn}, True negative: {tn}\"\n",
    "          .format(name=clfname, tp=np.sum(tp), fp=np.sum(fp), fn=np.sum(fn), tn=np.sum(tn)))\n",
    "    f1 = np.average(roundf1)\n",
    "    ppv, npv, specificity, sensitivity, accuracy = calculate_important_value(np.sum(tp), np.sum(tn),\n",
    "                                                                             np.sum(fp), np.sum(fn), len(data),f1)\n",
    "    # return ppv, npv, specificity, sensitivity, accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Pred:  [1 1 0 0 1 0 0 0 0 1 1 1 1 1 0 0 1 0 0 1 0 1 0 1]\n",
      "True:  [1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1]\n",
      "Mislabeled sample: \n",
      "Pred:  [1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 1]\n",
      "True:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1]\n",
      "Mislabeled sample: \n",
      "Pred:  [0 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 0 1 0]\n",
      "True:  [0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0]\n",
      "Mislabeled sample: \n",
      "Pred:  [1 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1]\n",
      "True:  [1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1]\n",
      "Mislabeled sample: \n",
      "Pred:  [1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 0 1 0 0 1 1]\n",
      "True:  [1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1]\n",
      "Mislabeled sample: \n",
      "Pred:  [1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1]\n",
      "True:  [1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1]\n",
      "Mislabeled sample: \n",
      "Pred:  [0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 0 1]\n",
      "True:  [0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1]\n",
      "Mislabeled sample: \n",
      "Pred:  [0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 0 1 1]\n",
      "True:  [0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1]\n",
      "Mislabeled sample: \n",
      "Pred:  [0 0 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1]\n",
      "True:  [0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1]\n",
      "Mislabeled sample: \n",
      "Pred:  [0 1 0 1 0 0 1 0 0 0 0 0 1 1 1 0 1 1 0 1 0 1 0]\n",
      "True:  [0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0]\n",
      "Mislabeled sample: \n",
      "Classifier: SVM linear\n",
      "True positive: 141, False positive: 0, False negative: 0, True negative: 98\n",
      "PPV:  1.0 NPV:  1.0 Specificity:  1.0 Sensitivity:  1.0\n",
      "Accuracy:  1.0 F1:  1.0\n",
      "Number of support vectors:  35\n",
      "SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Pred:  [1 1 1 0 1 0 0 0 0 1 1 1 1 1 0 0 1 0 0 1 0 1 0 1]\n",
      "True:  [1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1]\n",
      "Mislabeled sample: 26308457,\n",
      "Pred:  [1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 1]\n",
      "True:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1]\n",
      "Mislabeled sample: \n",
      "Pred:  [0 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 0 0 1 0]\n",
      "True:  [0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0]\n",
      "Mislabeled sample: \n",
      "Pred:  [1 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1]\n",
      "True:  [1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1]\n",
      "Mislabeled sample: \n",
      "Pred:  [1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 1 0 1 0 0 1 1]\n",
      "True:  [1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1]\n",
      "Mislabeled sample: \n",
      "Pred:  [1 1 1 0 0 0 0 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1]\n",
      "True:  [1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1]\n",
      "Mislabeled sample: \n",
      "Pred:  [0 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 0 1]\n",
      "True:  [0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1]\n",
      "Mislabeled sample: \n",
      "Pred:  [0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 0 0 1 1]\n",
      "True:  [0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1]\n",
      "Mislabeled sample: \n",
      "Pred:  [0 0 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1]\n",
      "True:  [0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1]\n",
      "Mislabeled sample: \n",
      "Pred:  [0 1 0 1 0 0 1 0 0 0 0 0 1 1 1 0 1 1 0 1 0 1 0]\n",
      "True:  [0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0]\n",
      "Mislabeled sample: \n",
      "Classifier: SVM rbf\n",
      "True positive: 141, False positive: 1, False negative: 0, True negative: 97\n",
      "PPV:  0.9929577464788732 NPV:  1.0 Specificity:  0.9897959183673469 Sensitivity:  1.0\n",
      "Accuracy:  0.99581589958159 F1:  0.9958333333333333\n",
      "Number of support vectors:  71\n"
     ]
    }
   ],
   "source": [
    "# # create linear SVM model\n",
    "linear_svc = svm.SVC(kernel='linear', class_weight='balanced', probability=True)\n",
    "print(linear_svc)\n",
    "\n",
    "# fit model and do 10-fold cv\n",
    "k_fold_cv(data, label, linear_svc, \"SVM linear\")\n",
    "\n",
    "# check number of support vectors\n",
    "print(\"Number of support vectors: \",len(linear_svc.support_vectors_))\n",
    "\n",
    "# create rbf SVM model with C=10 where (C*Error) is added into minimize function\n",
    "# C big means error matter more\n",
    "rbf_svc = svm.SVC(kernel='rbf', C=10)\n",
    "print(rbf_svc)\n",
    "\n",
    "# fit model and do 10-fold cv\n",
    "k_fold_cv(data, label, rbf_svc, \"SVM rbf\")\n",
    "print(\"Number of support vectors: \",len(rbf_svc.support_vectors_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
