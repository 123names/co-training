{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-13T23:07:57.193Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import com_func\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "\n",
    "Dataset = \"pubmed\"\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# parameters\n",
    "threshold = 30\n",
    "cutoff = 3\n",
    "\n",
    "pp_textual_emb_type = [\"tf_idf\",\"lsa\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T20:39:47.637934Z",
     "start_time": "2018-12-13T20:39:12.142985Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load text information\n",
    "Dataset = \"pubmed\"\n",
    "raw_filepath = \"../Data/\"+Dataset+\"/id_textual_combined.txt\"\n",
    "all_text_content = []\n",
    "with open(raw_filepath, 'r', encoding = 'utf8') as f:\n",
    "    # items[0] is paper ID, items[1] is title, items[2] is abstract\n",
    "    for line in f:\n",
    "        items = line.split(\"\\t\")\n",
    "        # lower case all character\n",
    "        paperID = items[0]\n",
    "        title = items[1].lower()\n",
    "        keywords = items[2].lower()\n",
    "        mesh = items[3].lower()\n",
    "        abstract = items[4].lower()\n",
    "        # textual information can be defined as all feature combined\n",
    "        content = title+\" \"+keywords+\" \"+mesh+\" \"+abstract\n",
    "        paper_text_content = {\"paperID\": paperID, \"combine_textual\":content}\n",
    "        all_text_content.append(paper_text_content)\n",
    "print(\"Total \", len(all_text_content), \" paper have text information\")\n",
    "# convert to dataframe so it's easy to process\n",
    "all_text_content = pd.DataFrame(all_text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T20:39:47.660327Z",
     "start_time": "2018-12-13T20:39:47.640586Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dummy(doc):\n",
    "    return doc\n",
    "def read_labeled_file(infile):\n",
    "    LabeledRecords_original = []\n",
    "    with open(infile, 'r', encoding = 'utf8') as f:\n",
    "        for line in f:\n",
    "            read_data = line.split(\"\\t\")\n",
    "            # get ride of bad formated lines\n",
    "            if(len(read_data)==13 or len(read_data)==12):\n",
    "                paper_detail = {\"paperID\": read_data[0], \"authorID\":read_data[1], \n",
    "                                \"co-author\": read_data[5], \"venue_id\": read_data[7]}\n",
    "                LabeledRecords_original.append(paper_detail)\n",
    "            else:\n",
    "                print(len(read_data))\n",
    "        f.close()\n",
    "    return pd.DataFrame(LabeledRecords_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T20:39:47.667385Z",
     "start_time": "2018-12-13T20:39:47.662717Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove author(positive sample) from other(negative sample)\n",
    "import random\n",
    "def extractNegativeSample(positiveSample, allSample):\n",
    "    negativeSample = [x for x in allSample if x not in positiveSample]\n",
    "    return negativeSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T20:39:47.683235Z",
     "start_time": "2018-12-13T20:39:47.669272Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LSA(cleaned_token, dim=100):\n",
    "    # Tf-idf Transformation\n",
    "    tfidf_vectorizer = TfidfVectorizer(use_idf=True, smooth_idf=True, sublinear_tf=True, tokenizer=dummy,\n",
    "                                               preprocessor=dummy, stop_words = None,min_df=cutoff)\n",
    "    tfidfMatrix = tfidf_vectorizer.fit_transform(cleaned_token).toarray()\n",
    "    if(tfidfMatrix.shape[1]<dim):\n",
    "        dim = tfidfMatrix.shape[1] -1\n",
    "    # tf-idf + svd\n",
    "    svd = TruncatedSVD(n_components=dim)\n",
    "    final_lsa_Matrix = svd.fit_transform(tfidfMatrix)\n",
    "    return final_lsa_Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T20:39:47.751136Z",
     "start_time": "2018-12-13T20:39:47.685641Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "# document relation wrt textual content\n",
    "# convert raw text to numerical feature vectors\n",
    "# bow(Bags of words) are used with uni-gram setting\n",
    "def raw_text_to_vector(raw_textual_content, emb_type=\"off\", stopword=True):\n",
    "    cleaned_token, sample_size= com_func.clean_batch_of_raw(raw_textual_content, stopword=stopword)\n",
    "    average_token_size = sum(sample_size)/len(sample_size)\n",
    "    print(\"Minimal token size: \", min(sample_size))\n",
    "    print(\"maximal token size: \", max(sample_size))\n",
    "    while True:\n",
    "        if emb_type == \"tf_idf\":\n",
    "            # using tf-idf\n",
    "            tfidf_vectorizer = TfidfVectorizer(use_idf=True, smooth_idf=True, sublinear_tf=True, tokenizer=dummy,\n",
    "                                               preprocessor=dummy, stop_words = None,min_df=cutoff)\n",
    "            print(tfidf_vectorizer)\n",
    "            result_vector = tfidf_vectorizer.fit_transform(cleaned_token).toarray()\n",
    "            #print(len(tfidf_vectorizer.vocabulary_))\n",
    "            #print(tfidf_vectorizer.get_feature_names())\n",
    "            break\n",
    "        elif emb_type == \"tf\":\n",
    "            # Document-Term frequence Matrix\n",
    "            count_vectorizer = CountVectorizer(tokenizer=dummy,preprocessor=dummy, min_df=cutoff)\n",
    "            result_vector = normalize(count_vectorizer.fit_transform(cleaned_token).toarray())\n",
    "            break\n",
    "        elif emb_type == \"lsa\":\n",
    "            # use lsa\n",
    "            result_vector = LSA(cleaned_token, dim=100)\n",
    "            break\n",
    "        elif emb_type == \"off\":\n",
    "            result_vector = pd.DataFrame()\n",
    "            break\n",
    "        else:\n",
    "            print(\"Embedding type not available, selecting default setting\")\n",
    "            emb_type=\"off\"\n",
    "    return result_vector, average_token_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-13T20:39:47.802753Z",
     "start_time": "2018-12-13T20:39:47.753437Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_score, recall_score,f1_score,accuracy_score\n",
    "# cross validation\n",
    "def k_fold_cv(data, label, clf, k=10):\n",
    "    kf = KFold(n_splits=k, shuffle=False)\n",
    "    allTrueLabel = []\n",
    "    allPredLabel = []\n",
    "    for train_index, test_index in kf.split(data):\n",
    "        # print(\"TRAIN:\", train_index, \" \\n TEST:\", test_index)\n",
    "        # split train and test\n",
    "        data_train, data_test = data[train_index], data[test_index]\n",
    "        label_train, label_test = label[train_index], label[test_index]\n",
    "        # fit data to clf\n",
    "        clf.fit(data_train, label_train)\n",
    "        # get predicted label\n",
    "        label_pred = clf.predict(data_test)\n",
    "        allTrueLabel.extend(label_test)\n",
    "        allPredLabel.extend(label_pred)\n",
    "        # print(\"True positive: {tp}, False positive: {fp}, False negative: {fn}, True negative: {tn}\"\n",
    "        # .format(tp=round_tp, fp=round_fp, fn=round_fn, tn=round_tn))\n",
    "\n",
    "    accuracy = accuracy_score(allTrueLabel, allPredLabel)\n",
    "    f1 = f1_score(allTrueLabel, allPredLabel,average='macro')\n",
    "    \n",
    "    print(metrics.classification_report(allTrueLabel, allPredLabel))\n",
    "    print(metrics.confusion_matrix(allTrueLabel, allPredLabel).ravel())\n",
    "    \n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-13T20:39:55.567Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimal token size:  22\n",
      "maximal token size:  302\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(148, 1295)\n",
      "1\n",
      "(148, 1295)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        40\n",
      "           1       1.00      1.00      1.00       108\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       148\n",
      "   macro avg       1.00      1.00      1.00       148\n",
      "weighted avg       1.00      1.00      1.00       148\n",
      "\n",
      "[ 40   0   0 108]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        40\n",
      "           1       1.00      1.00      1.00       108\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       148\n",
      "   macro avg       1.00      1.00      1.00       148\n",
      "weighted avg       1.00      1.00      1.00       148\n",
      "\n",
      "[ 40   0   0 108]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "w_shi_1\n",
      "Minimal token size:  22\n",
      "maximal token size:  302\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(148, 1295)\n",
      "1\n",
      "(148, 1295)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98        45\n",
      "           1       0.99      0.99      0.99       103\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       148\n",
      "   macro avg       0.98      0.98      0.98       148\n",
      "weighted avg       0.99      0.99      0.99       148\n",
      "\n",
      "[ 44   1   1 102]\n",
      "LR Accuracy:  0.9864864864864865\n",
      "LR F1:  0.9840345199568501\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        45\n",
      "           1       0.99      1.00      1.00       103\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       148\n",
      "   macro avg       1.00      0.99      0.99       148\n",
      "weighted avg       0.99      0.99      0.99       148\n",
      "\n",
      "[ 44   1   0 103]\n",
      "svc Accuracy:  0.9932432432432432\n",
      "svc F1:  0.991966563534712\n",
      "w_shi_2\n",
      "Minimal token size:  22\n",
      "maximal token size:  302\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(148, 1295)\n",
      "1\n",
      "(148, 1295)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97        36\n",
      "           1       0.98      1.00      0.99       112\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       148\n",
      "   macro avg       0.99      0.97      0.98       148\n",
      "weighted avg       0.99      0.99      0.99       148\n",
      "\n",
      "[ 34   2   0 112]\n",
      "LR Accuracy:  0.9864864864864865\n",
      "LR F1:  0.9812895069532237\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99        36\n",
      "           1       0.99      1.00      1.00       112\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       148\n",
      "   macro avg       1.00      0.99      0.99       148\n",
      "weighted avg       0.99      0.99      0.99       148\n",
      "\n",
      "[ 35   1   0 112]\n",
      "svc Accuracy:  0.9932432432432432\n",
      "svc F1:  0.9907355242566509\n",
      "For name:  d_matthews\n",
      "total sample size before apply threshold:  57\n",
      "Counter({'0000-0003-4611-8795': 36, '0000-0002-3579-3608': 11, '0000-0003-3562-9549': 9, '0000-0002-0516-7470': 1})\n",
      "['0000-0003-4611-8795']\n",
      "d_matthews  pass\n",
      "For name:  j_christensen\n",
      "total sample size before apply threshold:  203\n",
      "Counter({'0000-0002-4299-9479': 100, '0000-0003-1414-1886': 53, '0000-0002-7641-8302': 32, '0000-0002-6741-5839': 13, '0000-0002-2689-1169': 1, '0000-0002-9231-8029': 1, '0000-0003-4225-3359': 1, '0000-0003-2370-2702': 1, '0000-0002-2495-8905': 1})\n",
      "['0000-0002-7641-8302', '0000-0002-4299-9479', '0000-0003-1414-1886']\n",
      "j_christensen_0\n",
      "Minimal token size:  8\n",
      "maximal token size:  288\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(203, 1762)\n",
      "1\n",
      "(203, 1762)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       100\n",
      "           1       0.98      1.00      0.99       103\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       203\n",
      "   macro avg       0.99      0.99      0.99       203\n",
      "weighted avg       0.99      0.99      0.99       203\n",
      "\n",
      "[ 98   2   0 103]\n",
      "LR Accuracy:  0.9901477832512315\n",
      "LR F1:  0.9901418026418026\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       100\n",
      "           1       0.99      1.00      1.00       103\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       203\n",
      "   macro avg       1.00      0.99      1.00       203\n",
      "weighted avg       1.00      1.00      1.00       203\n",
      "\n",
      "[ 99   1   0 103]\n",
      "svc Accuracy:  0.9950738916256158\n",
      "svc F1:  0.9950719782487316\n",
      "j_christensen_1\n",
      "Minimal token size:  8\n",
      "maximal token size:  288\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(203, 1762)\n",
      "1\n",
      "(203, 1762)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.78      0.86        32\n",
      "           1       0.96      0.99      0.98       171\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       203\n",
      "   macro avg       0.96      0.89      0.92       203\n",
      "weighted avg       0.96      0.96      0.96       203\n",
      "\n",
      "[ 25   7   1 170]\n",
      "LR Accuracy:  0.9605911330049262\n",
      "LR F1:  0.9195402298850575\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.78      0.86        32\n",
      "           1       0.96      0.99      0.98       171\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       203\n",
      "   macro avg       0.96      0.89      0.92       203\n",
      "weighted avg       0.96      0.96      0.96       203\n",
      "\n",
      "[ 25   7   1 170]\n",
      "svc Accuracy:  0.9605911330049262\n",
      "svc F1:  0.9195402298850575\n",
      "j_christensen_2\n",
      "Minimal token size:  8\n",
      "maximal token size:  288\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(203, 1762)\n",
      "1\n",
      "(203, 1762)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96        53\n",
      "           1       0.99      0.99      0.99       150\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       203\n",
      "   macro avg       0.97      0.97      0.97       203\n",
      "weighted avg       0.98      0.98      0.98       203\n",
      "\n",
      "[ 51   2   2 148]\n",
      "LR Accuracy:  0.9802955665024631\n",
      "LR F1:  0.9744654088050315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96        53\n",
      "           1       0.99      0.99      0.99       150\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       203\n",
      "   macro avg       0.97      0.97      0.97       203\n",
      "weighted avg       0.98      0.98      0.98       203\n",
      "\n",
      "[ 51   2   2 148]\n",
      "svc Accuracy:  0.9802955665024631\n",
      "svc F1:  0.9744654088050315\n",
      "For name:  j_sampaio\n",
      "total sample size before apply threshold:  117\n",
      "Counter({'0000-0003-2335-9991': 61, '0000-0001-8145-5274': 48, '0000-0003-4359-493X': 5, '0000-0002-0460-3664': 3})\n",
      "['0000-0003-2335-9991', '0000-0001-8145-5274']\n",
      "j_sampaio_0\n",
      "Minimal token size:  7\n",
      "maximal token size:  233\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(117, 1069)\n",
      "1\n",
      "(117, 1069)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        61\n",
      "           1       1.00      1.00      1.00        56\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       117\n",
      "   macro avg       1.00      1.00      1.00       117\n",
      "weighted avg       1.00      1.00      1.00       117\n",
      "\n",
      "[61  0  0 56]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        61\n",
      "           1       1.00      1.00      1.00        56\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       117\n",
      "   macro avg       1.00      1.00      1.00       117\n",
      "weighted avg       1.00      1.00      1.00       117\n",
      "\n",
      "[61  0  0 56]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "j_sampaio_1\n",
      "Minimal token size:  7\n",
      "maximal token size:  233\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(117, 1069)\n",
      "1\n",
      "(117, 1069)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        48\n",
      "           1       1.00      1.00      1.00        69\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       117\n",
      "   macro avg       1.00      1.00      1.00       117\n",
      "weighted avg       1.00      1.00      1.00       117\n",
      "\n",
      "[48  0  0 69]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        48\n",
      "           1       1.00      1.00      1.00        69\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       117\n",
      "   macro avg       1.00      1.00      1.00       117\n",
      "weighted avg       1.00      1.00      1.00       117\n",
      "\n",
      "[48  0  0 69]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "For name:  j_dias\n",
      "total sample size before apply threshold:  31\n",
      "Counter({'0000-0002-7613-6241': 9, '0000-0002-1150-4357': 9, '0000-0003-3732-7122': 5, '0000-0003-2517-7905': 3, '0000-0002-0966-0537': 3, '0000-0003-4732-7230': 1, '0000-0002-6271-6501': 1})\n",
      "[]\n",
      "j_dias  pass\n",
      "For name:  p_nunes\n",
      "total sample size before apply threshold:  36\n",
      "Counter({'0000-0002-4598-685X': 19, '0000-0003-4740-8268': 12, '0000-0002-4641-8846': 4, '0000-0003-1693-1267': 1})\n",
      "[]\n",
      "p_nunes  pass\n",
      "For name:  c_bauer\n",
      "total sample size before apply threshold:  7\n",
      "Counter({'0000-0001-9511-2491': 4, '0000-0003-3466-7076': 1, '0000-0001-8288-8290': 1, '0000-0002-3368-6681': 1})\n",
      "[]\n",
      "c_bauer  pass\n",
      "For name:  r_patel\n",
      "total sample size before apply threshold:  182\n",
      "Counter({'0000-0002-1526-4303': 128, '0000-0002-7444-5550': 16, '0000-0002-4712-1921': 9, '0000-0003-1586-5595': 8, '0000-0002-3851-8257': 8, '0000-0001-6344-4141': 4, '0000-0001-7667-5918': 3, '0000-0002-8442-0349': 2, '0000-0001-5330-1438': 2, '0000-0002-5398-2496': 1, '0000-0002-3418-0260': 1})\n",
      "['0000-0002-1526-4303']\n",
      "r_patel  pass\n",
      "For name:  a_das\n",
      "total sample size before apply threshold:  74\n",
      "Counter({'0000-0002-0883-1816': 14, '0000-0002-7033-1441': 10, '0000-0003-0740-8140': 8, '0000-0001-5924-4235': 6, '0000-0001-7383-9606': 5, '0000-0002-5196-9589': 5, '0000-0002-7510-1805': 5, '0000-0003-1801-7487': 4, '0000-0002-1733-626X': 3, '0000-0003-0616-9715': 3, '0000-0002-7473-6139': 2, '0000-0003-4305-6007': 2, '0000-0002-2101-9056': 2, '0000-0003-0921-8877': 2, '0000-0001-5884-0852': 1, '0000-0002-0445-0012': 1, '0000-0002-0141-0963': 1})\n",
      "[]\n",
      "a_das  pass\n",
      "For name:  c_becker\n",
      "total sample size before apply threshold:  110\n",
      "Counter({'0000-0002-1388-1041': 40, '0000-0002-1716-7208': 27, '0000-0002-6369-2185': 17, '0000-0002-7035-6083': 11, '0000-0002-9179-7996': 7, '0000-0003-3406-4670': 6, '0000-0002-8385-0785': 2})\n",
      "['0000-0002-1388-1041']\n",
      "c_becker  pass\n",
      "For name:  k_zhu\n",
      "total sample size before apply threshold:  6\n",
      "Counter({'0000-0001-7664-7204': 3, '0000-0003-4361-1138': 1, '0000-0003-2784-3190': 1, '0000-0003-2293-3568': 1})\n",
      "[]\n",
      "k_zhu  pass\n",
      "For name:  a_machado\n",
      "total sample size before apply threshold:  150\n",
      "Counter({'0000-0002-8132-5610': 54, '0000-0002-5677-7332': 30, '0000-0003-4380-3711': 25, '0000-0001-6200-3686': 16, '0000-0003-0732-1571': 14, '0000-0003-1999-1206': 4, '0000-0003-1947-8605': 4, '0000-0001-8957-661X': 2, '0000-0001-9341-5827': 1})\n",
      "['0000-0002-8132-5610', '0000-0002-5677-7332']\n",
      "a_machado_0\n",
      "Minimal token size:  2\n",
      "maximal token size:  312\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(150, 1288)\n",
      "1\n",
      "(150, 1288)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97        54\n",
      "           1       0.97      1.00      0.98        96\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       150\n",
      "   macro avg       0.98      0.97      0.98       150\n",
      "weighted avg       0.98      0.98      0.98       150\n",
      "\n",
      "[51  3  0 96]\n",
      "LR Accuracy:  0.98\n",
      "LR F1:  0.9780219780219781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.96        54\n",
      "           1       0.96      1.00      0.98        96\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       150\n",
      "   macro avg       0.98      0.96      0.97       150\n",
      "weighted avg       0.97      0.97      0.97       150\n",
      "\n",
      "[50  4  0 96]\n",
      "svc Accuracy:  0.9733333333333334\n",
      "svc F1:  0.9705651491365777\n",
      "a_machado_1\n",
      "Minimal token size:  2\n",
      "maximal token size:  312\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(150, 1288)\n",
      "1\n",
      "(150, 1288)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.97        30\n",
      "           1       0.98      1.00      0.99       120\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       150\n",
      "   macro avg       0.99      0.97      0.98       150\n",
      "weighted avg       0.99      0.99      0.99       150\n",
      "\n",
      "[ 28   2   0 120]\n",
      "LR Accuracy:  0.9866666666666667\n",
      "LR F1:  0.9786263892846965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.90      0.95        30\n",
      "           1       0.98      1.00      0.99       120\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       150\n",
      "   macro avg       0.99      0.95      0.97       150\n",
      "weighted avg       0.98      0.98      0.98       150\n",
      "\n",
      "[ 27   3   0 120]\n",
      "svc Accuracy:  0.98\n",
      "svc F1:  0.967511371020143\n",
      "For name:  j_alexander\n",
      "total sample size before apply threshold:  31\n",
      "Counter({'0000-0001-6783-4382': 11, '0000-0003-2226-7913': 10, '0000-0002-2258-5738': 5, '0000-0001-9797-6322': 2, '0000-0002-6492-1621': 2, '0000-0001-7734-9428': 1})\n",
      "[]\n",
      "j_alexander  pass\n",
      "For name:  j_schneider\n",
      "total sample size before apply threshold:  40\n",
      "Counter({'0000-0001-8016-8687': 13, '0000-0002-6028-9956': 7, '0000-0003-1114-618X': 5, '0000-0001-7169-3973': 5, '0000-0003-1176-8309': 3, '0000-0001-5187-6756': 3, '0000-0002-5863-7747': 1, '0000-0001-6093-5404': 1, '0000-0001-5556-0919': 1, '0000-0001-9610-6501': 1})\n",
      "[]\n",
      "j_schneider  pass\n",
      "For name:  g_russo\n",
      "total sample size before apply threshold:  58\n",
      "Counter({'0000-0002-8764-7389': 22, '0000-0002-2716-369X': 11, '0000-0003-1493-1087': 7, '0000-0001-9321-1613': 5, '0000-0003-4687-7353': 5, '0000-0001-5001-3027': 4, '0000-0002-4565-3131': 2, '0000-0003-4215-1926': 1, '0000-0002-7779-6225': 1})\n",
      "[]\n",
      "g_russo  pass\n",
      "For name:  j_carvalho\n",
      "total sample size before apply threshold:  136\n",
      "Counter({'0000-0002-3015-7821': 49, '0000-0001-5256-1422': 33, '0000-0003-4495-057X': 22, '0000-0003-2362-0010': 15, '0000-0001-9743-438X': 9, '0000-0001-8091-5419': 3, '0000-0002-4235-1242': 2, '0000-0002-4027-735X': 2, '0000-0002-6263-344X': 1})\n",
      "['0000-0001-5256-1422', '0000-0002-3015-7821']\n",
      "j_carvalho_0\n",
      "Minimal token size:  23\n",
      "maximal token size:  280\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(136, 1048)\n",
      "1\n",
      "(136, 1048)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        49\n",
      "           1       0.99      1.00      0.99        87\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       136\n",
      "   macro avg       0.99      0.99      0.99       136\n",
      "weighted avg       0.99      0.99      0.99       136\n",
      "\n",
      "[48  1  0 87]\n",
      "LR Accuracy:  0.9926470588235294\n",
      "LR F1:  0.9919882179675994\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        49\n",
      "           1       0.99      1.00      0.99        87\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       136\n",
      "   macro avg       0.99      0.99      0.99       136\n",
      "weighted avg       0.99      0.99      0.99       136\n",
      "\n",
      "[48  1  0 87]\n",
      "svc Accuracy:  0.9926470588235294\n",
      "svc F1:  0.9919882179675994\n",
      "j_carvalho_1\n",
      "Minimal token size:  23\n",
      "maximal token size:  280\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(136, 1048)\n",
      "1\n",
      "(136, 1048)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98        33\n",
      "           1       0.99      1.00      1.00       103\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       136\n",
      "   macro avg       1.00      0.98      0.99       136\n",
      "weighted avg       0.99      0.99      0.99       136\n",
      "\n",
      "[ 32   1   0 103]\n",
      "LR Accuracy:  0.9926470588235294\n",
      "LR F1:  0.9898922333704943\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98        33\n",
      "           1       0.99      1.00      1.00       103\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       136\n",
      "   macro avg       1.00      0.98      0.99       136\n",
      "weighted avg       0.99      0.99      0.99       136\n",
      "\n",
      "[ 32   1   0 103]\n",
      "svc Accuracy:  0.9926470588235294\n",
      "svc F1:  0.9898922333704943\n",
      "For name:  y_nishikawa\n",
      "total sample size before apply threshold:  21\n",
      "Counter({'0000-0002-0739-8491': 10, '0000-0003-3313-1990': 8, '0000-0002-0088-8447': 2, '0000-0002-1113-6937': 1})\n",
      "[]\n",
      "y_nishikawa  pass\n",
      "For name:  j_ward\n",
      "total sample size before apply threshold:  22\n",
      "Counter({'0000-0001-9870-8936': 11, '0000-0002-4108-4330': 5, '0000-0002-4698-8857': 3, '0000-0002-4196-4653': 1, '0000-0003-0289-117X': 1, '0000-0002-4415-5544': 1})\n",
      "[]\n",
      "j_ward  pass\n",
      "For name:  m_singh\n",
      "total sample size before apply threshold:  133\n",
      "Counter({'0000-0002-8072-1769': 52, '0000-0003-3044-1010': 22, '0000-0002-2884-0074': 21, '0000-0002-8396-5451': 21, '0000-0001-8569-8599': 4, '0000-0002-9124-1859': 4, '0000-0001-8526-2955': 3, '0000-0002-9010-0990': 2, '0000-0002-5783-073X': 1, '0000-0003-0051-336X': 1, '0000-0001-9166-626X': 1, '0000-0002-0034-9726': 1})\n",
      "['0000-0002-8072-1769']\n",
      "m_singh  pass\n",
      "For name:  a_bhattacharyya\n",
      "total sample size before apply threshold:  14\n",
      "Counter({'0000-0002-1646-709X': 8, '0000-0002-5948-3364': 3, '0000-0001-7011-2102': 2, '0000-0003-1077-2082': 1})\n",
      "[]\n",
      "a_bhattacharyya  pass\n",
      "For name:  e_morris\n",
      "total sample size before apply threshold:  40\n",
      "Counter({'0000-0001-7046-3623': 29, '0000-0003-1893-7515': 7, '0000-0002-5011-6744': 3, '0000-0002-9913-6041': 1})\n",
      "[]\n",
      "e_morris  pass\n",
      "For name:  m_lewis\n",
      "total sample size before apply threshold:  177\n",
      "Counter({'0000-0002-8430-4479': 63, '0000-0002-5735-5318': 35, '0000-0002-6709-9215': 30, '0000-0001-6042-0865': 13, '0000-0002-2062-6006': 11, '0000-0001-9365-5345': 9, '0000-0002-6241-3690': 8, '0000-0001-5918-3444': 3, '0000-0002-1154-9096': 2, '0000-0002-9703-8456': 1, '0000-0003-4410-5720': 1, '0000-0003-0897-1621': 1})\n",
      "['0000-0002-5735-5318', '0000-0002-8430-4479', '0000-0002-6709-9215']\n",
      "m_lewis_0\n",
      "Minimal token size:  11\n",
      "maximal token size:  345\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(177, 1578)\n",
      "1\n",
      "(177, 1578)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.90      0.94        63\n",
      "           1       0.95      0.99      0.97       114\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       177\n",
      "   macro avg       0.97      0.95      0.96       177\n",
      "weighted avg       0.96      0.96      0.96       177\n",
      "\n",
      "[ 57   6   1 113]\n",
      "LR Accuracy:  0.96045197740113\n",
      "LR F1:  0.9560529209378215\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97        63\n",
      "           1       0.97      1.00      0.98       114\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       177\n",
      "   macro avg       0.98      0.97      0.97       177\n",
      "weighted avg       0.98      0.98      0.98       177\n",
      "\n",
      "[ 59   4   0 114]\n",
      "svc Accuracy:  0.9774011299435028\n",
      "svc F1:  0.9749858677218768\n",
      "m_lewis_1\n",
      "Minimal token size:  11\n",
      "maximal token size:  345\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(177, 1578)\n",
      "1\n",
      "(177, 1578)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.86      0.91        35\n",
      "           1       0.97      0.99      0.98       142\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       177\n",
      "   macro avg       0.97      0.93      0.94       177\n",
      "weighted avg       0.97      0.97      0.97       177\n",
      "\n",
      "[ 30   5   1 141]\n",
      "LR Accuracy:  0.9661016949152542\n",
      "LR F1:  0.9441287878787878\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.86      0.92        35\n",
      "           1       0.97      1.00      0.98       142\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       177\n",
      "   macro avg       0.98      0.93      0.95       177\n",
      "weighted avg       0.97      0.97      0.97       177\n",
      "\n",
      "[ 30   5   0 142]\n",
      "svc Accuracy:  0.9717514124293786\n",
      "svc F1:  0.9528879425073196\n",
      "m_lewis_2\n",
      "Minimal token size:  11\n",
      "maximal token size:  345\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(177, 1578)\n",
      "1\n",
      "(177, 1578)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.83      0.88        30\n",
      "           1       0.97      0.99      0.98       147\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       177\n",
      "   macro avg       0.95      0.91      0.93       177\n",
      "weighted avg       0.96      0.96      0.96       177\n",
      "\n",
      "[ 25   5   2 145]\n",
      "LR Accuracy:  0.96045197740113\n",
      "LR F1:  0.9268119794435583\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.83      0.91        30\n",
      "           1       0.97      1.00      0.98       147\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       177\n",
      "   macro avg       0.98      0.92      0.95       177\n",
      "weighted avg       0.97      0.97      0.97       177\n",
      "\n",
      "[ 25   5   0 147]\n",
      "svc Accuracy:  0.9717514124293786\n",
      "svc F1:  0.9461842505320766\n",
      "For name:  v_fernandes\n",
      "total sample size before apply threshold:  55\n",
      "Counter({'0000-0001-6060-9035': 17, '0000-0002-3873-2034': 16, '0000-0003-3979-7523': 15, '0000-0002-9671-3923': 6, '0000-0003-0568-2920': 1})\n",
      "[]\n",
      "v_fernandes  pass\n",
      "For name:  m_pinheiro\n",
      "total sample size before apply threshold:  54\n",
      "Counter({'0000-0001-8228-3435': 30, '0000-0002-6931-1355': 16, '0000-0002-5500-7408': 2, '0000-0003-0758-5526': 2, '0000-0003-2523-245X': 2, '0000-0001-5963-8947': 1, '0000-0001-8234-6790': 1})\n",
      "['0000-0001-8228-3435']\n",
      "m_pinheiro  pass\n",
      "For name:  j_petersen\n",
      "total sample size before apply threshold:  41\n",
      "Counter({'0000-0001-9615-1310': 12, '0000-0001-6116-5114': 10, '0000-0001-8612-2508': 5, '0000-0003-0138-0693': 5, '0000-0002-4071-0416': 4, '0000-0002-7715-0088': 2, '0000-0001-6857-982X': 1, '0000-0003-2976-308X': 1, '0000-0003-4939-5149': 1})\n",
      "[]\n",
      "j_petersen  pass\n",
      "For name:  k_shimizu\n",
      "total sample size before apply threshold:  103\n",
      "Counter({'0000-0002-0229-6541': 44, '0000-0003-2454-1795': 37, '0000-0003-1574-5526': 10, '0000-0001-8261-8098': 8, '0000-0002-2796-8666': 4})\n",
      "['0000-0002-0229-6541', '0000-0003-2454-1795']\n",
      "k_shimizu_0\n",
      "Minimal token size:  3\n",
      "maximal token size:  233\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(103, 787)\n",
      "1\n",
      "(103, 787)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.89      0.94        37\n",
      "           1       0.94      1.00      0.97        66\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       103\n",
      "   macro avg       0.97      0.95      0.96       103\n",
      "weighted avg       0.96      0.96      0.96       103\n",
      "\n",
      "[33  4  0 66]\n",
      "LR Accuracy:  0.9611650485436893\n",
      "LR F1:  0.9567226890756302\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.89      0.94        37\n",
      "           1       0.94      1.00      0.97        66\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       103\n",
      "   macro avg       0.97      0.95      0.96       103\n",
      "weighted avg       0.96      0.96      0.96       103\n",
      "\n",
      "[33  4  0 66]\n",
      "svc Accuracy:  0.9611650485436893\n",
      "svc F1:  0.9567226890756302\n",
      "k_shimizu_1\n",
      "Minimal token size:  3\n",
      "maximal token size:  233\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(103, 787)\n",
      "1\n",
      "(103, 787)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.96        44\n",
      "           1       0.98      0.95      0.97        59\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       103\n",
      "   macro avg       0.96      0.96      0.96       103\n",
      "weighted avg       0.96      0.96      0.96       103\n",
      "\n",
      "[43  1  3 56]\n",
      "LR Accuracy:  0.9611650485436893\n",
      "LR F1:  0.960536398467433\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.96        44\n",
      "           1       0.98      0.95      0.97        59\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       103\n",
      "   macro avg       0.96      0.96      0.96       103\n",
      "weighted avg       0.96      0.96      0.96       103\n",
      "\n",
      "[43  1  3 56]\n",
      "svc Accuracy:  0.9611650485436893\n",
      "svc F1:  0.960536398467433\n",
      "For name:  p_shaw\n",
      "total sample size before apply threshold:  57\n",
      "Counter({'0000-0002-8925-2567': 21, '0000-0003-1076-2669': 18, '0000-0003-3698-1608': 12, '0000-0002-3326-3670': 6})\n",
      "[]\n",
      "p_shaw  pass\n",
      "For name:  g_coppola\n",
      "total sample size before apply threshold:  142\n",
      "Counter({'0000-0002-9574-0081': 61, '0000-0002-8510-6925': 57, '0000-0003-0147-6142': 16, '0000-0003-2675-783X': 7, '0000-0001-7139-3719': 1})\n",
      "['0000-0002-9574-0081', '0000-0002-8510-6925']\n",
      "g_coppola_0\n",
      "Minimal token size:  3\n",
      "maximal token size:  365\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(142, 1227)\n",
      "1\n",
      "(142, 1227)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96        61\n",
      "           1       0.94      1.00      0.97        81\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       142\n",
      "   macro avg       0.97      0.96      0.96       142\n",
      "weighted avg       0.97      0.96      0.96       142\n",
      "\n",
      "[56  5  0 81]\n",
      "LR Accuracy:  0.9647887323943662\n",
      "LR F1:  0.9636624187522391\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.92      0.95        61\n",
      "           1       0.94      0.99      0.96        81\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       142\n",
      "   macro avg       0.96      0.95      0.96       142\n",
      "weighted avg       0.96      0.96      0.96       142\n",
      "\n",
      "[56  5  1 80]\n",
      "svc Accuracy:  0.9577464788732394\n",
      "svc F1:  0.9565039820298141\n",
      "g_coppola_1\n",
      "Minimal token size:  3\n",
      "maximal token size:  365\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(142, 1227)\n",
      "1\n",
      "(142, 1227)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.93      0.95        57\n",
      "           1       0.95      0.98      0.97        85\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       142\n",
      "   macro avg       0.96      0.95      0.96       142\n",
      "weighted avg       0.96      0.96      0.96       142\n",
      "\n",
      "[53  4  2 83]\n",
      "LR Accuracy:  0.9577464788732394\n",
      "LR F1:  0.9557724252491695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.96        57\n",
      "           1       0.97      0.98      0.97        85\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       142\n",
      "   macro avg       0.96      0.96      0.96       142\n",
      "weighted avg       0.96      0.96      0.96       142\n",
      "\n",
      "[54  3  2 83]\n",
      "svc Accuracy:  0.9647887323943662\n",
      "svc F1:  0.9632562231537545\n",
      "For name:  a_sinclair\n",
      "total sample size before apply threshold:  109\n",
      "Counter({'0000-0003-2741-7992': 64, '0000-0001-8510-8691': 31, '0000-0002-2628-1686': 9, '0000-0002-5602-5958': 5})\n",
      "['0000-0003-2741-7992', '0000-0001-8510-8691']\n",
      "a_sinclair_0\n",
      "Minimal token size:  10\n",
      "maximal token size:  301\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(109, 966)\n",
      "1\n",
      "(109, 966)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.98        64\n",
      "           1       0.94      1.00      0.97        45\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       109\n",
      "   macro avg       0.97      0.98      0.97       109\n",
      "weighted avg       0.97      0.97      0.97       109\n",
      "\n",
      "[61  3  0 45]\n",
      "LR Accuracy:  0.9724770642201835\n",
      "LR F1:  0.9718709677419355\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.98        64\n",
      "           1       0.94      1.00      0.97        45\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       109\n",
      "   macro avg       0.97      0.98      0.97       109\n",
      "weighted avg       0.97      0.97      0.97       109\n",
      "\n",
      "[61  3  0 45]\n",
      "svc Accuracy:  0.9724770642201835\n",
      "svc F1:  0.9718709677419355\n",
      "a_sinclair_1\n",
      "Minimal token size:  10\n",
      "maximal token size:  301\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(109, 966)\n",
      "1\n",
      "(109, 966)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97        31\n",
      "           1       0.97      1.00      0.99        78\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       109\n",
      "   macro avg       0.99      0.97      0.98       109\n",
      "weighted avg       0.98      0.98      0.98       109\n",
      "\n",
      "[29  2  0 78]\n",
      "LR Accuracy:  0.981651376146789\n",
      "LR F1:  0.9770042194092826\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97        31\n",
      "           1       0.97      1.00      0.99        78\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       109\n",
      "   macro avg       0.99      0.97      0.98       109\n",
      "weighted avg       0.98      0.98      0.98       109\n",
      "\n",
      "[29  2  0 78]\n",
      "svc Accuracy:  0.981651376146789\n",
      "svc F1:  0.9770042194092826\n",
      "For name:  y_pan\n",
      "total sample size before apply threshold:  46\n",
      "Counter({'0000-0001-7709-0508': 15, '0000-0002-6311-2945': 14, '0000-0002-8587-6065': 7, '0000-0002-5547-0849': 3, '0000-0002-1173-1074': 2, '0000-0001-5133-1342': 1, '0000-0002-3945-6377': 1, '0000-0002-0090-1285': 1, '0000-0002-6894-7271': 1, '0000-0002-9195-3776': 1})\n",
      "[]\n",
      "y_pan  pass\n",
      "For name:  m_ramos\n",
      "total sample size before apply threshold:  251\n",
      "Counter({'0000-0002-7554-8324': 187, '0000-0002-8950-2079': 22, '0000-0003-3230-8045': 13, '0000-0002-2157-9774': 8, '0000-0001-6176-5048': 7, '0000-0001-8849-6386': 3, '0000-0001-5224-5665': 3, '0000-0002-2582-7616': 2, '0000-0001-5832-0945': 1, '0000-0001-6594-6591': 1, '0000-0001-6821-3692': 1, '0000-0002-3117-4498': 1, '0000-0003-1133-4164': 1, '0000-0002-9480-782X': 1})\n",
      "['0000-0002-7554-8324']\n",
      "m_ramos  pass\n",
      "For name:  j_tsai\n",
      "total sample size before apply threshold:  153\n",
      "Counter({'0000-0003-2723-6841': 83, '0000-0002-8657-3744': 38, '0000-0002-5227-8894': 16, '0000-0001-5202-722X': 7, '0000-0002-8666-2739': 5, '0000-0002-5332-2818': 2, '0000-0003-1693-9437': 1, '0000-0003-4921-3982': 1})\n",
      "['0000-0003-2723-6841', '0000-0002-8657-3744']\n",
      "j_tsai_0\n",
      "Minimal token size:  6\n",
      "maximal token size:  302\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(153, 1337)\n",
      "1\n",
      "(153, 1337)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.98      0.91        83\n",
      "           1       0.96      0.79      0.87        70\n",
      "\n",
      "   micro avg       0.89      0.89      0.89       153\n",
      "   macro avg       0.90      0.88      0.89       153\n",
      "weighted avg       0.90      0.89      0.89       153\n",
      "\n",
      "[81  2 15 55]\n",
      "LR Accuracy:  0.8888888888888888\n",
      "LR F1:  0.8855848326221792\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94        83\n",
      "           1       0.94      0.91      0.93        70\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       153\n",
      "   macro avg       0.94      0.93      0.93       153\n",
      "weighted avg       0.93      0.93      0.93       153\n",
      "\n",
      "[79  4  6 64]\n",
      "svc Accuracy:  0.934640522875817\n",
      "svc F1:  0.9340062111801242\n",
      "j_tsai_1\n",
      "Minimal token size:  6\n",
      "maximal token size:  302\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(153, 1337)\n",
      "1\n",
      "(153, 1337)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.76      0.87        38\n",
      "           1       0.93      1.00      0.96       115\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       153\n",
      "   macro avg       0.96      0.88      0.91       153\n",
      "weighted avg       0.95      0.94      0.94       153\n",
      "\n",
      "[ 29   9   0 115]\n",
      "LR Accuracy:  0.9411764705882353\n",
      "LR F1:  0.9140073690126773\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.87      0.93        38\n",
      "           1       0.96      1.00      0.98       115\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       153\n",
      "   macro avg       0.98      0.93      0.95       153\n",
      "weighted avg       0.97      0.97      0.97       153\n",
      "\n",
      "[ 33   5   0 115]\n",
      "svc Accuracy:  0.9673202614379085\n",
      "svc F1:  0.9541504345220257\n",
      "For name:  f_dai\n",
      "total sample size before apply threshold:  34\n",
      "Counter({'0000-0002-7651-8549': 18, '0000-0003-0850-6906': 11, '0000-0002-9229-5576': 4, '0000-0002-2983-4880': 1})\n",
      "[]\n",
      "f_dai  pass\n",
      "For name:  t_martin\n",
      "total sample size before apply threshold:  83\n",
      "Counter({'0000-0002-4028-4867': 43, '0000-0001-7165-9812': 28, '0000-0002-7872-4194': 7, '0000-0003-2800-5308': 2, '0000-0002-1609-078X': 1, '0000-0002-7302-1190': 1, '0000-0002-6242-6782': 1})\n",
      "['0000-0002-4028-4867']\n",
      "t_martin  pass\n",
      "For name:  t_o'brien\n",
      "total sample size before apply threshold:  262\n",
      "Counter({'0000-0002-7198-8621': 202, '0000-0002-9161-8070': 39, '0000-0001-9028-5481': 20, '0000-0002-5031-736X': 1})\n",
      "['0000-0002-9161-8070', '0000-0002-7198-8621']\n",
      "t_o'brien_0\n",
      "Minimal token size:  9\n",
      "maximal token size:  297\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(262, 2164)\n",
      "1\n",
      "(262, 2164)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96        39\n",
      "           1       1.00      0.99      0.99       223\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       262\n",
      "   macro avg       0.97      0.98      0.98       262\n",
      "weighted avg       0.99      0.99      0.99       262\n",
      "\n",
      "[ 38   1   2 221]\n",
      "LR Accuracy:  0.9885496183206107\n",
      "LR F1:  0.9776418717109943\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.92      0.95        39\n",
      "           1       0.99      1.00      0.99       223\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       262\n",
      "   macro avg       0.98      0.96      0.97       262\n",
      "weighted avg       0.98      0.98      0.98       262\n",
      "\n",
      "[ 36   3   1 222]\n",
      "svc Accuracy:  0.9847328244274809\n",
      "svc F1:  0.9692199248120301\n",
      "t_o'brien_1\n",
      "Minimal token size:  9\n",
      "maximal token size:  297\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(262, 2164)\n",
      "1\n",
      "(262, 2164)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       202\n",
      "           1       0.98      0.95      0.97        60\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       262\n",
      "   macro avg       0.98      0.97      0.98       262\n",
      "weighted avg       0.98      0.98      0.98       262\n",
      "\n",
      "[201   1   3  57]\n",
      "LR Accuracy:  0.9847328244274809\n",
      "LR F1:  0.9781247390832428\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       202\n",
      "           1       0.98      0.93      0.96        60\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       262\n",
      "   macro avg       0.98      0.96      0.97       262\n",
      "weighted avg       0.98      0.98      0.98       262\n",
      "\n",
      "[201   1   4  56]\n",
      "svc Accuracy:  0.9809160305343512\n",
      "svc F1:  0.9724899724899725\n",
      "For name:  s_may\n",
      "total sample size before apply threshold:  115\n",
      "Counter({'0000-0003-1813-7745': 59, '0000-0001-5282-3250': 47, '0000-0002-7228-8440': 7, '0000-0001-6762-7500': 2})\n",
      "['0000-0003-1813-7745', '0000-0001-5282-3250']\n",
      "s_may_0\n",
      "Minimal token size:  7\n",
      "maximal token size:  262\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(115, 1074)\n",
      "1\n",
      "(115, 1074)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        47\n",
      "           1       1.00      1.00      1.00        68\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       115\n",
      "   macro avg       1.00      1.00      1.00       115\n",
      "weighted avg       1.00      1.00      1.00       115\n",
      "\n",
      "[47  0  0 68]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        47\n",
      "           1       0.99      1.00      0.99        68\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       115\n",
      "   macro avg       0.99      0.99      0.99       115\n",
      "weighted avg       0.99      0.99      0.99       115\n",
      "\n",
      "[46  1  0 68]\n",
      "svc Accuracy:  0.991304347826087\n",
      "svc F1:  0.9909740208774822\n",
      "s_may_1\n",
      "Minimal token size:  7\n",
      "maximal token size:  262\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(115, 1074)\n",
      "1\n",
      "(115, 1074)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98        59\n",
      "           1       1.00      0.96      0.98        56\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       115\n",
      "   macro avg       0.98      0.98      0.98       115\n",
      "weighted avg       0.98      0.98      0.98       115\n",
      "\n",
      "[59  0  2 54]\n",
      "LR Accuracy:  0.9826086956521739\n",
      "LR F1:  0.9825757575757575\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        59\n",
      "           1       1.00      1.00      1.00        56\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       115\n",
      "   macro avg       1.00      1.00      1.00       115\n",
      "weighted avg       1.00      1.00      1.00       115\n",
      "\n",
      "[59  0  0 56]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "For name:  z_cai\n",
      "total sample size before apply threshold:  244\n",
      "Counter({'0000-0002-8724-7684': 200, '0000-0002-8937-4943': 27, '0000-0002-9180-675X': 11, '0000-0003-2884-1429': 6})\n",
      "['0000-0002-8724-7684']\n",
      "z_cai  pass\n",
      "For name:  a_pereira\n",
      "total sample size before apply threshold:  205\n",
      "Counter({'0000-0003-1378-4273': 47, '0000-0001-9980-441X': 19, '0000-0002-3478-4718': 15, '0000-0002-1053-8715': 14, '0000-0002-7392-2255': 9, '0000-0001-9430-9399': 7, '0000-0002-8587-262X': 7, '0000-0003-2351-1084': 7, '0000-0003-1587-4264': 7, '0000-0003-1344-2118': 7, '0000-0003-3097-7704': 5, '0000-0001-5062-1241': 5, '0000-0002-3897-2732': 5, '0000-0003-3665-7592': 5, '0000-0001-5206-4063': 4, '0000-0001-7616-4683': 4, '0000-0003-4532-6947': 3, '0000-0002-0131-3354': 3, '0000-0002-8573-7364': 3, '0000-0002-4788-0338': 3, '0000-0003-1698-3374': 3, '0000-0002-7616-0444': 3, '0000-0003-2534-1007': 2, '0000-0001-8335-7694': 2, '0000-0003-0824-1063': 2, '0000-0001-7066-1769': 2, '0000-0001-9479-5550': 2, '0000-0003-2291-1350': 2, '0000-0002-5834-9374': 1, '0000-0003-4203-6311': 1, '0000-0002-5468-0932': 1, '0000-0002-2563-6174': 1, '0000-0002-1068-2880': 1, '0000-0003-0038-2064': 1, '0000-0003-3803-2043': 1, '0000-0002-6733-5425': 1})\n",
      "['0000-0003-1378-4273']\n",
      "a_pereira  pass\n",
      "For name:  d_patel\n",
      "total sample size before apply threshold:  33\n",
      "Counter({'0000-0002-1154-3444': 9, '0000-0002-5744-568X': 8, '0000-0002-2236-7757': 5, '0000-0002-1110-0125': 3, '0000-0002-7198-1163': 2, '0000-0002-3746-8171': 2, '0000-0002-0375-2318': 2, '0000-0002-9592-1990': 2})\n",
      "[]\n",
      "d_patel  pass\n",
      "For name:  a_james\n",
      "total sample size before apply threshold:  154\n",
      "Counter({'0000-0002-4125-4053': 64, '0000-0002-1411-9307': 37, '0000-0002-0873-3714': 29, '0000-0001-8523-0857': 9, '0000-0002-6174-6696': 4, '0000-0001-8454-6219': 3, '0000-0003-4573-932X': 2, '0000-0001-5655-1213': 2, '0000-0002-0023-4363': 2, '0000-0001-9274-7803': 1, '0000-0002-2002-622X': 1})\n",
      "['0000-0002-4125-4053', '0000-0002-1411-9307']\n",
      "a_james_0\n",
      "Minimal token size:  12\n",
      "maximal token size:  339\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(154, 1465)\n",
      "1\n",
      "(154, 1465)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98        64\n",
      "           1       0.98      1.00      0.99        90\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       154\n",
      "   macro avg       0.99      0.98      0.99       154\n",
      "weighted avg       0.99      0.99      0.99       154\n",
      "\n",
      "[62  2  0 90]\n",
      "LR Accuracy:  0.987012987012987\n",
      "LR F1:  0.9865689865689866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.98        64\n",
      "           1       0.97      1.00      0.98        90\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       154\n",
      "   macro avg       0.98      0.98      0.98       154\n",
      "weighted avg       0.98      0.98      0.98       154\n",
      "\n",
      "[61  3  0 90]\n",
      "svc Accuracy:  0.9805194805194806\n",
      "svc F1:  0.9798032786885246\n",
      "a_james_1\n",
      "Minimal token size:  12\n",
      "maximal token size:  339\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(154, 1465)\n",
      "1\n",
      "(154, 1465)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.89      0.94        37\n",
      "           1       0.97      1.00      0.98       117\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       154\n",
      "   macro avg       0.98      0.95      0.96       154\n",
      "weighted avg       0.97      0.97      0.97       154\n",
      "\n",
      "[ 33   4   0 117]\n",
      "LR Accuracy:  0.974025974025974\n",
      "LR F1:  0.9630252100840335\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.86      0.93        37\n",
      "           1       0.96      1.00      0.98       117\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       154\n",
      "   macro avg       0.98      0.93      0.95       154\n",
      "weighted avg       0.97      0.97      0.97       154\n",
      "\n",
      "[ 32   5   0 117]\n",
      "svc Accuracy:  0.9675324675324676\n",
      "svc F1:  0.9533078648960038\n",
      "For name:  c_cao\n",
      "total sample size before apply threshold:  74\n",
      "Counter({'0000-0003-2139-1648': 25, '0000-0003-2830-4383': 20, '0000-0001-8621-8403': 19, '0000-0002-0320-1110': 5, '0000-0002-3407-7837': 4, '0000-0001-6909-5739': 1})\n",
      "[]\n",
      "c_cao  pass\n",
      "For name:  c_brown\n",
      "total sample size before apply threshold:  384\n",
      "Counter({'0000-0002-0294-2419': 85, '0000-0002-8959-0101': 60, '0000-0003-2305-846X': 49, '0000-0002-9637-9355': 44, '0000-0003-2506-4871': 33, '0000-0003-0079-7067': 28, '0000-0003-4776-3403': 13, '0000-0002-7271-4091': 12, '0000-0002-0210-1820': 11, '0000-0003-2057-3976': 8, '0000-0002-1559-3238': 8, '0000-0001-6001-2677': 8, '0000-0003-1602-9214': 7, '0000-0003-3060-5652': 6, '0000-0002-7758-6447': 4, '0000-0002-9905-6391': 3, '0000-0003-4780-6485': 2, '0000-0002-9616-2084': 2, '0000-0001-9979-1815': 1})\n",
      "['0000-0002-0294-2419', '0000-0003-2305-846X', '0000-0002-9637-9355', '0000-0003-2506-4871', '0000-0002-8959-0101']\n",
      "c_brown_0\n",
      "Minimal token size:  7\n",
      "maximal token size:  398\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(384, 2955)\n",
      "1\n",
      "(384, 2955)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97        49\n",
      "           1       0.99      1.00      1.00       335\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       384\n",
      "   macro avg       1.00      0.97      0.98       384\n",
      "weighted avg       0.99      0.99      0.99       384\n",
      "\n",
      "[ 46   3   0 335]\n",
      "LR Accuracy:  0.9921875\n",
      "LR F1:  0.9819817001642293\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96        49\n",
      "           1       0.99      1.00      0.99       335\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       384\n",
      "   macro avg       0.99      0.96      0.98       384\n",
      "weighted avg       0.99      0.99      0.99       384\n",
      "\n",
      "[ 45   4   0 335]\n",
      "svc Accuracy:  0.9895833333333334\n",
      "svc F1:  0.9757560452048741\n",
      "c_brown_1\n",
      "Minimal token size:  7\n",
      "maximal token size:  398\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(384, 2955)\n",
      "1\n",
      "(384, 2955)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96        60\n",
      "           1       0.99      0.99      0.99       324\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       384\n",
      "   macro avg       0.98      0.97      0.98       384\n",
      "weighted avg       0.99      0.99      0.99       384\n",
      "\n",
      "[ 57   3   2 322]\n",
      "LR Accuracy:  0.9869791666666666\n",
      "LR F1:  0.9751395165153889\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.93      0.96        60\n",
      "           1       0.99      1.00      0.99       324\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       384\n",
      "   macro avg       0.99      0.97      0.97       384\n",
      "weighted avg       0.99      0.99      0.99       384\n",
      "\n",
      "[ 56   4   1 323]\n",
      "svc Accuracy:  0.9869791666666666\n",
      "svc F1:  0.9747922328567489\n",
      "c_brown_2\n",
      "Minimal token size:  7\n",
      "maximal token size:  398\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(384, 2955)\n",
      "1\n",
      "(384, 2955)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95        85\n",
      "           1       0.99      0.98      0.99       299\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       384\n",
      "   macro avg       0.97      0.97      0.97       384\n",
      "weighted avg       0.98      0.98      0.98       384\n",
      "\n",
      "[ 82   3   5 294]\n",
      "LR Accuracy:  0.9791666666666666\n",
      "LR F1:  0.9700327766505384\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96        85\n",
      "           1       0.98      0.99      0.99       299\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       384\n",
      "   macro avg       0.98      0.97      0.97       384\n",
      "weighted avg       0.98      0.98      0.98       384\n",
      "\n",
      "[ 80   5   2 297]\n",
      "svc Accuracy:  0.9817708333333334\n",
      "svc F1:  0.9732182888798111\n",
      "c_brown_3\n",
      "Minimal token size:  7\n",
      "maximal token size:  398\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(384, 2955)\n",
      "1\n",
      "(384, 2955)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.85      0.92        33\n",
      "           1       0.99      1.00      0.99       351\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       384\n",
      "   macro avg       0.99      0.92      0.96       384\n",
      "weighted avg       0.99      0.99      0.99       384\n",
      "\n",
      "[ 28   5   0 351]\n",
      "LR Accuracy:  0.9869791666666666\n",
      "LR F1:  0.9554803255501194\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.82      0.90        33\n",
      "           1       0.98      1.00      0.99       351\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       384\n",
      "   macro avg       0.99      0.91      0.95       384\n",
      "weighted avg       0.98      0.98      0.98       384\n",
      "\n",
      "[ 27   6   0 351]\n",
      "svc Accuracy:  0.984375\n",
      "svc F1:  0.9457627118644067\n",
      "c_brown_4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimal token size:  7\n",
      "maximal token size:  398\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(384, 2955)\n",
      "1\n",
      "(384, 2955)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.98        44\n",
      "           1       0.99      1.00      1.00       340\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       384\n",
      "   macro avg       1.00      0.98      0.99       384\n",
      "weighted avg       0.99      0.99      0.99       384\n",
      "\n",
      "[ 42   2   0 340]\n",
      "LR Accuracy:  0.9947916666666666\n",
      "LR F1:  0.9869058173634317\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.98        44\n",
      "           1       0.99      1.00      1.00       340\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       384\n",
      "   macro avg       1.00      0.98      0.99       384\n",
      "weighted avg       0.99      0.99      0.99       384\n",
      "\n",
      "[ 42   2   0 340]\n",
      "svc Accuracy:  0.9947916666666666\n",
      "svc F1:  0.9869058173634317\n",
      "For name:  y_liang\n",
      "total sample size before apply threshold:  30\n",
      "Counter({'0000-0002-4798-4882': 18, '0000-0002-7224-9687': 5, '0000-0002-7225-7062': 4, '0000-0002-6440-6144': 2, '0000-0001-7756-1621': 1})\n",
      "[]\n",
      "y_liang  pass\n",
      "For name:  y_fan\n",
      "total sample size before apply threshold:  50\n",
      "Counter({'0000-0001-8477-8458': 18, '0000-0001-9677-3777': 11, '0000-0002-1865-4550': 8, '0000-0002-8897-9836': 3, '0000-0002-7919-4148': 3, '0000-0001-8914-4796': 3, '0000-0003-3743-3988': 2, '0000-0002-6551-9394': 1, '0000-0002-4010-9719': 1})\n",
      "[]\n",
      "y_fan  pass\n",
      "For name:  j_simon\n",
      "total sample size before apply threshold:  93\n",
      "Counter({'0000-0003-0214-3745': 54, '0000-0003-0858-0698': 36, '0000-0003-4824-5667': 1, '0000-0001-6081-4127': 1, '0000-0001-7513-9363': 1})\n",
      "['0000-0003-0214-3745', '0000-0003-0858-0698']\n",
      "j_simon_0\n",
      "Minimal token size:  82\n",
      "maximal token size:  283\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(93, 1008)\n",
      "1\n",
      "(93, 1008)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99        54\n",
      "           1       1.00      0.97      0.99        39\n",
      "\n",
      "   micro avg       0.99      0.99      0.99        93\n",
      "   macro avg       0.99      0.99      0.99        93\n",
      "weighted avg       0.99      0.99      0.99        93\n",
      "\n",
      "[54  0  1 38]\n",
      "LR Accuracy:  0.989247311827957\n",
      "LR F1:  0.9889193375431906\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99        54\n",
      "           1       1.00      0.97      0.99        39\n",
      "\n",
      "   micro avg       0.99      0.99      0.99        93\n",
      "   macro avg       0.99      0.99      0.99        93\n",
      "weighted avg       0.99      0.99      0.99        93\n",
      "\n",
      "[54  0  1 38]\n",
      "svc Accuracy:  0.989247311827957\n",
      "svc F1:  0.9889193375431906\n",
      "j_simon_1\n",
      "Minimal token size:  82\n",
      "maximal token size:  283\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(93, 1008)\n",
      "1\n",
      "(93, 1008)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97        36\n",
      "           1       1.00      0.96      0.98        57\n",
      "\n",
      "   micro avg       0.98      0.98      0.98        93\n",
      "   macro avg       0.97      0.98      0.98        93\n",
      "weighted avg       0.98      0.98      0.98        93\n",
      "\n",
      "[36  0  2 55]\n",
      "LR Accuracy:  0.978494623655914\n",
      "LR F1:  0.9775579150579151\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96        36\n",
      "           1       0.98      0.96      0.97        57\n",
      "\n",
      "   micro avg       0.97      0.97      0.97        93\n",
      "   macro avg       0.96      0.97      0.97        93\n",
      "weighted avg       0.97      0.97      0.97        93\n",
      "\n",
      "[35  1  2 55]\n",
      "svc Accuracy:  0.967741935483871\n",
      "svc F1:  0.9661777185113347\n",
      "For name:  m_jeong\n",
      "total sample size before apply threshold:  41\n",
      "Counter({'0000-0002-7019-8089': 34, '0000-0003-0669-1386': 3, '0000-0003-2869-1475': 2, '0000-0003-4850-8121': 2})\n",
      "['0000-0002-7019-8089']\n",
      "m_jeong  pass\n",
      "For name:  j_barrett\n",
      "total sample size before apply threshold:  130\n",
      "Counter({'0000-0002-1720-7724': 116, '0000-0002-2222-0579': 9, '0000-0002-7524-6035': 1, '0000-0002-3316-5894': 1, '0000-0002-5573-0401': 1, '0000-0002-4048-1692': 1, '0000-0002-3736-0662': 1})\n",
      "['0000-0002-1720-7724']\n",
      "j_barrett  pass\n",
      "For name:  d_elliott\n",
      "total sample size before apply threshold:  216\n",
      "Counter({'0000-0001-9959-6841': 129, '0000-0002-6081-5442': 59, '0000-0003-1052-7407': 21, '0000-0001-9837-7890': 7})\n",
      "['0000-0002-6081-5442', '0000-0001-9959-6841']\n",
      "d_elliott_0\n",
      "Minimal token size:  4\n",
      "maximal token size:  354\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(216, 1706)\n",
      "1\n",
      "(216, 1706)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99        59\n",
      "           1       1.00      0.99      1.00       157\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       216\n",
      "   macro avg       0.99      1.00      0.99       216\n",
      "weighted avg       1.00      1.00      1.00       216\n",
      "\n",
      "[ 59   0   1 156]\n",
      "LR Accuracy:  0.9953703703703703\n",
      "LR F1:  0.9942008752382742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98        59\n",
      "           1       0.99      0.99      0.99       157\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       216\n",
      "   macro avg       0.99      0.99      0.99       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "[ 58   1   1 156]\n",
      "svc Accuracy:  0.9907407407407407\n",
      "svc F1:  0.9883407103530174\n",
      "d_elliott_1\n",
      "Minimal token size:  4\n",
      "maximal token size:  354\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(216, 1706)\n",
      "1\n",
      "(216, 1706)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       129\n",
      "           1       0.98      1.00      0.99        87\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       216\n",
      "   macro avg       0.99      0.99      0.99       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "[127   2   0  87]\n",
      "LR Accuracy:  0.9907407407407407\n",
      "LR F1:  0.9904119318181819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       129\n",
      "           1       0.98      1.00      0.99        87\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       216\n",
      "   macro avg       0.99      0.99      0.99       216\n",
      "weighted avg       0.99      0.99      0.99       216\n",
      "\n",
      "[127   2   0  87]\n",
      "svc Accuracy:  0.9907407407407407\n",
      "svc F1:  0.9904119318181819\n",
      "For name:  p_antunes\n",
      "total sample size before apply threshold:  41\n",
      "Counter({'0000-0002-3553-2678': 25, '0000-0003-3324-4151': 10, '0000-0001-9129-3539': 5, '0000-0003-1969-1860': 1})\n",
      "[]\n",
      "p_antunes  pass\n",
      "For name:  x_yuan\n",
      "total sample size before apply threshold:  71\n",
      "Counter({'0000-0002-1632-8460': 38, '0000-0002-8063-9431': 13, '0000-0001-5395-9109': 11, '0000-0002-2891-1354': 5, '0000-0002-6900-6983': 2, '0000-0001-6983-7368': 1, '0000-0001-7280-7207': 1})\n",
      "['0000-0002-1632-8460']\n",
      "x_yuan  pass\n",
      "For name:  t_kim\n",
      "total sample size before apply threshold:  568\n",
      "Counter({'0000-0003-4982-4441': 109, '0000-0001-5193-1428': 95, '0000-0003-4087-8021': 48, '0000-0003-0806-8969': 39, '0000-0001-6568-2469': 34, '0000-0002-9578-5722': 27, '0000-0001-9827-7531': 27, '0000-0003-2920-9038': 23, '0000-0002-7975-2437': 23, '0000-0001-9802-0568': 22, '0000-0003-3950-7557': 22, '0000-0002-4032-1285': 17, '0000-0001-5328-0913': 15, '0000-0002-2116-4579': 14, '0000-0002-4375-8095': 11, '0000-0001-7071-1455': 10, '0000-0002-5239-3833': 9, '0000-0002-5104-6565': 4, '0000-0002-0691-9072': 4, '0000-0002-9355-7574': 3, '0000-0003-4835-0707': 3, '0000-0002-7683-7259': 2, '0000-0002-6944-4385': 2, '0000-0002-2225-1199': 2, '0000-0002-3594-826X': 1, '0000-0002-6494-1868': 1, '0000-0001-5162-5420': 1})\n",
      "['0000-0001-6568-2469', '0000-0001-5193-1428', '0000-0003-4982-4441', '0000-0003-4087-8021', '0000-0003-0806-8969']\n",
      "t_kim_0\n",
      "Minimal token size:  0\n",
      "maximal token size:  304\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(568, 3361)\n",
      "1\n",
      "(568, 3361)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.75      0.85        48\n",
      "           1       0.98      1.00      0.99       520\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       568\n",
      "   macro avg       0.98      0.87      0.92       568\n",
      "weighted avg       0.98      0.98      0.98       568\n",
      "\n",
      "[ 36  12   1 519]\n",
      "LR Accuracy:  0.977112676056338\n",
      "LR F1:  0.9173448256562378\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.92      0.95        48\n",
      "           1       0.99      1.00      1.00       520\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       568\n",
      "   macro avg       0.99      0.96      0.97       568\n",
      "weighted avg       0.99      0.99      0.99       568\n",
      "\n",
      "[ 44   4   1 519]\n",
      "svc Accuracy:  0.9911971830985915\n",
      "svc F1:  0.9707213476427592\n",
      "t_kim_1\n",
      "Minimal token size:  0\n",
      "maximal token size:  304\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(568, 3361)\n",
      "1\n",
      "(568, 3361)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.93      0.94        95\n",
      "           1       0.99      0.99      0.99       473\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       568\n",
      "   macro avg       0.97      0.96      0.96       568\n",
      "weighted avg       0.98      0.98      0.98       568\n",
      "\n",
      "[ 88   7   4 469]\n",
      "LR Accuracy:  0.9806338028169014\n",
      "LR F1:  0.9647926610053927\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.93      0.95        95\n",
      "           1       0.99      0.99      0.99       473\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       568\n",
      "   macro avg       0.98      0.96      0.97       568\n",
      "weighted avg       0.98      0.98      0.98       568\n",
      "\n",
      "[ 88   7   3 470]\n",
      "svc Accuracy:  0.9823943661971831\n",
      "svc F1:  0.9678551216751556\n",
      "t_kim_2\n",
      "Minimal token size:  0\n",
      "maximal token size:  304\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(568, 3361)\n",
      "1\n",
      "(568, 3361)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.77      0.83        39\n",
      "           1       0.98      0.99      0.99       529\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       568\n",
      "   macro avg       0.95      0.88      0.91       568\n",
      "weighted avg       0.98      0.98      0.98       568\n",
      "\n",
      "[ 30   9   3 526]\n",
      "LR Accuracy:  0.9788732394366197\n",
      "LR F1:  0.9110275689223057\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.72      0.82        39\n",
      "           1       0.98      1.00      0.99       529\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       568\n",
      "   macro avg       0.97      0.86      0.91       568\n",
      "weighted avg       0.98      0.98      0.98       568\n",
      "\n",
      "[ 28  11   1 528]\n",
      "svc Accuracy:  0.9788732394366197\n",
      "svc F1:  0.906146728354263\n",
      "t_kim_3\n",
      "Minimal token size:  0\n",
      "maximal token size:  304\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(568, 3361)\n",
      "1\n",
      "(568, 3361)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.88      0.85        34\n",
      "           1       0.99      0.99      0.99       534\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       568\n",
      "   macro avg       0.90      0.93      0.92       568\n",
      "weighted avg       0.98      0.98      0.98       568\n",
      "\n",
      "[ 30   4   7 527]\n",
      "LR Accuracy:  0.9806338028169014\n",
      "LR F1:  0.9173708920187793\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.79      0.84        34\n",
      "           1       0.99      0.99      0.99       534\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       568\n",
      "   macro avg       0.94      0.89      0.92       568\n",
      "weighted avg       0.98      0.98      0.98       568\n",
      "\n",
      "[ 27   7   3 531]\n",
      "svc Accuracy:  0.9823943661971831\n",
      "svc F1:  0.9172108208955224\n",
      "t_kim_4\n",
      "Minimal token size:  0\n",
      "maximal token size:  304\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(568, 3361)\n",
      "1\n",
      "(568, 3361)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89       109\n",
      "           1       0.97      0.97      0.97       459\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       568\n",
      "   macro avg       0.93      0.93      0.93       568\n",
      "weighted avg       0.96      0.96      0.96       568\n",
      "\n",
      "[ 97  12  12 447]\n",
      "LR Accuracy:  0.9577464788732394\n",
      "LR F1:  0.9318822330155303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.93      0.91       109\n",
      "           1       0.98      0.97      0.98       459\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       568\n",
      "   macro avg       0.94      0.95      0.94       568\n",
      "weighted avg       0.97      0.96      0.97       568\n",
      "\n",
      "[101   8  12 447]\n",
      "svc Accuracy:  0.9647887323943662\n",
      "svc F1:  0.9440140359177558\n",
      "For name:  a_cruz\n",
      "total sample size before apply threshold:  80\n",
      "Counter({'0000-0002-0465-4111': 38, '0000-0002-8251-8422': 13, '0000-0002-1662-3072': 10, '0000-0003-0368-9731': 9, '0000-0003-4537-1318': 7, '0000-0002-4591-4362': 3})\n",
      "['0000-0002-0465-4111']\n",
      "a_cruz  pass\n",
      "For name:  a_mora\n",
      "total sample size before apply threshold:  84\n",
      "Counter({'0000-0002-0785-5795': 54, '0000-0002-6397-4836': 20, '0000-0003-1344-1131': 5, '0000-0003-1354-4739': 3, '0000-0002-9132-5622': 2})\n",
      "['0000-0002-0785-5795']\n",
      "a_mora  pass\n",
      "For name:  j_walker\n",
      "total sample size before apply threshold:  253\n",
      "Counter({'0000-0002-8922-083X': 71, '0000-0002-5349-1689': 70, '0000-0002-2050-1641': 64, '0000-0002-2995-0398': 17, '0000-0002-8683-0026': 15, '0000-0001-6034-7514': 9, '0000-0002-9732-5738': 4, '0000-0001-5151-1693': 1, '0000-0003-1349-2633': 1, '0000-0002-8241-9424': 1})\n",
      "['0000-0002-2050-1641', '0000-0002-8922-083X', '0000-0002-5349-1689']\n",
      "j_walker_0\n",
      "Minimal token size:  6\n",
      "maximal token size:  409\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(253, 1962)\n",
      "1\n",
      "(253, 1962)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        64\n",
      "           1       0.99      1.00      1.00       189\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       253\n",
      "   macro avg       1.00      0.99      0.99       253\n",
      "weighted avg       1.00      1.00      1.00       253\n",
      "\n",
      "[ 63   1   0 189]\n",
      "LR Accuracy:  0.9960474308300395\n",
      "LR F1:  0.994743730912264\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        64\n",
      "           1       0.99      1.00      1.00       189\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       253\n",
      "   macro avg       1.00      0.99      0.99       253\n",
      "weighted avg       1.00      1.00      1.00       253\n",
      "\n",
      "[ 63   1   0 189]\n",
      "svc Accuracy:  0.9960474308300395\n",
      "svc F1:  0.994743730912264\n",
      "j_walker_1\n",
      "Minimal token size:  6\n",
      "maximal token size:  409\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(253, 1962)\n",
      "1\n",
      "(253, 1962)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97        71\n",
      "           1       0.98      1.00      0.99       182\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       253\n",
      "   macro avg       0.99      0.97      0.98       253\n",
      "weighted avg       0.98      0.98      0.98       253\n",
      "\n",
      "[ 67   4   0 182]\n",
      "LR Accuracy:  0.9841897233201581\n",
      "LR F1:  0.980072463768116\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97        71\n",
      "           1       0.98      1.00      0.99       182\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       253\n",
      "   macro avg       0.99      0.97      0.98       253\n",
      "weighted avg       0.98      0.98      0.98       253\n",
      "\n",
      "[ 67   4   0 182]\n",
      "svc Accuracy:  0.9841897233201581\n",
      "svc F1:  0.980072463768116\n",
      "j_walker_2\n",
      "Minimal token size:  6\n",
      "maximal token size:  409\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(253, 1962)\n",
      "1\n",
      "(253, 1962)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.93      0.96        70\n",
      "           1       0.97      0.99      0.98       183\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       253\n",
      "   macro avg       0.98      0.96      0.97       253\n",
      "weighted avg       0.98      0.98      0.98       253\n",
      "\n",
      "[ 65   5   1 182]\n",
      "LR Accuracy:  0.9762845849802372\n",
      "LR F1:  0.9698330683624801\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.96        70\n",
      "           1       0.97      1.00      0.98       183\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       253\n",
      "   macro avg       0.98      0.96      0.97       253\n",
      "weighted avg       0.98      0.98      0.98       253\n",
      "\n",
      "[ 64   6   0 183]\n",
      "svc Accuracy:  0.9762845849802372\n",
      "svc F1:  0.9695474241694753\n",
      "For name:  j_alves\n",
      "total sample size before apply threshold:  53\n",
      "Counter({'0000-0001-5914-2087': 15, '0000-0001-7221-871X': 13, '0000-0001-7554-2419': 8, '0000-0001-7182-0936': 6, '0000-0002-5736-6519': 4, '0000-0003-3131-9834': 3, '0000-0002-9599-5463': 3, '0000-0002-4355-0921': 1})\n",
      "[]\n",
      "j_alves  pass\n",
      "For name:  j_seo\n",
      "total sample size before apply threshold:  146\n",
      "Counter({'0000-0002-1927-2618': 56, '0000-0003-0242-1805': 47, '0000-0002-5039-2503': 12, '0000-0001-8881-7952': 10, '0000-0001-5095-4046': 6, '0000-0003-3471-7803': 3, '0000-0001-5844-4585': 3, '0000-0002-6582-8162': 2, '0000-0001-5534-2508': 2, '0000-0002-3329-1540': 2, '0000-0002-2878-4551': 1, '0000-0003-2117-5750': 1, '0000-0001-9338-643X': 1})\n",
      "['0000-0003-0242-1805', '0000-0002-1927-2618']\n",
      "j_seo_0\n",
      "Minimal token size:  6\n",
      "maximal token size:  267\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(146, 1233)\n",
      "1\n",
      "(146, 1233)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85        47\n",
      "           1       0.93      0.93      0.93        99\n",
      "\n",
      "   micro avg       0.90      0.90      0.90       146\n",
      "   macro avg       0.89      0.89      0.89       146\n",
      "weighted avg       0.90      0.90      0.90       146\n",
      "\n",
      "[40  7  7 92]\n",
      "LR Accuracy:  0.9041095890410958\n",
      "LR F1:  0.8901783795400816\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.81      0.87        47\n",
      "           1       0.92      0.98      0.95        99\n",
      "\n",
      "   micro avg       0.92      0.92      0.92       146\n",
      "   macro avg       0.93      0.89      0.91       146\n",
      "weighted avg       0.93      0.92      0.92       146\n",
      "\n",
      "[38  9  2 97]\n",
      "svc Accuracy:  0.9246575342465754\n",
      "svc F1:  0.9099523409027195\n",
      "j_seo_1\n",
      "Minimal token size:  6\n",
      "maximal token size:  267\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(146, 1233)\n",
      "1\n",
      "(146, 1233)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.80      0.88        56\n",
      "           1       0.89      0.99      0.94        90\n",
      "\n",
      "   micro avg       0.92      0.92      0.92       146\n",
      "   macro avg       0.93      0.90      0.91       146\n",
      "weighted avg       0.92      0.92      0.92       146\n",
      "\n",
      "[45 11  1 89]\n",
      "LR Accuracy:  0.9178082191780822\n",
      "LR F1:  0.9095975232198142\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.91      0.94        56\n",
      "           1       0.95      0.99      0.97        90\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       146\n",
      "   macro avg       0.96      0.95      0.96       146\n",
      "weighted avg       0.96      0.96      0.96       146\n",
      "\n",
      "[51  5  1 89]\n",
      "svc Accuracy:  0.958904109589041\n",
      "svc F1:  0.9559178743961353\n",
      "For name:  y_tang\n",
      "total sample size before apply threshold:  66\n",
      "Counter({'0000-0003-4888-6771': 34, '0000-0003-2718-544X': 17, '0000-0001-9312-1378': 6, '0000-0002-2649-5270': 5, '0000-0002-8807-9264': 2, '0000-0001-7919-1409': 1, '0000-0003-1096-1764': 1})\n",
      "['0000-0003-4888-6771']\n",
      "y_tang  pass\n",
      "For name:  a_norman\n",
      "total sample size before apply threshold:  28\n",
      "Counter({'0000-0002-1282-394X': 16, '0000-0002-4208-2708': 4, '0000-0002-9499-758X': 4, '0000-0002-4332-6049': 3, '0000-0001-6368-521X': 1})\n",
      "[]\n",
      "a_norman  pass\n",
      "For name:  s_tanaka\n",
      "total sample size before apply threshold:  80\n",
      "Counter({'0000-0002-3468-7694': 38, '0000-0001-5157-3317': 24, '0000-0002-1262-3876': 7, '0000-0003-2002-5582': 6, '0000-0002-7101-0690': 4, '0000-0002-2898-9557': 1})\n",
      "['0000-0002-3468-7694']\n",
      "s_tanaka  pass\n",
      "For name:  c_wen\n",
      "total sample size before apply threshold:  36\n",
      "Counter({'0000-0002-5345-0756': 25, '0000-0002-1181-8786': 6, '0000-0002-7684-8820': 2, '0000-0002-5174-1576': 1, '0000-0002-4445-1589': 1, '0000-0002-2538-0439': 1})\n",
      "[]\n",
      "c_wen  pass\n",
      "For name:  c_myers\n",
      "total sample size before apply threshold:  100\n",
      "Counter({'0000-0002-2776-4823': 92, '0000-0003-1492-9008': 6, '0000-0001-7788-8595': 1, '0000-0001-9860-3931': 1})\n",
      "['0000-0002-2776-4823']\n",
      "c_myers  pass\n",
      "For name:  v_santos\n",
      "total sample size before apply threshold:  30\n",
      "Counter({'0000-0003-0194-7397': 10, '0000-0002-5370-4867': 9, '0000-0001-8693-0759': 4, '0000-0003-3581-5595': 4, '0000-0002-8518-5408': 1, '0000-0003-1283-7388': 1, '0000-0002-9426-6197': 1})\n",
      "[]\n",
      "v_santos  pass\n",
      "For name:  j_brown\n",
      "total sample size before apply threshold:  290\n",
      "Counter({'0000-0002-2797-5428': 66, '0000-0001-5269-7661': 47, '0000-0002-6936-035X': 27, '0000-0001-8155-677X': 25, '0000-0002-6839-5948': 24, '0000-0002-1447-8633': 15, '0000-0002-0653-4615': 14, '0000-0001-8502-4252': 11, '0000-0002-3155-0334': 10, '0000-0002-2002-3010': 10, '0000-0002-7535-2874': 9, '0000-0002-4681-9586': 8, '0000-0002-4128-4359': 5, '0000-0002-9838-7201': 4, '0000-0001-6486-8667': 3, '0000-0003-3705-0290': 2, '0000-0002-1261-4574': 2, '0000-0002-1100-7457': 2, '0000-0001-6738-9653': 2, '0000-0001-5823-3083': 1, '0000-0002-9125-8474': 1, '0000-0002-2973-1021': 1, '0000-0003-2979-779X': 1})\n",
      "['0000-0001-5269-7661', '0000-0002-2797-5428']\n",
      "j_brown_0\n",
      "Minimal token size:  5\n",
      "maximal token size:  578\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(290, 2447)\n",
      "1\n",
      "(290, 2447)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97        47\n",
      "           1       1.00      0.99      0.99       243\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       290\n",
      "   macro avg       0.98      0.99      0.98       290\n",
      "weighted avg       0.99      0.99      0.99       290\n",
      "\n",
      "[ 46   1   2 241]\n",
      "LR Accuracy:  0.9896551724137931\n",
      "LR F1:  0.9811177428106349\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97        47\n",
      "           1       0.99      1.00      0.99       243\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       290\n",
      "   macro avg       0.99      0.97      0.98       290\n",
      "weighted avg       0.99      0.99      0.99       290\n",
      "\n",
      "[ 44   3   0 243]\n",
      "svc Accuracy:  0.9896551724137931\n",
      "svc F1:  0.9804489988539069\n",
      "j_brown_1\n",
      "Minimal token size:  5\n",
      "maximal token size:  578\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(290, 2447)\n",
      "1\n",
      "(290, 2447)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.92      0.95        66\n",
      "           1       0.98      1.00      0.99       224\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       290\n",
      "   macro avg       0.98      0.96      0.97       290\n",
      "weighted avg       0.98      0.98      0.98       290\n",
      "\n",
      "[ 61   5   1 223]\n",
      "LR Accuracy:  0.9793103448275862\n",
      "LR F1:  0.9699253318584071\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.92      0.95        66\n",
      "           1       0.98      1.00      0.99       224\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       290\n",
      "   macro avg       0.98      0.96      0.97       290\n",
      "weighted avg       0.98      0.98      0.98       290\n",
      "\n",
      "[ 61   5   1 223]\n",
      "svc Accuracy:  0.9793103448275862\n",
      "svc F1:  0.9699253318584071\n",
      "For name:  b_pandey\n",
      "total sample size before apply threshold:  42\n",
      "Counter({'0000-0001-6862-4424': 28, '0000-0002-8453-9665': 11, '0000-0001-7870-6060': 2, '0000-0002-3712-5961': 1})\n",
      "[]\n",
      "b_pandey  pass\n",
      "For name:  d_morgan\n",
      "total sample size before apply threshold:  86\n",
      "Counter({'0000-0002-2291-1740': 50, '0000-0002-7410-6591': 27, '0000-0001-8725-9477': 7, '0000-0001-7403-4586': 1, '0000-0002-4911-0046': 1})\n",
      "['0000-0002-2291-1740']\n",
      "d_morgan  pass\n",
      "For name:  r_smith\n",
      "total sample size before apply threshold:  789\n",
      "Counter({'0000-0002-2381-2349': 587, '0000-0002-5252-9649': 43, '0000-0001-5645-8422': 31, '0000-0002-9174-7681': 19, '0000-0001-8483-6777': 19, '0000-0003-1599-9171': 13, '0000-0003-0245-2265': 13, '0000-0001-9746-1230': 10, '0000-0002-8343-794X': 8, '0000-0002-6881-5690': 8, '0000-0003-4000-2919': 6, '0000-0002-3540-1133': 6, '0000-0003-2502-5098': 6, '0000-0001-9634-2918': 4, '0000-0002-6825-888X': 4, '0000-0003-1209-9653': 3, '0000-0002-9044-9199': 3, '0000-0003-2340-0042': 2, '0000-0002-3794-3788': 2, '0000-0002-7413-4189': 1, '0000-0001-7479-7778': 1})\n",
      "['0000-0002-2381-2349', '0000-0002-5252-9649', '0000-0001-5645-8422']\n",
      "r_smith_0\n",
      "Minimal token size:  6\n",
      "maximal token size:  299\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(789, 4116)\n",
      "1\n",
      "(789, 4116)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.81      0.88        31\n",
      "           1       0.99      1.00      1.00       758\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       789\n",
      "   macro avg       0.98      0.90      0.94       789\n",
      "weighted avg       0.99      0.99      0.99       789\n",
      "\n",
      "[ 25   6   1 757]\n",
      "LR Accuracy:  0.991128010139417\n",
      "LR F1:  0.936295373542337\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.77      0.86        31\n",
      "           1       0.99      1.00      0.99       758\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       789\n",
      "   macro avg       0.98      0.89      0.93       789\n",
      "weighted avg       0.99      0.99      0.99       789\n",
      "\n",
      "[ 24   7   1 757]\n",
      "svc Accuracy:  0.9898605830164765\n",
      "svc F1:  0.9259433076778674\n",
      "r_smith_1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimal token size:  6\n",
      "maximal token size:  299\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function dummy at 0x7f5865d60e18>, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=<function dummy at 0x7f5865d60e18>, use_idf=True,\n",
      "        vocabulary=None)\n",
      "(789, 4116)\n",
      "1\n",
      "(789, 4116)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       587\n",
      "           1       0.93      0.99      0.95       202\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       789\n",
      "   macro avg       0.96      0.98      0.97       789\n",
      "weighted avg       0.98      0.98      0.98       789\n",
      "\n",
      "[571  16   3 199]\n",
      "LR Accuracy:  0.9759188846641318\n",
      "LR F1:  0.9690356242138073\n"
     ]
    }
   ],
   "source": [
    "# load the file\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "fileDir = \"../Data/\"+Dataset+\"/canopies_labeled/\"\n",
    "listfiles = os.listdir(fileDir)\n",
    "\n",
    "for embedding in pp_textual_emb_type:\n",
    "    # collect statistic to output\n",
    "    allname = []\n",
    "    average_token_size = []\n",
    "    positive_sample_size = []\n",
    "    negative_sample_size = []\n",
    "\n",
    "    all_LR_accuracy = []\n",
    "    all_LR_f1 = []\n",
    "    all_svcLinear_accuracy = []\n",
    "    all_svcLinear_f1 = []\n",
    "\n",
    "    # read all file in labeled group\n",
    "    for file in listfiles:\n",
    "        # group name\n",
    "        temp = file.split(\"_\")\n",
    "        name = temp[1]+\"_\"+temp[-1]\n",
    "        print(\"For name: \",name)\n",
    "        # read needed content in labeled file\n",
    "        labeled_data = read_labeled_file(fileDir+file)\n",
    "        # merge textual from all raw data to labeled dataset\n",
    "        labeled_data = pd.merge(left=labeled_data,right=all_text_content, how='left', left_on='paperID', right_on='paperID')\n",
    "        # collect all labeled sample\n",
    "        all_labeled_sample = labeled_data[\"paperID\"].tolist()\n",
    "        print(\"total sample size before apply threshold: \",len(labeled_data))\n",
    "        # count number of paper each author write based on author ID\n",
    "        paperCounter = collections.Counter(labeled_data[\"authorID\"])\n",
    "        print(paperCounter)\n",
    "        # collect per class statistic\n",
    "        for k in list(paperCounter):\n",
    "            if paperCounter[k] < threshold:\n",
    "                del paperCounter[k]\n",
    "        temp =list(paperCounter.keys())\n",
    "        print(temp)\n",
    "        # remove authors that write smaller than threshold number of authors\n",
    "        temp = labeled_data[labeled_data.authorID.isin(temp)]\n",
    "        author_list = set(temp[\"authorID\"])\n",
    "        # if only have one class or no class pass the threshold, not applicable\n",
    "        if(len(paperCounter)==0) or (len(paperCounter)==1):\n",
    "            print(name,\" pass\")\n",
    "        else:\n",
    "            counter = 0\n",
    "            # loop through each author have label, one vs rest\n",
    "            for author in author_list:\n",
    "                author_name = name+'_'+str(counter)\n",
    "                allname.append(author_name)\n",
    "                print(author_name)\n",
    "                mask = labeled_data[\"authorID\"] == author\n",
    "                temp = labeled_data[mask]\n",
    "                positive_sample_pid = temp[\"paperID\"].tolist()\n",
    "                negative_sample_pid = extractNegativeSample(positive_sample_pid, all_labeled_sample)\n",
    "                # append to statistic collection\n",
    "                positive_sample_size.append(len(positive_sample_pid))\n",
    "                negative_sample_size.append(len(negative_sample_pid))\n",
    "                # form positive and negative (negative class come from similar name group)\n",
    "                all_authors = []\n",
    "                all_authors.append(positive_sample_pid)\n",
    "                all_authors.append(negative_sample_pid)\n",
    "                appended_data = []\n",
    "                for label, pid in enumerate(all_authors):\n",
    "                    # create df save one author data \n",
    "                    authordf = pd.DataFrame({\"paperID\":pid})\n",
    "                    authordf['label'] = label\n",
    "                    appended_data.append(authordf)\n",
    "                processed_data = pd.concat(appended_data, axis=0,ignore_index=True)\n",
    "                # shuffle the data\n",
    "                processed_data = processed_data.sample(frac=1).reset_index(drop=True)\n",
    "                # extract true label and it's corresponeding pid for matching\n",
    "                label = processed_data[\"label\"]\n",
    "                pid = processed_data[\"paperID\"]\n",
    "                \n",
    "                # alignment\n",
    "                labeled_data = pd.merge(processed_data, labeled_data, on=\"paperID\")\n",
    "\n",
    "                # list of different data field\n",
    "                part_collection = []\n",
    "                # select feature wanted to fit to clustering/classification algorithm\n",
    "                data_textual, data_token_size = raw_text_to_vector(labeled_data[\"combine_textual\"], emb_type=embedding)\n",
    "                \n",
    "                average_token_size.append(data_token_size)\n",
    "                print(data_textual.shape)\n",
    "                part_collection.append(data_textual)\n",
    "                # merge different part of data data together by concatenate it all together\n",
    "                # remove empty emb (when emb set off)\n",
    "                part_collection = [part for part in part_collection if len(part)!=0]\n",
    "                print(len(part_collection))\n",
    "                if len(part_collection)>1:\n",
    "                    combinedata = np.concatenate(part_collection,axis=1)\n",
    "                elif len(part_collection)==1:\n",
    "                    if isinstance(part_collection[0], pd.DataFrame):\n",
    "                        combinedata = part_collection[0].values\n",
    "                    else:\n",
    "                        combinedata = part_collection[0]\n",
    "                else:\n",
    "                    print(\"No data available\")\n",
    "                    break\n",
    "                print(combinedata.shape)\n",
    "                # using converted feature vector to train classifier\n",
    "                # using logistic regression\n",
    "                clf = LogisticRegression(class_weight=\"balanced\")\n",
    "                LRaccuracy, LRmarcof1 = k_fold_cv(combinedata, label, clf, k=10)\n",
    "                print(\"LR Accuracy: \",LRaccuracy)\n",
    "                print(\"LR F1: \", LRmarcof1)\n",
    "                all_LR_accuracy.append(LRaccuracy)\n",
    "                all_LR_f1.append(LRmarcof1)\n",
    "                # using SVM with linear kernal\n",
    "                clf = SVC(kernel='linear',class_weight=\"balanced\")\n",
    "                svcaccuracy, svcmarcof1 = k_fold_cv(combinedata, label, clf, k=10)\n",
    "                print(\"svc Accuracy: \",svcaccuracy)\n",
    "                print(\"svc F1: \", svcmarcof1)\n",
    "                all_svcLinear_accuracy.append(svcaccuracy)\n",
    "                all_svcLinear_f1.append(svcmarcof1)\n",
    "                counter+=1\n",
    "    # write evaluation result to excel\n",
    "    output = pd.DataFrame({'Author Name':allname, \"sample average token\":average_token_size,\n",
    "                           \"positive sample size\":positive_sample_size,\"negative sample size\":negative_sample_size, \n",
    "                           \"svc(linear) accuracy\":all_svcLinear_accuracy, \"svc(linear) f1\": all_svcLinear_f1, \n",
    "                           \"logistic regression accuracy\":all_LR_accuracy, \"logistic regression f1\": all_LR_f1})\n",
    "\n",
    "    savePath = \"../result/\"+Dataset+\"/binary_local_emb/\"\n",
    "    if not os.path.exists(savePath):\n",
    "        os.makedirs(savePath)\n",
    "    filename = \"textual=\"+embedding+\"_threshold=\"+str(threshold)+\".csv\"\n",
    "    output.to_csv(savePath+filename, encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-13T23:01:15.090Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# accuracy\n",
    "from statistics import mean \n",
    "cleaned_svcLinear_accuracy = [x for x in all_svcLinear_accuracy if isinstance(x, float)]\n",
    "cleaned_lr_accuracy = [x for x in all_LR_accuracy if isinstance(x, float)]\n",
    "print(len(cleaned_svcLinear_accuracy))\n",
    "print(len(cleaned_lr_accuracy))\n",
    "print(mean(cleaned_svcLinear_accuracy))\n",
    "print(mean(cleaned_lr_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-13T23:01:15.356Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# f1\n",
    "from statistics import mean \n",
    "# remove string from result\n",
    "cleaned_svcLinear_f1 = [x for x in all_svcLinear_f1 if isinstance(x, float)]\n",
    "cleaned_lr_f1 = [x for x in all_LR_f1 if isinstance(x, float)]\n",
    "print(len(cleaned_svcLinear_f1))\n",
    "print(len(cleaned_lr_f1))\n",
    "print(mean(cleaned_svcLinear_f1))\n",
    "print(mean(cleaned_lr_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T23:55:25.455010Z",
     "start_time": "2018-12-12T23:55:25.451064Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(listfiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-11-29T21:26:31.930Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%who"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
