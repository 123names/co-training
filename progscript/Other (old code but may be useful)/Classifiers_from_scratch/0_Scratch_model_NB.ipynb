{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # # test data\n",
    "# # Dataset = [[6,148,72,35,0,33.6,0.627,50,1],[1,85,66,29,0,26.6,0.351,31,0],[8,183,64,0,0,23.3,0.672,32,1],\n",
    "# #            [1,89,66,23,94,28.1,0.167,21,0],[0,137,40,35,168,43.1,2.288,33,1]]\n",
    "# Dataset = [[1,20,1], [2,21,0], [3,22,1], [4,22,0]]\n",
    "# data = []\n",
    "# label = []\n",
    "# for line in Dataset:\n",
    "#     data.append(line[:-1])\n",
    "#     label.append(line[-1])\n",
    "\n",
    "# print(data)\n",
    "# print(label)\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "data = iris.data\n",
    "label = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# naive bayes model with Gaussian distribution\n",
    "class naive_bayes(object):\n",
    "    \n",
    "    import math\n",
    "    import numpy as np\n",
    "    \n",
    "    def __init__(self, priors=None):\n",
    "        self.priors = priors\n",
    "\n",
    "    def fit(self, data, label):\n",
    "        if(len(data)!=len(label)):\n",
    "            print(\"You have inconsistent length between data and label\")\n",
    "        else:\n",
    "            # check if input is numpy array or not\n",
    "            if type(data) is self.np.ndarray:\n",
    "                data = data.tolist()\n",
    "            self.data = data\n",
    "            self.label = label\n",
    "            self.summaries = self.class_statistic(data,label)\n",
    "            print(self.summaries)\n",
    "            return self\n",
    "    # separate data for each class\n",
    "    def Separate_data_by_class(self, data, label):\n",
    "        separated = {}\n",
    "        for i in range(len(data)):\n",
    "            if(label[i] not in separated):\n",
    "                separated[label[i]] = []\n",
    "            separated[label[i]].append(data[i])\n",
    "        # count sample size of each class\n",
    "        class_count = {}\n",
    "        for key, value in separated.items():\n",
    "            class_count[key] = len(value)\n",
    "        self.count_per_class = class_count\n",
    "        # calculate log priors based on class\n",
    "        totalSample = len(data)\n",
    "        priors = {}\n",
    "        for key, value in class_count.items():\n",
    "            priors[key] = self.math.log(value/totalSample)\n",
    "        self.class_priors = priors\n",
    "        return separated\n",
    "    \n",
    "    # calculate central middle or central tendency of the data,\n",
    "    # use it as the middle of our gaussian distribution when calculating probabilities.\n",
    "    def mean(self,numbers):\n",
    "        return sum(numbers)/float(len(numbers))\n",
    "    \n",
    "    # standard deviation describes the variation of spread of the data\n",
    "    # standard deviation calculated as the square root of the variance\n",
    "    # The variance is calculated as the average of the squared differences for each attribute value from the mean. \n",
    "    def stdev(self,numbers):\n",
    "        avg = self.mean(numbers)\n",
    "        # len(numbers-1) for Bessel's correction \n",
    "        variance = sum([pow(x-avg,2) for x in numbers])/float(len(numbers)-1)\n",
    "        return self.math.sqrt(variance)\n",
    "    \n",
    "    def calculate_statistic(self,inputData):\n",
    "        summaries = [(self.mean(feature), self.stdev(feature)) for feature in zip(*inputData)]\n",
    "        return summaries\n",
    "    \n",
    "    # calculate overall(all samples) mean and standard deviation\n",
    "    def overall_statistic(self):\n",
    "        return self.calculate_statistic(data)\n",
    "    \n",
    "    # calculate class based mean and standard deviation\n",
    "    def class_statistic(self,data,label):\n",
    "        separated = self.Separate_data_by_class(data,label)\n",
    "        class_summaries = {}\n",
    "        for classLabel, classdata in separated.items():\n",
    "            class_summaries[classLabel] = self.calculate_statistic(classdata)\n",
    "        return class_summaries\n",
    "    \n",
    "    # Calculate Gaussian Probability Density Function\n",
    "    # Calculate the probability of an feature of X belonging to a class\n",
    "    def GaussianProbability(self,x, mean, stdev):\n",
    "        exponent = self.math.exp(-(self.math.pow(x-mean,2)/(2*self.math.pow(stdev,2))))\n",
    "        return self.math.log( exponent/ (self.math.sqrt(2*self.math.pi) * stdev))\n",
    "    \n",
    "    # combine the probabilities of all of the feature for a data instance and \n",
    "    # come up with a probability of the entire data instance belonging to the class.\n",
    "    def calculateClassProbabilities(self, inputVector):\n",
    "        probabilities = {}\n",
    "        for classLabel, classSummaries in self.summaries.items():\n",
    "            probabilities[classLabel] = 1\n",
    "            for i in range(len(classSummaries)):\n",
    "                mean, stdev = classSummaries[i]\n",
    "                x = inputVector[i]\n",
    "                probabilities[classLabel] += self.GaussianProbability(x, mean, stdev)\n",
    "                probabilities[classLabel] += self.class_priors[classLabel]\n",
    "        return probabilities\n",
    "    \n",
    "    \n",
    "    # find the higher probability score as final label\n",
    "    # calculate class probability when predict\n",
    "    def predict(self, inputVector):\n",
    "        probabilities = self.calculateClassProbabilities(inputVector)\n",
    "        #print(probabilities)\n",
    "        bestLabel, bestProb = None, -1\n",
    "        for classLabel, probability in probabilities.items():\n",
    "            if bestLabel is None or probability > bestProb:\n",
    "                bestProb = probability\n",
    "                bestLabel = classLabel\n",
    "        return bestLabel\n",
    "    \n",
    "    def predicts(self, inputVector):\n",
    "        results = []\n",
    "        for vector in inputVector:\n",
    "            #print(vector)\n",
    "            results.append(self.predict(vector))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test\n",
    "clf = naive_bayes().fit(data, label)\n",
    "print(clf)\n",
    "print(clf.predicts(data[-5:]))\n",
    "#1: 39.0991315159432, 0: 60.900868484056815"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "print(data[:5])\n",
    "print(gnb.fit(data, label).predict_log_proba(data[-5:]))\n",
    "gnb.predict(data[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Multinomial naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T23:05:16.107024Z",
     "start_time": "2019-04-19T23:05:15.373565Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultinomialNB(object):\n",
    "    \n",
    "    import numpy as np\n",
    "    import math\n",
    "    \n",
    "    def __init__(self, alpha=1.0):\n",
    "        # smooth parameter\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, data, label):\n",
    "        # check if input is numpy array or not\n",
    "        if type(data) is not np.ndarray:\n",
    "            data = np.asarray(data)\n",
    "        # total sample\n",
    "        totalSample = len(data)\n",
    "        # separate the data based on class\n",
    "        separated = {}\n",
    "        for i in range(len(data)):\n",
    "            if(label[i] not in separated):\n",
    "                separated[label[i]] = []\n",
    "            separated[label[i]].append(data[i])\n",
    "        # count sample size of each class\n",
    "        class_count = {}\n",
    "        for key, value in separated.items():\n",
    "            class_count[key] = len(value)\n",
    "        self.count_per_class = class_count\n",
    "        # calculate class log prior proba\n",
    "        priors = {}\n",
    "        for key, value in class_count.items():\n",
    "            priors[key] = self.math.log(value/totalSample)\n",
    "        self.class_log_prior_ = priors\n",
    "        # count total feature per class (by adding feature from each sample together)\n",
    "        # in text classification case, will be total term count per class\n",
    "        print(\"Sample with it's class label: \",separated)\n",
    "        feature_sum = {}\n",
    "        smooth_feature_sum = {}\n",
    "        for classLabel, classFeature in separated.items():\n",
    "            class_feature_sum = [sum(x) for x in zip(*classFeature)]\n",
    "            # smoothing: add alpha to every feature to prevent zero estimate\n",
    "            smoothed = [x+1 for x in class_feature_sum]\n",
    "            feature_sum[classLabel] = class_feature_sum\n",
    "            smooth_feature_sum[classLabel] = smoothed\n",
    "        print(\"No smoothing class based count: \", feature_sum)\n",
    "        print(\"Smoothed class based count: \", smooth_feature_sum)\n",
    "        # calculate log probability\n",
    "        probabilities = {}\n",
    "        for classLabel, classFeature in smooth_feature_sum.items():\n",
    "            cond_prob_list = []\n",
    "            for f in classFeature:\n",
    "                print(f, \" \", sum(classFeature), \" \" , f/sum(classFeature))\n",
    "                cond_prob_list.append(self.np.log(f/sum(classFeature)))\n",
    "            probabilities[classLabel] = cond_prob_list\n",
    "        self.log_cond_prob_feature = probabilities\n",
    "        return self\n",
    "\n",
    "    def joint_log_likelihood(self, data):\n",
    "        if type(data) is np.ndarray:\n",
    "            data = data.tolist()\n",
    "        result_list = []\n",
    "        for datapoint in data:\n",
    "            result_proba = {}\n",
    "            for classLabel, classFeature in self.log_cond_prob_feature.items():\n",
    "#                 print([a*b for a,b in zip(classFeature,datapoint)])\n",
    "                result_proba[classLabel] = sum([a*b for a,b in zip(classFeature,datapoint)]) + self.class_log_prior_[classLabel]\n",
    "            result_list.append(result_proba)\n",
    "        return result_list\n",
    "    \n",
    "    def dict_to_array(self, input_list):\n",
    "        array = []\n",
    "        for point in input_list:\n",
    "            array.append(list(point.values()))\n",
    "        return array\n",
    "    \n",
    "    def predict_log_proba(self, data):\n",
    "        jointlogli = self.joint_log_likelihood(data)\n",
    "        jointlogli = self.dict_to_array(jointlogli)\n",
    "        # normalize likelihood\n",
    "        # exponential\n",
    "        exp_logli = np.exp(jointlogli)\n",
    "        # sum over classes\n",
    "        sum_exp_logli = np.sum(exp_logli, axis=1)\n",
    "        # take log\n",
    "        log_sum_exp_logli = np.log(sum_exp_logli)\n",
    "        return jointlogli - np.atleast_2d(log_sum_exp_logli).T\n",
    "    \n",
    "    def predict_proba(self, data):\n",
    "        return np.exp(self.predict_log_proba(data))\n",
    "\n",
    "    def predict(self, data):\n",
    "        result = []\n",
    "        for feature_proba in self.joint_log_likelihood(data):\n",
    "            bestProb, bestLabel = -1, None\n",
    "            for key, value in feature_proba.items():\n",
    "                if bestLabel==None or bestProb < value :\n",
    "                    bestProb = value\n",
    "                    bestLabel = key\n",
    "            result.append(bestLabel)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-22T02:48:06.595996Z",
     "start_time": "2018-12-22T02:48:06.460781Z"
    }
   },
   "source": [
    "Denote $x = \\{x_1,x_2,...,x_i\\}$ and it represent one input data point, $\\{x_1, x_2, ..., x_i\\}$ represents feature of input data point.\n",
    "\n",
    "1. Bayes Theorem\n",
    "\\begin{equation}\n",
    "P(y|x) = \\frac{P(y)*P(x|y)}{P(x)}\n",
    "\\end{equation}\n",
    "\n",
    "2. Bayes Theorem with independent (\"naive\") assumption (Each feature in data point is independent from each other)\n",
    "\\begin{equation}\n",
    "P(y|x)=\\frac{P(y)*\\prod_{i=1}^{|x|}P(x_i|y)}{P(x)}\n",
    "\\end{equation}\n",
    "\n",
    "3. $P(x)$ is a constant (equally weight for different feature)\n",
    "\\begin{equation}\n",
    "P(y|x) = P(y)*\\prod_{i=1}^{|x|}P(x_i|y)\n",
    "\\end{equation}\n",
    "\n",
    "4. Prior probabilities P(y)\n",
    "\\begin{equation}\n",
    "P(y) = \\frac{Class Sample Count}{Total Sample Count}\n",
    "\\end{equation}\n",
    "\n",
    "5. Multinomial distribution\n",
    "\\begin{equation}\n",
    "P(x_i|y) = P(t|y) = \\frac{\\sum_{j=0}^{D_{y}}x_{t, j}}{\\sum_{t'\\in T}\\sum_{j=0}^{D_{y}}x_{t', j}}\n",
    "\\end{equation}\n",
    "\n",
    "6. Add one smoothing\n",
    "\\begin{equation}\n",
    "P(x_i|y) = P(t|y) = \\frac{\\sum_{j=0}^{D_{y}}x_{t, j} + 1}{\\sum_{t'\\in T}\\sum_{j=0}^{D_{y}}x_{t', j} + |T|}\n",
    "\\end{equation}\n",
    "\n",
    "7. Apply log to avoid underflow errors\n",
    "\\begin{equation}\n",
    "P(y|x_1,x_2,....,x_i) = \\log (P(y)) + \\sum_{i=1}^{|X|}\\log(P(x_i|y))\n",
    "\\end{equation}\n",
    "\n",
    "8. For new sample (Test sample), we calculate joint log likelihood by using per class statistic we just trained, times input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T23:05:16.149729Z",
     "start_time": "2019-04-19T23:05:16.109214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample with it's class label:  {0: [array([2, 1, 0, 0, 0, 0]), array([2, 0, 1, 0, 0, 0]), array([1, 0, 0, 1, 0, 0])], 1: [array([1, 0, 0, 0, 1, 1])]}\n",
      "No smoothing class based count:  {0: [5, 1, 1, 1, 0, 0], 1: [1, 0, 0, 0, 1, 1]}\n",
      "Smoothed class based count:  {0: [6, 2, 2, 2, 1, 1], 1: [2, 1, 1, 1, 2, 2]}\n",
      "6   14   0.42857142857142855\n",
      "2   14   0.14285714285714285\n",
      "2   14   0.14285714285714285\n",
      "2   14   0.14285714285714285\n",
      "1   14   0.07142857142857142\n",
      "1   14   0.07142857142857142\n",
      "2   9   0.2222222222222222\n",
      "1   9   0.1111111111111111\n",
      "1   9   0.1111111111111111\n",
      "1   9   0.1111111111111111\n",
      "2   9   0.2222222222222222\n",
      "2   9   0.2222222222222222\n",
      "log prior prob:  {0: -0.2876820724517809, 1: -1.3862943611198906}\n",
      "log prob {0: [-0.8472978603872037, -1.9459101490553135, -1.9459101490553135, -1.9459101490553135, -2.639057329615259, -2.639057329615259], 1: [-1.5040773967762742, -2.1972245773362196, -2.1972245773362196, -2.1972245773362196, -1.5040773967762742, -1.5040773967762742]}\n",
      "Test samples: \n",
      " [[3 0 0 0 1 1]\n",
      " [0 1 1 0 1 1]]\n",
      "[{0: -8.10769031284391, 1: -8.906681345001262}, {0: -9.457617029792926, 1: -8.788898309344878}]\n",
      "[[-0.37141358 -1.17040461]\n",
      " [-1.08239313 -0.41367441]]\n",
      "[[0.68975861 0.31024139]\n",
      " [0.3387838  0.6612162 ]]\n",
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array([\n",
    "    [2,1,0,0,0,0],\n",
    "    [2,0,1,0,0,0],\n",
    "    [1,0,0,1,0,0],\n",
    "    [1,0,0,0,1,1]\n",
    "])\n",
    "y = np.array([0,0,0,1])\n",
    "X_test = np.array([[3,0,0,0,1,1],[0,1,1,0,1,1]])\n",
    "nb = MultinomialNB().fit(X, y)\n",
    "print(\"log prior prob: \",nb.class_log_prior_)\n",
    "print(\"log prob\", nb.log_cond_prob_feature)\n",
    "print(\"Test samples: \\n\", X_test)\n",
    "#print(nb.predict_proba(X_test))\n",
    "print(nb.joint_log_likelihood(X_test))\n",
    "print(nb.predict_log_proba(X_test))\n",
    "print(nb.predict_proba(X_test))\n",
    "print(nb.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T21:39:23.227742Z",
     "start_time": "2019-04-19T21:39:23.202582Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.28768207 -1.38629436]\n",
      "[[-0.84729786 -1.94591015 -1.94591015 -1.94591015 -2.63905733 -2.63905733]\n",
      " [-1.5040774  -2.19722458 -2.19722458 -2.19722458 -1.5040774  -1.5040774 ]]\n",
      "Test samples: \n",
      " [[3 0 0 0 1 1]\n",
      " [0 1 1 0 1 1]]\n",
      "[[-8.10769031 -8.90668135]\n",
      " [-9.45761703 -8.78889831]]\n",
      "<class 'numpy.ndarray'>\n",
      "[[-0.37141358 -1.17040461]\n",
      " [-1.08239313 -0.41367441]]\n",
      "[[0.68975861 0.31024139]\n",
      " [0.3387838  0.6612162 ]]\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X, y)\n",
    "print(clf.class_log_prior_)\n",
    "print(clf.feature_log_prob_)\n",
    "\n",
    "print(\"Test samples: \\n\", X_test)\n",
    "print(clf._joint_log_likelihood(X_test))\n",
    "print(type(clf._joint_log_likelihood(X_test)))\n",
    "\n",
    "print(clf.predict_log_proba(X_test))\n",
    "print(clf.predict_proba(X_test))\n",
    "print(clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
