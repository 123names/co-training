{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "# Load tf idf model from disk\n",
    "loadDir = \"..Data/\"+Dataset+\"/models/tfidf/\"\n",
    "tfidf_vectorizer = joblib.load(loadDir+'TfidfVectorizer(use_idf=True, smooth_idf=True, sublinear_tf=True, norm=None,min_df=3)'+'.pkl')\n",
    "# Load data\n",
    "allContent = [paper.content for paper in allpaperCollection]\n",
    "print(allContent[:2])\n",
    "tfidf_matrix = tfidf_vectorizer.transform(allContent)\n",
    "for i in range(0, len(allpaperCollection)):\n",
    "    allpaperCollection[i].add_vec_location(i)\n",
    "print(tfidf_matrix[:2].toarray())\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(allpaperCollection[-1].pid)\n",
    "print(allpaperCollection[-1].vec_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def extractNegativeSample(positiveSample, allSample):\n",
    "    negativeSample = [x for x in allSample if x not in positiveSample]\n",
    "    print(\"Total Negative sample size:\", len(negativeSample))\n",
    "    return negativeSample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# collect class vectors from tf-idf matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def extractVectors(author_pids, NegativeSample_pid, allpaperCollection):\n",
    "    # extract class one vectors\n",
    "    author_features = []\n",
    "    for pid in author_pids:\n",
    "        vec_index = -1\n",
    "        for paper in allpaperCollection:\n",
    "            if(pid == paper.pid):\n",
    "                vec_index = paper.vec_index\n",
    "        if(vec_index==-1):\n",
    "            print(\"Error, not get vector index\")\n",
    "        author_features.extend(np.insert(tfidf_matrix[vec_index].toarray(), 0, pid, axis=1))\n",
    "    print(\"Positive sample size: \", len(author_features))\n",
    "    classOne = pd.DataFrame(author_features)\n",
    "    classOne[\"label\"] = 0\n",
    "    # extract class two vectors\n",
    "    other_features = []\n",
    "    for pid in NegativeSample_pid:\n",
    "        vec_index = -1\n",
    "        for paper in allpaperCollection:\n",
    "            if(pid == paper.pid):\n",
    "                vec_index = paper.vec_index\n",
    "        if(vec_index==-1):\n",
    "            print(\"Error, not get vector index\")\n",
    "        other_features.extend(np.insert(tfidf_matrix[vec_index].toarray(), 0, pid, axis=1))\n",
    "    print(\"Negative sample size: \", len(other_features))\n",
    "    classTwo = pd.DataFrame(other_features)\n",
    "    classTwo[\"label\"] = 1\n",
    "    return classOne, classTwo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combine data from different class get all data\n",
    "def combineClassesData(classOne,classTwo):\n",
    "    combinedData = pd.concat([classOne, classTwo])\n",
    "    combinedData = combinedData.sample(frac=1).reset_index(drop=True)\n",
    "    # take the paper id out\n",
    "    paperID = [int(i) for i in combinedData[0]]\n",
    "    # split data and label\n",
    "    data = combinedData.drop([0,'label'], axis=1)\n",
    "    label = combinedData['label']\n",
    "    print(\"Total sample size and shape: \",data.shape)\n",
    "    return data, label, paperID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Principal Component Analysis (PCA) applied to this data identifies the combination of attributes\n",
    "# (principal components, or directions in the feature space) that account for the most variance in the data.\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "def visualizeWithPCA(plotSavingPath,name, data, label):\n",
    "    visualize_setting = \"PCA\"\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_transformed = pd.DataFrame(pca.fit_transform(X=data, y=label))\n",
    "    pca_transformed[\"label\"] = label\n",
    "    #print(pca_transformed)\n",
    "    plt.scatter(pca_transformed[label==0][0], pca_transformed[label==0][1], label='Positive sample', c='red')\n",
    "    plt.scatter(pca_transformed[label==1][0], pca_transformed[label==1][1], label='Other', c='blue')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.savefig((plotSavingPath+name+\"_\"+visualize_setting+\".png\").encode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import Normalizer\n",
    "def visualizeWithTSNE(plotSavingPath,name, data, label):\n",
    "    visualize_setting = \"TSNE\"\n",
    "    tsne_transformed = TSNE(n_components=2, init = \"pca\").fit_transform(data)\n",
    "    tsne_transformed_normalized = Normalizer(norm='l2').fit_transform(tsne_transformed)\n",
    "    tsne_transformed_normalized = pd.DataFrame(tsne_transformed)\n",
    "    tsne_transformed_normalized[\"label\"] = label\n",
    "    #print(tsne_transformed_normalized)\n",
    "    plt.scatter(tsne_transformed_normalized[label==0][0], tsne_transformed_normalized[label==0][1], label='Positive sample', c='red')\n",
    "    plt.scatter(tsne_transformed_normalized[label==1][0], tsne_transformed_normalized[label==1][1], label='Other', c='blue')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.savefig((plotSavingPath+name+\"_\"+visualize_setting+\".png\").encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import (precision_score, recall_score,f1_score,accuracy_score)\n",
    "\n",
    "def k_fold_cv_mnb_model(author_name,data,label,k=10):\n",
    "    kf = KFold(n_splits=10, shuffle=False)\n",
    "    allTrueLabel = []\n",
    "    allPredLabel = []\n",
    "    for counter,(train_index, test_index) in enumerate(kf.split(data)):\n",
    "        # split train and test\n",
    "        data_train, data_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        label_train, test_true_label = label.iloc[train_index], label.iloc[test_index]\n",
    "        # fit data to classifier\n",
    "        classifier = MultinomialNB().fit(data_train, label_train)\n",
    "        # get predicted label\n",
    "        label_pred = classifier.predict(data_test)\n",
    "        allTrueLabel.extend(test_true_label)\n",
    "        allPredLabel.extend(label_pred)\n",
    "        # find out which sample cause the issue\n",
    "        print(\"Pred: \",label_pred)\n",
    "        print(\"True: \", test_true_label.values.tolist())\n",
    "        print(\"Mislabeled sample: \",end='')\n",
    "        for i in range(len(test_true_label)):\n",
    "            if(label_pred[i]!=test_true_label[test_index[i]]):\n",
    "                print(str(paperID[test_index[i]])+\",\",end='')\n",
    "        print()\n",
    "        # print(\"True positive: {tp}, False positive: {fp}, False negative: {fn}, True negative: {tn}\"\n",
    "        # .format(tp=round_tp, fp=round_fp, fn=round_fn, tn=round_tn))\n",
    "    \n",
    "    accuracy = accuracy_score(allTrueLabel, allPredLabel)\n",
    "    f1 = f1_score(allTrueLabel, allPredLabel,average='binary')\n",
    "    precision = precision_score(allTrueLabel, allPredLabel)\n",
    "    recall = recall_score(allTrueLabel, allPredLabel)\n",
    "    tn,fp,fn,tp = metrics.confusion_matrix(allTrueLabel, allPredLabel).ravel()\n",
    "    \n",
    "    print(\"Author: \", author_name)\n",
    "    print(\"Classifier: \", classifier)\n",
    "    print(metrics.classification_report(allTrueLabel, allPredLabel))\n",
    "    print(metrics.confusion_matrix(allTrueLabel, allPredLabel).ravel())\n",
    "    print(\"Accuracy: \",accuracy)\n",
    "    print(\"F1: \", f1)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    \n",
    "    return accuracy, f1, precision, recall, tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract vector for classification\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "fileDir = \"../Data/\"+Dataset+\"/filteredSameNameAuthor/filter=10/\"\n",
    "fileList = os.listdir(fileDir)\n",
    "print(fileList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loop through files in directory |\n",
    "# add name to name list\n",
    "# author as positive sample, other as all samples\n",
    "# train classifier for each author and save the result to the file\n",
    "name_list = []\n",
    "# create name list for all authors have same name\n",
    "for file in fileList:\n",
    "    if not file.startswith('.'):\n",
    "        if not re.match(r'\\D*\\d+.txt$', file):\n",
    "            # fix the coding issue\n",
    "            name_list.append(file.encode(\"utf-8\", \"surrogateescape\").decode('utf8','surrogateescape')[:-4])\n",
    "# print(name_list)\n",
    "\n",
    "# loop through all the author and gather result\n",
    "allauthor = []\n",
    "authorSampleSize = []\n",
    "allSampleSize = []\n",
    "allaccuracy = []\n",
    "allf1 = []\n",
    "allprecision = []\n",
    "allrecall = []\n",
    "alltn = []\n",
    "allfp = []\n",
    "allfn = []\n",
    "alltp = []\n",
    "\n",
    "for name in name_list:\n",
    "    other_pids = []\n",
    "    # read other sample\n",
    "    with open((fileDir+name+\".txt\").encode('utf-8'), 'r', encoding = 'utf8') as f:\n",
    "        for line in f:\n",
    "            other_pids.extend(line.strip().split(\" \"))\n",
    "#     print(name)\n",
    "    for file in fileList:\n",
    "        file=file.encode(\"utf-8\", \"surrogateescape\").decode('utf8','surrogateescape')\n",
    "        if not file.startswith('.'):\n",
    "            if re.match(r'\\D*\\d+.txt$', file):\n",
    "                if name in file:\n",
    "                    print(os.path.splitext(file)[0])\n",
    "                    # add author to list for final output\n",
    "                    allauthor.append(os.path.splitext(file)[0])\n",
    "                    author_pids = []\n",
    "                    # read author sample\n",
    "                    with open((fileDir+os.path.splitext(file)[0]+\".txt\").encode('utf-8'), 'r', encoding = 'utf8') as f:\n",
    "                        for line in f:\n",
    "                            author_pids.extend(line.strip().split(\" \"))\n",
    "                    # print properties\n",
    "                    authorSampleSize.append(len(author_pids))\n",
    "                    allSampleSize.append(len(other_pids))\n",
    "                    print(len(author_pids))\n",
    "                    print(len(other_pids))\n",
    "                    # remove author(positive sample) from other(all sample) to create negative sample\n",
    "                    NegativeSample_pid = extractNegativeSample(author_pids, other_pids)\n",
    "                    print(len(NegativeSample_pid))\n",
    "                    # collect all vector\n",
    "                    classOne, classTwo = extractVectors(author_pids,NegativeSample_pid,allpaperCollection)\n",
    "                    print(classOne.shape)\n",
    "                    print(classTwo.shape)\n",
    "                    # combine data from different class get all data\n",
    "                    data, label, paperID= combineClassesData(classOne, classTwo)\n",
    "                    # PCA visualize data\n",
    "                    plotSavingPath = \"../plot/tf_idf/\"\n",
    "                    visualizeWithPCA(plotSavingPath,os.path.splitext(file)[0],data,label)\n",
    "                    # TSNE visualize data\n",
    "                    visualizeWithTSNE(plotSavingPath,os.path.splitext(file)[0],data,label)\n",
    "                    # train classifier\n",
    "                    accuracy, f1, precision, recall, tn, fp, fn, tp= k_fold_cv_mnb_model(os.path.splitext(file)[0],data,label,k=10)\n",
    "                    allaccuracy.append(accuracy)\n",
    "                    allf1.append(f1)\n",
    "                    allprecision.append(precision)\n",
    "                    allrecall.append(recall)\n",
    "                    alltn.append(tn)\n",
    "                    alltp.append(tp)\n",
    "                    allfn.append(fn)\n",
    "                    allfp.append(fp)\n",
    "# write evaluation result to excel\n",
    "output = pd.DataFrame({'author':allauthor,\"AuthorSampleSize\":authorSampleSize,\n",
    "                       \"accuracy\":allaccuracy,\"f1\":allf1, \"precision\":allprecision,\n",
    "                      \"recall\":allrecall, \"AllSameNameSampleCount\":allSampleSize,\n",
    "                      \"True positive\": alltp, \"True negative\":alltn,\n",
    "                      \"False positive\": allfp, \"False negative\": allfn})\n",
    "filename = \"author_clf_mnb_tf_idf_filter=10.csv\"\n",
    "output.to_csv(\"../result/\"+Dataset+\"/\"+filename, encoding='utf-8',index=False)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hard code to read the file one by one\n",
    "# author as positive sample, other as all samples\n",
    "author_pids = []\n",
    "other_pids = []\n",
    "with open((fileDir+\"luís alves0.txt\").encode('utf-8'), 'r', encoding = 'utf8') as f:\n",
    "    for line in f:\n",
    "        author_pids.extend(line.strip().split(\" \"))\n",
    "\n",
    "with open((fileDir+\"luís alves.txt\").encode('utf-8'), 'r', encoding = 'utf8') as f:\n",
    "    for line in f:\n",
    "        other_pids.extend(line.strip().split(\" \"))\n",
    "print(author_pids[0])\n",
    "print(other_pids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# size of each class\n",
    "print(len(author_pids))\n",
    "print(len(other_pids))\n",
    "print(len(allpaperCollection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NegativeSample_pid = extractNegativeSample(author_pids, other_pids)\n",
    "print(len(NegativeSample_pid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classOne, classTwo = extractVectors(author_pids,NegativeSample_pid,allpaperCollection)\n",
    "print(classOne.shape)\n",
    "print(classTwo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data, label, paperID= combineClassesData(classOne, classTwo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotSavingPath = \"../plot/tf_idf/\"\n",
    "name = \"luís alves0\"\n",
    "visualizeWithPCA(plotSavingPath,name,data,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TSNE\n",
    "plotSavingPath = \"../plot/tf_idf/\"\n",
    "visualizeWithTSNE(plotSavingPath,name,data,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k_fold_cv_mnb_model(name,data,label,k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.externals import joblib\n",
    "def train_save_mnb_model(model_name, saving_path):\n",
    "    # create data for train and data for test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data,label)\n",
    "    # train model\n",
    "    mnb = MultinomialNB()\n",
    "    y_pred = mnb.fit(X_train, y_train).predict(X_test)\n",
    "    print(y_pred)\n",
    "    print(y_test)\n",
    "    print(metrics.confusion_matrix(y_test, y_pred).ravel())\n",
    "    print(metrics.f1_score(y_test, y_pred,average='micro'))\n",
    "    # save model\n",
    "    joblib.dump(mnb, saving_path+model_name+\"_tf_idf_mnb.pkl\")\n",
    "    print(\"Done\")\n",
    "\n",
    "ModelSavingPath = \"../Data/\"+Dataset+\"/models/MultinomialNB/\"\n",
    "name = \"michael wagner0\"\n",
    "train_save_mnb_model(name,ModelSavingPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
