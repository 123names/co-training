{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T05:22:30.497371Z",
     "start_time": "2018-12-14T05:22:27.940082Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import com_func\n",
    "import pandas as pd\n",
    "\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "Dataset = \"pubmed\"\n",
    "# parameters\n",
    "threshold = 30\n",
    "cutoff = 3\n",
    "\n",
    "pp_textual = \"tf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T05:23:17.716167Z",
     "start_time": "2018-12-14T05:22:30.503199Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total  3151504  paper have text information\n"
     ]
    }
   ],
   "source": [
    "# load text information\n",
    "Dataset = \"pubmed\"\n",
    "raw_filepath = \"../../Data/\"+Dataset+\"/id_textual_combined.txt\"\n",
    "all_text_content = []\n",
    "with open(raw_filepath, 'r', encoding = 'utf8') as f:\n",
    "    # items[0] is paper ID, items[1] is title, items[2] is abstract\n",
    "    for line in f:\n",
    "        items = line.split(\"\\t\")\n",
    "        # lower case all character\n",
    "        paperID = items[0]\n",
    "        title = items[1].lower()\n",
    "        keywords = items[2].lower()\n",
    "        mesh = items[3].lower()\n",
    "        abstract = items[4].lower()\n",
    "        # keyword and mesh\n",
    "        key_mesh = keywords+\" \"+mesh\n",
    "        # textual information can be defined as all feature combined\n",
    "        content = title+\" \"+keywords+\" \"+mesh+\" \"+abstract\n",
    "        paper_text_content = {\"paperID\": paperID, \"title\":title, \"keywords_mesh\":key_mesh,\n",
    "                              \"abstract\": abstract, \"combine_textual\":content}\n",
    "        all_text_content.append(paper_text_content)\n",
    "print(\"Total \", len(all_text_content), \" paper have text information\")\n",
    "# convert to dataframe so it's easy to process\n",
    "all_text_content = pd.DataFrame(all_text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T05:23:17.738253Z",
     "start_time": "2018-12-14T05:23:17.718517Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dummy(doc):\n",
    "    return doc\n",
    "def read_labeled_file(infile):\n",
    "    LabeledRecords_original = []\n",
    "    with open(infile, 'r', encoding = 'utf8') as f:\n",
    "        for line in f:\n",
    "            read_data = line.split(\"\\t\")\n",
    "            # get ride of bad formated lines\n",
    "            if(len(read_data)==13 or len(read_data)==12):\n",
    "                paper_detail = {\"paperID\": read_data[0], \"authorID\":read_data[1], \n",
    "                                \"co-author\": read_data[5], \"venue_id\": read_data[7]}\n",
    "                LabeledRecords_original.append(paper_detail)\n",
    "            else:\n",
    "                print(len(read_data))\n",
    "        f.close()\n",
    "    return pd.DataFrame(LabeledRecords_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T05:23:17.745112Z",
     "start_time": "2018-12-14T05:23:17.740416Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove author(positive sample) from other(negative sample)\n",
    "import random\n",
    "def extractNegativeSample(positiveSample, allSample):\n",
    "    negativeSample = [x for x in allSample if x not in positiveSample]\n",
    "    return negativeSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T05:23:17.762865Z",
     "start_time": "2018-12-14T05:23:17.747148Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LSA(cleaned_token, dim=100):\n",
    "    # Tf-idf Transformation\n",
    "    tfidf_vectorizer = TfidfVectorizer(use_idf=True, smooth_idf=True, sublinear_tf=True, tokenizer=dummy,\n",
    "                                               preprocessor=dummy, stop_words = None,min_df=cutoff)\n",
    "    tfidfMatrix = tfidf_vectorizer.fit_transform(cleaned_token).toarray()\n",
    "    if(tfidfMatrix.shape[1]<dim):\n",
    "        dim = tfidfMatrix.shape[1] -1\n",
    "    # tf-idf + svd\n",
    "    svd = TruncatedSVD(n_components=dim)\n",
    "    final_lsa_Matrix = svd.fit_transform(tfidfMatrix)\n",
    "    print(svd.explained_variance_ratio_.sum())\n",
    "    return final_lsa_Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T05:23:17.826716Z",
     "start_time": "2018-12-14T05:23:17.764946Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# document relation wrt textual content\n",
    "# convert raw text to numerical feature vectors\n",
    "# bow(Bags of words) are used with uni-gram setting\n",
    "def raw_text_to_vector(raw_textual_content, emb_type=\"off\", stopword=True):\n",
    "    cleaned_token, sample_size= com_func.clean_batch_of_raw(raw_textual_content, stopword=stopword)\n",
    "    average_token_size = sum(sample_size)/len(sample_size)\n",
    "    print(\"Minimal token size: \", min(sample_size))\n",
    "    print(\"maximal token size: \", max(sample_size))\n",
    "    while True:\n",
    "        if emb_type == \"tf_idf\":\n",
    "            # using tf-idf\n",
    "            tfidf_vectorizer = TfidfVectorizer(use_idf=True, smooth_idf=True, sublinear_tf=True, tokenizer=dummy,\n",
    "                                               preprocessor=dummy, stop_words = None,min_df=cutoff)\n",
    "            print(tfidf_vectorizer)\n",
    "            result_vector = tfidf_vectorizer.fit_transform(cleaned_token).toarray()\n",
    "            #print(len(tfidf_vectorizer.vocabulary_))\n",
    "            #print(tfidf_vectorizer.get_feature_names())\n",
    "            break\n",
    "        elif emb_type == \"tf\":\n",
    "            # Document-Term frequence Matrix\n",
    "            count_vectorizer = CountVectorizer(tokenizer=dummy,preprocessor=dummy, min_df=cutoff)\n",
    "            result_vector = normalize(count_vectorizer.fit_transform(cleaned_token).toarray())\n",
    "            break\n",
    "        elif emb_type == \"lsa\":\n",
    "            # use lsa\n",
    "            result_vector = LSA(cleaned_token, dim=100)\n",
    "            break\n",
    "        elif emb_type == \"off\":\n",
    "            result_vector = pd.DataFrame()\n",
    "            break\n",
    "        else:\n",
    "            print(\"Embedding type not available, selecting default setting\")\n",
    "            emb_type=\"off\"\n",
    "    return result_vector, average_token_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T05:24:25.972375Z",
     "start_time": "2018-12-14T05:24:20.957605Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For name:  j_read\n",
      "total sample size before apply threshold:  136\n",
      "Counter({'0000-0002-5159-1192': 57, '0000-0002-9029-5185': 39, '0000-0002-9697-0962': 31, '0000-0002-4739-9245': 3, '0000-0003-0605-5259': 3, '0000-0003-4316-7006': 1, '0000-0002-0784-0091': 1, '0000-0002-3888-6631': 1})\n",
      "['0000-0002-9697-0962', '0000-0002-9029-5185', '0000-0002-5159-1192']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-ad32d1e96618>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;31m# for each name group\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;31m# split test and train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;31m# read in data in name group\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mgroup_pid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabeled_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"paperID\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "# load the file\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "fileDir = \"../../Data/\"+Dataset+\"/canopies_labeled/\"\n",
    "listfiles = os.listdir(fileDir)\n",
    "\n",
    "# different feature\n",
    "train_feature = [\"keywords_mesh\",\"abstract\", \"combine_textual\"]\n",
    "for feature in train_feature:\n",
    "    # collect statistic to output\n",
    "    allname = []\n",
    "    average_token_size = []\n",
    "    positive_sample_size = []\n",
    "    negative_sample_size = []\n",
    "\n",
    "\n",
    "    all_mnb_accuracy = []\n",
    "    all_mnb_f1 = []\n",
    "    all_LR_accuracy = []\n",
    "    all_LR_f1 = []\n",
    "    all_svcLinear_accuracy = []\n",
    "    all_svcLinear_f1 = []\n",
    "\n",
    "    # read all file in labeled group\n",
    "    for file in listfiles:\n",
    "        # group name\n",
    "        temp = file.split(\"_\")\n",
    "        name = temp[1]+\"_\"+temp[-1]\n",
    "        print(\"For name: \",name)\n",
    "        # read needed content in labeled file\n",
    "        labeled_data = read_labeled_file(fileDir+file)\n",
    "        # merge textual from all raw data to labeled dataset\n",
    "        labeled_data = pd.merge(left=labeled_data,right=all_text_content, how='left', left_on='paperID', right_on='paperID')\n",
    "        # collect all labeled sample\n",
    "        all_labeled_sample = labeled_data[\"paperID\"].tolist()\n",
    "        print(\"total sample size before apply threshold: \",len(labeled_data))\n",
    "        # count number of paper each author write based on author ID\n",
    "        paperCounter = collections.Counter(labeled_data[\"authorID\"])\n",
    "        print(paperCounter)\n",
    "        # collect per class statistic\n",
    "        for k in list(paperCounter):\n",
    "            if paperCounter[k] < threshold:\n",
    "                del paperCounter[k]\n",
    "        temp =list(paperCounter.keys())\n",
    "        print(temp)\n",
    "        # remove authors that write smaller than threshold number of authors\n",
    "        temp = labeled_data[labeled_data.authorID.isin(temp)]\n",
    "        author_list = set(temp[\"authorID\"])\n",
    "        # if only have one class or no class pass the threshold, not applicable\n",
    "        if(len(paperCounter)==0) or (len(paperCounter)==1):\n",
    "            print(name,\" pass\")\n",
    "        else:\n",
    "            # for each name group\n",
    "            # split test and train\n",
    "            train_test_split(labeled_data[\"authorID\"], shuffle=False)\n",
    "            # read in data in name group \n",
    "            group_pid = labeled_data[\"paperID\"]\n",
    "            # list of different data field\n",
    "            part_collection = []\n",
    "            # select feature wanted to fit to clustering/classification algorithm\n",
    "            data_textual, data_token_size = raw_text_to_vector(labeled_data[feature], emb_type=pp_textual)\n",
    "            average_token_size.append(data_token_size)\n",
    "            print(data_textual.shape)\n",
    "            part_collection.append(data_textual)\n",
    "            # merge different part of data data together by concatenate it all together\n",
    "            # remove empty emb (when emb set off)\n",
    "            part_collection = [part for part in part_collection if len(part)!=0]\n",
    "            print(len(part_collection))\n",
    "            if len(part_collection)>1:\n",
    "                combinedata = np.concatenate(part_collection,axis=1)\n",
    "            elif len(part_collection)==1:\n",
    "                if isinstance(part_collection[0], pd.DataFrame):\n",
    "                    combinedata = part_collection[0].values\n",
    "                else:\n",
    "                    combinedata = part_collection[0]\n",
    "            else:\n",
    "                print(\"No data available\")\n",
    "                break\n",
    "            print(combinedata.shape)\n",
    "            print(combinedata[0])\n",
    "            \n",
    "            svcModels = []\n",
    "            lrModels = []\n",
    "            counter = 0\n",
    "            # loop through each author and train classifier\n",
    "            for author in author_list:\n",
    "                author_name = name+'_'+str(counter)\n",
    "                allname.append(author_name)\n",
    "                print(author_name)\n",
    "                mask = labeled_data[\"authorID\"] == author\n",
    "                temp = labeled_data[mask]\n",
    "                positive_sample_pid = temp[\"paperID\"].tolist()\n",
    "                negative_sample_pid = extractNegativeSample(positive_sample_pid, all_labeled_sample)\n",
    "                # append to statistic collection\n",
    "                positive_sample_size.append(len(positive_sample_pid))\n",
    "                negative_sample_size.append(len(negative_sample_pid))\n",
    "                # form positive and negative (negative class come from similar name group)\n",
    "                all_authors = []\n",
    "                all_authors.append(positive_sample_pid)\n",
    "                all_authors.append(negative_sample_pid)\n",
    "                appended_data = []\n",
    "                for label, pid in enumerate(all_authors):\n",
    "                    # create df save one author data \n",
    "                    authordf = pd.DataFrame({\"paperID\":pid})\n",
    "                    authordf['label'] = label\n",
    "                    appended_data.append(authordf)\n",
    "                processed_data = pd.concat(appended_data, axis=0,ignore_index=True)\n",
    "                # extract true label and it's corresponeding pid for matching\n",
    "                label = processed_data[\"label\"]\n",
    "                pid = processed_data[\"paperID\"]\n",
    "                \n",
    "                # alignment\n",
    "                processed_data = pd.merge(group_pid, processed_data, on=\"paperID\")\n",
    "                \n",
    "                print(processed_data[:50])\n",
    "                print(group_pid[:50])\n",
    "                \n",
    "                \n",
    "#                 # using converted feature vector to train classifier\n",
    "#                 # using logistic regression\n",
    "#                 clf = LogisticRegression()\n",
    "#                 clf.fit(combinedata)\n",
    "#                 svcModels.append()\n",
    "#                 # using SVM with linear kernal\n",
    "#                 clf = SVC(kernel='linear')\n",
    "#                 svcaccuracy, svcmarcof1 = k_fold_cv(combinedata, label, clf, k=10)\n",
    "#                 print(\"svc Accuracy: \",svcaccuracy)\n",
    "#                 print(\"svc F1: \", svcmarcof1)\n",
    "#                 all_svcLinear_accuracy.append(svcaccuracy)\n",
    "#                 all_svcLinear_f1.append(svcmarcof1)\n",
    "#                 counter+=1\n",
    "            break\n",
    "#     # write evaluation result to excel\n",
    "#     output = pd.DataFrame({'Author Name':allname, \"sample average token\":average_token_size,\n",
    "#                            \"positive sample size\":positive_sample_size,\"negative sample size\":negative_sample_size, \n",
    "#                            \"MNB Accuracy\":all_mnb_accuracy, \"MNB F1\": all_mnb_f1, \n",
    "#                            \"svc(linear) accuracy\":all_svcLinear_accuracy, \"svc(linear) f1\": all_svcLinear_f1, \n",
    "#                            \"logistic regression accuracy\":all_LR_accuracy, \"logistic regression f1\": all_LR_f1})\n",
    "\n",
    "#     savePath = \"../../result/\"+Dataset+\"/binary_clf/\"+feature+\"/\"\n",
    "#     if not os.path.exists(savePath):\n",
    "#         os.makedirs(savePath)\n",
    "#     filename = \"textual=\"+pp_textual+\"_threshold=\"+str(threshold)+\".csv\"\n",
    "#     output.to_csv(savePath+filename, encoding='utf-8',index=False)\n",
    "#     print(feature, \" Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
