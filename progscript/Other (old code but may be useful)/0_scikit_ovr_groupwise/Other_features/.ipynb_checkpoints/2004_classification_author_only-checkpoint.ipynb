{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T03:09:14.720994Z",
     "start_time": "2018-11-30T03:09:14.673209Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "import pickle\n",
    "import com_func\n",
    "import pandas as pd\n",
    "\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "threshold = 10\n",
    "cutoff = 3\n",
    "Dataset = \"pubmed\"\n",
    "\n",
    "# only using paper representation, paper ciatation, and paper co_author\n",
    "coauthor_emb_type = \"tf\"\n",
    "pp_textual_emb_type = \"off\"\n",
    "citation_emb_type = \"off\"\n",
    "\n",
    "# other could added later\n",
    "venue_emb_type = \"off\"\n",
    "year_emb_type = \"off\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T03:09:15.682953Z",
     "start_time": "2018-11-30T03:09:15.287610Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read trained rec to rec tf-idf textual graph\n",
    "def read_textual_embedding(Dataset = \"pubmed\", emb_type = \"off\"):\n",
    "    textual_emb = []\n",
    "    while True:\n",
    "        if emb_type == \"lsa\":\n",
    "            loadDir = \"../Data/\"+Dataset+\"/models/LSA/textual/\"\n",
    "            with open(loadDir+'lsa_d100_emb.pickle', \"rb\") as input_file:\n",
    "                emb = pickle.load(input_file)\n",
    "            with open(loadDir+'feature_pid.pickle', \"rb\") as input_file:\n",
    "                pid = pickle.load(input_file)\n",
    "                \n",
    "            print(emb.shape)\n",
    "            print(len(pid))\n",
    "            break\n",
    "        elif emb_type == \"pv_dm\":\n",
    "            loadDir = \"../Data/\"+Dataset+\"/vectors/d2v/Doc2Vec(dmm,d100,n5,w5,mc3,s0.001,t24).txt\"\n",
    "            with open(loadDir, 'r', encoding = 'utf8') as f:\n",
    "                for line in f:\n",
    "                    read_data = line.split(\" \")\n",
    "                    paper_Vectors = read_data\n",
    "                    textual_emb.append(paper_Vectors)\n",
    "            f.close()\n",
    "\n",
    "            print(\"Total vector records:\",len(textual_emb))\n",
    "            print(textual_emb[0])\n",
    "            break\n",
    "        elif emb_type == \"pv_dbow\":\n",
    "            loadDir = \"../Data/\"+Dataset+\"/vectors/d2v/Doc2Vec(dbow,d100,n5,mc3,s0.001,t24).txt\"\n",
    "            with open(loadDir, 'r', encoding = 'utf8') as f:\n",
    "                for line in f:\n",
    "                    read_data = line.split(\" \")\n",
    "                    paper_Vectors = read_data\n",
    "                    textual_emb.append(paper_Vectors)\n",
    "            f.close()\n",
    "            \n",
    "            print(\"Total vector records:\",len(textual_emb))\n",
    "            print(textual_emb[0])\n",
    "            break\n",
    "        elif emb_type == \"off\":\n",
    "            break\n",
    "        else:\n",
    "            print(\"Embedding type not available, selecting default setting\")\n",
    "            emb_type=\"pv_dbow\"\n",
    "    return textual_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T03:09:15.793954Z",
     "start_time": "2018-11-30T03:09:15.686339Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read trained rec to rec node2vec citation graph\n",
    "def read_citation_embedding(Dataset = \"pubmed\", emb_type = \"off\"):\n",
    "    citation_emb = []\n",
    "    while True:\n",
    "        if emb_type == \"n2v\":\n",
    "            citation_emb_dir = \"../Data/\"+Dataset+\"/vectors/\"+emb_type+\"/n2v.txt\"\n",
    "            with open(citation_emb_dir, 'r', encoding = 'utf8') as f:\n",
    "                for line in f:\n",
    "                    read_data = line.split(\" \")\n",
    "                    if(len(read_data)==101):\n",
    "                        paper_Vectors = read_data\n",
    "                        citation_emb.append(paper_Vectors)\n",
    "            f.close()\n",
    "            print(\"Total vector records:\",len(citation_emb))\n",
    "            print(citation_emb[:3])\n",
    "            break\n",
    "        elif emb_type == \"off\":\n",
    "            break\n",
    "        else:\n",
    "            print(\"Embedding type not available, selecting default setting\")\n",
    "            emb_type=\"n2v\"\n",
    "    return citation_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T03:09:16.130547Z",
     "start_time": "2018-11-30T03:09:16.025902Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_labeled_file(infile):\n",
    "    LabeledRecords_original = []\n",
    "    with open(infile, 'r', encoding = 'utf8') as f:\n",
    "        for line in f:\n",
    "            read_data = line.split(\"\\t\")\n",
    "            # get ride of bad formated lines\n",
    "            if(len(read_data)==13 or len(read_data)==12):\n",
    "                paper_detail = {\"paperID\": read_data[0], \"authorID\":read_data[1], \n",
    "                                \"total_author\": read_data[3], \"co-author\": read_data[5],\n",
    "                                \"venue_id\": read_data[7], \"publish_year\": read_data[10]}\n",
    "                LabeledRecords_original.append(paper_detail)\n",
    "            else:\n",
    "                print(len(read_data))\n",
    "        f.close()\n",
    "    return pd.DataFrame(LabeledRecords_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T03:09:16.713360Z",
     "start_time": "2018-11-30T03:09:16.578363Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# co-author relation to emb\n",
    "def co_author_to_vector(raw_co_author_data, emb_type=\"off\"):\n",
    "    while True:\n",
    "        if emb_type == \"tf\":\n",
    "            co_author_vectorizer = TfidfVectorizer(use_idf=False, sublinear_tf=False, norm=None, \n",
    "                                                   min_df=0, analyzer='word', stop_words = None)\n",
    "            print(co_author_vectorizer)\n",
    "            result_vector = co_author_vectorizer.fit_transform(raw_co_author_data).toarray()\n",
    "            #print(co_author_vectorizer.get_feature_names())\n",
    "            #print(len(co_author_vectorizer.vocabulary_))\n",
    "            break\n",
    "        elif emb_type == \"off\":\n",
    "            result_vector = pd.DataFrame()\n",
    "            break\n",
    "        else:\n",
    "            print(\"Embedding type not available, selecting default setting\")\n",
    "            emb_type=\"tf\"\n",
    "    return result_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T03:09:17.315273Z",
     "start_time": "2018-11-30T03:09:17.246249Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# author-venue relation to emb\n",
    "def venue_to_vector(raw_venue_id, emb_type=\"off\"):\n",
    "    while True:\n",
    "        if emb_type == \"tf\":\n",
    "            venue_vectorizer = TfidfVectorizer(use_idf=False, sublinear_tf=False, norm=None, \n",
    "                                       min_df=0, analyzer='word', stop_words = None)\n",
    "            result_vector = venue_vectorizer.fit_transform(raw_venue_id).toarray()\n",
    "            #print(len(venue_vectorizer.vocabulary_))\n",
    "            break\n",
    "        elif emb_type == \"off\":\n",
    "            result_vector = pd.DataFrame()\n",
    "            break\n",
    "        else:\n",
    "            print(\"Embedding type not available, selecting default setting\")\n",
    "            emb_type=\"tf\"\n",
    "    return result_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T03:09:17.936435Z",
     "start_time": "2018-11-30T03:09:17.839731Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# author-year relation to emb\n",
    "def year_to_vector(raw_year, emb_type=\"off\"):\n",
    "    while True:\n",
    "        if emb_type == \"tf\":\n",
    "            count_vectorizer = CountVectorizer()\n",
    "            result_vector = count_vectorizer.fit_transform(raw_year).toarray()\n",
    "            #print(len(count_vectorizer.vocabulary_))\n",
    "            break\n",
    "        elif emb_type == \"off\":\n",
    "            result_vector = pd.DataFrame()\n",
    "            break\n",
    "        else:\n",
    "            print(\"Embedding type not available, selecting default setting\")\n",
    "            emb_type=\"tf\"\n",
    "    return result_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T03:09:18.703541Z",
     "start_time": "2018-11-30T03:09:18.552809Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_embedding(all_embedding, pid):\n",
    "    extracted_emb = []\n",
    "    wanted_pid = pid.values.tolist()\n",
    "    # only if embedding exist\n",
    "    if len(all_embedding)>0:\n",
    "        for paper_embedding in all_embedding:\n",
    "            if paper_embedding[0] in wanted_pid:\n",
    "                extracted_emb.append(paper_embedding)\n",
    "    \n",
    "    extracted_emb = pd.DataFrame(extracted_emb)\n",
    "    # only if embedding exist\n",
    "    if len(all_embedding)>0:\n",
    "        # reorder embedding with pid and fill empty record with 0\n",
    "        extracted_emb = pd.merge(pid.to_frame(), extracted_emb, left_on='paperID', right_on=0, how='outer')\n",
    "        # remove index\n",
    "        extracted_emb.drop(['paperID', 0], axis=1, inplace=True)\n",
    "    return extracted_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T03:09:19.563602Z",
     "start_time": "2018-11-30T03:09:19.247456Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_score, recall_score,f1_score,accuracy_score\n",
    "# cross validation\n",
    "def k_fold_cv(data, label, clf, k=10):\n",
    "    kf = KFold(n_splits=k, shuffle=False)\n",
    "    allTrueLabel = []\n",
    "    allPredLabel = []\n",
    "    for train_index, test_index in kf.split(data):\n",
    "        # print(\"TRAIN:\", train_index, \" \\n TEST:\", test_index)\n",
    "        # split train and test\n",
    "        data_train, data_test = data[train_index], data[test_index]\n",
    "        label_train, label_test = label[train_index], label[test_index]\n",
    "        # fit data to clf\n",
    "        clf.fit(data_train, label_train)\n",
    "        # get predicted label\n",
    "        label_pred = clf.predict(data_test)\n",
    "        allTrueLabel.extend(label_test)\n",
    "        allPredLabel.extend(label_pred)\n",
    "        # print(\"True positive: {tp}, False positive: {fp}, False negative: {fn}, True negative: {tn}\"\n",
    "        # .format(tp=round_tp, fp=round_fp, fn=round_fn, tn=round_tn))\n",
    "\n",
    "    accuracy = accuracy_score(allTrueLabel, allPredLabel)\n",
    "    f1 = f1_score(allTrueLabel, allPredLabel,average='macro')\n",
    "    \n",
    "    print(metrics.classification_report(allTrueLabel, allPredLabel))\n",
    "    print(metrics.confusion_matrix(allTrueLabel, allPredLabel).ravel())\n",
    "    \n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T03:09:20.321636Z",
     "start_time": "2018-11-30T03:09:20.293165Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# read pretrained embeddings\n",
    "all_textual_embedding = read_textual_embedding(Dataset, pp_textual_emb_type)\n",
    "all_citation_embedding = read_citation_embedding(Dataset, citation_emb_type)\n",
    "print(len(all_textual_embedding))\n",
    "print(len(all_citation_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T03:49:51.996219Z",
     "start_time": "2018-11-30T03:15:37.504417Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For name:  j_read\n",
      "total sample size before apply threshold:  136\n",
      "Counter({'0000-0002-5159-1192': 57, '0000-0002-9029-5185': 39, '0000-0002-9697-0962': 31, '0000-0002-4739-9245': 3, '0000-0003-0605-5259': 3, '0000-0003-4316-7006': 1, '0000-0002-0784-0091': 1, '0000-0002-3888-6631': 1})\n",
      "['0000-0002-9697-0962', '0000-0002-9029-5185', '0000-0002-5159-1192']\n",
      "Total sample size after apply threshold:  127\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(127, 263)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(127, 263)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.71      0.83        31\n",
      "          1       0.93      0.67      0.78        39\n",
      "          2       0.73      0.98      0.84        57\n",
      "\n",
      "avg / total       0.86      0.82      0.82       127\n",
      "\n",
      "[22  1  8  0 26 13  0  1 56]\n",
      "svc Accuracy:  0.8188976377952756\n",
      "svc F1:  0.8140429925842486\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.71      0.83        31\n",
      "          1       1.00      0.67      0.80        39\n",
      "          2       0.72      1.00      0.84        57\n",
      "\n",
      "avg / total       0.88      0.83      0.82       127\n",
      "\n",
      "[22  0  9  0 26 13  0  0 57]\n",
      "LR Accuracy:  0.8267716535433071\n",
      "LR F1:  0.8228079911209768\n",
      "For name:  f_esteves\n",
      "total sample size before apply threshold:  34\n",
      "Counter({'0000-0002-3046-1313': 18, '0000-0002-5403-0091': 12, '0000-0003-0589-0746': 3, '0000-0003-3172-6253': 1})\n",
      "['0000-0002-5403-0091', '0000-0002-3046-1313']\n",
      "Total sample size after apply threshold:  30\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(30, 63)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(30, 63)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        12\n",
      "          1       1.00      1.00      1.00        18\n",
      "\n",
      "avg / total       1.00      1.00      1.00        30\n",
      "\n",
      "[12  0  0 18]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        12\n",
      "          1       1.00      1.00      1.00        18\n",
      "\n",
      "avg / total       1.00      1.00      1.00        30\n",
      "\n",
      "[12  0  0 18]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  c_miller\n",
      "total sample size before apply threshold:  252\n",
      "Counter({'0000-0003-4341-1283': 51, '0000-0002-3989-7973': 40, '0000-0002-3813-1706': 39, '0000-0003-2772-9531': 27, '0000-0001-6082-9273': 22, '0000-0002-2601-4422': 22, '0000-0002-9448-8144': 19, '0000-0001-8628-4902': 15, '0000-0002-2936-7717': 6, '0000-0003-3898-9734': 6, '0000-0002-5074-6914': 2, '0000-0003-4266-6700': 1, '0000-0002-9286-9787': 1, '0000-0002-0821-0892': 1})\n",
      "['0000-0003-4341-1283', '0000-0002-9448-8144', '0000-0003-2772-9531', '0000-0001-6082-9273', '0000-0002-3813-1706', '0000-0001-8628-4902', '0000-0002-3989-7973', '0000-0002-2601-4422']\n",
      "Total sample size after apply threshold:  235\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(235, 683)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(235, 683)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.78      0.88        51\n",
      "          1       1.00      0.68      0.81        19\n",
      "          2       0.42      1.00      0.59        27\n",
      "          3       0.68      0.77      0.72        22\n",
      "          4       0.97      0.95      0.96        39\n",
      "          5       1.00      0.73      0.85        15\n",
      "          6       1.00      0.70      0.82        40\n",
      "          7       1.00      0.73      0.84        22\n",
      "\n",
      "avg / total       0.90      0.80      0.82       235\n",
      "\n",
      "[40  0  8  3  0  0  0  0  0 13  4  2  0  0  0  0  0  0 27  0  0  0  0  0\n",
      "  0  0  5 17  0  0  0  0  0  0  2  0 37  0  0  0  0  0  2  2  0 11  0  0\n",
      "  0  0 11  0  1  0 28  0  0  0  5  1  0  0  0 16]\n",
      "svc Accuracy:  0.8042553191489362\n",
      "svc F1:  0.8101574012452537\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.96      0.88        51\n",
      "          1       1.00      0.74      0.85        19\n",
      "          2       0.56      0.81      0.67        27\n",
      "          3       0.95      0.86      0.90        22\n",
      "          4       0.97      0.92      0.95        39\n",
      "          5       1.00      0.80      0.89        15\n",
      "          6       0.89      0.78      0.83        40\n",
      "          7       1.00      0.82      0.90        22\n",
      "\n",
      "avg / total       0.88      0.86      0.86       235\n",
      "\n",
      "[49  0  2  0  0  0  0  0  2 14  2  1  0  0  0  0  3  0 22  0  0  0  2  0\n",
      "  1  0  1 19  0  0  1  0  0  0  2  0 36  0  1  0  2  0  1  0  0 12  0  0\n",
      "  1  0  7  0  1  0 31  0  2  0  2  0  0  0  0 18]\n",
      "LR Accuracy:  0.8553191489361702\n",
      "LR F1:  0.8582150349255613\n",
      "For name:  r_jha\n",
      "total sample size before apply threshold:  11\n",
      "Counter({'0000-0002-2891-8353': 6, '0000-0003-0332-2542': 3, '0000-0003-1877-1973': 1, '0000-0002-7755-7443': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  a_lowe\n",
      "total sample size before apply threshold:  102\n",
      "Counter({'0000-0002-4691-8162': 69, '0000-0001-6650-7486': 22, '0000-0002-0558-3597': 10, '0000-0003-1139-2516': 1})\n",
      "['0000-0002-0558-3597', '0000-0002-4691-8162', '0000-0001-6650-7486']\n",
      "Total sample size after apply threshold:  101\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(101, 262)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(101, 262)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.50      0.67        10\n",
      "          1       0.90      1.00      0.95        69\n",
      "          2       1.00      0.86      0.93        22\n",
      "\n",
      "avg / total       0.93      0.92      0.91       101\n",
      "\n",
      "[ 5  5  0  0 69  0  0  3 19]\n",
      "svc Accuracy:  0.9207920792079208\n",
      "svc F1:  0.8462338048038015\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.50      0.67        10\n",
      "          1       0.90      1.00      0.95        69\n",
      "          2       1.00      0.86      0.93        22\n",
      "\n",
      "avg / total       0.93      0.92      0.91       101\n",
      "\n",
      "[ 5  5  0  0 69  0  0  3 19]\n",
      "LR Accuracy:  0.9207920792079208\n",
      "LR F1:  0.8462338048038015\n",
      "For name:  a_vega\n",
      "total sample size before apply threshold:  20\n",
      "Counter({'0000-0002-8207-9925': 10, '0000-0002-2178-2780': 8, '0000-0002-8148-5702': 1, '0000-0003-1082-0961': 1})\n",
      "['0000-0002-8207-9925']\n",
      "Total sample size after apply threshold:  10\n",
      "For name:  k_smith\n",
      "total sample size before apply threshold:  338\n",
      "Counter({'0000-0002-6736-4779': 133, '0000-0001-8088-566X': 75, '0000-0002-1323-627X': 29, '0000-0002-8914-6457': 23, '0000-0001-6828-7480': 19, '0000-0001-8150-5702': 15, '0000-0003-2793-3460': 14, '0000-0002-4530-6914': 13, '0000-0003-2802-4939': 8, '0000-0002-0932-1412': 4, '0000-0002-2424-6254': 1, '0000-0001-6957-5361': 1, '0000-0002-7807-2472': 1, '0000-0002-0346-2820': 1, '0000-0003-2060-9369': 1})\n",
      "['0000-0001-8150-5702', '0000-0002-1323-627X', '0000-0002-4530-6914', '0000-0002-6736-4779', '0000-0003-2793-3460', '0000-0002-8914-6457', '0000-0001-8088-566X', '0000-0001-6828-7480']\n",
      "Total sample size after apply threshold:  321\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(321, 719)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(321, 719)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        15\n",
      "          1       1.00      0.93      0.96        29\n",
      "          2       1.00      0.54      0.70        13\n",
      "          3       0.83      0.98      0.90       133\n",
      "          4       1.00      0.86      0.92        14\n",
      "          5       1.00      0.96      0.98        23\n",
      "          6       0.95      0.79      0.86        75\n",
      "          7       1.00      0.95      0.97        19\n",
      "\n",
      "avg / total       0.92      0.91      0.90       321\n",
      "\n",
      "[ 15   0   0   0   0   0   0   0   0  27   0   2   0   0   0   0   0   0\n",
      "   7   5   0   0   1   0   0   0   0 131   0   0   2   0   0   0   0   2\n",
      "  12   0   0   0   0   0   0   1   0  22   0   0   0   0   0  16   0   0\n",
      "  59   0   0   0   0   1   0   0   0  18]\n",
      "svc Accuracy:  0.9065420560747663\n",
      "svc F1:  0.9124713624172763\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        15\n",
      "          1       1.00      0.93      0.96        29\n",
      "          2       1.00      0.38      0.56        13\n",
      "          3       0.80      0.98      0.88       133\n",
      "          4       1.00      0.86      0.92        14\n",
      "          5       1.00      0.91      0.95        23\n",
      "          6       0.95      0.77      0.85        75\n",
      "          7       1.00      0.95      0.97        19\n",
      "\n",
      "avg / total       0.91      0.89      0.89       321\n",
      "\n",
      "[ 15   0   0   0   0   0   0   0   0  27   0   2   0   0   0   0   0   0\n",
      "   5   8   0   0   0   0   0   0   0 130   0   0   3   0   0   0   0   2\n",
      "  12   0   0   0   0   0   0   2   0  21   0   0   0   0   0  17   0   0\n",
      "  58   0   0   0   0   1   0   0   0  18]\n",
      "LR Accuracy:  0.8909657320872274\n",
      "LR F1:  0.8880917161388249\n",
      "For name:  j_gordon\n",
      "total sample size before apply threshold:  19\n",
      "Counter({'0000-0002-0061-2168': 12, '0000-0001-9494-0586': 4, '0000-0001-7811-9245': 2, '0000-0002-5911-4219': 1})\n",
      "['0000-0002-0061-2168']\n",
      "Total sample size after apply threshold:  12\n",
      "For name:  s_liao\n",
      "total sample size before apply threshold:  104\n",
      "Counter({'0000-0003-4129-0879': 46, '0000-0002-4312-5351': 43, '0000-0003-0943-0667': 10, '0000-0002-3122-8249': 2, '0000-0002-2372-9502': 1, '0000-0002-8872-2117': 1, '0000-0002-7339-2768': 1})\n",
      "['0000-0003-0943-0667', '0000-0003-4129-0879', '0000-0002-4312-5351']\n",
      "Total sample size after apply threshold:  99\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(99, 88)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(99, 88)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.30      0.43        10\n",
      "          1       0.75      0.78      0.77        46\n",
      "          2       0.81      0.88      0.84        43\n",
      "\n",
      "avg / total       0.78      0.78      0.77        99\n",
      "\n",
      "[ 3  7  0  1 36  9  0  5 38]\n",
      "svc Accuracy:  0.7777777777777778\n",
      "svc F1:  0.6796577732747946\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.10      0.18        10\n",
      "          1       0.73      0.80      0.76        46\n",
      "          2       0.81      0.88      0.84        43\n",
      "\n",
      "avg / total       0.79      0.77      0.74        99\n",
      "\n",
      "[ 1  9  0  0 37  9  0  5 38]\n",
      "LR Accuracy:  0.7676767676767676\n",
      "LR F1:  0.5963830747335902\n",
      "For name:  j_qian\n",
      "total sample size before apply threshold:  17\n",
      "Counter({'0000-0002-8793-9330': 6, '0000-0001-6145-045X': 6, '0000-0003-3162-2913': 1, '0000-0002-9522-6445': 1, '0000-0002-1325-6975': 1, '0000-0002-5438-0833': 1, '0000-0001-5043-020X': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  s_bernardi\n",
      "total sample size before apply threshold:  91\n",
      "Counter({'0000-0001-5672-0881': 38, '0000-0002-7429-3075': 30, '0000-0002-1050-3096': 17, '0000-0001-6130-8533': 6})\n",
      "['0000-0002-7429-3075', '0000-0001-5672-0881', '0000-0002-1050-3096']\n",
      "Total sample size after apply threshold:  85\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(85, 403)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(85, 403)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.97      0.98        30\n",
      "          1       1.00      0.95      0.97        38\n",
      "          2       0.85      1.00      0.92        17\n",
      "\n",
      "avg / total       0.97      0.96      0.97        85\n",
      "\n",
      "[29  0  1  0 36  2  0  0 17]\n",
      "svc Accuracy:  0.9647058823529412\n",
      "svc F1:  0.9583142464498396\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.98        30\n",
      "          1       0.95      1.00      0.97        38\n",
      "          2       1.00      0.82      0.90        17\n",
      "\n",
      "avg / total       0.97      0.96      0.96        85\n",
      "\n",
      "[30  0  0  0 38  0  1  2 14]\n",
      "LR Accuracy:  0.9647058823529412\n",
      "LR F1:  0.9537304460625454\n",
      "For name:  t_hill\n",
      "total sample size before apply threshold:  15\n",
      "Counter({'0000-0003-4159-9104': 7, '0000-0001-6996-9475': 3, '0000-0002-4125-7895': 2, '0000-0003-2980-4099': 2, '0000-0002-7995-9315': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  s_schindler\n",
      "total sample size before apply threshold:  51\n",
      "Counter({'0000-0002-9991-9513': 26, '0000-0002-7054-5431': 13, '0000-0003-1028-3115': 6, '0000-0003-1378-0053': 5, '0000-0002-1755-4304': 1})\n",
      "['0000-0002-7054-5431', '0000-0002-9991-9513']\n",
      "Total sample size after apply threshold:  39\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(39, 129)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(39, 129)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.92      0.89        13\n",
      "          1       0.96      0.92      0.94        26\n",
      "\n",
      "avg / total       0.93      0.92      0.92        39\n",
      "\n",
      "[12  1  2 24]\n",
      "svc Accuracy:  0.9230769230769231\n",
      "svc F1:  0.9150326797385622\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.92      0.96        13\n",
      "          1       0.96      1.00      0.98        26\n",
      "\n",
      "avg / total       0.98      0.97      0.97        39\n",
      "\n",
      "[12  1  0 26]\n",
      "LR Accuracy:  0.9743589743589743\n",
      "LR F1:  0.9705660377358492\n",
      "For name:  j_williams\n",
      "total sample size before apply threshold:  625\n",
      "Counter({'0000-0001-5188-7957': 141, '0000-0002-6063-7615': 82, '0000-0001-6665-6596': 79, '0000-0002-4688-3000': 66, '0000-0001-7152-765X': 51, '0000-0001-8251-4176': 28, '0000-0003-1235-5186': 26, '0000-0002-8883-7838': 25, '0000-0001-8331-3181': 20, '0000-0001-8377-5175': 15, '0000-0002-8861-0596': 14, '0000-0002-3804-2594': 14, '0000-0003-3815-0891': 14, '0000-0002-4497-4961': 10, '0000-0002-9801-9580': 9, '0000-0003-4400-5180': 5, '0000-0002-3500-914X': 5, '0000-0002-0195-6771': 4, '0000-0001-6105-0296': 3, '0000-0002-4681-3360': 3, '0000-0003-0161-0532': 3, '0000-0002-6511-1284': 3, '0000-0002-0195-5509': 2, '0000-0003-0500-1961': 2, '0000-0002-5355-3210': 1})\n",
      "['0000-0003-1235-5186', '0000-0002-8861-0596', '0000-0001-7152-765X', '0000-0001-6665-6596', '0000-0002-4497-4961', '0000-0002-6063-7615', '0000-0001-5188-7957', '0000-0002-3804-2594', '0000-0002-8883-7838', '0000-0001-8251-4176', '0000-0003-3815-0891', '0000-0001-8331-3181', '0000-0002-4688-3000', '0000-0001-8377-5175']\n",
      "Total sample size after apply threshold:  585\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(585, 1669)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(585, 1669)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.81      0.82        26\n",
      "          1       1.00      0.86      0.92        14\n",
      "          2       0.88      0.75      0.81        51\n",
      "          3       1.00      0.82      0.90        79\n",
      "          4       1.00      0.40      0.57        10\n",
      "          5       0.46      0.99      0.63        82\n",
      "          6       0.98      0.88      0.93       141\n",
      "          7       1.00      0.57      0.73        14\n",
      "          8       1.00      0.80      0.89        25\n",
      "          9       1.00      0.64      0.78        28\n",
      "         10       1.00      0.71      0.83        14\n",
      "         11       0.89      0.80      0.84        20\n",
      "         12       0.98      0.79      0.87        66\n",
      "         13       1.00      0.47      0.64        15\n",
      "\n",
      "avg / total       0.90      0.81      0.83       585\n",
      "\n",
      "[ 21   0   0   0   0   4   0   0   0   0   0   0   1   0   0  12   0   0\n",
      "   0   2   0   0   0   0   0   0   0   0   0   0  38   0   0  12   1   0\n",
      "   0   0   0   0   0   0   2   0   1  65   0   9   2   0   0   0   0   0\n",
      "   0   0   0   0   0   0   4   6   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0  81   0   0   0   0   0   1   0   0   1   0   1   0   0  15\n",
      " 124   0   0   0   0   0   0   0   1   0   1   0   0   3   0   8   0   0\n",
      "   0   1   0   0   0   0   1   0   0   4   0   0  20   0   0   0   0   0\n",
      "   0   0   1   0   0   9   0   0   0  18   0   0   0   0   0   0   0   0\n",
      "   0   4   0   0   0   0  10   0   0   0   0   0   0   0   0   4   0   0\n",
      "   0   0   0  16   0   0   0   0   0   0   0  14   0   0   0   0   0   0\n",
      "  52   0   0   0   0   0   0   8   0   0   0   0   0   0   0   7]\n",
      "svc Accuracy:  0.8136752136752137\n",
      "svc F1:  0.7978263411233806\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.85      0.90        26\n",
      "          1       1.00      0.86      0.92        14\n",
      "          2       0.89      0.82      0.86        51\n",
      "          3       1.00      0.86      0.93        79\n",
      "          4       1.00      0.40      0.57        10\n",
      "          5       0.57      0.99      0.73        82\n",
      "          6       0.92      0.97      0.94       141\n",
      "          7       1.00      0.64      0.78        14\n",
      "          8       1.00      0.88      0.94        25\n",
      "          9       1.00      0.50      0.67        28\n",
      "         10       1.00      0.64      0.78        14\n",
      "         11       0.84      0.80      0.82        20\n",
      "         12       0.98      0.89      0.94        66\n",
      "         13       1.00      0.53      0.70        15\n",
      "\n",
      "avg / total       0.90      0.86      0.86       585\n",
      "\n",
      "[ 22   0   0   0   0   3   0   0   0   0   0   0   1   0   0  12   0   0\n",
      "   0   2   0   0   0   0   0   0   0   0   0   0  42   0   0   7   2   0\n",
      "   0   0   0   0   0   0   0   0   1  68   0   8   2   0   0   0   0   0\n",
      "   0   0   0   0   0   0   4   5   1   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0  81   1   0   0   0   0   0   0   0   0   0   1   0   0   3\n",
      " 137   0   0   0   0   0   0   0   1   0   1   0   0   2   0   9   0   0\n",
      "   0   1   0   0   0   0   1   0   0   0   1   0  22   0   0   1   0   0\n",
      "   0   0   1   0   0  13   0   0   0  14   0   0   0   0   0   0   0   0\n",
      "   0   3   2   0   0   0   9   0   0   0   0   0   0   0   0   4   0   0\n",
      "   0   0   0  16   0   0   0   0   0   0   0   5   1   0   0   0   0   1\n",
      "  59   0   0   0   0   0   0   5   2   0   0   0   0   0   0   8]\n",
      "LR Accuracy:  0.8598290598290599\n",
      "LR F1:  0.8190564135949885\n",
      "For name:  s_jacobson\n",
      "total sample size before apply threshold:  28\n",
      "Counter({'0000-0002-9042-8750': 20, '0000-0002-3955-5746': 4, '0000-0002-4952-9007': 3, '0000-0001-9937-419X': 1})\n",
      "['0000-0002-9042-8750']\n",
      "Total sample size after apply threshold:  20\n",
      "For name:  e_andrade\n",
      "total sample size before apply threshold:  17\n",
      "Counter({'0000-0002-1941-580X': 7, '0000-0001-7080-7035': 5, '0000-0003-2016-8305': 4, '0000-0002-5030-1675': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  t_santos\n",
      "total sample size before apply threshold:  45\n",
      "Counter({'0000-0002-5365-4863': 18, '0000-0003-3765-5863': 14, '0000-0001-9072-5010': 3, '0000-0001-9947-6022': 2, '0000-0002-7694-306X': 2, '0000-0003-4171-5806': 1, '0000-0002-9744-0410': 1, '0000-0002-5325-3090': 1, '0000-0003-4620-0174': 1, '0000-0002-5738-4995': 1, '0000-0001-6892-0354': 1})\n",
      "['0000-0003-3765-5863', '0000-0002-5365-4863']\n",
      "Total sample size after apply threshold:  32\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(32, 83)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(32, 83)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        14\n",
      "          1       1.00      1.00      1.00        18\n",
      "\n",
      "avg / total       1.00      1.00      1.00        32\n",
      "\n",
      "[14  0  0 18]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.93      0.96        14\n",
      "          1       0.95      1.00      0.97        18\n",
      "\n",
      "avg / total       0.97      0.97      0.97        32\n",
      "\n",
      "[13  1  0 18]\n",
      "LR Accuracy:  0.96875\n",
      "LR F1:  0.967967967967968\n",
      "For name:  k_kim\n",
      "total sample size before apply threshold:  1111\n",
      "Counter({'0000-0002-6929-5359': 211, '0000-0001-9498-284X': 154, '0000-0002-5878-8895': 139, '0000-0002-1864-3392': 92, '0000-0002-7045-8004': 57, '0000-0001-7896-6751': 57, '0000-0002-7991-9428': 55, '0000-0002-4010-1063': 45, '0000-0002-2186-3484': 28, '0000-0002-4899-1929': 25, '0000-0003-0487-4242': 24, '0000-0002-3642-1486': 22, '0000-0001-9965-3535': 17, '0000-0002-4168-757X': 17, '0000-0001-6525-3744': 14, '0000-0002-3897-0278': 14, '0000-0002-1181-5112': 12, '0000-0003-1447-9385': 11, '0000-0002-7305-8786': 11, '0000-0002-2655-7806': 10, '0000-0003-3466-5353': 9, '0000-0002-7359-663X': 8, '0000-0003-4600-8668': 6, '0000-0002-1382-7088': 5, '0000-0002-9505-4882': 5, '0000-0003-3667-9900': 4, '0000-0001-9714-6038': 4, '0000-0002-4760-0228': 3, '0000-0003-4188-7915': 3, '0000-0001-9454-0427': 3, '0000-0002-0333-6808': 3, '0000-0003-2134-4964': 3, '0000-0002-6658-047X': 3, '0000-0003-1273-379X': 3, '0000-0002-7047-3183': 3, '0000-0002-1814-9546': 3, '0000-0003-4812-6297': 2, '0000-0001-6597-578X': 2, '0000-0002-5285-9138': 2, '0000-0002-6796-7844': 2, '0000-0002-1130-8698': 2, '0000-0001-8518-8150': 2, '0000-0002-7103-924X': 2, '0000-0002-5407-0202': 1, '0000-0001-6220-8411': 1, '0000-0002-7440-6703': 1, '0000-0002-1603-7559': 1, '0000-0003-0257-1707': 1, '0000-0001-8532-6517': 1, '0000-0001-6626-316X': 1, '0000-0002-3246-9861': 1, '0000-0002-7207-4389': 1, '0000-0001-9682-9654': 1, '0000-0002-0196-3832': 1, '0000-0001-8063-6081': 1, '0000-0003-2037-3333': 1, '0000-0001-8872-6751': 1})\n",
      "['0000-0003-1447-9385', '0000-0001-6525-3744', '0000-0001-9498-284X', '0000-0002-3642-1486', '0000-0001-9965-3535', '0000-0002-7305-8786', '0000-0002-4899-1929', '0000-0002-6929-5359', '0000-0002-7045-8004', '0000-0002-2186-3484', '0000-0002-5878-8895', '0000-0002-4010-1063', '0000-0003-0487-4242', '0000-0001-7896-6751', '0000-0002-3897-0278', '0000-0002-7991-9428', '0000-0002-1864-3392', '0000-0002-4168-757X', '0000-0002-2655-7806', '0000-0002-1181-5112']\n",
      "Total sample size after apply threshold:  1015\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(1015, 674)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(1015, 674)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.64      0.82      0.72        11\n",
      "          1       0.90      0.64      0.75        14\n",
      "          2       0.72      0.80      0.75       154\n",
      "          3       0.75      0.27      0.40        22\n",
      "          4       0.88      0.41      0.56        17\n",
      "          5       0.67      0.36      0.47        11\n",
      "          6       0.76      0.76      0.76        25\n",
      "          7       0.58      0.84      0.68       211\n",
      "          8       0.63      0.47      0.54        57\n",
      "          9       0.53      0.36      0.43        28\n",
      "         10       0.85      0.79      0.82       139\n",
      "         11       0.75      0.53      0.62        45\n",
      "         12       1.00      0.33      0.50        24\n",
      "         13       0.58      0.44      0.50        57\n",
      "         14       0.71      0.71      0.71        14\n",
      "         15       0.76      0.69      0.72        55\n",
      "         16       0.62      0.66      0.64        92\n",
      "         17       1.00      0.65      0.79        17\n",
      "         18       0.78      0.70      0.74        10\n",
      "         19       0.57      0.33      0.42        12\n",
      "\n",
      "avg / total       0.70      0.68      0.67      1015\n",
      "\n",
      "[  9   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0\n",
      "   0   0   0   9   1   0   0   0   0   0   0   3   0   0   0   0   0   0\n",
      "   1   0   0   0   0   0 123   0   0   0   0  14   0   1   4   0   0   2\n",
      "   0   4   6   0   0   0   0   0   1   6   0   0   0   4   0   2   3   1\n",
      "   0   0   0   1   4   0   0   0   0   0   1   0   7   0   0   9   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   4   0   6\n",
      "   0   0   0   0   0   1   0   0   0   0   0   0   0   0   2   0   0   0\n",
      "  19   1   1   0   0   0   0   0   0   0   2   0   0   0   0   0  14   0\n",
      "   0   1   0 177   3   3   2   2   0   2   0   1   4   0   0   2   1   0\n",
      "   2   0   1   0   1  19  27   0   1   0   0   2   0   0   3   0   0   0\n",
      "   0   0   0   1   0   0   0  10   3  10   0   2   0   1   0   0   0   0\n",
      "   0   1   0   1   7   0   0   0   0   6   5   0 110   1   0   2   0   1\n",
      "   6   0   0   0   0   0   5   0   0   1   0   8   0   0   1  24   0   4\n",
      "   0   0   1   0   1   0   0   0   0   0   0   0   0  14   0   0   1   0\n",
      "   8   0   0   0   1   0   0   0   3   0   6   0   0   0   3   6   2   0\n",
      "   3   2   0  25   0   1   6   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0  10   4   0   0   0   0   0   0   4   1   0   0\n",
      "   0   3   0   0   2   0   0   1   4  38   1   0   1   0   1   0   3   0\n",
      "   0   0   2  17   2   0   3   0   0   3   0   0  61   0   0   0   0   0\n",
      "   0   0   0   0   0   5   0   0   0   0   0   0   0   0   1  11   0   0\n",
      "   0   0   1   0   0   0   0   2   0   0   0   0   0   0   0   0   0   0\n",
      "   7   0   0   0   1   0   0   0   0   6   0   0   0   0   0   0   0   0\n",
      "   1   0   0   4]\n",
      "svc Accuracy:  0.6788177339901478\n",
      "svc F1:  0.626289363313796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.82      0.78        11\n",
      "          1       0.89      0.57      0.70        14\n",
      "          2       0.66      0.79      0.72       154\n",
      "          3       0.88      0.32      0.47        22\n",
      "          4       1.00      0.47      0.64        17\n",
      "          5       1.00      0.09      0.17        11\n",
      "          6       0.83      0.76      0.79        25\n",
      "          7       0.59      0.89      0.71       211\n",
      "          8       0.53      0.40      0.46        57\n",
      "          9       0.53      0.32      0.40        28\n",
      "         10       0.80      0.85      0.82       139\n",
      "         11       0.72      0.58      0.64        45\n",
      "         12       0.83      0.42      0.56        24\n",
      "         13       0.71      0.47      0.57        57\n",
      "         14       0.50      0.29      0.36        14\n",
      "         15       0.73      0.69      0.71        55\n",
      "         16       0.65      0.55      0.60        92\n",
      "         17       1.00      0.65      0.79        17\n",
      "         18       1.00      0.70      0.82        10\n",
      "         19       1.00      0.17      0.29        12\n",
      "\n",
      "avg / total       0.70      0.68      0.66      1015\n",
      "\n",
      "[  9   0   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   8   2   0   0   0   0   3   0   1   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0 122   0   0   0   0  13   3   1   9   1   0   1\n",
      "   0   2   2   0   0   0   0   0   3   7   0   0   0   5   0   2   3   1\n",
      "   0   0   0   0   1   0   0   0   0   0   1   0   8   0   0   7   0   0\n",
      "   0   0   0   0   0   0   1   0   0   0   0   0   1   0   0   1   0   6\n",
      "   0   1   2   0   0   0   0   0   0   0   0   0   0   0   3   0   0   0\n",
      "  19   0   2   0   0   0   0   0   0   0   1   0   0   0   0   0  10   0\n",
      "   0   0   0 188   2   1   2   3   0   3   0   1   1   0   0   0   1   0\n",
      "   6   0   0   0   1  19  23   1   3   0   0   0   0   0   3   0   0   0\n",
      "   0   1   2   0   0   0   0   7   3   9   0   1   0   2   0   0   3   0\n",
      "   0   0   0   0   6   0   0   0   0   4   3   0 118   2   0   0   0   1\n",
      "   5   0   0   0   0   0   4   0   0   0   0  10   0   0   0  26   0   3\n",
      "   0   0   2   0   0   0   0   0   1   0   0   0   0  11   0   0   0   1\n",
      "  10   0   0   0   1   0   0   0   1   0   6   0   0   0   1  10   3   0\n",
      "   2   1   0  27   0   2   4   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   1   0   0   0   4   8   1   0   0   0   0   0   6   1   0   0\n",
      "   1   2   0   0   3   0   0   0   4  38   0   0   0   0   1   0   9   0\n",
      "   0   0   1  17   4   1   4   0   2   2   0   0  51   0   0   0   0   0\n",
      "   0   0   0   0   0   5   0   0   0   0   0   0   0   0   1  11   0   0\n",
      "   0   0   1   0   0   0   0   2   0   0   0   0   0   0   0   0   0   0\n",
      "   7   0   0   0   0   0   0   0   0   7   0   0   1   0   0   0   0   0\n",
      "   2   0   0   2]\n",
      "LR Accuracy:  0.677832512315271\n",
      "LR F1:  0.5995057442130888\n",
      "For name:  d_ricci\n",
      "total sample size before apply threshold:  40\n",
      "Counter({'0000-0003-0015-6374': 26, '0000-0003-2853-4816': 12, '0000-0001-9678-904X': 1, '0000-0002-9790-0552': 1})\n",
      "['0000-0003-0015-6374', '0000-0003-2853-4816']\n",
      "Total sample size after apply threshold:  38\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(38, 139)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(38, 139)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        26\n",
      "          1       1.00      1.00      1.00        12\n",
      "\n",
      "avg / total       1.00      1.00      1.00        38\n",
      "\n",
      "[26  0  0 12]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        26\n",
      "          1       1.00      1.00      1.00        12\n",
      "\n",
      "avg / total       1.00      1.00      1.00        38\n",
      "\n",
      "[26  0  0 12]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  s_cameron\n",
      "total sample size before apply threshold:  66\n",
      "Counter({'0000-0002-6694-4130': 41, '0000-0002-3050-7262': 16, '0000-0001-9570-135X': 7, '0000-0001-5680-2641': 2})\n",
      "['0000-0002-6694-4130', '0000-0002-3050-7262']\n",
      "Total sample size after apply threshold:  57\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(57, 173)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(57, 173)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        41\n",
      "          1       1.00      1.00      1.00        16\n",
      "\n",
      "avg / total       1.00      1.00      1.00        57\n",
      "\n",
      "[41  0  0 16]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        41\n",
      "          1       1.00      1.00      1.00        16\n",
      "\n",
      "avg / total       1.00      1.00      1.00        57\n",
      "\n",
      "[41  0  0 16]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  t_wright\n",
      "total sample size before apply threshold:  31\n",
      "Counter({'0000-0001-5071-9978': 19, '0000-0002-5813-9991': 6, '0000-0001-8338-5935': 5, '0000-0001-7836-6705': 1})\n",
      "['0000-0001-5071-9978']\n",
      "Total sample size after apply threshold:  19\n",
      "For name:  r_cunha\n",
      "total sample size before apply threshold:  209\n",
      "Counter({'0000-0003-2550-6422': 151, '0000-0002-0203-3143': 24, '0000-0002-0849-3247': 12, '0000-0002-6622-7043': 12, '0000-0003-2228-5492': 8, '0000-0002-2382-7479': 2})\n",
      "['0000-0002-0849-3247', '0000-0002-6622-7043', '0000-0003-2550-6422', '0000-0002-0203-3143']\n",
      "Total sample size after apply threshold:  199\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(199, 473)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(199, 473)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.42      0.56        12\n",
      "          1       1.00      0.50      0.67        12\n",
      "          2       0.87      1.00      0.93       151\n",
      "          3       1.00      0.54      0.70        24\n",
      "\n",
      "avg / total       0.89      0.88      0.86       199\n",
      "\n",
      "[  5   0   7   0   1   6   5   0   0   0 151   0   0   0  11  13]\n",
      "svc Accuracy:  0.8793969849246231\n",
      "svc F1:  0.7135389235389236\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.17      0.29        12\n",
      "          1       1.00      0.42      0.59        12\n",
      "          2       0.83      1.00      0.91       151\n",
      "          3       1.00      0.42      0.59        24\n",
      "\n",
      "avg / total       0.87      0.84      0.81       199\n",
      "\n",
      "[  2   0  10   0   0   5   7   0   0   0 151   0   0   0  14  10]\n",
      "LR Accuracy:  0.8442211055276382\n",
      "LR F1:  0.5922729452141217\n",
      "For name:  s_fuchs\n",
      "total sample size before apply threshold:  32\n",
      "Counter({'0000-0002-1338-2699': 18, '0000-0001-9191-7970': 11, '0000-0002-0644-2876': 2, '0000-0001-7261-9214': 1})\n",
      "['0000-0001-9191-7970', '0000-0002-1338-2699']\n",
      "Total sample size after apply threshold:  29\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(29, 271)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(29, 271)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.73      0.84        11\n",
      "          1       0.86      1.00      0.92        18\n",
      "\n",
      "avg / total       0.91      0.90      0.89        29\n",
      "\n",
      "[ 8  3  0 18]\n",
      "svc Accuracy:  0.896551724137931\n",
      "svc F1:  0.8825910931174089\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.91      0.95        11\n",
      "          1       0.95      1.00      0.97        18\n",
      "\n",
      "avg / total       0.97      0.97      0.97        29\n",
      "\n",
      "[10  1  0 18]\n",
      "LR Accuracy:  0.9655172413793104\n",
      "LR F1:  0.9626769626769627\n",
      "For name:  m_nawaz\n",
      "total sample size before apply threshold:  9\n",
      "Counter({'0000-0002-0792-8296': 4, '0000-0001-9016-9229': 3, '0000-0003-4249-4259': 1, '0000-0003-3387-484X': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  k_harris\n",
      "total sample size before apply threshold:  47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'0000-0001-8486-1219': 15, '0000-0002-5930-6456': 10, '0000-0003-1769-2587': 8, '0000-0002-1199-2856': 7, '0000-0003-2162-9652': 3, '0000-0003-0302-2523': 2, '0000-0001-5190-4219': 1, '0000-0003-2806-1262': 1})\n",
      "['0000-0002-5930-6456', '0000-0001-8486-1219']\n",
      "Total sample size after apply threshold:  25\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(25, 42)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(25, 42)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      1.00      0.95        10\n",
      "          1       1.00      0.93      0.97        15\n",
      "\n",
      "avg / total       0.96      0.96      0.96        25\n",
      "\n",
      "[10  0  1 14]\n",
      "svc Accuracy:  0.96\n",
      "svc F1:  0.9589490968801313\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      1.00      0.95        10\n",
      "          1       1.00      0.93      0.97        15\n",
      "\n",
      "avg / total       0.96      0.96      0.96        25\n",
      "\n",
      "[10  0  1 14]\n",
      "LR Accuracy:  0.96\n",
      "LR F1:  0.9589490968801313\n",
      "For name:  r_daniel\n",
      "total sample size before apply threshold:  173\n",
      "Counter({'0000-0002-8646-7925': 123, '0000-0002-6483-5897': 37, '0000-0001-8835-8047': 8, '0000-0002-1753-6683': 5})\n",
      "['0000-0002-8646-7925', '0000-0002-6483-5897']\n",
      "Total sample size after apply threshold:  160\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(160, 787)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(160, 787)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.98       123\n",
      "          1       1.00      0.84      0.91        37\n",
      "\n",
      "avg / total       0.96      0.96      0.96       160\n",
      "\n",
      "[123   0   6  31]\n",
      "svc Accuracy:  0.9625\n",
      "svc F1:  0.9439775910364147\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      1.00      0.96       123\n",
      "          1       1.00      0.76      0.86        37\n",
      "\n",
      "avg / total       0.95      0.94      0.94       160\n",
      "\n",
      "[123   0   9  28]\n",
      "LR Accuracy:  0.94375\n",
      "LR F1:  0.9131221719457014\n",
      "For name:  k_xu\n",
      "total sample size before apply threshold:  37\n",
      "Counter({'0000-0002-2788-194X': 19, '0000-0003-2036-3469': 14, '0000-0002-3985-739X': 3, '0000-0001-7851-2629': 1})\n",
      "['0000-0003-2036-3469', '0000-0002-2788-194X']\n",
      "Total sample size after apply threshold:  33\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(33, 76)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(33, 76)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.86      0.92        14\n",
      "          1       0.90      1.00      0.95        19\n",
      "\n",
      "avg / total       0.95      0.94      0.94        33\n",
      "\n",
      "[12  2  0 19]\n",
      "svc Accuracy:  0.9393939393939394\n",
      "svc F1:  0.9365384615384615\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.86      0.92        14\n",
      "          1       0.90      1.00      0.95        19\n",
      "\n",
      "avg / total       0.95      0.94      0.94        33\n",
      "\n",
      "[12  2  0 19]\n",
      "LR Accuracy:  0.9393939393939394\n",
      "LR F1:  0.9365384615384615\n",
      "For name:  s_antunes\n",
      "total sample size before apply threshold:  54\n",
      "Counter({'0000-0002-6686-9919': 35, '0000-0002-5512-9093': 12, '0000-0003-3218-3924': 4, '0000-0002-2264-3774': 3})\n",
      "['0000-0002-5512-9093', '0000-0002-6686-9919']\n",
      "Total sample size after apply threshold:  47\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(47, 95)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(47, 95)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.75      0.86        12\n",
      "          1       0.92      1.00      0.96        35\n",
      "\n",
      "avg / total       0.94      0.94      0.93        47\n",
      "\n",
      "[ 9  3  0 35]\n",
      "svc Accuracy:  0.9361702127659575\n",
      "svc F1:  0.9080234833659491\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.67      0.80        12\n",
      "          1       0.90      1.00      0.95        35\n",
      "\n",
      "avg / total       0.92      0.91      0.91        47\n",
      "\n",
      "[ 8  4  0 35]\n",
      "LR Accuracy:  0.9148936170212766\n",
      "LR F1:  0.872972972972973\n",
      "For name:  k_cho\n",
      "total sample size before apply threshold:  126\n",
      "Counter({'0000-0002-7751-0469': 55, '0000-0001-6586-983X': 47, '0000-0002-5782-6028': 15, '0000-0003-2555-5048': 6, '0000-0003-3818-9403': 1, '0000-0003-1154-4065': 1, '0000-0003-2926-3958': 1})\n",
      "['0000-0001-6586-983X', '0000-0002-7751-0469', '0000-0002-5782-6028']\n",
      "Total sample size after apply threshold:  117\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(117, 148)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(117, 148)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.98      0.94        47\n",
      "          1       0.91      0.93      0.92        55\n",
      "          2       0.60      0.40      0.48        15\n",
      "\n",
      "avg / total       0.87      0.88      0.87       117\n",
      "\n",
      "[46  0  1  1 51  3  4  5  6]\n",
      "svc Accuracy:  0.8803418803418803\n",
      "svc F1:  0.7792314763743335\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      1.00      0.94        47\n",
      "          1       0.91      0.96      0.94        55\n",
      "          2       0.83      0.33      0.48        15\n",
      "\n",
      "avg / total       0.89      0.90      0.88       117\n",
      "\n",
      "[47  0  0  1 53  1  5  5  5]\n",
      "LR Accuracy:  0.8974358974358975\n",
      "LR F1:  0.7847478578452031\n",
      "For name:  j_sanderson\n",
      "total sample size before apply threshold:  31\n",
      "Counter({'0000-0001-7023-8442': 24, '0000-0003-3326-9842': 3, '0000-0002-1206-8833': 2, '0000-0003-1000-2897': 1, '0000-0002-4726-4885': 1})\n",
      "['0000-0001-7023-8442']\n",
      "Total sample size after apply threshold:  24\n",
      "For name:  s_uddin\n",
      "total sample size before apply threshold:  39\n",
      "Counter({'0000-0003-4698-2225': 13, '0000-0003-1886-6710': 8, '0000-0003-0091-6919': 8, '0000-0001-6045-6059': 8, '0000-0002-7285-4262': 2})\n",
      "['0000-0003-4698-2225']\n",
      "Total sample size after apply threshold:  13\n",
      "For name:  a_batista\n",
      "total sample size before apply threshold:  48\n",
      "Counter({'0000-0001-6652-3988': 14, '0000-0003-1904-0531': 9, '0000-0003-1593-0174': 8, '0000-0002-5672-8266': 6, '0000-0001-7366-1254': 5, '0000-0002-7788-1753': 3, '0000-0002-9617-8094': 2, '0000-0002-2287-4265': 1})\n",
      "['0000-0001-6652-3988']\n",
      "Total sample size after apply threshold:  14\n",
      "For name:  h_pereira\n",
      "total sample size before apply threshold:  70\n",
      "Counter({'0000-0003-1043-1675': 16, '0000-0002-5393-4443': 16, '0000-0002-1369-2099': 10, '0000-0003-4373-7005': 9, '0000-0002-7933-6097': 6, '0000-0002-0104-8714': 6, '0000-0002-1423-3038': 4, '0000-0001-9448-682X': 2, '0000-0002-3561-4980': 1})\n",
      "['0000-0003-1043-1675', '0000-0002-5393-4443', '0000-0002-1369-2099']\n",
      "Total sample size after apply threshold:  42\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(42, 220)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(42, 220)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.44      0.25      0.32        16\n",
      "          1       0.56      0.88      0.68        16\n",
      "          2       0.88      0.70      0.78        10\n",
      "\n",
      "avg / total       0.59      0.60      0.57        42\n",
      "\n",
      "[ 4 11  1  2 14  0  3  0  7]\n",
      "svc Accuracy:  0.5952380952380952\n",
      "svc F1:  0.5935682023486901\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.31      0.42        16\n",
      "          1       0.60      0.94      0.73        16\n",
      "          2       0.89      0.80      0.84        10\n",
      "\n",
      "avg / total       0.68      0.67      0.64        42\n",
      "\n",
      "[ 5 10  1  1 15  0  2  0  8]\n",
      "LR Accuracy:  0.6666666666666666\n",
      "LR F1:  0.6634930822992441\n",
      "For name:  a_patel\n",
      "total sample size before apply threshold:  262\n",
      "Counter({'0000-0003-1984-1400': 61, '0000-0001-7621-6463': 32, '0000-0002-6570-8582': 27, '0000-0003-1751-0421': 25, '0000-0002-3840-2473': 20, '0000-0002-5549-9166': 19, '0000-0001-7214-5901': 18, '0000-0003-0075-3304': 13, '0000-0003-3874-3216': 11, '0000-0002-7129-7548': 10, '0000-0002-4914-5062': 9, '0000-0002-3632-4977': 8, '0000-0001-8915-8995': 3, '0000-0003-3423-5134': 2, '0000-0002-2344-4179': 1, '0000-0001-7857-8724': 1, '0000-0003-4213-7454': 1, '0000-0002-9245-731X': 1})\n",
      "['0000-0002-6570-8582', '0000-0003-1751-0421', '0000-0002-5549-9166', '0000-0002-7129-7548', '0000-0003-3874-3216', '0000-0002-3840-2473', '0000-0001-7214-5901', '0000-0003-0075-3304', '0000-0003-1984-1400', '0000-0001-7621-6463']\n",
      "Total sample size after apply threshold:  236\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(236, 760)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(236, 760)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.74      0.83        27\n",
      "          1       1.00      0.84      0.91        25\n",
      "          2       0.89      0.89      0.89        19\n",
      "          3       1.00      0.60      0.75        10\n",
      "          4       1.00      0.73      0.84        11\n",
      "          5       1.00      0.95      0.97        20\n",
      "          6       1.00      1.00      1.00        18\n",
      "          7       0.30      0.77      0.43        13\n",
      "          8       1.00      0.90      0.95        61\n",
      "          9       0.75      0.84      0.79        32\n",
      "\n",
      "avg / total       0.91      0.85      0.87       236\n",
      "\n",
      "[20  0  1  0  0  0  0  4  0  2  0 21  1  0  0  0  0  2  0  1  1  0 17  0\n",
      "  0  0  0  1  0  0  0  0  0  6  0  0  0  3  0  1  0  0  0  0  8  0  0  2\n",
      "  0  1  0  0  0  0  0 19  0  1  0  0  0  0  0  0  0  0 18  0  0  0  0  0\n",
      "  0  0  0  0  0 10  0  3  0  0  0  0  0  0  0  5 55  1  0  0  0  0  0  0\n",
      "  0  5  0 27]\n",
      "svc Accuracy:  0.8516949152542372\n",
      "svc F1:  0.8384754009039777\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.81      0.90        27\n",
      "          1       1.00      0.88      0.94        25\n",
      "          2       1.00      1.00      1.00        19\n",
      "          3       1.00      0.90      0.95        10\n",
      "          4       1.00      1.00      1.00        11\n",
      "          5       1.00      0.95      0.97        20\n",
      "          6       1.00      1.00      1.00        18\n",
      "          7       1.00      0.77      0.87        13\n",
      "          8       0.84      1.00      0.91        61\n",
      "          9       0.88      0.91      0.89        32\n",
      "\n",
      "avg / total       0.94      0.93      0.93       236\n",
      "\n",
      "[22  0  0  0  0  0  0  0  3  2  0 22  0  0  0  0  0  0  2  1  0  0 19  0\n",
      "  0  0  0  0  0  0  0  0  0  9  0  0  0  0  1  0  0  0  0  0 11  0  0  0\n",
      "  0  0  0  0  0  0  0 19  0  0  1  0  0  0  0  0  0  0 18  0  0  0  0  0\n",
      "  0  0  0  0  0 10  2  1  0  0  0  0  0  0  0  0 61  0  0  0  0  0  0  0\n",
      "  0  0  3 29]\n",
      "LR Accuracy:  0.9322033898305084\n",
      "LR F1:  0.942817746274406\n",
      "For name:  r_graham\n",
      "total sample size before apply threshold:  52\n",
      "Counter({'0000-0002-8686-4867': 41, '0000-0002-5530-8120': 9, '0000-0003-3082-8784': 1, '0000-0003-0103-2971': 1})\n",
      "['0000-0002-8686-4867']\n",
      "Total sample size after apply threshold:  41\n",
      "For name:  a_nilsson\n",
      "total sample size before apply threshold:  42\n",
      "Counter({'0000-0001-5885-7101': 29, '0000-0002-5609-4988': 5, '0000-0002-1217-2163': 4, '0000-0002-9476-4516': 2, '0000-0001-5774-7189': 1, '0000-0003-1968-8696': 1})\n",
      "['0000-0001-5885-7101']\n",
      "Total sample size after apply threshold:  29\n",
      "For name:  m_soto\n",
      "total sample size before apply threshold:  97\n",
      "Counter({'0000-0002-4541-8182': 40, '0000-0003-2045-7238': 30, '0000-0002-4843-556X': 14, '0000-0002-2140-2012': 13})\n",
      "['0000-0002-4541-8182', '0000-0002-2140-2012', '0000-0002-4843-556X', '0000-0003-2045-7238']\n",
      "Total sample size after apply threshold:  97\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(97, 226)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(97, 226)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      1.00      0.96        40\n",
      "          1       1.00      1.00      1.00        13\n",
      "          2       0.86      0.86      0.86        14\n",
      "          3       1.00      0.90      0.95        30\n",
      "\n",
      "avg / total       0.95      0.95      0.95        97\n",
      "\n",
      "[40  0  0  0  0 13  0  0  2  0 12  0  1  0  2 27]\n",
      "svc Accuracy:  0.9484536082474226\n",
      "svc F1:  0.942091674970559\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      1.00      0.95        40\n",
      "          1       1.00      1.00      1.00        13\n",
      "          2       0.85      0.79      0.81        14\n",
      "          3       1.00      0.90      0.95        30\n",
      "\n",
      "avg / total       0.94      0.94      0.94        97\n",
      "\n",
      "[40  0  0  0  0 13  0  0  3  0 11  0  1  0  2 27]\n",
      "LR Accuracy:  0.9381443298969072\n",
      "LR F1:  0.9286410470620996\n",
      "For name:  g_guidi\n",
      "total sample size before apply threshold:  37\n",
      "Counter({'0000-0002-3061-9870': 15, '0000-0003-3199-6624': 11, '0000-0001-9535-9152': 5, '0000-0002-1393-326X': 4, '0000-0002-8857-0096': 2})\n",
      "['0000-0003-3199-6624', '0000-0002-3061-9870']\n",
      "Total sample size after apply threshold:  26\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(26, 1492)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(26, 1492)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        11\n",
      "          1       1.00      1.00      1.00        15\n",
      "\n",
      "avg / total       1.00      1.00      1.00        26\n",
      "\n",
      "[11  0  0 15]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      1.00      0.92        11\n",
      "          1       1.00      0.87      0.93        15\n",
      "\n",
      "avg / total       0.93      0.92      0.92        26\n",
      "\n",
      "[11  0  2 13]\n",
      "LR Accuracy:  0.9230769230769231\n",
      "LR F1:  0.9226190476190477\n",
      "For name:  e_andersson\n",
      "total sample size before apply threshold:  138\n",
      "Counter({'0000-0002-7864-1014': 36, '0000-0003-0088-8719': 33, '0000-0002-2854-0354': 29, '0000-0003-3095-465X': 26, '0000-0001-5856-6806': 10, '0000-0002-8608-625X': 2, '0000-0001-8453-2079': 1, '0000-0003-3398-0132': 1})\n",
      "['0000-0001-5856-6806', '0000-0003-3095-465X', '0000-0002-2854-0354', '0000-0003-0088-8719', '0000-0002-7864-1014']\n",
      "Total sample size after apply threshold:  134\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(134, 375)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(134, 375)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.20      0.31        10\n",
      "          1       0.96      0.92      0.94        26\n",
      "          2       0.79      0.93      0.86        29\n",
      "          3       1.00      0.97      0.98        33\n",
      "          4       0.90      1.00      0.95        36\n",
      "\n",
      "avg / total       0.90      0.90      0.89       134\n",
      "\n",
      "[ 2  1  6  0  1  0 24  1  0  1  0  0 27  0  2  1  0  0 32  0  0  0  0  0\n",
      " 36]\n",
      "svc Accuracy:  0.9029850746268657\n",
      "svc F1:  0.8075990882182833\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.30      0.46        10\n",
      "          1       0.96      0.92      0.94        26\n",
      "          2       1.00      1.00      1.00        29\n",
      "          3       0.97      1.00      0.99        33\n",
      "          4       0.84      1.00      0.91        36\n",
      "\n",
      "avg / total       0.94      0.93      0.92       134\n",
      "\n",
      "[ 3  1  0  1  5  0 24  0  0  2  0  0 29  0  0  0  0  0 33  0  0  0  0  0\n",
      " 36]\n",
      "LR Accuracy:  0.9328358208955224\n",
      "LR F1:  0.8598363928111319\n",
      "For name:  s_reid\n",
      "total sample size before apply threshold:  132\n",
      "Counter({'0000-0001-9916-7414': 43, '0000-0002-6103-4429': 42, '0000-0001-7779-4820': 34, '0000-0002-8068-6529': 12, '0000-0001-9415-5246': 1})\n",
      "['0000-0001-7779-4820', '0000-0001-9916-7414', '0000-0002-8068-6529', '0000-0002-6103-4429']\n",
      "Total sample size after apply threshold:  131\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(131, 396)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(131, 396)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.88      0.94        34\n",
      "          1       1.00      0.86      0.92        43\n",
      "          2       1.00      0.92      0.96        12\n",
      "          3       0.79      1.00      0.88        42\n",
      "\n",
      "avg / total       0.93      0.92      0.92       131\n",
      "\n",
      "[30  0  0  4  0 37  0  6  0  0 11  1  0  0  0 42]\n",
      "svc Accuracy:  0.916030534351145\n",
      "svc F1:  0.925808066361556\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.91      0.95        34\n",
      "          1       1.00      0.88      0.94        43\n",
      "          2       1.00      1.00      1.00        12\n",
      "          3       0.84      1.00      0.91        42\n",
      "\n",
      "avg / total       0.95      0.94      0.94       131\n",
      "\n",
      "[31  0  0  3  0 38  0  5  0  0 12  0  0  0  0 42]\n",
      "LR Accuracy:  0.9389312977099237\n",
      "LR F1:  0.9512903092613237\n",
      "For name:  a_maleki\n",
      "total sample size before apply threshold:  25\n",
      "Counter({'0000-0001-5490-3350': 15, '0000-0001-8261-8717': 5, '0000-0003-3203-7492': 4, '0000-0001-7888-1985': 1})\n",
      "['0000-0001-5490-3350']\n",
      "Total sample size after apply threshold:  15\n",
      "For name:  j_moon\n",
      "total sample size before apply threshold:  203\n",
      "Counter({'0000-0001-8071-1491': 96, '0000-0001-6327-0575': 23, '0000-0001-7776-6889': 19, '0000-0002-9182-5475': 17, '0000-0001-9760-297X': 13, '0000-0002-9274-4554': 12, '0000-0002-8625-6562': 8, '0000-0003-1282-4528': 7, '0000-0003-1569-3068': 2, '0000-0003-1428-414X': 2, '0000-0002-7049-892X': 1, '0000-0001-8246-931X': 1, '0000-0003-4742-8744': 1, '0000-0002-4630-3301': 1})\n",
      "['0000-0001-7776-6889', '0000-0002-9274-4554', '0000-0001-8071-1491', '0000-0001-9760-297X', '0000-0002-9182-5475', '0000-0001-6327-0575']\n",
      "Total sample size after apply threshold:  180\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(180, 623)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(180, 623)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.84      0.89        19\n",
      "          1       0.83      0.42      0.56        12\n",
      "          2       0.82      0.93      0.87        96\n",
      "          3       0.64      0.54      0.58        13\n",
      "          4       1.00      0.88      0.94        17\n",
      "          5       0.83      0.83      0.83        23\n",
      "\n",
      "avg / total       0.84      0.84      0.83       180\n",
      "\n",
      "[16  0  3  0  0  0  0  5  7  0  0  0  0  1 89  3  0  3  0  0  5  7  0  1\n",
      "  0  0  2  0 15  0  1  0  2  1  0 19]\n",
      "svc Accuracy:  0.8388888888888889\n",
      "svc F1:  0.7773189589845599\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.79      0.88        19\n",
      "          1       0.83      0.42      0.56        12\n",
      "          2       0.81      0.92      0.86        96\n",
      "          3       0.75      0.46      0.57        13\n",
      "          4       1.00      0.88      0.94        17\n",
      "          5       0.75      0.91      0.82        23\n",
      "\n",
      "avg / total       0.84      0.83      0.83       180\n",
      "\n",
      "[15  0  4  0  0  0  0  5  7  0  0  0  0  1 88  2  0  5  0  0  5  6  0  2\n",
      "  0  0  2  0 15  0  0  0  2  0  0 21]\n",
      "LR Accuracy:  0.8333333333333334\n",
      "LR F1:  0.7721852629940864\n",
      "For name:  t_abe\n",
      "total sample size before apply threshold:  50\n",
      "Counter({'0000-0003-3496-1953': 19, '0000-0002-4185-5254': 17, '0000-0001-5298-082X': 12, '0000-0003-1251-7448': 2})\n",
      "['0000-0002-4185-5254', '0000-0001-5298-082X', '0000-0003-3496-1953']\n",
      "Total sample size after apply threshold:  48\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(48, 117)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(48, 117)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97        17\n",
      "          1       1.00      0.92      0.96        12\n",
      "          2       1.00      1.00      1.00        19\n",
      "\n",
      "avg / total       0.98      0.98      0.98        48\n",
      "\n",
      "[17  0  0  1 11  0  0  0 19]\n",
      "svc Accuracy:  0.9791666666666666\n",
      "svc F1:  0.975983436853002\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97        17\n",
      "          1       1.00      0.92      0.96        12\n",
      "          2       1.00      1.00      1.00        19\n",
      "\n",
      "avg / total       0.98      0.98      0.98        48\n",
      "\n",
      "[17  0  0  1 11  0  0  0 19]\n",
      "LR Accuracy:  0.9791666666666666\n",
      "LR F1:  0.975983436853002\n",
      "For name:  x_fu\n",
      "total sample size before apply threshold:  16\n",
      "Counter({'0000-0001-6928-4396': 8, '0000-0001-9295-6314': 6, '0000-0002-8012-4753': 1, '0000-0002-4305-6624': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  f_ortega\n",
      "total sample size before apply threshold:  368\n",
      "Counter({'0000-0003-2001-1121': 205, '0000-0003-2111-769X': 86, '0000-0002-4730-9270': 38, '0000-0002-3172-2095': 22, '0000-0002-7431-354X': 9, '0000-0001-7850-2105': 7, '0000-0003-0231-2051': 1})\n",
      "['0000-0002-3172-2095', '0000-0003-2001-1121', '0000-0002-4730-9270', '0000-0003-2111-769X']\n",
      "Total sample size after apply threshold:  351\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(351, 723)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(351, 723)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.91      0.95        22\n",
      "          1       0.98      1.00      0.99       205\n",
      "          2       1.00      0.92      0.96        38\n",
      "          3       1.00      1.00      1.00        86\n",
      "\n",
      "avg / total       0.99      0.99      0.99       351\n",
      "\n",
      "[ 20   2   0   0   0 205   0   0   0   3  35   0   0   0   0  86]\n",
      "svc Accuracy:  0.9857549857549858\n",
      "svc F1:  0.9748092172997272\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.91      0.95        22\n",
      "          1       0.98      1.00      0.99       205\n",
      "          2       1.00      0.95      0.97        38\n",
      "          3       1.00      0.99      0.99        86\n",
      "\n",
      "avg / total       0.99      0.99      0.99       351\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 20   2   0   0   0 205   0   0   0   2  36   0   0   1   0  85]\n",
      "LR Accuracy:  0.9857549857549858\n",
      "LR F1:  0.9768644448416166\n",
      "For name:  r_morris\n",
      "total sample size before apply threshold:  409\n",
      "Counter({'0000-0001-7240-4563': 107, '0000-0001-7809-0315': 73, '0000-0001-8661-1520': 59, '0000-0002-7574-9388': 51, '0000-0003-3080-2613': 44, '0000-0002-5018-1239': 21, '0000-0001-7431-6401': 20, '0000-0001-7450-5923': 14, '0000-0001-5511-3457': 10, '0000-0003-4764-3639': 7, '0000-0001-7443-7406': 2, '0000-0002-9193-3417': 1})\n",
      "['0000-0003-3080-2613', '0000-0002-5018-1239', '0000-0001-7809-0315', '0000-0001-5511-3457', '0000-0001-7240-4563', '0000-0001-7431-6401', '0000-0002-7574-9388', '0000-0001-8661-1520', '0000-0001-7450-5923']\n",
      "Total sample size after apply threshold:  399\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(399, 1936)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(399, 1936)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.84      0.91        44\n",
      "          1       1.00      0.90      0.95        21\n",
      "          2       0.97      0.84      0.90        73\n",
      "          3       0.78      0.70      0.74        10\n",
      "          4       1.00      0.82      0.90       107\n",
      "          5       1.00      0.60      0.75        20\n",
      "          6       0.95      0.76      0.85        51\n",
      "          7       0.50      1.00      0.67        59\n",
      "          8       1.00      0.86      0.92        14\n",
      "\n",
      "avg / total       0.91      0.84      0.85       399\n",
      "\n",
      "[37  0  0  0  0  0  0  7  0  0 19  0  0  0  0  0  2  0  0  0 61  0  0  0\n",
      "  0 12  0  0  0  0  7  0  0  0  3  0  0  0  2  1 88  0  2 14  0  0  0  0\n",
      "  1  0 12  0  7  0  0  0  0  0  0  0 39 12  0  0  0  0  0  0  0  0 59  0\n",
      "  0  0  0  0  0  0  0  2 12]\n",
      "svc Accuracy:  0.8370927318295739\n",
      "svc F1:  0.8430683283300405\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.86      0.93        44\n",
      "          1       1.00      0.76      0.86        21\n",
      "          2       0.97      0.88      0.92        73\n",
      "          3       1.00      0.80      0.89        10\n",
      "          4       0.99      0.88      0.93       107\n",
      "          5       1.00      0.70      0.82        20\n",
      "          6       0.95      0.80      0.87        51\n",
      "          7       0.55      1.00      0.71        59\n",
      "          8       1.00      0.86      0.92        14\n",
      "\n",
      "avg / total       0.92      0.87      0.88       399\n",
      "\n",
      "[38  0  0  0  0  0  0  6  0  0 16  0  0  0  0  0  5  0  0  0 64  0  0  0\n",
      "  0  9  0  0  0  0  8  0  0  0  2  0  0  0  2  0 94  0  2  9  0  0  0  0\n",
      "  0  1 14  0  5  0  0  0  0  0  0  0 41 10  0  0  0  0  0  0  0  0 59  0\n",
      "  0  0  0  0  0  0  0  2 12]\n",
      "LR Accuracy:  0.8671679197994987\n",
      "LR F1:  0.8735477260637117\n",
      "For name:  w_fang\n",
      "total sample size before apply threshold:  43\n",
      "Counter({'0000-0002-8431-8256': 31, '0000-0002-9580-3716': 6, '0000-0002-2449-3749': 3, '0000-0002-9724-898X': 3})\n",
      "['0000-0002-8431-8256']\n",
      "Total sample size after apply threshold:  31\n",
      "For name:  m_amaral\n",
      "total sample size before apply threshold:  134\n",
      "Counter({'0000-0002-0828-8630': 101, '0000-0002-3209-3366': 21, '0000-0003-4966-2614': 6, '0000-0002-4301-2760': 4, '0000-0001-5607-6475': 1, '0000-0001-9686-1312': 1})\n",
      "['0000-0002-0828-8630', '0000-0002-3209-3366']\n",
      "Total sample size after apply threshold:  122\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(122, 442)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(122, 442)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.99       101\n",
      "          1       0.95      0.90      0.93        21\n",
      "\n",
      "avg / total       0.98      0.98      0.98       122\n",
      "\n",
      "[100   1   2  19]\n",
      "svc Accuracy:  0.9754098360655737\n",
      "svc F1:  0.956025471584765\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.99       101\n",
      "          1       1.00      0.86      0.92        21\n",
      "\n",
      "avg / total       0.98      0.98      0.97       122\n",
      "\n",
      "[101   0   3  18]\n",
      "LR Accuracy:  0.9754098360655737\n",
      "LR F1:  0.9542213883677297\n",
      "For name:  h_song\n",
      "total sample size before apply threshold:  210\n",
      "Counter({'0000-0001-5684-4059': 88, '0000-0001-5553-2539': 30, '0000-0002-3134-782X': 29, '0000-0003-3845-8079': 20, '0000-0002-7844-2293': 14, '0000-0001-5486-2560': 8, '0000-0002-8720-6436': 6, '0000-0002-2721-3626': 2, '0000-0002-3563-9504': 2, '0000-0003-2197-1562': 2, '0000-0002-9849-8091': 2, '0000-0002-2164-8813': 2, '0000-0001-6000-1572': 1, '0000-0001-5747-8847': 1, '0000-0002-2791-1723': 1, '0000-0002-4204-6459': 1, '0000-0003-2631-9223': 1})\n",
      "['0000-0002-3134-782X', '0000-0002-7844-2293', '0000-0001-5684-4059', '0000-0001-5553-2539', '0000-0003-3845-8079']\n",
      "Total sample size after apply threshold:  181\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(181, 1094)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(181, 1094)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.97      0.98        29\n",
      "          1       0.78      0.50      0.61        14\n",
      "          2       0.91      0.98      0.94        88\n",
      "          3       0.90      0.90      0.90        30\n",
      "          4       0.89      0.85      0.87        20\n",
      "\n",
      "avg / total       0.91      0.91      0.91       181\n",
      "\n",
      "[28  0  0  1  0  0  7  7  0  0  0  2 86  0  0  0  0  1 27  2  0  0  1  2\n",
      " 17]\n",
      "svc Accuracy:  0.9116022099447514\n",
      "svc F1:  0.8605674749404351\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.97      0.98        29\n",
      "          1       1.00      0.21      0.35        14\n",
      "          2       0.86      1.00      0.93        88\n",
      "          3       0.90      0.87      0.88        30\n",
      "          4       0.89      0.85      0.87        20\n",
      "\n",
      "avg / total       0.90      0.90      0.88       181\n",
      "\n",
      "[28  0  0  1  0  0  3 11  0  0  0  0 88  0  0  0  0  2 26  2  0  0  1  2\n",
      " 17]\n",
      "LR Accuracy:  0.8950276243093923\n",
      "LR F1:  0.8029727820586825\n",
      "For name:  h_dai\n",
      "total sample size before apply threshold:  6\n",
      "Counter({'0000-0003-1395-7904': 2, '0000-0002-1516-7255': 2, '0000-0003-3807-4585': 1, '0000-0001-6165-4196': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  y_nakajima\n",
      "total sample size before apply threshold:  12\n",
      "Counter({'0000-0001-6558-5378': 8, '0000-0001-9759-3487': 2, '0000-0002-7153-6238': 1, '0000-0002-7357-2910': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  t_warner\n",
      "total sample size before apply threshold:  68\n",
      "Counter({'0000-0003-3988-4408': 56, '0000-0003-0604-0110': 6, '0000-0003-1809-102X': 3, '0000-0001-8397-6030': 3})\n",
      "['0000-0003-3988-4408']\n",
      "Total sample size after apply threshold:  56\n",
      "For name:  s_saha\n",
      "total sample size before apply threshold:  111\n",
      "Counter({'0000-0001-9433-8894': 24, '0000-0001-6610-4820': 23, '0000-0002-0087-8652': 14, '0000-0001-8780-9117': 12, '0000-0001-6631-0464': 8, '0000-0003-3029-7995': 5, '0000-0003-1060-9402': 5, '0000-0003-1534-7130': 4, '0000-0002-5791-8635': 3, '0000-0002-8312-6711': 3, '0000-0003-1742-2974': 2, '0000-0003-2366-8620': 2, '0000-0002-7184-6906': 2, '0000-0002-7487-9885': 1, '0000-0001-6844-2516': 1, '0000-0002-6655-4001': 1, '0000-0001-9742-3306': 1})\n",
      "['0000-0002-0087-8652', '0000-0001-8780-9117', '0000-0001-9433-8894', '0000-0001-6610-4820']\n",
      "Total sample size after apply threshold:  73\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(73, 210)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(73, 210)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      1.00      0.97        14\n",
      "          1       1.00      0.67      0.80        12\n",
      "          2       0.82      0.96      0.88        24\n",
      "          3       1.00      0.96      0.98        23\n",
      "\n",
      "avg / total       0.93      0.92      0.92        73\n",
      "\n",
      "[14  0  0  0  0  8  4  0  1  0 23  0  0  0  1 22]\n",
      "svc Accuracy:  0.9178082191780822\n",
      "svc F1:  0.9069776009431182\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.79      0.85        14\n",
      "          1       0.91      0.83      0.87        12\n",
      "          2       0.82      0.96      0.88        24\n",
      "          3       1.00      0.96      0.98        23\n",
      "\n",
      "avg / total       0.91      0.90      0.90        73\n",
      "\n",
      "[11  1  2  0  0 10  2  0  1  0 23  0  0  0  1 22]\n",
      "LR Accuracy:  0.9041095890410958\n",
      "LR F1:  0.8945280564845782\n",
      "For name:  j_fernandez\n",
      "total sample size before apply threshold:  28\n",
      "Counter({'0000-0003-2222-3355': 7, '0000-0002-4190-7341': 5, '0000-0003-2969-8150': 5, '0000-0003-4756-6645': 5, '0000-0002-8533-1858': 1, '0000-0003-4427-3935': 1, '0000-0002-7315-2326': 1, '0000-0002-7629-6106': 1, '0000-0001-5894-9866': 1, '0000-0002-9528-6173': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  m_pan\n",
      "total sample size before apply threshold:  146\n",
      "Counter({'0000-0002-5188-7030': 128, '0000-0001-5535-2714': 10, '0000-0001-5697-6086': 6, '0000-0003-1485-3154': 1, '0000-0003-3350-8719': 1})\n",
      "['0000-0001-5535-2714', '0000-0002-5188-7030']\n",
      "Total sample size after apply threshold:  138\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(138, 119)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(138, 119)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.70      0.82        10\n",
      "          1       0.98      1.00      0.99       128\n",
      "\n",
      "avg / total       0.98      0.98      0.98       138\n",
      "\n",
      "[  7   3   0 128]\n",
      "svc Accuracy:  0.9782608695652174\n",
      "svc F1:  0.9059732000908471\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.50      0.67        10\n",
      "          1       0.96      1.00      0.98       128\n",
      "\n",
      "avg / total       0.97      0.96      0.96       138\n",
      "\n",
      "[  5   5   0 128]\n",
      "LR Accuracy:  0.9637681159420289\n",
      "LR F1:  0.8237547892720306\n",
      "For name:  a_simon\n",
      "total sample size before apply threshold:  117\n",
      "Counter({'0000-0002-6141-7921': 60, '0000-0002-0151-0120': 19, '0000-0002-6509-4541': 14, '0000-0001-6023-6427': 14, '0000-0002-1879-5628': 5, '0000-0002-3286-5776': 4, '0000-0003-4641-6186': 1})\n",
      "['0000-0002-6141-7921', '0000-0002-6509-4541', '0000-0002-0151-0120', '0000-0001-6023-6427']\n",
      "Total sample size after apply threshold:  107\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(107, 437)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(107, 437)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      1.00      0.93        60\n",
      "          1       1.00      0.64      0.78        14\n",
      "          2       1.00      0.89      0.94        19\n",
      "          3       1.00      0.86      0.92        14\n",
      "\n",
      "avg / total       0.93      0.92      0.91       107\n",
      "\n",
      "[60  0  0  0  5  9  0  0  2  0 17  0  2  0  0 12]\n",
      "svc Accuracy:  0.9158878504672897\n",
      "svc F1:  0.895090655328269\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      1.00      0.92        60\n",
      "          1       1.00      0.57      0.73        14\n",
      "          2       1.00      0.89      0.94        19\n",
      "          3       1.00      0.86      0.92        14\n",
      "\n",
      "avg / total       0.92      0.91      0.90       107\n",
      "\n",
      "[60  0  0  0  6  8  0  0  2  0 17  0  2  0  0 12]\n",
      "LR Accuracy:  0.9065420560747663\n",
      "LR F1:  0.8794677544677544\n",
      "For name:  r_freitas\n",
      "total sample size before apply threshold:  73\n",
      "Counter({'0000-0003-4900-3897': 48, '0000-0001-8605-2925': 6, '0000-0002-0123-7232': 6, '0000-0002-4448-6458': 5, '0000-0001-5811-5255': 5, '0000-0002-1645-4125': 2, '0000-0001-8836-1422': 1})\n",
      "['0000-0003-4900-3897']\n",
      "Total sample size after apply threshold:  48\n",
      "For name:  c_yun\n",
      "total sample size before apply threshold:  284\n",
      "Counter({'0000-0002-9466-4531': 149, '0000-0002-0041-2887': 98, '0000-0003-2204-8067': 36, '0000-0002-6747-4628': 1})\n",
      "['0000-0002-9466-4531', '0000-0002-0041-2887', '0000-0003-2204-8067']\n",
      "Total sample size after apply threshold:  283\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(283, 1324)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(283, 1324)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.94      0.89       149\n",
      "          1       0.92      0.87      0.89        98\n",
      "          2       0.68      0.47      0.56        36\n",
      "\n",
      "avg / total       0.85      0.86      0.85       283\n",
      "\n",
      "[140   3   6  11  85   2  15   4  17]\n",
      "svc Accuracy:  0.8551236749116607\n",
      "svc F1:  0.7803342600581601\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.93      0.87       149\n",
      "          1       0.88      0.89      0.88        98\n",
      "          2       0.62      0.28      0.38        36\n",
      "\n",
      "avg / total       0.82      0.83      0.81       283\n",
      "\n",
      "[138   6   5  10  87   1  20   6  10]\n",
      "LR Accuracy:  0.8303886925795053\n",
      "LR F1:  0.712842192049225\n",
      "For name:  j_huang\n",
      "total sample size before apply threshold:  443\n",
      "Counter({'0000-0001-8011-2317': 69, '0000-0001-9207-8953': 68, '0000-0001-5495-3577': 68, '0000-0002-2742-5557': 32, '0000-0001-8993-2506': 25, '0000-0002-7027-3042': 22, '0000-0003-3282-8892': 21, '0000-0003-0996-9451': 18, '0000-0002-7163-5156': 17, '0000-0002-4452-4557': 15, '0000-0001-7281-663X': 14, '0000-0002-4569-0629': 12, '0000-0002-5761-2177': 12, '0000-0002-9570-4101': 10, '0000-0001-9069-5739': 8, '0000-0001-9639-2907': 6, '0000-0002-0901-9635': 5, '0000-0003-4435-7274': 4, '0000-0002-4051-4482': 4, '0000-0002-5153-506X': 3, '0000-0001-7288-9724': 2, '0000-0003-1776-9863': 1, '0000-0003-2314-7104': 1, '0000-0002-7531-4691': 1, '0000-0001-8111-0583': 1, '0000-0001-6589-4963': 1, '0000-0002-6135-1256': 1, '0000-0002-5736-6148': 1, '0000-0002-4189-3779': 1})\n",
      "['0000-0003-3282-8892', '0000-0001-9207-8953', '0000-0001-8011-2317', '0000-0002-7163-5156', '0000-0001-7281-663X', '0000-0001-8993-2506', '0000-0002-9570-4101', '0000-0002-2742-5557', '0000-0002-4569-0629', '0000-0002-4452-4557', '0000-0002-7027-3042', '0000-0003-0996-9451', '0000-0002-5761-2177', '0000-0001-5495-3577']\n",
      "Total sample size after apply threshold:  403\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(403, 752)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(403, 752)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.71      0.77        21\n",
      "          1       0.93      1.00      0.96        68\n",
      "          2       0.85      0.93      0.89        69\n",
      "          3       0.69      0.53      0.60        17\n",
      "          4       0.88      0.50      0.64        14\n",
      "          5       1.00      0.92      0.96        25\n",
      "          6       1.00      0.90      0.95        10\n",
      "          7       0.96      0.69      0.80        32\n",
      "          8       0.89      0.67      0.76        12\n",
      "          9       0.47      0.53      0.50        15\n",
      "         10       0.67      0.73      0.70        22\n",
      "         11       1.00      0.89      0.94        18\n",
      "         12       1.00      0.67      0.80        12\n",
      "         13       0.74      0.94      0.83        68\n",
      "\n",
      "avg / total       0.85      0.84      0.83       403\n",
      "\n",
      "[15  0  0  2  0  0  0  0  0  0  1  0  0  3  0 68  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0 64  0  0  0  0  0  0  3  1  0  0  1  2  0  0  9  0  0\n",
      "  0  0  1  2  0  0  0  3  0  2  0  1  7  0  0  0  0  1  0  0  0  3  1  1\n",
      "  0  0  0 23  0  0  0  0  0  0  0  0  0  0  0  0  0  0  9  1  0  0  0  0\n",
      "  0  0  0  1  2  0  0  0  0 22  0  0  0  0  0  7  0  0  1  0  0  0  0  0\n",
      "  8  0  0  0  0  3  0  0  4  0  0  0  0  0  0  8  2  0  0  1  0  0  4  0\n",
      "  0  0  0  0  0  2 16  0  0  0  0  0  0  0  1  0  0  0  0  0  0 16  0  1\n",
      "  0  0  0  0  0  0  0  0  0  1  2  0  8  1  0  1  0  1  0  0  0  0  0  0\n",
      "  2  0  0 64]\n",
      "svc Accuracy:  0.8362282878411911\n",
      "svc F1:  0.7920902795700288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.71      0.77        21\n",
      "          1       0.92      1.00      0.96        68\n",
      "          2       0.81      0.96      0.88        69\n",
      "          3       0.80      0.47      0.59        17\n",
      "          4       0.78      0.50      0.61        14\n",
      "          5       0.96      0.96      0.96        25\n",
      "          6       1.00      0.90      0.95        10\n",
      "          7       0.96      0.81      0.88        32\n",
      "          8       0.88      0.58      0.70        12\n",
      "          9       0.67      0.53      0.59        15\n",
      "         10       0.65      0.68      0.67        22\n",
      "         11       1.00      0.89      0.94        18\n",
      "         12       1.00      0.75      0.86        12\n",
      "         13       0.80      0.97      0.88        68\n",
      "\n",
      "avg / total       0.86      0.85      0.85       403\n",
      "\n",
      "[15  1  0  1  0  1  0  0  0  0  1  0  0  2  0 68  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0 66  0  0  0  0  0  0  1  2  0  0  0  2  1  1  8  1  0\n",
      "  0  0  0  0  0  0  0  4  0  2  0  1  7  0  0  0  0  1  0  0  0  3  1  0\n",
      "  0  0  0 24  0  0  0  0  0  0  0  0  0  0  0  0  0  0  9  1  0  0  0  0\n",
      "  0  0  0  1  2  0  0  0  0 26  0  0  0  0  0  3  0  0  2  0  0  0  0  0\n",
      "  7  0  0  0  0  3  0  1  4  0  0  0  0  0  0  8  2  0  0  0  0  0  6  0\n",
      "  0  0  0  0  0  1 15  0  0  0  0  0  0  0  1  0  0  0  0  0  0 16  0  1\n",
      "  0  0  0  0  0  0  0  0  0  1  2  0  9  0  0  0  0  0  0  0  0  0  1  0\n",
      "  1  0  0 66]\n",
      "LR Accuracy:  0.8535980148883374\n",
      "LR F1:  0.8024691737940636\n",
      "For name:  p_santos\n",
      "total sample size before apply threshold:  92\n",
      "Counter({'0000-0003-3234-4265': 24, '0000-0002-2225-455X': 17, '0000-0003-3548-700X': 16, '0000-0001-9669-9837': 9, '0000-0002-8723-4373': 8, '0000-0002-2362-5527': 4, '0000-0002-3363-0098': 3, '0000-0003-3045-4591': 3, '0000-0001-7907-5133': 2, '0000-0002-4188-7766': 1, '0000-0002-0257-592X': 1, '0000-0003-2505-8420': 1, '0000-0002-2537-5904': 1, '0000-0003-1179-3096': 1, '0000-0001-9785-0180': 1})\n",
      "['0000-0003-3548-700X', '0000-0003-3234-4265', '0000-0002-2225-455X']\n",
      "Total sample size after apply threshold:  57\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(57, 158)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(57, 158)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.94      0.97        16\n",
      "          1       1.00      1.00      1.00        24\n",
      "          2       0.94      1.00      0.97        17\n",
      "\n",
      "avg / total       0.98      0.98      0.98        57\n",
      "\n",
      "[15  0  1  0 24  0  0  0 17]\n",
      "svc Accuracy:  0.9824561403508771\n",
      "svc F1:  0.9797235023041475\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.94      0.91        16\n",
      "          1       0.95      0.88      0.91        24\n",
      "          2       0.89      0.94      0.91        17\n",
      "\n",
      "avg / total       0.91      0.91      0.91        57\n",
      "\n",
      "[15  0  1  2 21  1  0  1 16]\n",
      "LR Accuracy:  0.9122807017543859\n",
      "LR F1:  0.9121400338791643\n",
      "For name:  n_young\n",
      "total sample size before apply threshold:  182\n",
      "Counter({'0000-0002-1739-3299': 72, '0000-0001-8756-229X': 66, '0000-0002-3323-2815': 30, '0000-0002-3263-4847': 8, '0000-0002-9323-9437': 6})\n",
      "['0000-0001-8756-229X', '0000-0002-3323-2815', '0000-0002-1739-3299']\n",
      "Total sample size after apply threshold:  168\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(168, 615)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(168, 615)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.95      0.98        66\n",
      "          1       1.00      0.80      0.89        30\n",
      "          2       0.89      1.00      0.94        72\n",
      "\n",
      "avg / total       0.95      0.95      0.95       168\n",
      "\n",
      "[63  0  3  0 24  6  0  0 72]\n",
      "svc Accuracy:  0.9464285714285714\n",
      "svc F1:  0.9356031818412119\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.95      0.98        66\n",
      "          1       1.00      0.77      0.87        30\n",
      "          2       0.88      1.00      0.94        72\n",
      "\n",
      "avg / total       0.95      0.94      0.94       168\n",
      "\n",
      "[63  0  3  0 23  7  0  0 72]\n",
      "LR Accuracy:  0.9404761904761905\n",
      "LR F1:  0.926577883137778\n",
      "For name:  d_ross\n",
      "total sample size before apply threshold:  25\n",
      "Counter({'0000-0002-8272-1877': 8, '0000-0002-8659-3833': 7, '0000-0001-7426-9561': 7, '0000-0002-5480-9978': 2, '0000-0001-6353-6951': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  q_wang\n",
      "total sample size before apply threshold:  348\n",
      "Counter({'0000-0002-2149-384X': 85, '0000-0001-7929-7692': 54, '0000-0001-9409-0251': 31, '0000-0002-7982-7275': 22, '0000-0001-5988-1293': 18, '0000-0002-5125-3724': 16, '0000-0002-6514-3470': 15, '0000-0002-1355-1616': 12, '0000-0001-7309-9580': 12, '0000-0002-2359-3262': 11, '0000-0002-0645-6514': 8, '0000-0002-9808-5035': 7, '0000-0002-4036-1818': 7, '0000-0001-7692-6721': 7, '0000-0001-8566-1120': 6, '0000-0002-6010-2178': 6, '0000-0002-9706-2421': 5, '0000-0003-2645-5807': 5, '0000-0002-6411-984X': 4, '0000-0003-3525-3422': 4, '0000-0002-8460-6821': 3, '0000-0003-3514-455X': 3, '0000-0002-0757-5208': 2, '0000-0001-7952-7101': 1, '0000-0001-5483-0243': 1, '0000-0002-6858-2778': 1, '0000-0003-3484-4810': 1, '0000-0003-3715-9106': 1})\n",
      "['0000-0002-2149-384X', '0000-0002-7982-7275', '0000-0002-2359-3262', '0000-0002-1355-1616', '0000-0001-5988-1293', '0000-0001-7309-9580', '0000-0001-9409-0251', '0000-0002-5125-3724', '0000-0001-7929-7692', '0000-0002-6514-3470']\n",
      "Total sample size after apply threshold:  276\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(276, 541)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(276, 541)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.88      0.88        85\n",
      "          1       1.00      1.00      1.00        22\n",
      "          2       1.00      0.64      0.78        11\n",
      "          3       0.92      1.00      0.96        12\n",
      "          4       0.73      0.89      0.80        18\n",
      "          5       0.70      0.58      0.64        12\n",
      "          6       1.00      0.97      0.98        31\n",
      "          7       0.93      0.81      0.87        16\n",
      "          8       0.81      0.89      0.85        54\n",
      "          9       1.00      0.87      0.93        15\n",
      "\n",
      "avg / total       0.89      0.88      0.88       276\n",
      "\n",
      "[75  0  0  1  4  0  0  0  5  0  0 22  0  0  0  0  0  0  0  0  3  0  7  0\n",
      "  0  1  0  0  0  0  0  0  0 12  0  0  0  0  0  0  1  0  0  0 16  0  0  0\n",
      "  1  0  0  0  0  0  1  7  0  0  4  0  0  0  0  0  0  0 30  0  1  0  2  0\n",
      "  0  0  0  1  0 13  0  0  3  0  0  0  1  1  0  1 48  0  2  0  0  0  0  0\n",
      "  0  0  0 13]\n",
      "svc Accuracy:  0.8804347826086957\n",
      "svc F1:  0.8679736571336593\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.91      0.89        85\n",
      "          1       1.00      1.00      1.00        22\n",
      "          2       1.00      0.73      0.84        11\n",
      "          3       0.92      0.92      0.92        12\n",
      "          4       0.94      0.83      0.88        18\n",
      "          5       0.71      0.42      0.53        12\n",
      "          6       1.00      0.97      0.98        31\n",
      "          7       0.93      0.88      0.90        16\n",
      "          8       0.78      0.93      0.85        54\n",
      "          9       1.00      0.87      0.93        15\n",
      "\n",
      "avg / total       0.89      0.89      0.89       276\n",
      "\n",
      "[77  0  0  1  1  0  0  0  6  0  0 22  0  0  0  0  0  0  0  0  2  0  8  0\n",
      "  0  0  0  0  1  0  1  0  0 11  0  0  0  0  0  0  1  0  0  0 15  0  0  0\n",
      "  2  0  3  0  0  0  0  5  0  0  4  0  0  0  0  0  0  0 30  0  1  0  1  0\n",
      "  0  0  0  1  0 14  0  0  2  0  0  0  0  1  0  1 50  0  2  0  0  0  0  0\n",
      "  0  0  0 13]\n",
      "LR Accuracy:  0.8876811594202898\n",
      "LR F1:  0.871535955125782\n",
      "For name:  c_cardoso\n",
      "total sample size before apply threshold:  52\n",
      "Counter({'0000-0001-6239-6651': 15, '0000-0003-3645-5368': 12, '0000-0001-7273-0676': 10, '0000-0002-9339-8075': 8, '0000-0003-3323-4447': 4, '0000-0002-7527-3973': 2, '0000-0003-1914-9553': 1})\n",
      "['0000-0001-6239-6651', '0000-0001-7273-0676', '0000-0003-3645-5368']\n",
      "Total sample size after apply threshold:  37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(37, 101)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(37, 101)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        15\n",
      "          1       1.00      1.00      1.00        10\n",
      "          2       1.00      1.00      1.00        12\n",
      "\n",
      "avg / total       1.00      1.00      1.00        37\n",
      "\n",
      "[15  0  0  0 10  0  0  0 12]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        15\n",
      "          1       1.00      1.00      1.00        10\n",
      "          2       1.00      1.00      1.00        12\n",
      "\n",
      "avg / total       1.00      1.00      1.00        37\n",
      "\n",
      "[15  0  0  0 10  0  0  0 12]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  j_matthews\n",
      "total sample size before apply threshold:  65\n",
      "Counter({'0000-0002-9815-8636': 46, '0000-0001-6184-1813': 7, '0000-0002-5993-7610': 5, '0000-0002-1832-4420': 4, '0000-0002-7282-8929': 1, '0000-0002-6888-9438': 1, '0000-0002-3968-8282': 1})\n",
      "['0000-0002-9815-8636']\n",
      "Total sample size after apply threshold:  46\n",
      "For name:  g_lee\n",
      "total sample size before apply threshold:  202\n",
      "Counter({'0000-0001-6910-9133': 102, '0000-0002-8441-0802': 27, '0000-0001-7895-5112': 18, '0000-0002-3028-867X': 15, '0000-0001-6250-8852': 13, '0000-0002-7619-8979': 5, '0000-0002-4521-8957': 4, '0000-0003-0932-4418': 4, '0000-0003-4122-7976': 3, '0000-0002-4676-4554': 3, '0000-0002-3488-8963': 2, '0000-0002-8705-9210': 2, '0000-0002-8492-650X': 1, '0000-0002-8206-1151': 1, '0000-0002-2587-2775': 1, '0000-0002-6412-9482': 1})\n",
      "['0000-0001-6250-8852', '0000-0001-6910-9133', '0000-0002-3028-867X', '0000-0002-8441-0802', '0000-0001-7895-5112']\n",
      "Total sample size after apply threshold:  175\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(175, 175)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(175, 175)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.92      0.96        13\n",
      "          1       0.93      0.98      0.96       102\n",
      "          2       1.00      0.87      0.93        15\n",
      "          3       0.88      0.85      0.87        27\n",
      "          4       0.94      0.89      0.91        18\n",
      "\n",
      "avg / total       0.94      0.94      0.94       175\n",
      "\n",
      "[ 12   1   0   0   0   0 100   0   1   1   0   2  13   0   0   0   4   0\n",
      "  23   0   0   0   0   2  16]\n",
      "svc Accuracy:  0.9371428571428572\n",
      "svc F1:  0.9255438940404185\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.92      0.96        13\n",
      "          1       0.92      0.98      0.95       102\n",
      "          2       1.00      0.87      0.93        15\n",
      "          3       0.88      0.78      0.82        27\n",
      "          4       0.94      0.89      0.91        18\n",
      "\n",
      "avg / total       0.93      0.93      0.92       175\n",
      "\n",
      "[ 12   1   0   0   0   0 100   0   1   1   0   2  13   0   0   0   6   0\n",
      "  21   0   0   0   0   2  16]\n",
      "LR Accuracy:  0.9257142857142857\n",
      "LR F1:  0.9148507706400096\n",
      "For name:  m_salem\n",
      "total sample size before apply threshold:  25\n",
      "Counter({'0000-0003-4955-0479': 12, '0000-0003-3214-3711': 9, '0000-0002-3961-7935': 3, '0000-0002-4189-857X': 1})\n",
      "['0000-0003-4955-0479']\n",
      "Total sample size after apply threshold:  12\n",
      "For name:  h_lai\n",
      "total sample size before apply threshold:  165\n",
      "Counter({'0000-0002-7958-2183': 146, '0000-0003-4334-0243': 10, '0000-0003-1834-4154': 6, '0000-0003-2521-0509': 2, '0000-0001-6044-8470': 1})\n",
      "['0000-0002-7958-2183', '0000-0003-4334-0243']\n",
      "Total sample size after apply threshold:  156\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(156, 151)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(156, 151)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.98       146\n",
      "          1       1.00      0.50      0.67        10\n",
      "\n",
      "avg / total       0.97      0.97      0.96       156\n",
      "\n",
      "[146   0   5   5]\n",
      "svc Accuracy:  0.967948717948718\n",
      "svc F1:  0.8249158249158249\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.97       146\n",
      "          1       1.00      0.20      0.33        10\n",
      "\n",
      "avg / total       0.95      0.95      0.93       156\n",
      "\n",
      "[146   0   8   2]\n",
      "LR Accuracy:  0.9487179487179487\n",
      "LR F1:  0.6533333333333333\n",
      "For name:  r_harris\n",
      "total sample size before apply threshold:  50\n",
      "Counter({'0000-0002-4377-5063': 26, '0000-0002-7943-5650': 8, '0000-0002-2636-1520': 6, '0000-0003-1787-7784': 3, '0000-0002-9247-0768': 3, '0000-0003-0954-1981': 2, '0000-0003-3322-1371': 2})\n",
      "['0000-0002-4377-5063']\n",
      "Total sample size after apply threshold:  26\n",
      "For name:  c_vaughan\n",
      "total sample size before apply threshold:  83\n",
      "Counter({'0000-0003-4314-7689': 73, '0000-0003-3988-8222': 7, '0000-0001-8714-4442': 2, '0000-0001-9147-8648': 1})\n",
      "['0000-0003-4314-7689']\n",
      "Total sample size after apply threshold:  73\n",
      "For name:  e_thompson\n",
      "total sample size before apply threshold:  181\n",
      "Counter({'0000-0002-9723-4924': 163, '0000-0001-8633-2417': 9, '0000-0002-6434-9290': 4, '0000-0003-3506-0401': 2, '0000-0002-5615-2893': 2, '0000-0002-7115-0001': 1})\n",
      "['0000-0002-9723-4924']\n",
      "Total sample size after apply threshold:  163\n",
      "For name:  r_gomes\n",
      "total sample size before apply threshold:  52\n",
      "Counter({'0000-0001-7155-0059': 15, '0000-0002-9197-8279': 10, '0000-0003-0278-4876': 10, '0000-0002-7242-6540': 6, '0000-0002-9012-3287': 6, '0000-0002-5984-0712': 4, '0000-0002-6375-7014': 1})\n",
      "['0000-0001-7155-0059', '0000-0002-9197-8279', '0000-0003-0278-4876']\n",
      "Total sample size after apply threshold:  35\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(35, 462)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(35, 462)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97        15\n",
      "          1       1.00      0.90      0.95        10\n",
      "          2       1.00      1.00      1.00        10\n",
      "\n",
      "avg / total       0.97      0.97      0.97        35\n",
      "\n",
      "[15  0  0  1  9  0  0  0 10]\n",
      "svc Accuracy:  0.9714285714285714\n",
      "svc F1:  0.9717034521788342\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97        15\n",
      "          1       1.00      0.90      0.95        10\n",
      "          2       1.00      1.00      1.00        10\n",
      "\n",
      "avg / total       0.97      0.97      0.97        35\n",
      "\n",
      "[15  0  0  1  9  0  0  0 10]\n",
      "LR Accuracy:  0.9714285714285714\n",
      "LR F1:  0.9717034521788342\n",
      "For name:  r_bennett\n",
      "total sample size before apply threshold:  93\n",
      "Counter({'0000-0002-7526-3425': 74, '0000-0002-7227-4831': 11, '0000-0002-5780-8786': 3, '0000-0002-3746-367X': 3, '0000-0002-5210-1386': 1, '0000-0002-1200-2068': 1})\n",
      "['0000-0002-7526-3425', '0000-0002-7227-4831']\n",
      "Total sample size after apply threshold:  85\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(85, 310)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(85, 310)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99        74\n",
      "          1       1.00      0.91      0.95        11\n",
      "\n",
      "avg / total       0.99      0.99      0.99        85\n",
      "\n",
      "[74  0  1 10]\n",
      "svc Accuracy:  0.9882352941176471\n",
      "svc F1:  0.9728347714924896\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.97        74\n",
      "          1       1.00      0.64      0.78        11\n",
      "\n",
      "avg / total       0.96      0.95      0.95        85\n",
      "\n",
      "[74  0  4  7]\n",
      "LR Accuracy:  0.9529411764705882\n",
      "LR F1:  0.8757309941520468\n",
      "For name:  m_collins\n",
      "total sample size before apply threshold:  57\n",
      "Counter({'0000-0002-7656-4975': 20, '0000-0003-3785-6008': 14, '0000-0003-3969-5797': 9, '0000-0002-2312-3172': 6, '0000-0003-2536-4508': 6, '0000-0003-1641-848X': 2})\n",
      "['0000-0002-7656-4975', '0000-0003-3785-6008']\n",
      "Total sample size after apply threshold:  34\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(34, 160)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(34, 160)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.90      0.92        20\n",
      "          1       0.87      0.93      0.90        14\n",
      "\n",
      "avg / total       0.91      0.91      0.91        34\n",
      "\n",
      "[18  2  1 13]\n",
      "svc Accuracy:  0.9117647058823529\n",
      "svc F1:  0.9098143236074271\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.90      0.90        20\n",
      "          1       0.86      0.86      0.86        14\n",
      "\n",
      "avg / total       0.88      0.88      0.88        34\n",
      "\n",
      "[18  2  2 12]\n",
      "LR Accuracy:  0.8823529411764706\n",
      "LR F1:  0.8785714285714286\n",
      "For name:  m_cowley\n",
      "total sample size before apply threshold:  132\n",
      "Counter({'0000-0001-7811-134X': 57, '0000-0002-9519-5714': 49, '0000-0003-0664-2891': 17, '0000-0001-8564-4224': 9})\n",
      "['0000-0003-0664-2891', '0000-0002-9519-5714', '0000-0001-7811-134X']\n",
      "Total sample size after apply threshold:  123\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(123, 775)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(123, 775)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        17\n",
      "          1       1.00      0.86      0.92        49\n",
      "          2       0.89      1.00      0.94        57\n",
      "\n",
      "avg / total       0.95      0.94      0.94       123\n",
      "\n",
      "[17  0  0  0 42  7  0  0 57]\n",
      "svc Accuracy:  0.943089430894309\n",
      "svc F1:  0.9550752278025004\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.82      0.90        17\n",
      "          1       1.00      0.88      0.93        49\n",
      "          2       0.86      1.00      0.93        57\n",
      "\n",
      "avg / total       0.94      0.93      0.93       123\n",
      "\n",
      "[14  0  3  0 43  6  0  0 57]\n",
      "LR Accuracy:  0.926829268292683\n",
      "LR F1:  0.9216125611466494\n",
      "For name:  p_teixeira\n",
      "total sample size before apply threshold:  213\n",
      "Counter({'0000-0002-7258-7977': 60, '0000-0002-6296-5137': 55, '0000-0001-7202-0527': 48, '0000-0003-2315-2261': 26, '0000-0003-2735-6608': 22, '0000-0002-7596-9735': 1, '0000-0002-1593-8064': 1})\n",
      "['0000-0001-7202-0527', '0000-0002-7258-7977', '0000-0003-2315-2261', '0000-0003-2735-6608', '0000-0002-6296-5137']\n",
      "Total sample size after apply threshold:  211\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(211, 378)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(211, 378)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.75      0.86        48\n",
      "          1       1.00      0.97      0.98        60\n",
      "          2       0.62      0.96      0.76        26\n",
      "          3       0.95      0.91      0.93        22\n",
      "          4       0.95      0.96      0.95        55\n",
      "\n",
      "avg / total       0.93      0.91      0.91       211\n",
      "\n",
      "[36  0 11  0  1  0 58  2  0  0  0  0 25  0  1  0  0  1 20  1  0  0  1  1\n",
      " 53]\n",
      "svc Accuracy:  0.909952606635071\n",
      "svc F1:  0.8965913950541463\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.83      0.91        48\n",
      "          1       1.00      1.00      1.00        60\n",
      "          2       0.76      0.96      0.85        26\n",
      "          3       0.95      0.91      0.93        22\n",
      "          4       0.93      0.96      0.95        55\n",
      "\n",
      "avg / total       0.95      0.94      0.94       211\n",
      "\n",
      "[40  0  6  0  2  0 60  0  0  0  0  0 25  0  1  0  0  1 20  1  0  0  1  1\n",
      " 53]\n",
      "LR Accuracy:  0.9383886255924171\n",
      "LR F1:  0.9266419331555319\n",
      "For name:  c_cox\n",
      "total sample size before apply threshold:  48\n",
      "Counter({'0000-0002-4927-979X': 24, '0000-0003-0625-328X': 21, '0000-0003-1074-3839': 2, '0000-0002-4486-0681': 1})\n",
      "['0000-0003-0625-328X', '0000-0002-4927-979X']\n",
      "Total sample size after apply threshold:  45\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(45, 210)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(45, 210)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.86      0.92        21\n",
      "          1       0.89      1.00      0.94        24\n",
      "\n",
      "avg / total       0.94      0.93      0.93        45\n",
      "\n",
      "[18  3  0 24]\n",
      "svc Accuracy:  0.9333333333333333\n",
      "svc F1:  0.9321266968325792\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.86      0.92        21\n",
      "          1       0.89      1.00      0.94        24\n",
      "\n",
      "avg / total       0.94      0.93      0.93        45\n",
      "\n",
      "[18  3  0 24]\n",
      "LR Accuracy:  0.9333333333333333\n",
      "LR F1:  0.9321266968325792\n",
      "For name:  s_hsu\n",
      "total sample size before apply threshold:  204\n",
      "Counter({'0000-0003-3399-055X': 124, '0000-0002-7231-0185': 65, '0000-0002-8214-1696': 12, '0000-0002-8830-5305': 1, '0000-0002-6666-4665': 1, '0000-0002-7232-9839': 1})\n",
      "['0000-0002-8214-1696', '0000-0002-7231-0185', '0000-0003-3399-055X']\n",
      "Total sample size after apply threshold:  201\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(201, 228)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(201, 228)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.67      0.70        12\n",
      "          1       0.91      0.77      0.83        65\n",
      "          2       0.87      0.94      0.90       124\n",
      "\n",
      "avg / total       0.87      0.87      0.87       201\n",
      "\n",
      "[  8   1   3   0  50  15   3   4 117]\n",
      "svc Accuracy:  0.8706467661691543\n",
      "svc F1:  0.8108201369070934\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.50      0.67        12\n",
      "          1       0.91      0.75      0.82        65\n",
      "          2       0.85      0.97      0.91       124\n",
      "\n",
      "avg / total       0.88      0.87      0.86       201\n",
      "\n",
      "[  6   1   5   0  49  16   0   4 120]\n",
      "LR Accuracy:  0.8706467661691543\n",
      "LR F1:  0.798618818596621\n",
      "For name:  f_williams\n",
      "total sample size before apply threshold:  149\n",
      "Counter({'0000-0002-2998-2744': 84, '0000-0002-6194-2734': 33, '0000-0002-3046-9235': 29, '0000-0003-4144-1411': 2, '0000-0001-7507-4870': 1})\n",
      "['0000-0002-2998-2744', '0000-0002-6194-2734', '0000-0002-3046-9235']\n",
      "Total sample size after apply threshold:  146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(146, 1028)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(146, 1028)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.92      0.91        84\n",
      "          1       1.00      0.91      0.95        33\n",
      "          2       0.74      0.79      0.77        29\n",
      "\n",
      "avg / total       0.89      0.89      0.89       146\n",
      "\n",
      "[77  0  7  2 30  1  6  0 23]\n",
      "svc Accuracy:  0.8904109589041096\n",
      "svc F1:  0.8767634075326383\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      1.00      0.95        84\n",
      "          1       1.00      0.91      0.95        33\n",
      "          2       1.00      0.83      0.91        29\n",
      "\n",
      "avg / total       0.95      0.95      0.94       146\n",
      "\n",
      "[84  0  0  3 30  0  5  0 24]\n",
      "LR Accuracy:  0.9452054794520548\n",
      "LR F1:  0.9375289280949657\n",
      "For name:  d_parsons\n",
      "total sample size before apply threshold:  30\n",
      "Counter({'0000-0002-3956-6031': 26, '0000-0002-1393-8431': 2, '0000-0002-9121-7859': 1, '0000-0002-5142-4466': 1})\n",
      "['0000-0002-3956-6031']\n",
      "Total sample size after apply threshold:  26\n",
      "For name:  a_choudhury\n",
      "total sample size before apply threshold:  56\n",
      "Counter({'0000-0002-3561-6580': 28, '0000-0001-5496-7346': 24, '0000-0002-7042-8139': 3, '0000-0002-8990-879X': 1})\n",
      "['0000-0001-5496-7346', '0000-0002-3561-6580']\n",
      "Total sample size after apply threshold:  52\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(52, 306)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(52, 306)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.88      0.93        24\n",
      "          1       0.90      1.00      0.95        28\n",
      "\n",
      "avg / total       0.95      0.94      0.94        52\n",
      "\n",
      "[21  3  0 28]\n",
      "svc Accuracy:  0.9423076923076923\n",
      "svc F1:  0.9412429378531073\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.92      0.96        24\n",
      "          1       0.93      1.00      0.97        28\n",
      "\n",
      "avg / total       0.96      0.96      0.96        52\n",
      "\n",
      "[22  2  0 28]\n",
      "LR Accuracy:  0.9615384615384616\n",
      "LR F1:  0.9610194902548725\n",
      "For name:  c_richter\n",
      "total sample size before apply threshold:  11\n",
      "Counter({'0000-0002-5658-6173': 4, '0000-0002-6591-1118': 4, '0000-0001-6017-1520': 2, '0000-0002-6839-7994': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  m_hossain\n",
      "total sample size before apply threshold:  102\n",
      "Counter({'0000-0003-1408-2273': 26, '0000-0002-1878-8145': 17, '0000-0003-3967-2544': 10, '0000-0003-3399-581X': 9, '0000-0003-3303-5755': 7, '0000-0003-1271-1515': 7, '0000-0003-4733-0018': 6, '0000-0002-9953-586X': 5, '0000-0001-8019-843X': 4, '0000-0001-7996-9233': 3, '0000-0002-1917-8701': 1, '0000-0002-0984-984X': 1, '0000-0002-7673-8410': 1, '0000-0002-0977-4593': 1, '0000-0003-2970-2324': 1, '0000-0001-6753-4216': 1, '0000-0002-3929-6211': 1, '0000-0002-6621-8737': 1})\n",
      "['0000-0003-3967-2544', '0000-0003-1408-2273', '0000-0002-1878-8145']\n",
      "Total sample size after apply threshold:  53\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(53, 127)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(53, 127)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.90      0.90        10\n",
      "          1       1.00      0.92      0.96        26\n",
      "          2       0.89      1.00      0.94        17\n",
      "\n",
      "avg / total       0.95      0.94      0.94        53\n",
      "\n",
      "[ 9  0  1  1 24  1  0  0 17]\n",
      "svc Accuracy:  0.9433962264150944\n",
      "svc F1:  0.9348148148148149\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.80      0.89        10\n",
      "          1       0.93      0.96      0.94        26\n",
      "          2       0.94      1.00      0.97        17\n",
      "\n",
      "avg / total       0.95      0.94      0.94        53\n",
      "\n",
      "[ 8  2  0  0 25  1  0  0 17]\n",
      "LR Accuracy:  0.9433962264150944\n",
      "LR F1:  0.9345712289108516\n",
      "For name:  v_alves\n",
      "total sample size before apply threshold:  24\n",
      "Counter({'0000-0002-8430-4380': 13, '0000-0002-6182-1748': 5, '0000-0003-1819-7051': 4, '0000-0002-1485-6032': 2})\n",
      "['0000-0002-8430-4380']\n",
      "Total sample size after apply threshold:  13\n",
      "For name:  j_becker\n",
      "total sample size before apply threshold:  177\n",
      "Counter({'0000-0003-4425-4726': 122, '0000-0002-6845-6122': 45, '0000-0003-4564-3192': 4, '0000-0001-5668-1544': 3, '0000-0002-5041-5488': 2, '0000-0002-7733-4522': 1})\n",
      "['0000-0002-6845-6122', '0000-0003-4425-4726']\n",
      "Total sample size after apply threshold:  167\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(167, 678)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(167, 678)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.82      0.90        45\n",
      "          1       0.94      1.00      0.97       122\n",
      "\n",
      "avg / total       0.96      0.95      0.95       167\n",
      "\n",
      "[ 37   8   0 122]\n",
      "svc Accuracy:  0.9520958083832335\n",
      "svc F1:  0.9353464963221061\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.82      0.90        45\n",
      "          1       0.94      1.00      0.97       122\n",
      "\n",
      "avg / total       0.96      0.95      0.95       167\n",
      "\n",
      "[ 37   8   0 122]\n",
      "LR Accuracy:  0.9520958083832335\n",
      "LR F1:  0.9353464963221061\n",
      "For name:  m_soares\n",
      "total sample size before apply threshold:  247\n",
      "Counter({'0000-0001-9701-836X': 75, '0000-0002-9314-4833': 68, '0000-0001-6071-0272': 44, '0000-0003-1579-8513': 32, '0000-0002-5213-2377': 10, '0000-0001-8860-0470': 7, '0000-0003-4227-4141': 4, '0000-0002-7181-1906': 3, '0000-0002-4614-8209': 2, '0000-0002-8059-7067': 1, '0000-0002-9013-2570': 1})\n",
      "['0000-0002-5213-2377', '0000-0001-6071-0272', '0000-0003-1579-8513', '0000-0002-9314-4833', '0000-0001-9701-836X']\n",
      "Total sample size after apply threshold:  229\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(229, 634)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(229, 634)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        10\n",
      "          1       1.00      0.84      0.91        44\n",
      "          2       0.96      0.78      0.86        32\n",
      "          3       1.00      0.79      0.89        68\n",
      "          4       0.73      0.99      0.84        75\n",
      "\n",
      "avg / total       0.90      0.87      0.88       229\n",
      "\n",
      "[10  0  0  0  0  0 37  0  0  7  0  0 25  0  7  0  0  0 54 14  0  0  1  0\n",
      " 74]\n",
      "svc Accuracy:  0.8733624454148472\n",
      "svc F1:  0.8994106612321124\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      1.00      0.95        10\n",
      "          1       1.00      0.93      0.96        44\n",
      "          2       1.00      0.81      0.90        32\n",
      "          3       1.00      0.91      0.95        68\n",
      "          4       0.83      0.99      0.90        75\n",
      "\n",
      "avg / total       0.94      0.93      0.93       229\n",
      "\n",
      "[10  0  0  0  0  0 41  0  0  3  0  0 26  0  6  0  0  0 62  6  1  0  0  0\n",
      " 74]\n",
      "LR Accuracy:  0.9301310043668122\n",
      "LR F1:  0.9339847474216445\n",
      "For name:  j_yi\n",
      "total sample size before apply threshold:  29\n",
      "Counter({'0000-0001-5299-9897': 18, '0000-0002-9296-8443': 9, '0000-0002-1025-865X': 1, '0000-0003-1718-6326': 1})\n",
      "['0000-0001-5299-9897']\n",
      "Total sample size after apply threshold:  18\n",
      "For name:  s_khan\n",
      "total sample size before apply threshold:  193\n",
      "Counter({'0000-0001-5147-145X': 61, '0000-0001-5654-2835': 42, '0000-0003-4185-8882': 29, '0000-0002-6792-3577': 7, '0000-0003-0910-4095': 7, '0000-0002-0310-0424': 6, '0000-0002-0763-2583': 6, '0000-0002-9564-5092': 5, '0000-0003-0273-1248': 5, '0000-0002-2689-8563': 4, '0000-0002-0948-5003': 4, '0000-0003-0772-6122': 4, '0000-0002-3845-541X': 3, '0000-0001-6732-768X': 2, '0000-0002-9643-6858': 2, '0000-0002-1894-7839': 1, '0000-0002-1589-6634': 1, '0000-0001-9377-6382': 1, '0000-0002-6307-2023': 1, '0000-0002-0823-4042': 1, '0000-0001-8678-2872': 1})\n",
      "['0000-0003-4185-8882', '0000-0001-5147-145X', '0000-0001-5654-2835']\n",
      "Total sample size after apply threshold:  132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(132, 942)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(132, 942)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.97      0.98        29\n",
      "          1       1.00      0.80      0.89        61\n",
      "          2       0.76      1.00      0.87        42\n",
      "\n",
      "avg / total       0.92      0.90      0.90       132\n",
      "\n",
      "[28  0  1  0 49 12  0  0 42]\n",
      "svc Accuracy:  0.9015151515151515\n",
      "svc F1:  0.9131148709010889\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.86      0.93        29\n",
      "          1       1.00      0.90      0.95        61\n",
      "          2       0.81      1.00      0.89        42\n",
      "\n",
      "avg / total       0.94      0.92      0.93       132\n",
      "\n",
      "[25  0  4  0 55  6  0  0 42]\n",
      "LR Accuracy:  0.9242424242424242\n",
      "LR F1:  0.9226062697571624\n",
      "For name:  a_rao\n",
      "total sample size before apply threshold:  93\n",
      "Counter({'0000-0002-2676-2762': 36, '0000-0003-0320-2962': 20, '0000-0002-2550-6097': 11, '0000-0001-6440-1274': 8, '0000-0003-2319-6539': 5, '0000-0002-2474-5010': 5, '0000-0003-4480-3190': 3, '0000-0003-4879-1123': 2, '0000-0002-7983-0773': 1, '0000-0002-0220-7131': 1, '0000-0002-6531-8728': 1})\n",
      "['0000-0002-2550-6097', '0000-0003-0320-2962', '0000-0002-2676-2762']\n",
      "Total sample size after apply threshold:  67\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(67, 184)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(67, 184)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.64      0.78        11\n",
      "          1       1.00      0.85      0.92        20\n",
      "          2       0.84      1.00      0.91        36\n",
      "\n",
      "avg / total       0.91      0.90      0.89        67\n",
      "\n",
      "[ 7  0  4  0 17  3  0  0 36]\n",
      "svc Accuracy:  0.8955223880597015\n",
      "svc F1:  0.8693630339199959\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.45      0.62        11\n",
      "          1       1.00      0.95      0.97        20\n",
      "          2       0.84      1.00      0.91        36\n",
      "\n",
      "avg / total       0.91      0.90      0.88        67\n",
      "\n",
      "[ 5  0  6  0 19  1  0  0 36]\n",
      "LR Accuracy:  0.8955223880597015\n",
      "LR F1:  0.8369171264740886\n",
      "For name:  d_cameron\n",
      "total sample size before apply threshold:  49\n",
      "Counter({'0000-0002-5439-6544': 29, '0000-0001-6274-2913': 17, '0000-0001-7520-7741': 2, '0000-0003-2567-0564': 1})\n",
      "['0000-0002-5439-6544', '0000-0001-6274-2913']\n",
      "Total sample size after apply threshold:  46\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(46, 154)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(46, 154)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      1.00      0.95        29\n",
      "          1       1.00      0.82      0.90        17\n",
      "\n",
      "avg / total       0.94      0.93      0.93        46\n",
      "\n",
      "[29  0  3 14]\n",
      "svc Accuracy:  0.9347826086956522\n",
      "svc F1:  0.9270227392913801\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      1.00      0.95        29\n",
      "          1       1.00      0.82      0.90        17\n",
      "\n",
      "avg / total       0.94      0.93      0.93        46\n",
      "\n",
      "[29  0  3 14]\n",
      "LR Accuracy:  0.9347826086956522\n",
      "LR F1:  0.9270227392913801\n",
      "For name:  c_morgan\n",
      "total sample size before apply threshold:  43\n",
      "Counter({'0000-0003-3131-9906': 15, '0000-0003-0931-5474': 11, '0000-0002-7511-0488': 11, '0000-0002-0118-1056': 3, '0000-0002-1508-2614': 2, '0000-0002-8191-3738': 1})\n",
      "['0000-0003-3131-9906', '0000-0003-0931-5474', '0000-0002-7511-0488']\n",
      "Total sample size after apply threshold:  37\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(37, 170)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(37, 170)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      1.00      0.71        15\n",
      "          1       1.00      0.45      0.62        11\n",
      "          2       1.00      0.45      0.62        11\n",
      "\n",
      "avg / total       0.82      0.68      0.66        37\n",
      "\n",
      "[15  0  0  6  5  0  6  0  5]\n",
      "svc Accuracy:  0.6756756756756757\n",
      "svc F1:  0.6547619047619048\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      1.00      0.75        15\n",
      "          1       1.00      0.55      0.71        11\n",
      "          2       1.00      0.55      0.71        11\n",
      "\n",
      "avg / total       0.84      0.73      0.72        37\n",
      "\n",
      "[15  0  0  5  6  0  5  0  6]\n",
      "LR Accuracy:  0.7297297297297297\n",
      "LR F1:  0.7205882352941174\n",
      "For name:  h_cui\n",
      "total sample size before apply threshold:  40\n",
      "Counter({'0000-0001-6394-4808': 11, '0000-0003-3358-8958': 10, '0000-0002-9870-748X': 9, '0000-0002-6343-1014': 9, '0000-0002-8627-8534': 1})\n",
      "['0000-0001-6394-4808', '0000-0003-3358-8958']\n",
      "Total sample size after apply threshold:  21\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(21, 55)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(21, 55)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.91      0.95        11\n",
      "          1       0.91      1.00      0.95        10\n",
      "\n",
      "avg / total       0.96      0.95      0.95        21\n",
      "\n",
      "[10  1  0 10]\n",
      "svc Accuracy:  0.9523809523809523\n",
      "svc F1:  0.9523809523809523\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        11\n",
      "          1       1.00      1.00      1.00        10\n",
      "\n",
      "avg / total       1.00      1.00      1.00        21\n",
      "\n",
      "[11  0  0 10]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  p_zhang\n",
      "total sample size before apply threshold:  137\n",
      "Counter({'0000-0002-1765-5965': 26, '0000-0003-3603-0175': 25, '0000-0003-2228-3569': 20, '0000-0002-2774-5534': 17, '0000-0002-5409-7480': 16, '0000-0001-5574-0899': 8, '0000-0002-6218-1885': 6, '0000-0002-1806-4200': 5, '0000-0001-9539-1136': 5, '0000-0003-0606-6855': 3, '0000-0001-6953-800X': 3, '0000-0002-8462-0340': 1, '0000-0001-7331-6020': 1, '0000-0003-3344-4823': 1})\n",
      "['0000-0002-5409-7480', '0000-0002-2774-5534', '0000-0003-3603-0175', '0000-0003-2228-3569', '0000-0002-1765-5965']\n",
      "Total sample size after apply threshold:  104\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(104, 161)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(104, 161)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.88      0.82        16\n",
      "          1       0.94      0.94      0.94        17\n",
      "          2       0.91      0.80      0.85        25\n",
      "          3       1.00      0.95      0.97        20\n",
      "          4       0.86      0.92      0.89        26\n",
      "\n",
      "avg / total       0.90      0.89      0.89       104\n",
      "\n",
      "[14  1  1  0  0  1 16  0  0  0  2  0 20  0  3  0  0  0 19  1  1  0  1  0\n",
      " 24]\n",
      "svc Accuracy:  0.8942307692307693\n",
      "svc F1:  0.8958035150776078\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.88      0.85        16\n",
      "          1       0.94      0.94      0.94        17\n",
      "          2       0.88      0.88      0.88        25\n",
      "          3       1.00      0.95      0.97        20\n",
      "          4       0.92      0.92      0.92        26\n",
      "\n",
      "avg / total       0.92      0.91      0.91       104\n",
      "\n",
      "[14  1  1  0  0  0 16  1  0  0  2  0 22  0  1  0  0  0 19  1  1  0  1  0\n",
      " 24]\n",
      "LR Accuracy:  0.9134615384615384\n",
      "LR F1:  0.9134194433017961\n",
      "For name:  j_fernandes\n",
      "total sample size before apply threshold:  208\n",
      "Counter({'0000-0002-2550-1640': 63, '0000-0003-1556-1698': 38, '0000-0001-5512-4092': 33, '0000-0002-8565-2942': 27, '0000-0002-6726-5324': 22, '0000-0002-9089-273X': 6, '0000-0001-8205-5870': 4, '0000-0001-6387-2939': 3, '0000-0002-4505-4809': 3, '0000-0001-6616-3513': 3, '0000-0003-0337-7084': 3, '0000-0003-1519-8032': 2, '0000-0003-0934-9244': 1})\n",
      "['0000-0002-2550-1640', '0000-0003-1556-1698', '0000-0002-8565-2942', '0000-0001-5512-4092', '0000-0002-6726-5324']\n",
      "Total sample size after apply threshold:  183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(183, 329)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(183, 329)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      1.00      0.90        63\n",
      "          1       1.00      0.89      0.94        38\n",
      "          2       0.92      0.85      0.88        27\n",
      "          3       0.97      0.97      0.97        33\n",
      "          4       1.00      0.64      0.78        22\n",
      "\n",
      "avg / total       0.92      0.91      0.90       183\n",
      "\n",
      "[63  0  0  0  0  3 34  0  1  0  4  0 23  0  0  1  0  0 32  0  6  0  2  0\n",
      " 14]\n",
      "svc Accuracy:  0.907103825136612\n",
      "svc F1:  0.8953069153069153\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.98      0.91        63\n",
      "          1       1.00      0.89      0.94        38\n",
      "          2       0.96      0.89      0.92        27\n",
      "          3       0.97      0.97      0.97        33\n",
      "          4       0.88      0.68      0.77        22\n",
      "\n",
      "avg / total       0.92      0.91      0.91       183\n",
      "\n",
      "[62  0  0  0  1  3 34  0  1  0  2  0 24  0  1  1  0  0 32  0  6  0  1  0\n",
      " 15]\n",
      "LR Accuracy:  0.912568306010929\n",
      "LR F1:  0.9023117191000403\n",
      "For name:  a_jain\n",
      "total sample size before apply threshold:  67\n",
      "Counter({'0000-0001-7808-6180': 15, '0000-0003-0250-3608': 11, '0000-0001-5320-0880': 9, '0000-0003-2235-5139': 7, '0000-0003-4032-1442': 6, '0000-0002-3281-9729': 5, '0000-0003-2827-6263': 4, '0000-0001-9415-0883': 4, '0000-0001-5658-367X': 3, '0000-0002-2457-8144': 1, '0000-0002-8481-7119': 1, '0000-0002-3950-2601': 1})\n",
      "['0000-0003-0250-3608', '0000-0001-7808-6180']\n",
      "Total sample size after apply threshold:  26\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(26, 63)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(26, 63)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.91      0.87        11\n",
      "          1       0.93      0.87      0.90        15\n",
      "\n",
      "avg / total       0.89      0.88      0.89        26\n",
      "\n",
      "[10  1  2 13]\n",
      "svc Accuracy:  0.8846153846153846\n",
      "svc F1:  0.8830584707646176\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.91      0.95        11\n",
      "          1       0.94      1.00      0.97        15\n",
      "\n",
      "avg / total       0.96      0.96      0.96        26\n",
      "\n",
      "[10  1  0 15]\n",
      "LR Accuracy:  0.9615384615384616\n",
      "LR F1:  0.9600614439324117\n",
      "For name:  d_zhang\n",
      "total sample size before apply threshold:  94\n",
      "Counter({'0000-0002-4175-5982': 17, '0000-0002-7665-2182': 12, '0000-0003-0779-6438': 11, '0000-0003-4280-0068': 8, '0000-0001-9295-4992': 7, '0000-0001-9508-8209': 7, '0000-0001-6930-5994': 6, '0000-0001-9478-5344': 6, '0000-0001-5809-0027': 5, '0000-0002-4149-4938': 4, '0000-0002-1581-2357': 4, '0000-0001-5956-4618': 2, '0000-0001-7063-7742': 2, '0000-0002-2541-837X': 1, '0000-0001-6259-7082': 1, '0000-0002-4515-2070': 1})\n",
      "['0000-0002-4175-5982', '0000-0002-7665-2182', '0000-0003-0779-6438']\n",
      "Total sample size after apply threshold:  40\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(40, 77)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(40, 77)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.88      0.86        17\n",
      "          1       0.90      0.75      0.82        12\n",
      "          2       0.92      1.00      0.96        11\n",
      "\n",
      "avg / total       0.88      0.88      0.87        40\n",
      "\n",
      "[15  1  1  3  9  0  0  0 11]\n",
      "svc Accuracy:  0.875\n",
      "svc F1:  0.8772821381517034\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.88      0.88        17\n",
      "          1       0.91      0.83      0.87        12\n",
      "          2       0.92      1.00      0.96        11\n",
      "\n",
      "avg / total       0.90      0.90      0.90        40\n",
      "\n",
      "[15  1  1  2 10  0  0  0 11]\n",
      "LR Accuracy:  0.9\n",
      "LR F1:  0.9028132992327365\n",
      "For name:  b_huang\n",
      "total sample size before apply threshold:  48\n",
      "Counter({'0000-0001-9082-2216': 16, '0000-0002-1981-5838': 12, '0000-0002-1246-7447': 9, '0000-0001-6189-814X': 5, '0000-0001-5009-3928': 3, '0000-0003-2838-6315': 3})\n",
      "['0000-0001-9082-2216', '0000-0002-1981-5838']\n",
      "Total sample size after apply threshold:  28\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(28, 145)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(28, 145)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.94      0.81        16\n",
      "          1       0.86      0.50      0.63        12\n",
      "\n",
      "avg / total       0.78      0.75      0.73        28\n",
      "\n",
      "[15  1  6  6]\n",
      "svc Accuracy:  0.75\n",
      "svc F1:  0.7211948790896159\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.94      0.83        16\n",
      "          1       0.88      0.58      0.70        12\n",
      "\n",
      "avg / total       0.80      0.79      0.78        28\n",
      "\n",
      "[15  1  5  7]\n",
      "LR Accuracy:  0.7857142857142857\n",
      "LR F1:  0.7666666666666667\n",
      "For name:  m_chong\n",
      "total sample size before apply threshold:  43\n",
      "Counter({'0000-0001-9324-5901': 20, '0000-0002-9586-6303': 20, '0000-0003-0587-2505': 1, '0000-0002-5507-1987': 1, '0000-0002-7324-1660': 1})\n",
      "['0000-0001-9324-5901', '0000-0002-9586-6303']\n",
      "Total sample size after apply threshold:  40\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(40, 90)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(40, 90)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.95      0.86        20\n",
      "          1       0.94      0.75      0.83        20\n",
      "\n",
      "avg / total       0.86      0.85      0.85        40\n",
      "\n",
      "[19  1  5 15]\n",
      "svc Accuracy:  0.85\n",
      "svc F1:  0.8484848484848484\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.95      0.90        20\n",
      "          1       0.94      0.85      0.89        20\n",
      "\n",
      "avg / total       0.90      0.90      0.90        40\n",
      "\n",
      "[19  1  3 17]\n",
      "LR Accuracy:  0.9\n",
      "LR F1:  0.899749373433584\n",
      "For name:  m_cerqueira\n",
      "total sample size before apply threshold:  41\n",
      "Counter({'0000-0002-8342-2967': 12, '0000-0002-2430-7758': 12, '0000-0001-6614-3942': 11, '0000-0002-3505-6982': 4, '0000-0001-7237-5053': 2})\n",
      "['0000-0002-8342-2967', '0000-0002-2430-7758', '0000-0001-6614-3942']\n",
      "Total sample size after apply threshold:  35\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(35, 81)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(35, 81)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        12\n",
      "          1       1.00      1.00      1.00        12\n",
      "          2       1.00      1.00      1.00        11\n",
      "\n",
      "avg / total       1.00      1.00      1.00        35\n",
      "\n",
      "[12  0  0  0 12  0  0  0 11]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        12\n",
      "          1       1.00      1.00      1.00        12\n",
      "          2       1.00      1.00      1.00        11\n",
      "\n",
      "avg / total       1.00      1.00      1.00        35\n",
      "\n",
      "[12  0  0  0 12  0  0  0 11]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  p_yang\n",
      "total sample size before apply threshold:  227\n",
      "Counter({'0000-0001-6330-6048': 102, '0000-0003-3473-4611': 46, '0000-0003-1098-3138': 17, '0000-0002-4004-2518': 17, '0000-0002-0463-1024': 14, '0000-0002-2334-5664': 10, '0000-0002-4635-1215': 6, '0000-0001-7554-5281': 5, '0000-0001-9227-3919': 4, '0000-0002-6610-1758': 3, '0000-0001-5247-1953': 1, '0000-0003-1840-6204': 1, '0000-0001-8472-0205': 1})\n",
      "['0000-0001-6330-6048', '0000-0003-3473-4611', '0000-0002-0463-1024', '0000-0003-1098-3138', '0000-0002-4004-2518', '0000-0002-2334-5664']\n",
      "Total sample size after apply threshold:  206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(206, 440)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(206, 440)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.86      0.85       102\n",
      "          1       0.79      0.83      0.81        46\n",
      "          2       0.53      0.71      0.61        14\n",
      "          3       0.92      0.65      0.76        17\n",
      "          4       0.64      0.53      0.58        17\n",
      "          5       0.78      0.70      0.74        10\n",
      "\n",
      "avg / total       0.80      0.79      0.79       206\n",
      "\n",
      "[88  7  2  0  5  0  8 38  0  0  0  0  1  0 10  1  0  2  1  0  5 11  0  0\n",
      "  5  3  0  0  9  0  1  0  2  0  0  7]\n",
      "svc Accuracy:  0.7912621359223301\n",
      "svc F1:  0.7241746887676611\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.92      0.88       102\n",
      "          1       0.80      0.89      0.85        46\n",
      "          2       0.64      0.50      0.56        14\n",
      "          3       1.00      0.82      0.90        17\n",
      "          4       0.89      0.47      0.62        17\n",
      "          5       0.80      0.80      0.80        10\n",
      "\n",
      "avg / total       0.84      0.83      0.83       206\n",
      "\n",
      "[94  5  2  0  1  0  5 41  0  0  0  0  2  3  7  0  0  2  2  0  1 14  0  0\n",
      "  7  2  0  0  8  0  1  0  1  0  0  8]\n",
      "LR Accuracy:  0.8349514563106796\n",
      "LR F1:  0.7677667257599529\n",
      "For name:  j_marques\n",
      "total sample size before apply threshold:  183\n",
      "Counter({'0000-0001-8865-8189': 30, '0000-0001-8157-8864': 28, '0000-0002-3800-7756': 27, '0000-0001-8910-4735': 18, '0000-0002-3457-3320': 14, '0000-0002-8124-3156': 12, '0000-0001-9436-1613': 11, '0000-0002-7234-9477': 8, '0000-0002-3724-5664': 8, '0000-0002-7333-9158': 6, '0000-0002-1014-0483': 5, '0000-0003-2199-9362': 4, '0000-0002-8740-642X': 4, '0000-0003-3429-0774': 2, '0000-0002-1644-7195': 2, '0000-0002-2523-6365': 1, '0000-0003-0972-1149': 1, '0000-0002-2354-433X': 1, '0000-0001-9834-1361': 1})\n",
      "['0000-0002-3457-3320', '0000-0002-8124-3156', '0000-0002-3800-7756', '0000-0001-8910-4735', '0000-0001-8865-8189', '0000-0001-9436-1613', '0000-0001-8157-8864']\n",
      "Total sample size after apply threshold:  140\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(140, 285)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(140, 285)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.50      0.67        14\n",
      "          1       0.91      0.83      0.87        12\n",
      "          2       0.89      0.93      0.91        27\n",
      "          3       0.94      0.83      0.88        18\n",
      "          4       0.92      0.77      0.84        30\n",
      "          5       0.35      0.82      0.49        11\n",
      "          6       1.00      0.96      0.98        28\n",
      "\n",
      "avg / total       0.89      0.83      0.84       140\n",
      "\n",
      "[ 7  0  1  0  1  5  0  0 10  0  0  0  2  0  0  0 25  0  0  2  0  0  0  0\n",
      " 15  0  3  0  0  1  0  1 23  5  0  0  0  2  0  0  9  0  0  0  0  0  1  0\n",
      " 27]\n",
      "svc Accuracy:  0.8285714285714286\n",
      "svc F1:  0.8046205769990936\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.64      0.78        14\n",
      "          1       0.92      0.92      0.92        12\n",
      "          2       0.76      0.96      0.85        27\n",
      "          3       0.94      0.94      0.94        18\n",
      "          4       0.79      0.87      0.83        30\n",
      "          5       1.00      0.55      0.71        11\n",
      "          6       0.96      0.96      0.96        28\n",
      "\n",
      "avg / total       0.89      0.87      0.87       140\n",
      "\n",
      "[ 9  0  3  0  2  0  0  0 11  0  0  1  0  0  0  0 26  0  1  0  0  0  0  1\n",
      " 17  0  0  0  0  1  1  1 26  0  1  0  0  3  0  2  6  0  0  0  0  0  1  0\n",
      " 27]\n",
      "LR Accuracy:  0.8714285714285714\n",
      "LR F1:  0.8559633879686348\n",
      "For name:  n_ali\n",
      "total sample size before apply threshold:  14\n",
      "Counter({'0000-0001-8121-0939': 6, '0000-0003-2063-2745': 3, '0000-0003-1245-4299': 2, '0000-0002-8292-0091': 1, '0000-0003-0858-7849': 1, '0000-0003-2924-6429': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  h_ng\n",
      "total sample size before apply threshold:  109\n",
      "Counter({'0000-0002-9210-5349': 53, '0000-0001-6352-8417': 41, '0000-0002-4055-8151': 13, '0000-0001-6519-1942': 1, '0000-0003-2397-840X': 1})\n",
      "['0000-0001-6352-8417', '0000-0002-9210-5349', '0000-0002-4055-8151']\n",
      "Total sample size after apply threshold:  107\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(107, 235)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(107, 235)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.71      0.83        41\n",
      "          1       0.83      1.00      0.91        53\n",
      "          2       0.93      1.00      0.96        13\n",
      "\n",
      "avg / total       0.91      0.89      0.88       107\n",
      "\n",
      "[29 11  1  0 53  0  0  0 13]\n",
      "svc Accuracy:  0.8878504672897196\n",
      "svc F1:  0.8991724325057658\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.73      0.85        41\n",
      "          1       0.83      1.00      0.91        53\n",
      "          2       1.00      1.00      1.00        13\n",
      "\n",
      "avg / total       0.91      0.90      0.89       107\n",
      "\n",
      "[30 11  0  0 53  0  0  0 13]\n",
      "LR Accuracy:  0.897196261682243\n",
      "LR F1:  0.9170177761727057\n",
      "For name:  m_viana\n",
      "total sample size before apply threshold:  139\n",
      "Counter({'0000-0002-0464-4845': 34, '0000-0003-4356-8109': 31, '0000-0002-4073-3802': 29, '0000-0001-9665-2115': 26, '0000-0001-9288-2108': 13, '0000-0002-3074-767X': 5, '0000-0002-5657-5570': 1})\n",
      "['0000-0001-9665-2115', '0000-0003-4356-8109', '0000-0002-4073-3802', '0000-0001-9288-2108', '0000-0002-0464-4845']\n",
      "Total sample size after apply threshold:  133\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(133, 440)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(133, 440)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.92      0.86        26\n",
      "          1       0.97      0.94      0.95        31\n",
      "          2       1.00      0.97      0.98        29\n",
      "          3       0.92      0.85      0.88        13\n",
      "          4       1.00      0.97      0.99        34\n",
      "\n",
      "avg / total       0.94      0.94      0.94       133\n",
      "\n",
      "[24  1  0  1  0  2 29  0  0  0  1  0 28  0  0  2  0  0 11  0  1  0  0  0\n",
      " 33]\n",
      "svc Accuracy:  0.9398496240601504\n",
      "svc F1:  0.9310986592981105\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.92      0.86        26\n",
      "          1       0.97      0.94      0.95        31\n",
      "          2       1.00      0.97      0.98        29\n",
      "          3       0.92      0.85      0.88        13\n",
      "          4       1.00      0.97      0.99        34\n",
      "\n",
      "avg / total       0.94      0.94      0.94       133\n",
      "\n",
      "[24  1  0  1  0  2 29  0  0  0  1  0 28  0  0  2  0  0 11  0  1  0  0  0\n",
      " 33]\n",
      "LR Accuracy:  0.9398496240601504\n",
      "LR F1:  0.9310986592981105\n",
      "For name:  t_inoue\n",
      "total sample size before apply threshold:  70\n",
      "Counter({'0000-0002-2728-0060': 52, '0000-0003-3289-4478': 9, '0000-0002-7710-1526': 8, '0000-0003-0582-0908': 1})\n",
      "['0000-0002-2728-0060']\n",
      "Total sample size after apply threshold:  52\n",
      "For name:  b_meyer\n",
      "total sample size before apply threshold:  92\n",
      "Counter({'0000-0002-0388-9568': 60, '0000-0003-1100-0260': 25, '0000-0002-2549-1825': 3, '0000-0002-7903-5710': 2, '0000-0001-9321-1277': 1, '0000-0002-6530-4588': 1})\n",
      "['0000-0002-0388-9568', '0000-0003-1100-0260']\n",
      "Total sample size after apply threshold:  85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(85, 518)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(85, 518)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.92      0.96        60\n",
      "          1       0.83      1.00      0.91        25\n",
      "\n",
      "avg / total       0.95      0.94      0.94        85\n",
      "\n",
      "[55  5  0 25]\n",
      "svc Accuracy:  0.9411764705882353\n",
      "svc F1:  0.9328063241106719\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.93      0.97        60\n",
      "          1       0.86      1.00      0.93        25\n",
      "\n",
      "avg / total       0.96      0.95      0.95        85\n",
      "\n",
      "[56  4  0 25]\n",
      "LR Accuracy:  0.9529411764705882\n",
      "LR F1:  0.9457215836526182\n",
      "For name:  c_liao\n",
      "total sample size before apply threshold:  35\n",
      "Counter({'0000-0002-1324-9644': 11, '0000-0001-5168-6493': 11, '0000-0001-9777-3701': 6, '0000-0003-3459-1913': 6, '0000-0003-4156-0912': 1})\n",
      "['0000-0002-1324-9644', '0000-0001-5168-6493']\n",
      "Total sample size after apply threshold:  22\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(22, 50)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(22, 50)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        11\n",
      "          1       1.00      1.00      1.00        11\n",
      "\n",
      "avg / total       1.00      1.00      1.00        22\n",
      "\n",
      "[11  0  0 11]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        11\n",
      "          1       1.00      1.00      1.00        11\n",
      "\n",
      "avg / total       1.00      1.00      1.00        22\n",
      "\n",
      "[11  0  0 11]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  k_wheeler\n",
      "total sample size before apply threshold:  28\n",
      "Counter({'0000-0003-3728-8928': 18, '0000-0001-6752-7542': 6, '0000-0002-6806-4233': 2, '0000-0003-2056-9977': 2})\n",
      "['0000-0003-3728-8928']\n",
      "Total sample size after apply threshold:  18\n",
      "For name:  m_rizzo\n",
      "total sample size before apply threshold:  152\n",
      "Counter({'0000-0002-9549-8504': 68, '0000-0002-3856-5010': 53, '0000-0001-5924-8615': 18, '0000-0002-1023-4260': 9, '0000-0003-4343-8937': 4})\n",
      "['0000-0002-9549-8504', '0000-0001-5924-8615', '0000-0002-3856-5010']\n",
      "Total sample size after apply threshold:  139\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(139, 378)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(139, 378)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.99        68\n",
      "          1       1.00      0.89      0.94        18\n",
      "          2       1.00      1.00      1.00        53\n",
      "\n",
      "avg / total       0.99      0.99      0.99       139\n",
      "\n",
      "[68  0  0  2 16  0  0  0 53]\n",
      "svc Accuracy:  0.9856115107913669\n",
      "svc F1:  0.975561238988349\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.99        68\n",
      "          1       1.00      0.89      0.94        18\n",
      "          2       1.00      1.00      1.00        53\n",
      "\n",
      "avg / total       0.99      0.99      0.99       139\n",
      "\n",
      "[68  0  0  2 16  0  0  0 53]\n",
      "LR Accuracy:  0.9856115107913669\n",
      "LR F1:  0.975561238988349\n",
      "For name:  y_shi\n",
      "total sample size before apply threshold:  67\n",
      "Counter({'0000-0001-6933-4971': 17, '0000-0003-4530-2056': 10, '0000-0003-2943-5465': 7, '0000-0001-6029-6526': 5, '0000-0001-7421-3306': 5, '0000-0001-7713-0813': 4, '0000-0003-4273-8663': 3, '0000-0001-9406-7967': 3, '0000-0003-1804-6990': 3, '0000-0002-6715-7681': 2, '0000-0002-7887-3050': 2, '0000-0001-7256-3628': 2, '0000-0001-6085-7880': 1, '0000-0003-0965-5751': 1, '0000-0001-7502-9201': 1, '0000-0002-3284-4449': 1})\n",
      "['0000-0001-6933-4971', '0000-0003-4530-2056']\n",
      "Total sample size after apply threshold:  27\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(27, 93)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(27, 93)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97        17\n",
      "          1       1.00      0.90      0.95        10\n",
      "\n",
      "avg / total       0.97      0.96      0.96        27\n",
      "\n",
      "[17  0  1  9]\n",
      "svc Accuracy:  0.9629629629629629\n",
      "svc F1:  0.9593984962406015\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97        17\n",
      "          1       1.00      0.90      0.95        10\n",
      "\n",
      "avg / total       0.97      0.96      0.96        27\n",
      "\n",
      "[17  0  1  9]\n",
      "LR Accuracy:  0.9629629629629629\n",
      "LR F1:  0.9593984962406015\n",
      "For name:  c_luo\n",
      "total sample size before apply threshold:  78\n",
      "Counter({'0000-0003-0524-5886': 36, '0000-0002-6453-7435': 18, '0000-0003-2193-3670': 15, '0000-0002-3477-5969': 5, '0000-0001-5876-5266': 1, '0000-0002-0879-3127': 1, '0000-0003-1152-0557': 1, '0000-0001-8806-1139': 1})\n",
      "['0000-0003-0524-5886', '0000-0002-6453-7435', '0000-0003-2193-3670']\n",
      "Total sample size after apply threshold:  69\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(69, 134)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(69, 134)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        36\n",
      "          1       1.00      1.00      1.00        18\n",
      "          2       1.00      1.00      1.00        15\n",
      "\n",
      "avg / total       1.00      1.00      1.00        69\n",
      "\n",
      "[36  0  0  0 18  0  0  0 15]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        36\n",
      "          1       1.00      1.00      1.00        18\n",
      "          2       1.00      1.00      1.00        15\n",
      "\n",
      "avg / total       1.00      1.00      1.00        69\n",
      "\n",
      "[36  0  0  0 18  0  0  0 15]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  j_arthur\n",
      "total sample size before apply threshold:  42\n",
      "Counter({'0000-0002-4796-0207': 24, '0000-0002-9185-6108': 11, '0000-0003-4540-4511': 6, '0000-0003-0344-4478': 1})\n",
      "['0000-0002-4796-0207', '0000-0002-9185-6108']\n",
      "Total sample size after apply threshold:  35\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(35, 184)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(35, 184)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        24\n",
      "          1       1.00      1.00      1.00        11\n",
      "\n",
      "avg / total       1.00      1.00      1.00        35\n",
      "\n",
      "[24  0  0 11]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        24\n",
      "          1       1.00      1.00      1.00        11\n",
      "\n",
      "avg / total       1.00      1.00      1.00        35\n",
      "\n",
      "[24  0  0 11]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  m_ansari\n",
      "total sample size before apply threshold:  34\n",
      "Counter({'0000-0001-6365-7104': 9, '0000-0002-8718-3078': 8, '0000-0001-7678-4639': 7, '0000-0003-2790-8353': 5, '0000-0002-9106-0978': 3, '0000-0002-0112-238X': 2})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  g_anderson\n",
      "total sample size before apply threshold:  103\n",
      "Counter({'0000-0001-5087-7837': 65, '0000-0001-7545-9893': 35, '0000-0002-5768-6360': 1, '0000-0001-6544-8007': 1, '0000-0003-2046-3152': 1})\n",
      "['0000-0001-7545-9893', '0000-0001-5087-7837']\n",
      "Total sample size after apply threshold:  100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(100, 351)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(100, 351)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.89      0.94        35\n",
      "          1       0.94      1.00      0.97        65\n",
      "\n",
      "avg / total       0.96      0.96      0.96       100\n",
      "\n",
      "[31  4  0 65]\n",
      "svc Accuracy:  0.96\n",
      "svc F1:  0.9547715965626413\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.89      0.94        35\n",
      "          1       0.94      1.00      0.97        65\n",
      "\n",
      "avg / total       0.96      0.96      0.96       100\n",
      "\n",
      "[31  4  0 65]\n",
      "LR Accuracy:  0.96\n",
      "LR F1:  0.9547715965626413\n",
      "For name:  m_hidalgo\n",
      "total sample size before apply threshold:  279\n",
      "Counter({'0000-0002-3765-3318': 238, '0000-0002-4450-3772': 31, '0000-0001-9862-6578': 5, '0000-0002-3494-9658': 3, '0000-0003-0684-0740': 2})\n",
      "['0000-0002-3765-3318', '0000-0002-4450-3772']\n",
      "Total sample size after apply threshold:  269\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(269, 1109)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(269, 1109)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99       238\n",
      "          1       0.97      0.94      0.95        31\n",
      "\n",
      "avg / total       0.99      0.99      0.99       269\n",
      "\n",
      "[237   1   2  29]\n",
      "svc Accuracy:  0.9888475836431226\n",
      "svc F1:  0.9722651819775234\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99       238\n",
      "          1       1.00      0.81      0.89        31\n",
      "\n",
      "avg / total       0.98      0.98      0.98       269\n",
      "\n",
      "[238   0   6  25]\n",
      "LR Accuracy:  0.9776951672862454\n",
      "LR F1:  0.9402045050385299\n",
      "For name:  k_jacobsen\n",
      "total sample size before apply threshold:  113\n",
      "Counter({'0000-0002-4198-6246': 93, '0000-0002-1121-2979': 17, '0000-0002-3450-0850': 2, '0000-0003-0135-0988': 1})\n",
      "['0000-0002-4198-6246', '0000-0002-1121-2979']\n",
      "Total sample size after apply threshold:  110\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(110, 1386)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(110, 1386)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97        93\n",
      "          1       1.00      0.65      0.79        17\n",
      "\n",
      "avg / total       0.95      0.95      0.94       110\n",
      "\n",
      "[93  0  6 11]\n",
      "svc Accuracy:  0.9454545454545454\n",
      "svc F1:  0.8772321428571429\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97        93\n",
      "          1       1.00      0.65      0.79        17\n",
      "\n",
      "avg / total       0.95      0.95      0.94       110\n",
      "\n",
      "[93  0  6 11]\n",
      "LR Accuracy:  0.9454545454545454\n",
      "LR F1:  0.8772321428571429\n",
      "For name:  s_kelly\n",
      "total sample size before apply threshold:  102\n",
      "Counter({'0000-0003-4002-048X': 31, '0000-0001-8583-5362': 26, '0000-0002-8245-0181': 20, '0000-0003-3533-5268': 12, '0000-0002-0375-1040': 11, '0000-0002-3078-8404': 2})\n",
      "['0000-0002-8245-0181', '0000-0001-8583-5362', '0000-0003-3533-5268', '0000-0002-0375-1040', '0000-0003-4002-048X']\n",
      "Total sample size after apply threshold:  100\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(100, 304)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(100, 304)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        20\n",
      "          1       0.77      0.65      0.71        26\n",
      "          2       1.00      0.67      0.80        12\n",
      "          3       1.00      0.36      0.53        11\n",
      "          4       0.59      0.87      0.70        31\n",
      "\n",
      "avg / total       0.81      0.76      0.76       100\n",
      "\n",
      "[20  0  0  0  0  0 17  0  0  9  0  0  8  0  4  0  1  0  4  6  0  4  0  0\n",
      " 27]\n",
      "svc Accuracy:  0.76\n",
      "svc F1:  0.7485930735930736\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        20\n",
      "          1       0.95      0.81      0.88        26\n",
      "          2       1.00      0.67      0.80        12\n",
      "          3       1.00      0.45      0.62        11\n",
      "          4       0.69      1.00      0.82        31\n",
      "\n",
      "avg / total       0.89      0.85      0.85       100\n",
      "\n",
      "[20  0  0  0  0  0 21  0  0  5  0  0  8  0  4  0  1  0  5  5  0  0  0  0\n",
      " 31]\n",
      "LR Accuracy:  0.85\n",
      "LR F1:  0.8231578947368421\n",
      "For name:  s_james\n",
      "total sample size before apply threshold:  59\n",
      "Counter({'0000-0001-9369-3288': 29, '0000-0003-0651-9842': 13, '0000-0001-7955-0491': 8, '0000-0001-6758-5726': 7, '0000-0003-1150-0628': 1, '0000-0002-8128-2139': 1})\n",
      "['0000-0003-0651-9842', '0000-0001-9369-3288']\n",
      "Total sample size after apply threshold:  42\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(42, 162)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(42, 162)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      1.00      0.93        13\n",
      "          1       1.00      0.93      0.96        29\n",
      "\n",
      "avg / total       0.96      0.95      0.95        42\n",
      "\n",
      "[13  0  2 27]\n",
      "svc Accuracy:  0.9523809523809523\n",
      "svc F1:  0.9464285714285714\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        13\n",
      "          1       1.00      1.00      1.00        29\n",
      "\n",
      "avg / total       1.00      1.00      1.00        42\n",
      "\n",
      "[13  0  0 29]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  p_persson\n",
      "total sample size before apply threshold:  80\n",
      "Counter({'0000-0001-9172-3068': 39, '0000-0001-7600-3230': 26, '0000-0001-9140-6724': 8, '0000-0003-4468-032X': 7})\n",
      "['0000-0001-7600-3230', '0000-0001-9172-3068']\n",
      "Total sample size after apply threshold:  65\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(65, 172)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(65, 172)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.85      0.92        26\n",
      "          1       0.91      1.00      0.95        39\n",
      "\n",
      "avg / total       0.94      0.94      0.94        65\n",
      "\n",
      "[22  4  0 39]\n",
      "svc Accuracy:  0.9384615384615385\n",
      "svc F1:  0.9339430894308943\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.77      0.87        26\n",
      "          1       0.87      1.00      0.93        39\n",
      "\n",
      "avg / total       0.92      0.91      0.90        65\n",
      "\n",
      "[20  6  0 39]\n",
      "LR Accuracy:  0.9076923076923077\n",
      "LR F1:  0.8990683229813665\n",
      "For name:  y_tanaka\n",
      "total sample size before apply threshold:  20\n",
      "Counter({'0000-0002-0674-660X': 12, '0000-0002-6190-4586': 5, '0000-0001-9598-5583': 2, '0000-0002-5163-7752': 1})\n",
      "['0000-0002-0674-660X']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sample size after apply threshold:  12\n",
      "For name:  c_gao\n",
      "total sample size before apply threshold:  189\n",
      "Counter({'0000-0001-5084-7208': 144, '0000-0003-3429-3473': 17, '0000-0001-7386-692X': 13, '0000-0002-1445-7939': 11, '0000-0003-2792-5022': 2, '0000-0003-2736-3920': 1, '0000-0002-5456-451X': 1})\n",
      "['0000-0003-3429-3473', '0000-0002-1445-7939', '0000-0001-7386-692X', '0000-0001-5084-7208']\n",
      "Total sample size after apply threshold:  185\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(185, 131)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(185, 131)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.88      0.88        17\n",
      "          1       0.82      0.82      0.82        11\n",
      "          2       1.00      0.92      0.96        13\n",
      "          3       0.97      0.98      0.98       144\n",
      "\n",
      "avg / total       0.96      0.96      0.96       185\n",
      "\n",
      "[ 15   1   0   1   0   9   0   2   0   0  12   1   2   1   0 141]\n",
      "svc Accuracy:  0.9567567567567568\n",
      "svc F1:  0.9090783265177729\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.82      0.90        17\n",
      "          1       0.88      0.64      0.74        11\n",
      "          2       1.00      0.69      0.82        13\n",
      "          3       0.93      0.99      0.96       144\n",
      "\n",
      "avg / total       0.94      0.94      0.93       185\n",
      "\n",
      "[ 14   0   0   3   0   7   0   4   0   0   9   4   0   1   0 143]\n",
      "LR Accuracy:  0.9351351351351351\n",
      "LR F1:  0.8544953183801876\n",
      "For name:  w_jung\n",
      "total sample size before apply threshold:  33\n",
      "Counter({'0000-0002-8697-9584': 17, '0000-0002-6853-2885': 8, '0000-0001-5266-3795': 4, '0000-0001-9590-3859': 2, '0000-0002-1615-750X': 2})\n",
      "['0000-0002-8697-9584']\n",
      "Total sample size after apply threshold:  17\n",
      "For name:  s_lewis\n",
      "total sample size before apply threshold:  306\n",
      "Counter({'0000-0003-1861-4652': 112, '0000-0002-8343-612X': 69, '0000-0002-2049-1586': 35, '0000-0003-1210-2314': 27, '0000-0003-4555-4907': 20, '0000-0001-9537-5822': 19, '0000-0002-6929-6626': 15, '0000-0001-7262-3168': 7, '0000-0003-4557-4123': 1, '0000-0002-5250-7415': 1})\n",
      "['0000-0003-4555-4907', '0000-0001-9537-5822', '0000-0002-2049-1586', '0000-0002-8343-612X', '0000-0003-1861-4652', '0000-0002-6929-6626', '0000-0003-1210-2314']\n",
      "Total sample size after apply threshold:  297\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(297, 1721)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(297, 1721)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.60      0.75        20\n",
      "          1       1.00      0.68      0.81        19\n",
      "          2       0.93      0.74      0.83        35\n",
      "          3       1.00      0.80      0.89        69\n",
      "          4       0.71      1.00      0.83       112\n",
      "          5       1.00      0.87      0.93        15\n",
      "          6       1.00      0.70      0.83        27\n",
      "\n",
      "avg / total       0.88      0.84      0.84       297\n",
      "\n",
      "[ 12   0   0   0   8   0   0   0  13   1   0   5   0   0   0   0  26   0\n",
      "   9   0   0   0   0   1  55  13   0   0   0   0   0   0 112   0   0   0\n",
      "   0   0   0   2  13   0   0   0   0   0   8   0  19]\n",
      "svc Accuracy:  0.8417508417508418\n",
      "svc F1:  0.8374808199043403\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.60      0.75        20\n",
      "          1       1.00      0.68      0.81        19\n",
      "          2       0.96      0.74      0.84        35\n",
      "          3       1.00      0.88      0.94        69\n",
      "          4       0.75      1.00      0.85       112\n",
      "          5       1.00      0.87      0.93        15\n",
      "          6       1.00      0.78      0.88        27\n",
      "\n",
      "avg / total       0.90      0.87      0.87       297\n",
      "\n",
      "[ 12   0   0   0   8   0   0   0  13   1   0   5   0   0   0   0  26   0\n",
      "   9   0   0   0   0   0  61   8   0   0   0   0   0   0 112   0   0   0\n",
      "   0   0   0   2  13   0   0   0   0   0   6   0  21]\n",
      "LR Accuracy:  0.8686868686868687\n",
      "LR F1:  0.8568863537876273\n",
      "For name:  w_han\n",
      "total sample size before apply threshold:  34\n",
      "Counter({'0000-0001-9702-0523': 18, '0000-0001-8678-7147': 9, '0000-0003-2252-9311': 3, '0000-0002-4544-2908': 3, '0000-0002-7567-1883': 1})\n",
      "['0000-0001-9702-0523']\n",
      "Total sample size after apply threshold:  18\n",
      "For name:  m_shah\n",
      "total sample size before apply threshold:  17\n",
      "Counter({'0000-0003-0299-8903': 10, '0000-0001-6126-7102': 2, '0000-0001-6599-7233': 2, '0000-0003-2191-7611': 1, '0000-0002-4354-9760': 1, '0000-0002-9740-8429': 1})\n",
      "['0000-0003-0299-8903']\n",
      "Total sample size after apply threshold:  10\n",
      "For name:  c_arango\n",
      "total sample size before apply threshold:  185\n",
      "Counter({'0000-0003-3382-4754': 177, '0000-0003-1098-830X': 6, '0000-0001-5920-5340': 1, '0000-0002-2970-4074': 1})\n",
      "['0000-0003-3382-4754']\n",
      "Total sample size after apply threshold:  177\n",
      "For name:  r_young\n",
      "total sample size before apply threshold:  361\n",
      "Counter({'0000-0002-6806-6503': 117, '0000-0001-8001-2914': 87, '0000-0002-6380-6314': 70, '0000-0001-7003-3017': 38, '0000-0001-6073-9489': 24, '0000-0002-1062-5691': 10, '0000-0002-5719-2205': 9, '0000-0001-7485-0604': 6})\n",
      "['0000-0001-8001-2914', '0000-0002-6806-6503', '0000-0001-7003-3017', '0000-0002-1062-5691', '0000-0001-6073-9489', '0000-0002-6380-6314']\n",
      "Total sample size after apply threshold:  346\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(346, 701)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(346, 701)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.89      0.79        87\n",
      "          1       0.99      0.95      0.97       117\n",
      "          2       1.00      0.71      0.83        38\n",
      "          3       1.00      0.50      0.67        10\n",
      "          4       1.00      0.71      0.83        24\n",
      "          5       0.74      0.80      0.77        70\n",
      "\n",
      "avg / total       0.87      0.85      0.85       346\n",
      "\n",
      "[ 77   1   0   0   0   9   2 111   0   0   0   4   8   0  27   0   0   3\n",
      "   4   0   0   5   0   1   4   0   0   0  17   3  14   0   0   0   0  56]\n",
      "svc Accuracy:  0.846820809248555\n",
      "svc F1:  0.8081623463191372\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.66      0.99      0.79        87\n",
      "          1       0.98      0.95      0.97       117\n",
      "          2       0.97      0.79      0.87        38\n",
      "          3       1.00      0.20      0.33        10\n",
      "          4       1.00      0.67      0.80        24\n",
      "          5       1.00      0.77      0.87        70\n",
      "\n",
      "avg / total       0.91      0.86      0.86       346\n",
      "\n",
      "[ 86   0   1   0   0   0   6 111   0   0   0   0   8   0  30   0   0   0\n",
      "   6   2   0   2   0   0   8   0   0   0  16   0  16   0   0   0   0  54]\n",
      "LR Accuracy:  0.8641618497109826\n",
      "LR F1:  0.7719517353458447\n",
      "For name:  r_coleman\n",
      "total sample size before apply threshold:  34\n",
      "Counter({'0000-0003-4136-5914': 15, '0000-0002-5194-8550': 13, '0000-0001-7118-524X': 3, '0000-0002-9731-7498': 3})\n",
      "['0000-0003-4136-5914', '0000-0002-5194-8550']\n",
      "Total sample size after apply threshold:  28\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(28, 179)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(28, 179)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97        15\n",
      "          1       1.00      0.92      0.96        13\n",
      "\n",
      "avg / total       0.97      0.96      0.96        28\n",
      "\n",
      "[15  0  1 12]\n",
      "svc Accuracy:  0.9642857142857143\n",
      "svc F1:  0.9638709677419355\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97        15\n",
      "          1       1.00      0.92      0.96        13\n",
      "\n",
      "avg / total       0.97      0.96      0.96        28\n",
      "\n",
      "[15  0  1 12]\n",
      "LR Accuracy:  0.9642857142857143\n",
      "LR F1:  0.9638709677419355\n",
      "For name:  b_kang\n",
      "total sample size before apply threshold:  20\n",
      "Counter({'0000-0001-5902-0549': 10, '0000-0001-6946-2279': 5, '0000-0003-2637-4695': 2, '0000-0003-0901-4903': 1, '0000-0002-4299-2170': 1, '0000-0002-1690-7753': 1})\n",
      "['0000-0001-5902-0549']\n",
      "Total sample size after apply threshold:  10\n",
      "For name:  s_carter\n",
      "total sample size before apply threshold:  205\n",
      "Counter({'0000-0002-3585-9400': 124, '0000-0003-2617-8694': 44, '0000-0002-9080-519X': 15, '0000-0002-4670-0884': 12, '0000-0002-9817-0029': 5, '0000-0002-3619-8640': 2, '0000-0002-8169-4483': 2, '0000-0002-2907-9651': 1})\n",
      "['0000-0002-3585-9400', '0000-0002-4670-0884', '0000-0003-2617-8694', '0000-0002-9080-519X']\n",
      "Total sample size after apply threshold:  195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(195, 439)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(195, 439)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.94      0.96       124\n",
      "          1       1.00      0.58      0.74        12\n",
      "          2       0.77      0.98      0.86        44\n",
      "          3       1.00      0.80      0.89        15\n",
      "\n",
      "avg / total       0.93      0.92      0.92       195\n",
      "\n",
      "[117   0   7   0   1   7   4   0   1   0  43   0   1   0   2  12]\n",
      "svc Accuracy:  0.9179487179487179\n",
      "svc F1:  0.8611868468986674\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      1.00      0.94       124\n",
      "          1       1.00      0.58      0.74        12\n",
      "          2       1.00      0.77      0.87        44\n",
      "          3       1.00      0.93      0.97        15\n",
      "\n",
      "avg / total       0.93      0.92      0.91       195\n",
      "\n",
      "[124   0   0   0   5   7   0   0  10   0  34   0   1   0   0  14]\n",
      "LR Accuracy:  0.9179487179487179\n",
      "LR F1:  0.8783870394578199\n",
      "For name:  c_thomas\n",
      "total sample size before apply threshold:  102\n",
      "Counter({'0000-0003-2822-1334': 56, '0000-0001-5855-1196': 15, '0000-0003-0316-6391': 15, '0000-0001-8704-3262': 8, '0000-0003-3091-5757': 2, '0000-0002-0351-0466': 2, '0000-0001-6662-6362': 2, '0000-0001-5706-3940': 1, '0000-0001-6536-4591': 1})\n",
      "['0000-0001-5855-1196', '0000-0003-2822-1334', '0000-0003-0316-6391']\n",
      "Total sample size after apply threshold:  86\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(86, 280)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(86, 280)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.53      0.70        15\n",
      "          1       0.76      0.96      0.85        56\n",
      "          2       0.71      0.33      0.45        15\n",
      "\n",
      "avg / total       0.79      0.78      0.75        86\n",
      "\n",
      "[ 8  7  0  0 54  2  0 10  5]\n",
      "svc Accuracy:  0.7790697674418605\n",
      "svc F1:  0.6668637764152998\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.53      0.70        15\n",
      "          1       0.77      1.00      0.87        56\n",
      "          2       1.00      0.33      0.50        15\n",
      "\n",
      "avg / total       0.85      0.80      0.77        86\n",
      "\n",
      "[ 8  7  0  0 56  0  0 10  5]\n",
      "LR Accuracy:  0.8023255813953488\n",
      "LR F1:  0.6879564093922031\n",
      "For name:  m_gutierrez\n",
      "total sample size before apply threshold:  32\n",
      "Counter({'0000-0003-3199-0337': 30, '0000-0003-0964-6222': 2})\n",
      "['0000-0003-3199-0337']\n",
      "Total sample size after apply threshold:  30\n",
      "For name:  s_moon\n",
      "total sample size before apply threshold:  85\n",
      "Counter({'0000-0001-6248-9049': 30, '0000-0002-7513-4404': 20, '0000-0001-7282-2888': 16, '0000-0002-3803-6354': 16, '0000-0002-2249-7500': 1, '0000-0002-4662-7859': 1, '0000-0002-4989-0150': 1})\n",
      "['0000-0001-6248-9049', '0000-0001-7282-2888', '0000-0002-3803-6354', '0000-0002-7513-4404']\n",
      "Total sample size after apply threshold:  82\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(82, 93)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(82, 93)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.90      0.93        30\n",
      "          1       0.93      0.81      0.87        16\n",
      "          2       0.59      0.62      0.61        16\n",
      "          3       0.65      0.75      0.70        20\n",
      "\n",
      "avg / total       0.81      0.79      0.80        82\n",
      "\n",
      "[27  0  1  2  0 13  2  1  0  1 10  5  1  0  4 15]\n",
      "svc Accuracy:  0.7926829268292683\n",
      "svc F1:  0.7753590435226361\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.93      0.95        30\n",
      "          1       0.87      0.81      0.84        16\n",
      "          2       0.62      0.62      0.62        16\n",
      "          3       0.68      0.75      0.71        20\n",
      "\n",
      "avg / total       0.81      0.80      0.81        82\n",
      "\n",
      "[28  0  1  1  0 13  2  1  0  1 10  5  1  1  3 15]\n",
      "LR Accuracy:  0.8048780487804879\n",
      "LR F1:  0.7817869835194877\n",
      "For name:  r_pereira\n",
      "total sample size before apply threshold:  202\n",
      "Counter({'0000-0001-6857-5968': 74, '0000-0003-3704-2848': 39, '0000-0002-3889-798X': 29, '0000-0002-8076-4822': 14, '0000-0002-7514-6130': 14, '0000-0003-1800-1450': 8, '0000-0001-7279-5728': 6, '0000-0003-2767-8535': 5, '0000-0003-1146-7506': 3, '0000-0003-1553-9693': 3, '0000-0001-8500-7364': 2, '0000-0002-3834-3709': 2, '0000-0002-5618-7690': 1, '0000-0002-9841-4775': 1, '0000-0002-2176-016X': 1})\n",
      "['0000-0002-8076-4822', '0000-0002-7514-6130', '0000-0002-3889-798X', '0000-0001-6857-5968', '0000-0003-3704-2848']\n",
      "Total sample size after apply threshold:  170\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(170, 377)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(170, 377)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.71      0.83        14\n",
      "          1       0.92      0.86      0.89        14\n",
      "          2       0.96      0.90      0.93        29\n",
      "          3       0.93      0.95      0.94        74\n",
      "          4       0.78      0.90      0.83        39\n",
      "\n",
      "avg / total       0.91      0.90      0.90       170\n",
      "\n",
      "[10  0  0  1  3  0 12  0  1  1  0  0 26  0  3  0  1  0 70  3  0  0  1  3\n",
      " 35]\n",
      "svc Accuracy:  0.9\n",
      "svc F1:  0.8847448599126452\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.64      0.78        14\n",
      "          1       1.00      0.86      0.92        14\n",
      "          2       0.96      0.90      0.93        29\n",
      "          3       0.92      0.96      0.94        74\n",
      "          4       0.76      0.87      0.81        39\n",
      "\n",
      "avg / total       0.90      0.89      0.89       170\n",
      "\n",
      "[ 9  0  0  1  4  0 12  0  1  1  0  0 26  0  3  0  0  0 71  3  0  0  1  4\n",
      " 34]\n",
      "LR Accuracy:  0.8941176470588236\n",
      "LR F1:  0.8768356415635425\n",
      "For name:  a_nielsen\n",
      "total sample size before apply threshold:  132\n",
      "Counter({'0000-0003-4372-9961': 70, '0000-0001-6616-0187': 27, '0000-0003-4464-8549': 17, '0000-0002-6469-4473': 7, '0000-0002-4837-9449': 3, '0000-0001-9842-5303': 2, '0000-0002-4741-7992': 2, '0000-0002-8955-9374': 2, '0000-0003-2199-2857': 1, '0000-0002-7130-6432': 1})\n",
      "['0000-0001-6616-0187', '0000-0003-4372-9961', '0000-0003-4464-8549']\n",
      "Total sample size after apply threshold:  114\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(114, 270)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(114, 270)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.67      0.80        27\n",
      "          1       0.88      1.00      0.93        70\n",
      "          2       1.00      0.94      0.97        17\n",
      "\n",
      "avg / total       0.92      0.91      0.91       114\n",
      "\n",
      "[18  9  0  0 70  0  0  1 16]\n",
      "svc Accuracy:  0.9122807017543859\n",
      "svc F1:  0.901010101010101\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.63      0.77        27\n",
      "          1       0.86      1.00      0.93        70\n",
      "          2       1.00      0.94      0.97        17\n",
      "\n",
      "avg / total       0.92      0.90      0.90       114\n",
      "\n",
      "[17 10  0  0 70  0  0  1 16]\n",
      "LR Accuracy:  0.9035087719298246\n",
      "LR F1:  0.8898588534350124\n",
      "For name:  j_conde\n",
      "total sample size before apply threshold:  84\n",
      "Counter({'0000-0001-8422-6792': 35, '0000-0002-2187-479X': 29, '0000-0002-5677-3024': 19, '0000-0001-8739-6893': 1})\n",
      "['0000-0001-8422-6792', '0000-0002-5677-3024', '0000-0002-2187-479X']\n",
      "Total sample size after apply threshold:  83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(83, 131)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(83, 131)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        35\n",
      "          1       1.00      1.00      1.00        19\n",
      "          2       1.00      1.00      1.00        29\n",
      "\n",
      "avg / total       1.00      1.00      1.00        83\n",
      "\n",
      "[35  0  0  0 19  0  0  0 29]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        35\n",
      "          1       1.00      1.00      1.00        19\n",
      "          2       1.00      1.00      1.00        29\n",
      "\n",
      "avg / total       1.00      1.00      1.00        83\n",
      "\n",
      "[35  0  0  0 19  0  0  0 29]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  k_wright\n",
      "total sample size before apply threshold:  59\n",
      "Counter({'0000-0003-0040-9247': 18, '0000-0002-9020-1572': 15, '0000-0003-3865-9743': 12, '0000-0002-0387-3048': 7, '0000-0001-6202-1737': 6, '0000-0003-0700-6010': 1})\n",
      "['0000-0003-0040-9247', '0000-0002-9020-1572', '0000-0003-3865-9743']\n",
      "Total sample size after apply threshold:  45\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(45, 2167)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(45, 2167)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.78      0.74        18\n",
      "          1       0.92      0.80      0.86        15\n",
      "          2       0.75      0.75      0.75        12\n",
      "\n",
      "avg / total       0.79      0.78      0.78        45\n",
      "\n",
      "[14  1  3  3 12  0  3  0  9]\n",
      "svc Accuracy:  0.7777777777777778\n",
      "svc F1:  0.7813283208020049\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.89      0.84        18\n",
      "          1       0.93      0.93      0.93        15\n",
      "          2       0.90      0.75      0.82        12\n",
      "\n",
      "avg / total       0.87      0.87      0.87        45\n",
      "\n",
      "[16  1  1  1 14  0  3  0  9]\n",
      "LR Accuracy:  0.8666666666666667\n",
      "LR F1:  0.8645401382243488\n",
      "For name:  m_parker\n",
      "total sample size before apply threshold:  280\n",
      "Counter({'0000-0002-3101-1138': 232, '0000-0002-7172-5231': 13, '0000-0003-1007-4612': 11, '0000-0002-3772-3742': 10, '0000-0002-1052-9296': 6, '0000-0002-3170-3505': 4, '0000-0002-1597-4858': 3, '0000-0001-9845-9108': 1})\n",
      "['0000-0002-3101-1138', '0000-0003-1007-4612', '0000-0002-7172-5231', '0000-0002-3772-3742']\n",
      "Total sample size after apply threshold:  266\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(266, 873)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(266, 873)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.99      0.98       232\n",
      "          1       1.00      0.82      0.90        11\n",
      "          2       1.00      0.85      0.92        13\n",
      "          3       0.80      0.80      0.80        10\n",
      "\n",
      "avg / total       0.97      0.97      0.97       266\n",
      "\n",
      "[230   0   0   2   2   9   0   0   2   0  11   0   2   0   0   8]\n",
      "svc Accuracy:  0.9699248120300752\n",
      "svc F1:  0.8998931623931624\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      1.00      0.96       232\n",
      "          1       1.00      0.45      0.62        11\n",
      "          2       1.00      0.46      0.63        13\n",
      "          3       1.00      0.20      0.33        10\n",
      "\n",
      "avg / total       0.93      0.92      0.90       266\n",
      "\n",
      "[232   0   0   0   6   5   0   0   7   0   6   0   8   0   0   2]\n",
      "LR Accuracy:  0.9210526315789473\n",
      "LR F1:  0.6366533279073975\n",
      "For name:  h_huang\n",
      "total sample size before apply threshold:  224\n",
      "Counter({'0000-0002-3386-0934': 87, '0000-0002-3382-305X': 24, '0000-0001-7640-7702': 18, '0000-0003-2657-3635': 16, '0000-0003-1461-5762': 16, '0000-0001-5497-0158': 14, '0000-0002-3778-4457': 9, '0000-0002-5647-7049': 6, '0000-0002-0919-4644': 5, '0000-0003-1743-7850': 5, '0000-0002-4564-7604': 5, '0000-0002-9665-2489': 4, '0000-0002-0534-2718': 4, '0000-0002-5948-317X': 3, '0000-0002-4104-9471': 2, '0000-0001-8346-1571': 1, '0000-0002-2650-3736': 1, '0000-0001-8237-0168': 1, '0000-0002-1188-9760': 1, '0000-0001-6455-676X': 1, '0000-0003-4184-3744': 1})\n",
      "['0000-0001-7640-7702', '0000-0002-3382-305X', '0000-0001-5497-0158', '0000-0003-2657-3635', '0000-0002-3386-0934', '0000-0003-1461-5762']\n",
      "Total sample size after apply threshold:  175\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(175, 906)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(175, 906)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.78      0.72        18\n",
      "          1       0.80      0.67      0.73        24\n",
      "          2       0.54      1.00      0.70        14\n",
      "          3       1.00      0.81      0.90        16\n",
      "          4       0.98      0.95      0.97        87\n",
      "          5       1.00      0.62      0.77        16\n",
      "\n",
      "avg / total       0.89      0.86      0.86       175\n",
      "\n",
      "[14  3  1  0  0  0  4 16  2  0  2  0  0  0 14  0  0  0  0  0  3 13  0  0\n",
      "  2  1  1  0 83  0  1  0  5  0  0 10]\n",
      "svc Accuracy:  0.8571428571428571\n",
      "svc F1:  0.7960200362766522\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.83      0.81        18\n",
      "          1       0.78      0.88      0.82        24\n",
      "          2       0.71      0.86      0.77        14\n",
      "          3       1.00      0.94      0.97        16\n",
      "          4       0.98      0.95      0.97        87\n",
      "          5       1.00      0.75      0.86        16\n",
      "\n",
      "avg / total       0.91      0.90      0.90       175\n",
      "\n",
      "[15  2  1  0  0  0  2 21  0  0  1  0  1  1 12  0  0  0  0  0  0 15  1  0\n",
      "  1  2  1  0 83  0  0  1  3  0  0 12]\n",
      "LR Accuracy:  0.9028571428571428\n",
      "LR F1:  0.866422473776518\n",
      "For name:  j_terry\n",
      "total sample size before apply threshold:  57\n",
      "Counter({'0000-0002-6829-5736': 35, '0000-0001-5464-8679': 20, '0000-0003-4255-5509': 1, '0000-0002-6314-1412': 1})\n",
      "['0000-0001-5464-8679', '0000-0002-6829-5736']\n",
      "Total sample size after apply threshold:  55\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(55, 207)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(55, 207)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.45      0.62        20\n",
      "          1       0.76      1.00      0.86        35\n",
      "\n",
      "avg / total       0.85      0.80      0.78        55\n",
      "\n",
      "[ 9 11  0 35]\n",
      "svc Accuracy:  0.8\n",
      "svc F1:  0.7424435930183058\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.45      0.62        20\n",
      "          1       0.76      1.00      0.86        35\n",
      "\n",
      "avg / total       0.85      0.80      0.78        55\n",
      "\n",
      "[ 9 11  0 35]\n",
      "LR Accuracy:  0.8\n",
      "LR F1:  0.7424435930183058\n",
      "For name:  y_xu\n",
      "total sample size before apply threshold:  137\n",
      "Counter({'0000-0002-2195-1695': 47, '0000-0002-6689-7768': 19, '0000-0002-6406-7832': 17, '0000-0001-6643-3173': 9, '0000-0002-0763-9953': 8, '0000-0002-4479-6157': 8, '0000-0001-7429-4724': 5, '0000-0002-5578-4960': 4, '0000-0002-1887-0632': 4, '0000-0002-9834-3006': 3, '0000-0002-9945-3514': 3, '0000-0001-8488-0399': 2, '0000-0001-9106-0049': 1, '0000-0003-4549-6110': 1, '0000-0002-2341-7971': 1, '0000-0003-4420-6353': 1, '0000-0002-7963-6890': 1, '0000-0002-7962-6668': 1, '0000-0003-1355-0055': 1, '0000-0002-1563-8811': 1})\n",
      "['0000-0002-6406-7832', '0000-0002-2195-1695', '0000-0002-6689-7768']\n",
      "Total sample size after apply threshold:  83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(83, 154)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(83, 154)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.71      0.83        17\n",
      "          1       0.84      1.00      0.91        47\n",
      "          2       0.87      0.68      0.76        19\n",
      "\n",
      "avg / total       0.88      0.87      0.86        83\n",
      "\n",
      "[12  3  2  0 47  0  0  6 13]\n",
      "svc Accuracy:  0.8674698795180723\n",
      "svc F1:  0.8349711494909314\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.65      0.79        17\n",
      "          1       0.82      1.00      0.90        47\n",
      "          2       1.00      0.79      0.88        19\n",
      "\n",
      "avg / total       0.90      0.88      0.87        83\n",
      "\n",
      "[11  6  0  0 47  0  0  4 15]\n",
      "LR Accuracy:  0.8795180722891566\n",
      "LR F1:  0.8573044602456368\n",
      "For name:  a_melo\n",
      "total sample size before apply threshold:  48\n",
      "Counter({'0000-0001-6455-7834': 26, '0000-0002-9153-0773': 11, '0000-0002-4606-7791': 7, '0000-0001-5682-2116': 4})\n",
      "['0000-0001-6455-7834', '0000-0002-9153-0773']\n",
      "Total sample size after apply threshold:  37\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(37, 84)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(37, 84)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98        26\n",
      "          1       1.00      0.91      0.95        11\n",
      "\n",
      "avg / total       0.97      0.97      0.97        37\n",
      "\n",
      "[26  0  1 10]\n",
      "svc Accuracy:  0.972972972972973\n",
      "svc F1:  0.9667565139263252\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98        26\n",
      "          1       1.00      0.91      0.95        11\n",
      "\n",
      "avg / total       0.97      0.97      0.97        37\n",
      "\n",
      "[26  0  1 10]\n",
      "LR Accuracy:  0.972972972972973\n",
      "LR F1:  0.9667565139263252\n",
      "For name:  r_doyle\n",
      "total sample size before apply threshold:  11\n",
      "Counter({'0000-0001-6229-4700': 5, '0000-0001-5001-1945': 4, '0000-0003-1019-6783': 1, '0000-0002-4704-7178': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  m_bernardo\n",
      "total sample size before apply threshold:  250\n",
      "Counter({'0000-0001-8748-6717': 216, '0000-0002-9204-7230': 22, '0000-0002-5823-6636': 11, '0000-0003-2661-5380': 1})\n",
      "['0000-0002-9204-7230', '0000-0001-8748-6717', '0000-0002-5823-6636']\n",
      "Total sample size after apply threshold:  249\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(249, 586)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(249, 586)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.91      0.91        22\n",
      "          1       0.99      0.99      0.99       216\n",
      "          2       1.00      1.00      1.00        11\n",
      "\n",
      "avg / total       0.98      0.98      0.98       249\n",
      "\n",
      "[ 20   2   0   2 214   0   0   0  11]\n",
      "svc Accuracy:  0.9839357429718876\n",
      "svc F1:  0.9666105499438832\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.91      0.95        22\n",
      "          1       0.99      1.00      1.00       216\n",
      "          2       1.00      1.00      1.00        11\n",
      "\n",
      "avg / total       0.99      0.99      0.99       249\n",
      "\n",
      "[ 20   2   0   0 216   0   0   0  11]\n",
      "LR Accuracy:  0.9919678714859438\n",
      "LR F1:  0.9825908858166922\n",
      "For name:  j_soares\n",
      "total sample size before apply threshold:  49\n",
      "Counter({'0000-0001-6558-4973': 17, '0000-0002-2775-131X': 8, '0000-0002-7105-2815': 6, '0000-0003-3464-6208': 5, '0000-0002-7241-8719': 5, '0000-0001-8496-156X': 3, '0000-0003-3908-0741': 2, '0000-0001-5277-4575': 2, '0000-0001-6534-1824': 1})\n",
      "['0000-0001-6558-4973']\n",
      "Total sample size after apply threshold:  17\n",
      "For name:  j_richard\n",
      "total sample size before apply threshold:  179\n",
      "Counter({'0000-0002-0440-2387': 110, '0000-0003-1503-3035': 57, '0000-0001-5750-0418': 10, '0000-0003-2514-8282': 2})\n",
      "['0000-0002-0440-2387', '0000-0003-1503-3035', '0000-0001-5750-0418']\n",
      "Total sample size after apply threshold:  177\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(177, 307)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(177, 307)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      1.00      0.96       110\n",
      "          1       1.00      0.91      0.95        57\n",
      "          2       1.00      0.70      0.82        10\n",
      "\n",
      "avg / total       0.96      0.95      0.95       177\n",
      "\n",
      "[110   0   0   5  52   0   3   0   7]\n",
      "svc Accuracy:  0.9548022598870056\n",
      "svc F1:  0.9141900442778109\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      1.00      0.96       110\n",
      "          1       1.00      0.89      0.94        57\n",
      "          2       1.00      0.70      0.82        10\n",
      "\n",
      "avg / total       0.95      0.95      0.95       177\n",
      "\n",
      "[110   0   0   6  51   0   3   0   7]\n",
      "LR Accuracy:  0.9491525423728814\n",
      "LR F1:  0.909557515388494\n",
      "For name:  p_robinson\n",
      "total sample size before apply threshold:  275\n",
      "Counter({'0000-0002-7878-0313': 133, '0000-0002-0736-9199': 119, '0000-0002-3156-3418': 19, '0000-0002-0577-3147': 4})\n",
      "['0000-0002-0736-9199', '0000-0002-7878-0313', '0000-0002-3156-3418']\n",
      "Total sample size after apply threshold:  271\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(271, 1067)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(271, 1067)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.87      0.93       119\n",
      "          1       0.85      1.00      0.92       133\n",
      "          2       1.00      0.63      0.77        19\n",
      "\n",
      "avg / total       0.93      0.92      0.91       271\n",
      "\n",
      "[103  16   0   0 133   0   0   7  12]\n",
      "svc Accuracy:  0.915129151291513\n",
      "svc F1:  0.8741789004095065\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.88      0.94       119\n",
      "          1       0.85      1.00      0.92       133\n",
      "          2       1.00      0.53      0.69        19\n",
      "\n",
      "avg / total       0.93      0.92      0.91       271\n",
      "\n",
      "[105  14   0   0 133   0   0   9  10]\n",
      "LR Accuracy:  0.915129151291513\n",
      "LR F1:  0.8491901324424292\n",
      "For name:  c_zou\n",
      "total sample size before apply threshold:  32\n",
      "Counter({'0000-0003-2484-7292': 22, '0000-0001-8569-3747': 8, '0000-0003-4305-5055': 1, '0000-0002-9712-4282': 1})\n",
      "['0000-0003-2484-7292']\n",
      "Total sample size after apply threshold:  22\n",
      "For name:  s_rana\n",
      "total sample size before apply threshold:  42\n",
      "Counter({'0000-0002-8039-1149': 30, '0000-0001-9197-8378': 9, '0000-0003-0628-7076': 2, '0000-0002-6604-997X': 1})\n",
      "['0000-0002-8039-1149']\n",
      "Total sample size after apply threshold:  30\n",
      "For name:  a_nunes\n",
      "total sample size before apply threshold:  61\n",
      "Counter({'0000-0003-2760-3277': 18, '0000-0001-9102-3600': 11, '0000-0001-8893-9247': 9, '0000-0001-8844-8333': 5, '0000-0002-3296-0183': 5, '0000-0002-0595-5821': 4, '0000-0002-5001-3534': 2, '0000-0002-4789-0253': 2, '0000-0003-4440-0391': 2, '0000-0001-6847-5764': 2, '0000-0001-8665-4459': 1})\n",
      "['0000-0001-9102-3600', '0000-0003-2760-3277']\n",
      "Total sample size after apply threshold:  29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(29, 79)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(29, 79)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.45      0.62        11\n",
      "          1       0.75      1.00      0.86        18\n",
      "\n",
      "avg / total       0.84      0.79      0.77        29\n",
      "\n",
      "[ 5  6  0 18]\n",
      "svc Accuracy:  0.7931034482758621\n",
      "svc F1:  0.7410714285714286\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.45      0.62        11\n",
      "          1       0.75      1.00      0.86        18\n",
      "\n",
      "avg / total       0.84      0.79      0.77        29\n",
      "\n",
      "[ 5  6  0 18]\n",
      "LR Accuracy:  0.7931034482758621\n",
      "LR F1:  0.7410714285714286\n",
      "For name:  s_jeong\n",
      "total sample size before apply threshold:  93\n",
      "Counter({'0000-0001-6178-8338': 33, '0000-0002-1958-8436': 21, '0000-0002-6376-7001': 13, '0000-0002-6480-7685': 7, '0000-0002-9084-5183': 6, '0000-0001-8995-3497': 5, '0000-0002-8370-3566': 1, '0000-0002-4004-3510': 1, '0000-0001-9175-9642': 1, '0000-0001-9197-1184': 1, '0000-0002-9868-621X': 1, '0000-0002-3309-0693': 1, '0000-0001-9575-0354': 1, '0000-0001-9588-1928': 1})\n",
      "['0000-0002-6376-7001', '0000-0002-1958-8436', '0000-0001-6178-8338']\n",
      "Total sample size after apply threshold:  67\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(67, 138)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(67, 138)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.92      0.96        13\n",
      "          1       0.91      0.95      0.93        21\n",
      "          2       0.97      0.97      0.97        33\n",
      "\n",
      "avg / total       0.96      0.96      0.96        67\n",
      "\n",
      "[12  1  0  0 20  1  0  1 32]\n",
      "svc Accuracy:  0.9552238805970149\n",
      "svc F1:  0.9533098426121681\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.92      0.96        13\n",
      "          1       0.89      0.76      0.82        21\n",
      "          2       0.86      0.97      0.91        33\n",
      "\n",
      "avg / total       0.90      0.90      0.89        67\n",
      "\n",
      "[12  1  0  0 16  5  0  1 32]\n",
      "LR Accuracy:  0.8955223880597015\n",
      "LR F1:  0.8982661782661783\n",
      "For name:  b_olsen\n",
      "total sample size before apply threshold:  213\n",
      "Counter({'0000-0002-4646-691X': 167, '0000-0002-7272-7140': 35, '0000-0001-9758-3641': 6, '0000-0001-5608-2779': 3, '0000-0002-6551-6812': 2})\n",
      "['0000-0002-7272-7140', '0000-0002-4646-691X']\n",
      "Total sample size after apply threshold:  202\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(202, 442)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(202, 442)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.74      0.85        35\n",
      "          1       0.95      1.00      0.97       167\n",
      "\n",
      "avg / total       0.96      0.96      0.95       202\n",
      "\n",
      "[ 26   9   0 167]\n",
      "svc Accuracy:  0.9554455445544554\n",
      "svc F1:  0.9131099746690245\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.69      0.81        35\n",
      "          1       0.94      1.00      0.97       167\n",
      "\n",
      "avg / total       0.95      0.95      0.94       202\n",
      "\n",
      "[ 24  11   0 167]\n",
      "LR Accuracy:  0.9455445544554455\n",
      "LR F1:  0.890837632031442\n",
      "For name:  m_reilly\n",
      "total sample size before apply threshold:  20\n",
      "Counter({'0000-0001-8029-0084': 17, '0000-0002-5526-8245': 1, '0000-0001-8746-3224': 1, '0000-0003-2506-3190': 1})\n",
      "['0000-0001-8029-0084']\n",
      "Total sample size after apply threshold:  17\n",
      "For name:  d_nguyen\n",
      "total sample size before apply threshold:  25\n",
      "Counter({'0000-0002-4997-555X': 8, '0000-0002-3283-3504': 7, '0000-0001-6420-7308': 3, '0000-0002-6811-5897': 2, '0000-0001-6432-4467': 2, '0000-0002-1694-0617': 1, '0000-0001-7720-3592': 1, '0000-0002-9680-5772': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  r_santos\n",
      "total sample size before apply threshold:  184\n",
      "Counter({'0000-0003-3737-8296': 33, '0000-0002-1577-1663': 29, '0000-0001-5071-443X': 20, '0000-0002-3085-5128': 16, '0000-0001-7720-6806': 13, '0000-0002-7394-7604': 13, '0000-0002-4830-0470': 11, '0000-0001-6182-1708': 8, '0000-0002-7604-5753': 7, '0000-0001-5240-6799': 6, '0000-0003-0126-7420': 6, '0000-0002-8368-8618': 4, '0000-0001-8183-9649': 4, '0000-0001-6071-8100': 4, '0000-0002-0070-5735': 2, '0000-0003-4395-8078': 2, '0000-0001-7922-5357': 1, '0000-0002-9133-2187': 1, '0000-0002-7861-4366': 1, '0000-0002-5431-4756': 1, '0000-0001-6328-8097': 1, '0000-0001-5845-5698': 1})\n",
      "['0000-0003-3737-8296', '0000-0002-4830-0470', '0000-0001-7720-6806', '0000-0001-5071-443X', '0000-0002-1577-1663', '0000-0002-3085-5128', '0000-0002-7394-7604']\n",
      "Total sample size after apply threshold:  135\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(135, 235)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(135, 235)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        33\n",
      "          1       0.88      0.64      0.74        11\n",
      "          2       0.92      0.85      0.88        13\n",
      "          3       0.95      0.90      0.92        20\n",
      "          4       0.74      1.00      0.85        29\n",
      "          5       1.00      0.81      0.90        16\n",
      "          6       1.00      0.85      0.92        13\n",
      "\n",
      "avg / total       0.92      0.90      0.90       135\n",
      "\n",
      "[33  0  0  0  0  0  0  0  7  1  1  2  0  0  0  0 11  0  2  0  0  0  1  0\n",
      " 18  1  0  0  0  0  0  0 29  0  0  0  0  0  0  3 13  0  0  0  0  0  2  0\n",
      " 11]\n",
      "svc Accuracy:  0.9037037037037037\n",
      "svc F1:  0.8865826565164666\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97        33\n",
      "          1       1.00      0.64      0.78        11\n",
      "          2       0.92      0.85      0.88        13\n",
      "          3       0.95      0.90      0.92        20\n",
      "          4       0.78      0.97      0.86        29\n",
      "          5       1.00      0.88      0.93        16\n",
      "          6       1.00      0.92      0.96        13\n",
      "\n",
      "avg / total       0.92      0.91      0.91       135\n",
      "\n",
      "[33  0  0  0  0  0  0  0  7  1  1  2  0  0  0  0 11  0  2  0  0  0  0  0\n",
      " 18  2  0  0  1  0  0  0 28  0  0  0  0  0  0  2 14  0  1  0  0  0  0  0\n",
      " 12]\n",
      "LR Accuracy:  0.9111111111111111\n",
      "LR F1:  0.9009021044315162\n",
      "For name:  f_ferreira\n",
      "total sample size before apply threshold:  224\n",
      "Counter({'0000-0003-0989-2335': 125, '0000-0002-7571-1830': 18, '0000-0002-9160-7355': 18, '0000-0001-5765-576X': 16, '0000-0003-1516-1221': 15, '0000-0003-3326-1250': 12, '0000-0001-9616-295X': 5, '0000-0001-8714-2615': 5, '0000-0001-5177-6237': 4, '0000-0002-8857-2438': 3, '0000-0001-5815-2136': 2, '0000-0001-8818-6521': 1})\n",
      "['0000-0003-0989-2335', '0000-0003-1516-1221', '0000-0002-7571-1830', '0000-0003-3326-1250', '0000-0001-5765-576X', '0000-0002-9160-7355']\n",
      "Total sample size after apply threshold:  204\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(204, 649)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(204, 649)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.99      0.95       125\n",
      "          1       0.71      0.33      0.45        15\n",
      "          2       1.00      1.00      1.00        18\n",
      "          3       1.00      0.92      0.96        12\n",
      "          4       0.88      0.94      0.91        16\n",
      "          5       0.94      0.83      0.88        18\n",
      "\n",
      "avg / total       0.91      0.92      0.91       204\n",
      "\n",
      "[124   1   0   0   0   0   9   5   0   0   0   1   0   0  18   0   0   0\n",
      "   1   0   0  11   0   0   1   0   0   0  15   0   0   1   0   0   2  15]\n",
      "svc Accuracy:  0.9215686274509803\n",
      "svc F1:  0.859392866298237\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      1.00      0.95       125\n",
      "          1       0.83      0.33      0.48        15\n",
      "          2       1.00      1.00      1.00        18\n",
      "          3       1.00      0.92      0.96        12\n",
      "          4       0.88      0.94      0.91        16\n",
      "          5       1.00      0.72      0.84        18\n",
      "\n",
      "avg / total       0.92      0.92      0.91       204\n",
      "\n",
      "[125   0   0   0   0   0  10   5   0   0   0   0   0   0  18   0   0   0\n",
      "   1   0   0  11   0   0   1   0   0   0  15   0   2   1   0   0   2  13]\n",
      "LR Accuracy:  0.9166666666666666\n",
      "LR F1:  0.854580416466812\n",
      "For name:  y_ng\n",
      "total sample size before apply threshold:  19\n",
      "Counter({'0000-0003-4598-1829': 11, '0000-0001-9142-2126': 4, '0000-0002-7140-1616': 2, '0000-0002-4590-3364': 1, '0000-0002-7213-5030': 1})\n",
      "['0000-0003-4598-1829']\n",
      "Total sample size after apply threshold:  11\n",
      "For name:  j_madsen\n",
      "total sample size before apply threshold:  69\n",
      "Counter({'0000-0001-7625-9498': 28, '0000-0003-1664-7645': 24, '0000-0003-1411-9080': 8, '0000-0003-3246-0215': 8, '0000-0002-6874-2970': 1})\n",
      "['0000-0001-7625-9498', '0000-0003-1664-7645']\n",
      "Total sample size after apply threshold:  52\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(52, 211)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(52, 211)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.96      0.98        28\n",
      "          1       0.96      1.00      0.98        24\n",
      "\n",
      "avg / total       0.98      0.98      0.98        52\n",
      "\n",
      "[27  1  0 24]\n",
      "svc Accuracy:  0.9807692307692307\n",
      "svc F1:  0.9807050092764378\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.96      0.96        28\n",
      "          1       0.96      0.96      0.96        24\n",
      "\n",
      "avg / total       0.96      0.96      0.96        52\n",
      "\n",
      "[27  1  1 23]\n",
      "LR Accuracy:  0.9615384615384616\n",
      "LR F1:  0.9613095238095238\n",
      "For name:  d_collins\n",
      "total sample size before apply threshold:  31\n",
      "Counter({'0000-0001-6754-9290': 8, '0000-0002-6248-9644': 7, '0000-0002-3283-0733': 6, '0000-0003-2274-0889': 5, '0000-0003-2484-1640': 2, '0000-0002-8432-7021': 1, '0000-0001-8891-1893': 1, '0000-0002-7981-3586': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  l_davies\n",
      "total sample size before apply threshold:  96\n",
      "Counter({'0000-0001-8801-3559': 62, '0000-0002-0451-8670': 19, '0000-0002-4876-6270': 11, '0000-0002-2986-705X': 4})\n",
      "['0000-0001-8801-3559', '0000-0002-4876-6270', '0000-0002-0451-8670']\n",
      "Total sample size after apply threshold:  92\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(92, 444)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(92, 444)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97        62\n",
      "          1       1.00      0.73      0.84        11\n",
      "          2       1.00      0.95      0.97        19\n",
      "\n",
      "avg / total       0.96      0.96      0.95        92\n",
      "\n",
      "[62  0  0  3  8  0  1  0 18]\n",
      "svc Accuracy:  0.9565217391304348\n",
      "svc F1:  0.9279427453769559\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      1.00      0.95        62\n",
      "          1       1.00      0.55      0.71        11\n",
      "          2       1.00      0.89      0.94        19\n",
      "\n",
      "avg / total       0.93      0.92      0.92        92\n",
      "\n",
      "[62  0  0  5  6  0  2  0 17]\n",
      "LR Accuracy:  0.9239130434782609\n",
      "LR F1:  0.8656305609606013\n",
      "For name:  m_mora\n",
      "total sample size before apply threshold:  131\n",
      "Counter({'0000-0002-5765-2320': 104, '0000-0002-8393-0216': 22, '0000-0002-2979-3601': 4, '0000-0003-0627-6764': 1})\n",
      "['0000-0002-8393-0216', '0000-0002-5765-2320']\n",
      "Total sample size after apply threshold:  126\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(126, 654)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(126, 654)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.69      1.00      0.81        22\n",
      "          1       1.00      0.90      0.95       104\n",
      "\n",
      "avg / total       0.95      0.92      0.93       126\n",
      "\n",
      "[22  0 10 94]\n",
      "svc Accuracy:  0.9206349206349206\n",
      "svc F1:  0.8821548821548821\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.59      0.74        22\n",
      "          1       0.92      1.00      0.96       104\n",
      "\n",
      "avg / total       0.93      0.93      0.92       126\n",
      "\n",
      "[ 13   9   0 104]\n",
      "LR Accuracy:  0.9285714285714286\n",
      "LR F1:  0.8506912442396313\n",
      "For name:  a_fontana\n",
      "total sample size before apply threshold:  203\n",
      "Counter({'0000-0002-6660-5315': 65, '0000-0002-5453-461X': 59, '0000-0002-5391-7520': 44, '0000-0002-8481-1219': 16, '0000-0002-4791-8746': 14, '0000-0003-3820-2823': 3, '0000-0003-1556-2770': 2})\n",
      "['0000-0002-5391-7520', '0000-0002-5453-461X', '0000-0002-4791-8746', '0000-0002-6660-5315', '0000-0002-8481-1219']\n",
      "Total sample size after apply threshold:  198\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(198, 702)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(198, 702)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.89      0.93        44\n",
      "          1       0.79      1.00      0.88        59\n",
      "          2       1.00      0.64      0.78        14\n",
      "          3       1.00      0.89      0.94        65\n",
      "          4       1.00      1.00      1.00        16\n",
      "\n",
      "avg / total       0.93      0.91      0.91       198\n",
      "\n",
      "[39  5  0  0  0  0 59  0  0  0  0  5  9  0  0  1  6  0 58  0  0  0  0  0\n",
      " 16]\n",
      "svc Accuracy:  0.9141414141414141\n",
      "svc F1:  0.9069733140086569\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.98      0.96        44\n",
      "          1       0.88      0.98      0.93        59\n",
      "          2       1.00      0.71      0.83        14\n",
      "          3       1.00      0.92      0.96        65\n",
      "          4       1.00      1.00      1.00        16\n",
      "\n",
      "avg / total       0.95      0.94      0.94       198\n",
      "\n",
      "[43  1  0  0  0  1 58  0  0  0  0  4 10  0  0  2  3  0 60  0  0  0  0  0\n",
      " 16]\n",
      "LR Accuracy:  0.9444444444444444\n",
      "LR F1:  0.9353777777777778\n",
      "For name:  r_chen\n",
      "total sample size before apply threshold:  367\n",
      "Counter({'0000-0002-8371-8629': 179, '0000-0001-6344-1442': 34, '0000-0003-0291-006X': 32, '0000-0001-6892-0602': 32, '0000-0003-3987-033X': 24, '0000-0002-7505-5415': 21, '0000-0003-1455-5093': 20, '0000-0001-9186-6747': 11, '0000-0002-5340-248X': 4, '0000-0002-8237-6612': 3, '0000-0001-6968-4955': 2, '0000-0003-1919-3335': 1, '0000-0003-4581-8204': 1, '0000-0001-8395-4392': 1, '0000-0001-9750-6670': 1, '0000-0003-1298-9381': 1})\n",
      "['0000-0003-0291-006X', '0000-0001-9186-6747', '0000-0001-6892-0602', '0000-0002-8371-8629', '0000-0003-1455-5093', '0000-0003-3987-033X', '0000-0002-7505-5415', '0000-0001-6344-1442']\n",
      "Total sample size after apply threshold:  353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(353, 645)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(353, 645)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.84      0.78        32\n",
      "          1       1.00      0.64      0.78        11\n",
      "          2       0.76      0.78      0.77        32\n",
      "          3       0.91      0.99      0.95       179\n",
      "          4       0.75      0.75      0.75        20\n",
      "          5       0.65      0.54      0.59        24\n",
      "          6       0.81      0.62      0.70        21\n",
      "          7       0.96      0.74      0.83        34\n",
      "\n",
      "avg / total       0.86      0.86      0.85       353\n",
      "\n",
      "[ 27   0   1   2   0   1   1   0   1   7   0   2   0   1   0   0   3   0\n",
      "  25   2   0   2   0   0   1   0   0 177   0   1   0   0   0   0   0   5\n",
      "  15   0   0   0   3   0   4   1   0  13   2   1   2   0   3   1   0   2\n",
      "  13   0   0   0   0   4   5   0   0  25]\n",
      "svc Accuracy:  0.8555240793201133\n",
      "svc F1:  0.7694530039755298\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.81      0.75        32\n",
      "          1       1.00      0.45      0.62        11\n",
      "          2       0.87      0.81      0.84        32\n",
      "          3       0.90      1.00      0.94       179\n",
      "          4       0.94      0.80      0.86        20\n",
      "          5       0.60      0.50      0.55        24\n",
      "          6       0.73      0.52      0.61        21\n",
      "          7       0.93      0.79      0.86        34\n",
      "\n",
      "avg / total       0.85      0.86      0.85       353\n",
      "\n",
      "[ 26   0   0   2   0   2   2   0   0   5   0   5   0   1   0   0   3   0\n",
      "  26   2   0   1   0   0   0   0   0 179   0   0   0   0   1   0   0   2\n",
      "  16   0   0   1   4   0   2   4   0  12   1   1   3   0   2   2   0   3\n",
      "  11   0   0   0   0   4   1   1   1  27]\n",
      "LR Accuracy:  0.8555240793201133\n",
      "LR F1:  0.7550621591777846\n",
      "For name:  s_krause\n",
      "total sample size before apply threshold:  70\n",
      "Counter({'0000-0002-5259-4651': 43, '0000-0003-1943-2703': 11, '0000-0002-8532-4244': 11, '0000-0002-7062-8472': 5})\n",
      "['0000-0003-1943-2703', '0000-0002-5259-4651', '0000-0002-8532-4244']\n",
      "Total sample size after apply threshold:  65\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(65, 330)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(65, 330)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        11\n",
      "          1       0.88      1.00      0.93        43\n",
      "          2       1.00      0.45      0.62        11\n",
      "\n",
      "avg / total       0.92      0.91      0.89        65\n",
      "\n",
      "[11  0  0  0 43  0  0  6  5]\n",
      "svc Accuracy:  0.9076923076923077\n",
      "svc F1:  0.8532608695652174\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.82      0.90        11\n",
      "          1       0.81      1.00      0.90        43\n",
      "          2       1.00      0.27      0.43        11\n",
      "\n",
      "avg / total       0.88      0.85      0.82        65\n",
      "\n",
      "[ 9  2  0  0 43  0  0  8  3]\n",
      "LR Accuracy:  0.8461538461538461\n",
      "LR F1:  0.7414682539682539\n",
      "For name:  t_smith\n",
      "total sample size before apply threshold:  603\n",
      "Counter({'0000-0002-3650-9381': 154, '0000-0003-1673-2954': 113, '0000-0002-2120-2766': 85, '0000-0002-6279-9685': 84, '0000-0003-3528-6793': 65, '0000-0003-4453-9713': 32, '0000-0002-5197-5030': 26, '0000-0002-3945-630X': 10, '0000-0001-7894-6814': 9, '0000-0002-5750-0706': 6, '0000-0002-5495-8906': 4, '0000-0003-3762-6253': 4, '0000-0002-0479-4261': 3, '0000-0003-2389-461X': 2, '0000-0001-6272-8871': 2, '0000-0001-7683-2653': 1, '0000-0002-2104-2264': 1, '0000-0001-9068-4642': 1, '0000-0002-1881-2766': 1})\n",
      "['0000-0002-3945-630X', '0000-0003-4453-9713', '0000-0003-3528-6793', '0000-0002-6279-9685', '0000-0003-1673-2954', '0000-0002-3650-9381', '0000-0002-2120-2766', '0000-0002-5197-5030']\n",
      "Total sample size after apply threshold:  569\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(569, 1071)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(569, 1071)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gao137/intel/intelpython3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/gao137/intel/intelpython3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00        10\n",
      "          1       1.00      0.56      0.72        32\n",
      "          2       0.98      0.72      0.83        65\n",
      "          3       0.54      1.00      0.70        84\n",
      "          4       0.96      0.86      0.91       113\n",
      "          5       0.98      0.93      0.95       154\n",
      "          6       0.99      0.93      0.96        85\n",
      "          7       1.00      0.77      0.87        26\n",
      "\n",
      "avg / total       0.90      0.86      0.86       569\n",
      "\n",
      "[  0   0   0   8   1   1   0   0   0  18   0  12   1   1   0   0   0   0\n",
      "  47  17   0   0   1   0   0   0   0  84   0   0   0   0   0   0   1  14\n",
      "  97   1   0   0   0   0   0   9   2 143   0   0   0   0   0   6   0   0\n",
      "  79   0   0   0   0   6   0   0   0  20]\n",
      "svc Accuracy:  0.8576449912126538\n",
      "svc F1:  0.742359346431851\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00        10\n",
      "          1       1.00      0.62      0.77        32\n",
      "          2       0.98      0.82      0.89        65\n",
      "          3       0.64      0.90      0.75        84\n",
      "          4       0.94      0.89      0.92       113\n",
      "          5       0.88      0.97      0.92       154\n",
      "          6       0.99      0.94      0.96        85\n",
      "          7       1.00      0.77      0.87        26\n",
      "\n",
      "avg / total       0.88      0.88      0.87       569\n",
      "\n",
      "[  0   0   0   7   2   1   0   0   0  20   1   7   2   2   0   0   0   0\n",
      "  53   9   1   1   1   0   0   0   0  76   1   7   0   0   0   0   0   6\n",
      " 101   6   0   0   0   0   0   5   0 149   0   0   0   0   0   3   0   2\n",
      "  80   0   0   0   0   5   0   1   0  20]\n",
      "LR Accuracy:  0.8769771528998243\n",
      "LR F1:  0.7608331744664307\n",
      "For name:  a_biswas\n",
      "total sample size before apply threshold:  10\n",
      "Counter({'0000-0003-2010-9524': 3, '0000-0002-5828-7230': 3, '0000-0002-0393-6280': 2, '0000-0002-7446-4639': 2})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  j_day\n",
      "total sample size before apply threshold:  16\n",
      "Counter({'0000-0001-9520-3465': 5, '0000-0003-1686-4885': 2, '0000-0001-8681-9831': 2, '0000-0001-6274-9197': 2, '0000-0001-6803-5865': 1, '0000-0003-4324-3486': 1, '0000-0003-1035-2117': 1, '0000-0003-4277-4816': 1, '0000-0003-3133-943X': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  d_truong\n",
      "total sample size before apply threshold:  13\n",
      "Counter({'0000-0003-4946-8969': 7, '0000-0002-1720-1744': 4, '0000-0003-3200-1297': 2})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  s_pan\n",
      "total sample size before apply threshold:  101\n",
      "Counter({'0000-0003-3154-6690': 34, '0000-0002-8247-2110': 12, '0000-0002-1189-4199': 11, '0000-0003-2082-4077': 10, '0000-0001-6451-4666': 10, '0000-0002-7581-1831': 9, '0000-0003-2620-7272': 6, '0000-0001-6565-3836': 5, '0000-0003-0794-527X': 4})\n",
      "['0000-0003-2082-4077', '0000-0001-6451-4666', '0000-0002-1189-4199', '0000-0002-8247-2110', '0000-0003-3154-6690']\n",
      "Total sample size after apply threshold:  77\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(77, 211)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(77, 211)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.80      0.84        10\n",
      "          1       0.67      0.80      0.73        10\n",
      "          2       1.00      0.64      0.78        11\n",
      "          3       0.65      0.92      0.76        12\n",
      "          4       0.91      0.85      0.88        34\n",
      "\n",
      "avg / total       0.85      0.82      0.82        77\n",
      "\n",
      "[ 8  0  0  1  1  0  8  0  0  2  0  1  7  3  0  0  1  0 11  0  1  2  0  2\n",
      " 29]\n",
      "svc Accuracy:  0.8181818181818182\n",
      "svc F1:  0.7969128673302903\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.90      0.95        10\n",
      "          1       1.00      0.80      0.89        10\n",
      "          2       1.00      0.64      0.78        11\n",
      "          3       0.71      1.00      0.83        12\n",
      "          4       0.89      0.94      0.91        34\n",
      "\n",
      "avg / total       0.91      0.88      0.88        77\n",
      "\n",
      "[ 9  0  0  0  1  0  8  0  0  2  0  0  7  3  1  0  0  0 12  0  0  0  0  2\n",
      " 32]\n",
      "LR Accuracy:  0.8831168831168831\n",
      "LR F1:  0.8711814017803128\n",
      "For name:  a_andrade\n",
      "total sample size before apply threshold:  52\n",
      "Counter({'0000-0001-9569-6503': 18, '0000-0002-5689-6606': 13, '0000-0003-4902-8728': 10, '0000-0002-8107-7338': 9, '0000-0002-3540-6858': 1, '0000-0001-7128-3472': 1})\n",
      "['0000-0002-5689-6606', '0000-0003-4902-8728', '0000-0001-9569-6503']\n",
      "Total sample size after apply threshold:  41\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(41, 142)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(41, 142)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.92      0.96        13\n",
      "          1       1.00      1.00      1.00        10\n",
      "          2       0.95      1.00      0.97        18\n",
      "\n",
      "avg / total       0.98      0.98      0.98        41\n",
      "\n",
      "[12  0  1  0 10  0  0  0 18]\n",
      "svc Accuracy:  0.975609756097561\n",
      "svc F1:  0.9776576576576576\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.85      0.92        13\n",
      "          1       1.00      1.00      1.00        10\n",
      "          2       0.90      1.00      0.95        18\n",
      "\n",
      "avg / total       0.96      0.95      0.95        41\n",
      "\n",
      "[11  0  2  0 10  0  0  0 18]\n",
      "LR Accuracy:  0.9512195121951219\n",
      "LR F1:  0.9546783625730993\n",
      "For name:  t_oliveira\n",
      "total sample size before apply threshold:  41\n",
      "Counter({'0000-0002-2654-0879': 17, '0000-0003-0843-7541': 11, '0000-0003-0509-0562': 9, '0000-0001-7040-7189': 1, '0000-0003-3947-1881': 1, '0000-0002-9200-3625': 1, '0000-0001-6055-058X': 1})\n",
      "['0000-0003-0843-7541', '0000-0002-2654-0879']\n",
      "Total sample size after apply threshold:  28\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(28, 156)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(28, 156)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      1.00      0.96        11\n",
      "          1       1.00      0.94      0.97        17\n",
      "\n",
      "avg / total       0.97      0.96      0.96        28\n",
      "\n",
      "[11  0  1 16]\n",
      "svc Accuracy:  0.9642857142857143\n",
      "svc F1:  0.9631093544137023\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      1.00      0.96        11\n",
      "          1       1.00      0.94      0.97        17\n",
      "\n",
      "avg / total       0.97      0.96      0.96        28\n",
      "\n",
      "[11  0  1 16]\n",
      "LR Accuracy:  0.9642857142857143\n",
      "LR F1:  0.9631093544137023\n",
      "For name:  n_romano\n",
      "total sample size before apply threshold:  11\n",
      "Counter({'0000-0003-2765-4912': 7, '0000-0002-9541-8885': 2, '0000-0002-6105-1827': 1, '0000-0001-7276-6994': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  t_hara\n",
      "total sample size before apply threshold:  23\n",
      "Counter({'0000-0003-2668-6218': 15, '0000-0003-0450-6829': 6, '0000-0002-6565-0720': 1, '0000-0002-0235-238X': 1})\n",
      "['0000-0003-2668-6218']\n",
      "Total sample size after apply threshold:  15\n",
      "For name:  t_wong\n",
      "total sample size before apply threshold:  14\n",
      "Counter({'0000-0002-1045-2698': 9, '0000-0002-5752-7917': 2, '0000-0001-9234-4529': 1, '0000-0001-6187-8851': 1, '0000-0001-8611-4911': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  s_ross\n",
      "total sample size before apply threshold:  25\n",
      "Counter({'0000-0002-2302-8415': 17, '0000-0001-7305-3451': 3, '0000-0002-3094-3769': 2, '0000-0003-3512-9579': 1, '0000-0001-5676-4489': 1, '0000-0001-5523-2376': 1})\n",
      "['0000-0002-2302-8415']\n",
      "Total sample size after apply threshold:  17\n",
      "For name:  d_richardson\n",
      "total sample size before apply threshold:  456\n",
      "Counter({'0000-0003-0960-6415': 231, '0000-0002-7751-1058': 167, '0000-0002-3992-8610': 22, '0000-0003-0247-9118': 17, '0000-0002-3189-2190': 12, '0000-0002-0054-6850': 7})\n",
      "['0000-0002-3189-2190', '0000-0003-0960-6415', '0000-0002-7751-1058', '0000-0002-3992-8610', '0000-0003-0247-9118']\n",
      "Total sample size after apply threshold:  449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(449, 1208)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(449, 1208)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.67      0.80        12\n",
      "          1       0.88      1.00      0.94       231\n",
      "          2       0.99      0.89      0.94       167\n",
      "          3       1.00      0.82      0.90        22\n",
      "          4       1.00      0.65      0.79        17\n",
      "\n",
      "avg / total       0.93      0.93      0.92       449\n",
      "\n",
      "[  8   4   0   0   0   0 231   0   0   0   0  19 148   0   0   0   4   0\n",
      "  18   0   0   5   1   0  11]\n",
      "svc Accuracy:  0.9265033407572383\n",
      "svc F1:  0.8715291637077114\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.50      0.67        12\n",
      "          1       0.88      1.00      0.94       231\n",
      "          2       0.99      0.90      0.94       167\n",
      "          3       1.00      0.73      0.84        22\n",
      "          4       1.00      0.71      0.83        17\n",
      "\n",
      "avg / total       0.93      0.92      0.92       449\n",
      "\n",
      "[  6   5   1   0   0   0 231   0   0   0   0  17 150   0   0   0   6   0\n",
      "  16   0   0   4   1   0  12]\n",
      "LR Accuracy:  0.9242761692650334\n",
      "LR F1:  0.842403936051849\n",
      "For name:  j_moraes\n",
      "total sample size before apply threshold:  26\n",
      "Counter({'0000-0002-5766-6802': 13, '0000-0002-8563-6432': 7, '0000-0002-4490-8307': 4, '0000-0002-3067-5194': 2})\n",
      "['0000-0002-5766-6802']\n",
      "Total sample size after apply threshold:  13\n",
      "For name:  e_moreno\n",
      "total sample size before apply threshold:  83\n",
      "Counter({'0000-0002-2309-4826': 26, '0000-0001-5040-452X': 21, '0000-0001-9490-7030': 14, '0000-0002-8434-2483': 8, '0000-0003-0491-7951': 5, '0000-0002-2301-4558': 4, '0000-0002-7197-5679': 3, '0000-0001-8520-8086': 1, '0000-0002-2733-0267': 1})\n",
      "['0000-0002-2309-4826', '0000-0001-9490-7030', '0000-0001-5040-452X']\n",
      "Total sample size after apply threshold:  61\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(61, 143)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(61, 143)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.85      0.92        26\n",
      "          1       1.00      0.86      0.92        14\n",
      "          2       0.78      1.00      0.88        21\n",
      "\n",
      "avg / total       0.92      0.90      0.90        61\n",
      "\n",
      "[22  0  4  0 12  2  0  0 21]\n",
      "svc Accuracy:  0.9016393442622951\n",
      "svc F1:  0.9049145299145299\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.92      0.96        26\n",
      "          1       1.00      0.86      0.92        14\n",
      "          2       0.84      1.00      0.91        21\n",
      "\n",
      "avg / total       0.94      0.93      0.94        61\n",
      "\n",
      "[24  0  2  0 12  2  0  0 21]\n",
      "LR Accuracy:  0.9344262295081968\n",
      "LR F1:  0.9320401337792642\n",
      "For name:  r_little\n",
      "total sample size before apply threshold:  4\n",
      "Counter({'0000-0002-4000-946X': 2, '0000-0002-7732-157X': 1, '0000-0003-1870-3241': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  t_kobayashi\n",
      "total sample size before apply threshold:  150\n",
      "Counter({'0000-0002-4008-454X': 85, '0000-0002-0237-3623': 22, '0000-0002-2738-373X': 10, '0000-0002-7650-1763': 10, '0000-0002-0903-6259': 6, '0000-0002-9202-7643': 5, '0000-0001-7297-8524': 5, '0000-0002-6952-8669': 4, '0000-0003-0963-2525': 2, '0000-0003-4264-5117': 1})\n",
      "['0000-0002-4008-454X', '0000-0002-2738-373X', '0000-0002-0237-3623', '0000-0002-7650-1763']\n",
      "Total sample size after apply threshold:  127\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(127, 325)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(127, 325)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.94      0.94        85\n",
      "          1       1.00      0.70      0.82        10\n",
      "          2       0.71      0.91      0.80        22\n",
      "          3       1.00      0.60      0.75        10\n",
      "\n",
      "avg / total       0.90      0.89      0.89       127\n",
      "\n",
      "[80  0  5  0  2  7  1  0  2  0 20  0  2  0  2  6]\n",
      "svc Accuracy:  0.889763779527559\n",
      "svc F1:  0.8273004815961472\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.99      0.90        85\n",
      "          1       1.00      0.70      0.82        10\n",
      "          2       0.93      0.59      0.72        22\n",
      "          3       1.00      0.40      0.57        10\n",
      "\n",
      "avg / total       0.87      0.85      0.84       127\n",
      "\n",
      "[84  0  1  0  3  7  0  0  9  0 13  0  6  0  0  4]\n",
      "LR Accuracy:  0.8503937007874016\n",
      "LR F1:  0.7538939818351583\n",
      "For name:  a_lin\n",
      "total sample size before apply threshold:  46\n",
      "Counter({'0000-0003-4236-7233': 27, '0000-0001-6310-9765': 10, '0000-0001-9783-1270': 5, '0000-0003-0072-612X': 3, '0000-0001-8545-2222': 1})\n",
      "['0000-0003-4236-7233', '0000-0001-6310-9765']\n",
      "Total sample size after apply threshold:  37\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(37, 55)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(37, 55)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      1.00      0.95        27\n",
      "          1       1.00      0.70      0.82        10\n",
      "\n",
      "avg / total       0.93      0.92      0.91        37\n",
      "\n",
      "[27  0  3  7]\n",
      "svc Accuracy:  0.918918918918919\n",
      "svc F1:  0.8854489164086687\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      1.00      0.92        27\n",
      "          1       1.00      0.50      0.67        10\n",
      "\n",
      "avg / total       0.89      0.86      0.85        37\n",
      "\n",
      "[27  0  5  5]\n",
      "LR Accuracy:  0.8648648648648649\n",
      "LR F1:  0.7909604519774012\n",
      "For name:  a_miranda\n",
      "total sample size before apply threshold:  70\n",
      "Counter({'0000-0001-6998-5686': 48, '0000-0001-5807-5820': 11, '0000-0003-3957-6288': 4, '0000-0003-4964-2197': 2, '0000-0002-9066-6935': 2, '0000-0003-4872-0632': 2, '0000-0002-7297-9639': 1})\n",
      "['0000-0001-5807-5820', '0000-0001-6998-5686']\n",
      "Total sample size after apply threshold:  59\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(59, 586)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(59, 586)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        11\n",
      "          1       1.00      1.00      1.00        48\n",
      "\n",
      "avg / total       1.00      1.00      1.00        59\n",
      "\n",
      "[11  0  0 48]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        11\n",
      "          1       1.00      1.00      1.00        48\n",
      "\n",
      "avg / total       1.00      1.00      1.00        59\n",
      "\n",
      "[11  0  0 48]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  h_vogel\n",
      "total sample size before apply threshold:  15\n",
      "Counter({'0000-0001-9821-7731': 5, '0000-0002-9902-8120': 4, '0000-0003-2404-9485': 4, '0000-0003-0072-4239': 2})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  m_campos\n",
      "total sample size before apply threshold:  148\n",
      "Counter({'0000-0001-7738-9892': 107, '0000-0003-3217-9001': 12, '0000-0003-4313-7069': 8, '0000-0003-1012-6240': 6, '0000-0002-0883-0610': 5, '0000-0002-5233-3769': 5, '0000-0003-4683-0176': 3, '0000-0002-9516-6526': 2})\n",
      "['0000-0001-7738-9892', '0000-0003-3217-9001']\n",
      "Total sample size after apply threshold:  119\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(119, 260)\n",
      "(0, 0)\n",
      "(0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(119, 260)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       107\n",
      "          1       1.00      1.00      1.00        12\n",
      "\n",
      "avg / total       1.00      1.00      1.00       119\n",
      "\n",
      "[107   0   0  12]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99       107\n",
      "          1       1.00      0.83      0.91        12\n",
      "\n",
      "avg / total       0.98      0.98      0.98       119\n",
      "\n",
      "[107   0   2  10]\n",
      "LR Accuracy:  0.9831932773109243\n",
      "LR F1:  0.9499158249158248\n",
      "For name:  d_stewart\n",
      "total sample size before apply threshold:  294\n",
      "Counter({'0000-0002-8157-7746': 210, '0000-0001-7360-8592': 77, '0000-0002-6764-4842': 3, '0000-0002-8499-7105': 1, '0000-0002-4087-5544': 1, '0000-0001-5144-1234': 1, '0000-0002-3690-9844': 1})\n",
      "['0000-0001-7360-8592', '0000-0002-8157-7746']\n",
      "Total sample size after apply threshold:  287\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(287, 519)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(287, 519)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.88      0.94        77\n",
      "          1       0.96      1.00      0.98       210\n",
      "\n",
      "avg / total       0.97      0.97      0.97       287\n",
      "\n",
      "[ 68   9   0 210]\n",
      "svc Accuracy:  0.9686411149825784\n",
      "svc F1:  0.9584760067518688\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.86      0.92        77\n",
      "          1       0.95      1.00      0.97       210\n",
      "\n",
      "avg / total       0.96      0.96      0.96       287\n",
      "\n",
      "[ 66  11   0 210]\n",
      "LR Accuracy:  0.9616724738675958\n",
      "LR F1:  0.948777440656791\n",
      "For name:  j_abrantes\n",
      "total sample size before apply threshold:  57\n",
      "Counter({'0000-0002-8391-7134': 42, '0000-0003-1902-9017': 11, '0000-0003-4585-9831': 4})\n",
      "['0000-0003-1902-9017', '0000-0002-8391-7134']\n",
      "Total sample size after apply threshold:  53\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(53, 118)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(53, 118)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      1.00      0.96        11\n",
      "          1       1.00      0.98      0.99        42\n",
      "\n",
      "avg / total       0.98      0.98      0.98        53\n",
      "\n",
      "[11  0  1 41]\n",
      "svc Accuracy:  0.9811320754716981\n",
      "svc F1:  0.9722367731796753\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        11\n",
      "          1       1.00      1.00      1.00        42\n",
      "\n",
      "avg / total       1.00      1.00      1.00        53\n",
      "\n",
      "[11  0  0 42]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  j_arroyo\n",
      "total sample size before apply threshold:  109\n",
      "Counter({'0000-0002-1971-1721': 65, '0000-0003-4749-2519': 18, '0000-0002-5992-5011': 10, '0000-0002-5674-6739': 10, '0000-0001-7658-8750': 6})\n",
      "['0000-0003-4749-2519', '0000-0002-5992-5011', '0000-0002-5674-6739', '0000-0002-1971-1721']\n",
      "Total sample size after apply threshold:  103\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(103, 412)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(103, 412)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      1.00      0.86        18\n",
      "          1       1.00      0.70      0.82        10\n",
      "          2       1.00      0.90      0.95        10\n",
      "          3       0.98      0.95      0.97        65\n",
      "\n",
      "avg / total       0.95      0.93      0.93       103\n",
      "\n",
      "[18  0  0  0  2  7  0  1  1  0  9  0  3  0  0 62]\n",
      "svc Accuracy:  0.9320388349514563\n",
      "svc F1:  0.8991976724900486\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.72      0.84        18\n",
      "          1       1.00      0.70      0.82        10\n",
      "          2       1.00      1.00      1.00        10\n",
      "          3       0.89      1.00      0.94        65\n",
      "\n",
      "avg / total       0.93      0.92      0.92       103\n",
      "\n",
      "[13  0  0  5  0  7  0  3  0  0 10  0  0  0  0 65]\n",
      "LR Accuracy:  0.9223300970873787\n",
      "LR F1:  0.9010670186728267\n",
      "For name:  a_giuliani\n",
      "total sample size before apply threshold:  196\n",
      "Counter({'0000-0002-4640-804X': 155, '0000-0003-1710-4933': 36, '0000-0002-4315-1699': 4, '0000-0002-6823-2807': 1})\n",
      "['0000-0003-1710-4933', '0000-0002-4640-804X']\n",
      "Total sample size after apply threshold:  191\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(191, 513)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(191, 513)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.83      0.91        36\n",
      "          1       0.96      1.00      0.98       155\n",
      "\n",
      "avg / total       0.97      0.97      0.97       191\n",
      "\n",
      "[ 30   6   0 155]\n",
      "svc Accuracy:  0.9685863874345549\n",
      "svc F1:  0.9450517836593786\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.72      0.84        36\n",
      "          1       0.94      1.00      0.97       155\n",
      "\n",
      "avg / total       0.95      0.95      0.94       191\n",
      "\n",
      "[ 26  10   0 155]\n",
      "LR Accuracy:  0.9476439790575916\n",
      "LR F1:  0.9037298387096774\n",
      "For name:  f_campos\n",
      "total sample size before apply threshold:  49\n",
      "Counter({'0000-0001-8376-0977': 14, '0000-0002-5948-472X': 12, '0000-0002-1132-3257': 10, '0000-0001-8332-5043': 9, '0000-0001-9826-751X': 2, '0000-0001-5828-2862': 2})\n",
      "['0000-0001-8376-0977', '0000-0002-5948-472X', '0000-0002-1132-3257']\n",
      "Total sample size after apply threshold:  36\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(36, 129)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(36, 129)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.79      0.88        14\n",
      "          1       1.00      0.92      0.96        12\n",
      "          2       0.71      1.00      0.83        10\n",
      "\n",
      "avg / total       0.92      0.89      0.89        36\n",
      "\n",
      "[11  0  3  0 11  1  0  0 10]\n",
      "svc Accuracy:  0.8888888888888888\n",
      "svc F1:  0.8899516908212561\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.79      0.85        14\n",
      "          1       0.92      1.00      0.96        12\n",
      "          2       0.73      0.80      0.76        10\n",
      "\n",
      "avg / total       0.87      0.86      0.86        36\n",
      "\n",
      "[11  0  3  0 12  0  1  1  8]\n",
      "LR Accuracy:  0.8611111111111112\n",
      "LR F1:  0.8560195360195361\n",
      "For name:  a_mitchell\n",
      "total sample size before apply threshold:  436\n",
      "Counter({'0000-0001-6014-598X': 188, '0000-0002-0868-4000': 98, '0000-0002-2463-2956': 65, '0000-0001-8996-1067': 24, '0000-0001-8655-7966': 23, '0000-0002-9946-183X': 20, '0000-0003-1062-0716': 6, '0000-0001-5022-5898': 4, '0000-0003-2001-1738': 4, '0000-0003-0969-1680': 3, '0000-0003-3352-3046': 1})\n",
      "['0000-0002-9946-183X', '0000-0001-6014-598X', '0000-0001-8655-7966', '0000-0002-2463-2956', '0000-0002-0868-4000', '0000-0001-8996-1067']\n",
      "Total sample size after apply threshold:  418\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(418, 1043)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(418, 1043)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.85      0.92        20\n",
      "          1       0.80      0.99      0.89       188\n",
      "          2       1.00      0.74      0.85        23\n",
      "          3       1.00      0.95      0.98        65\n",
      "          4       1.00      0.72      0.84        98\n",
      "          5       0.94      0.71      0.81        24\n",
      "\n",
      "avg / total       0.91      0.89      0.89       418\n",
      "\n",
      "[ 17   3   0   0   0   0   0 187   0   0   0   1   0   6  17   0   0   0\n",
      "   0   3   0  62   0   0   0  27   0   0  71   0   0   7   0   0   0  17]\n",
      "svc Accuracy:  0.8875598086124402\n",
      "svc F1:  0.8805697354533013\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.85      0.92        20\n",
      "          1       0.79      1.00      0.88       188\n",
      "          2       1.00      0.61      0.76        23\n",
      "          3       1.00      0.92      0.96        65\n",
      "          4       1.00      0.77      0.87        98\n",
      "          5       1.00      0.58      0.74        24\n",
      "\n",
      "avg / total       0.91      0.88      0.88       418\n",
      "\n",
      "[ 17   3   0   0   0   0   0 188   0   0   0   0   0   9  14   0   0   0\n",
      "   0   5   0  60   0   0   0  23   0   0  75   0   0  10   0   0   0  14]\n",
      "LR Accuracy:  0.8803827751196173\n",
      "LR F1:  0.8536998186735735\n",
      "For name:  c_murray\n",
      "total sample size before apply threshold:  112\n",
      "Counter({'0000-0002-0951-5700': 41, '0000-0001-6736-1546': 28, '0000-0002-2398-3914': 23, '0000-0002-5499-6857': 15, '0000-0003-4471-0509': 4, '0000-0002-4713-8475': 1})\n",
      "['0000-0001-6736-1546', '0000-0002-2398-3914', '0000-0002-5499-6857', '0000-0002-0951-5700']\n",
      "Total sample size after apply threshold:  107\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(107, 296)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(107, 296)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.71      0.83        28\n",
      "          1       1.00      0.91      0.95        23\n",
      "          2       1.00      0.67      0.80        15\n",
      "          3       0.73      1.00      0.85        41\n",
      "\n",
      "avg / total       0.90      0.86      0.86       107\n",
      "\n",
      "[20  0  0  8  0 21  0  2  0  0 10  5  0  0  0 41]\n",
      "svc Accuracy:  0.8598130841121495\n",
      "svc F1:  0.8583099031552639\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.64      0.78        28\n",
      "          1       0.95      0.87      0.91        23\n",
      "          2       1.00      0.53      0.70        15\n",
      "          3       0.68      1.00      0.81        41\n",
      "\n",
      "avg / total       0.87      0.81      0.81       107\n",
      "\n",
      "[18  1  0  9  0 20  0  3  0  0  8  7  0  0  0 41]\n",
      "LR Accuracy:  0.8130841121495327\n",
      "LR F1:  0.7998082416937347\n",
      "For name:  m_grant\n",
      "total sample size before apply threshold:  39\n",
      "Counter({'0000-0002-1380-2104': 28, '0000-0002-7838-8725': 9, '0000-0003-1003-4071': 1, '0000-0002-0377-2036': 1})\n",
      "['0000-0002-1380-2104']\n",
      "Total sample size after apply threshold:  28\n",
      "For name:  d_scott\n",
      "total sample size before apply threshold:  145\n",
      "Counter({'0000-0001-5226-1972': 65, '0000-0002-6726-2078': 64, '0000-0002-6878-9840': 10, '0000-0003-2230-0090': 2, '0000-0003-4918-2610': 2, '0000-0001-8560-0248': 1, '0000-0002-2592-1522': 1})\n",
      "['0000-0002-6726-2078', '0000-0001-5226-1972', '0000-0002-6878-9840']\n",
      "Total sample size after apply threshold:  139\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(139, 750)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(139, 750)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.98      0.95        64\n",
      "          1       0.98      0.97      0.98        65\n",
      "          2       1.00      0.60      0.75        10\n",
      "\n",
      "avg / total       0.95      0.95      0.95       139\n",
      "\n",
      "[63  1  0  2 63  0  4  0  6]\n",
      "svc Accuracy:  0.9496402877697842\n",
      "svc F1:  0.8913708690330476\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.98      0.96        64\n",
      "          1       0.98      1.00      0.99        65\n",
      "          2       1.00      0.60      0.75        10\n",
      "\n",
      "avg / total       0.97      0.96      0.96       139\n",
      "\n",
      "[63  1  0  0 65  0  4  0  6]\n",
      "LR Accuracy:  0.9640287769784173\n",
      "LR F1:  0.9013994910941476\n",
      "For name:  s_mohan\n",
      "total sample size before apply threshold:  50\n",
      "Counter({'0000-0002-5305-9685': 43, '0000-0002-4797-9565': 4, '0000-0001-5628-2631': 2, '0000-0001-8980-0730': 1})\n",
      "['0000-0002-5305-9685']\n",
      "Total sample size after apply threshold:  43\n",
      "For name:  n_wong\n",
      "total sample size before apply threshold:  24\n",
      "Counter({'0000-0003-3788-8114': 13, '0000-0002-7003-6020': 9, '0000-0003-4393-7541': 1, '0000-0002-5932-1015': 1})\n",
      "['0000-0003-3788-8114']\n",
      "Total sample size after apply threshold:  13\n",
      "For name:  k_anderson\n",
      "total sample size before apply threshold:  171\n",
      "Counter({'0000-0003-1657-2161': 78, '0000-0002-9324-9598': 44, '0000-0001-9843-404X': 22, '0000-0001-5613-5893': 14, '0000-0002-3289-2598': 6, '0000-0003-3927-8117': 4, '0000-0002-1472-3352': 2, '0000-0002-5458-6735': 1})\n",
      "['0000-0001-9843-404X', '0000-0002-9324-9598', '0000-0001-5613-5893', '0000-0003-1657-2161']\n",
      "Total sample size after apply threshold:  158\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(158, 453)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(158, 453)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.82      0.90        22\n",
      "          1       1.00      0.75      0.86        44\n",
      "          2       1.00      0.79      0.88        14\n",
      "          3       0.81      1.00      0.90        78\n",
      "\n",
      "avg / total       0.91      0.89      0.88       158\n",
      "\n",
      "[18  0  0  4  0 33  0 11  0  0 11  3  0  0  0 78]\n",
      "svc Accuracy:  0.8860759493670886\n",
      "svc F1:  0.883423645320197\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.68      0.81        22\n",
      "          1       1.00      0.70      0.83        44\n",
      "          2       1.00      0.71      0.83        14\n",
      "          3       0.76      1.00      0.87        78\n",
      "\n",
      "avg / total       0.88      0.85      0.84       158\n",
      "\n",
      "[15  0  0  7  0 31  0 13  0  0 10  4  0  0  0 78]\n",
      "LR Accuracy:  0.8481012658227848\n",
      "LR F1:  0.8343693693693693\n",
      "For name:  m_king\n",
      "total sample size before apply threshold:  58\n",
      "Counter({'0000-0002-2587-9117': 26, '0000-0001-6030-5154': 13, '0000-0001-9895-7297': 9, '0000-0001-5611-9498': 7, '0000-0002-9558-8622': 2, '0000-0001-7993-8808': 1})\n",
      "['0000-0001-6030-5154', '0000-0002-2587-9117']\n",
      "Total sample size after apply threshold:  39\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(39, 62)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(39, 62)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.92      0.96        13\n",
      "          1       0.96      1.00      0.98        26\n",
      "\n",
      "avg / total       0.98      0.97      0.97        39\n",
      "\n",
      "[12  1  0 26]\n",
      "svc Accuracy:  0.9743589743589743\n",
      "svc F1:  0.9705660377358492\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.92      0.96        13\n",
      "          1       0.96      1.00      0.98        26\n",
      "\n",
      "avg / total       0.98      0.97      0.97        39\n",
      "\n",
      "[12  1  0 26]\n",
      "LR Accuracy:  0.9743589743589743\n",
      "LR F1:  0.9705660377358492\n",
      "For name:  a_srivastava\n",
      "total sample size before apply threshold:  49\n",
      "Counter({'0000-0002-2031-4643': 14, '0000-0002-0211-7814': 13, '0000-0001-9866-8145': 6, '0000-0001-7042-4317': 5, '0000-0001-8340-856X': 3, '0000-0001-9871-5781': 3, '0000-0001-5345-6405': 2, '0000-0002-7046-405X': 1, '0000-0002-4590-7947': 1, '0000-0002-5295-7176': 1})\n",
      "['0000-0002-2031-4643', '0000-0002-0211-7814']\n",
      "Total sample size after apply threshold:  27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(27, 105)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(27, 105)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        14\n",
      "          1       1.00      1.00      1.00        13\n",
      "\n",
      "avg / total       1.00      1.00      1.00        27\n",
      "\n",
      "[14  0  0 13]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      1.00      0.97        14\n",
      "          1       1.00      0.92      0.96        13\n",
      "\n",
      "avg / total       0.97      0.96      0.96        27\n",
      "\n",
      "[14  0  1 12]\n",
      "LR Accuracy:  0.9629629629629629\n",
      "LR F1:  0.9627586206896552\n",
      "For name:  m_scholz\n",
      "total sample size before apply threshold:  42\n",
      "Counter({'0000-0001-8440-6785': 31, '0000-0002-4300-3020': 9, '0000-0001-9887-9831': 2})\n",
      "['0000-0001-8440-6785']\n",
      "Total sample size after apply threshold:  31\n",
      "For name:  y_ju\n",
      "total sample size before apply threshold:  27\n",
      "Counter({'0000-0002-5120-6960': 14, '0000-0001-8325-1494': 9, '0000-0003-0103-1207': 3, '0000-0002-5514-4189': 1})\n",
      "['0000-0002-5120-6960']\n",
      "Total sample size after apply threshold:  14\n",
      "For name:  d_stanley\n",
      "total sample size before apply threshold:  6\n",
      "Counter({'0000-0001-9806-5694': 4, '0000-0001-5992-8901': 1, '0000-0001-8948-8409': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  c_nogueira\n",
      "total sample size before apply threshold:  303\n",
      "Counter({'0000-0003-2950-3632': 279, '0000-0002-0853-5304': 16, '0000-0001-8464-0045': 4, '0000-0002-9152-754X': 4})\n",
      "['0000-0002-0853-5304', '0000-0003-2950-3632']\n",
      "Total sample size after apply threshold:  295\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(295, 392)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(295, 392)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.62      0.77        16\n",
      "          1       0.98      1.00      0.99       279\n",
      "\n",
      "avg / total       0.98      0.98      0.98       295\n",
      "\n",
      "[ 10   6   0 279]\n",
      "svc Accuracy:  0.9796610169491525\n",
      "svc F1:  0.8792962356792144\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.31      0.48        16\n",
      "          1       0.96      1.00      0.98       279\n",
      "\n",
      "avg / total       0.96      0.96      0.95       295\n",
      "\n",
      "[  5  11   0 279]\n",
      "LR Accuracy:  0.9627118644067797\n",
      "LR F1:  0.7284291572516528\n",
      "For name:  j_cooper\n",
      "total sample size before apply threshold:  147\n",
      "Counter({'0000-0003-1339-4750': 85, '0000-0001-6009-3542': 24, '0000-0001-8163-2306': 19, '0000-0002-9014-4395': 14, '0000-0002-8626-7827': 4, '0000-0002-4932-1740': 1})\n",
      "['0000-0002-9014-4395', '0000-0001-6009-3542', '0000-0001-8163-2306', '0000-0003-1339-4750']\n",
      "Total sample size after apply threshold:  142\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(142, 549)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(142, 549)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.57      0.73        14\n",
      "          1       1.00      0.71      0.83        24\n",
      "          2       1.00      0.74      0.85        19\n",
      "          3       0.83      1.00      0.90        85\n",
      "\n",
      "avg / total       0.90      0.87      0.87       142\n",
      "\n",
      "[ 8  0  0  6  0 17  0  7  0  0 14  5  0  0  0 85]\n",
      "svc Accuracy:  0.8732394366197183\n",
      "svc F1:  0.8273202968973598\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.43      0.60        14\n",
      "          1       1.00      0.71      0.83        24\n",
      "          2       1.00      0.68      0.81        19\n",
      "          3       0.80      1.00      0.89        85\n",
      "\n",
      "avg / total       0.88      0.85      0.84       142\n",
      "\n",
      "[ 6  0  0  8  0 17  0  7  0  0 13  6  0  0  0 85]\n",
      "LR Accuracy:  0.852112676056338\n",
      "LR F1:  0.7829551621759673\n",
      "For name:  k_lau\n",
      "total sample size before apply threshold:  121\n",
      "Counter({'0000-0003-2125-6841': 81, '0000-0003-3676-9228': 18, '0000-0001-8438-0319': 17, '0000-0002-7713-1928': 4, '0000-0003-2197-5539': 1})\n",
      "['0000-0001-8438-0319', '0000-0003-2125-6841', '0000-0003-3676-9228']\n",
      "Total sample size after apply threshold:  116\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(116, 242)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(116, 242)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.94      0.97        17\n",
      "          1       0.95      1.00      0.98        81\n",
      "          2       1.00      0.83      0.91        18\n",
      "\n",
      "avg / total       0.97      0.97      0.96       116\n",
      "\n",
      "[16  1  0  0 81  0  0  3 15]\n",
      "svc Accuracy:  0.9655172413793104\n",
      "svc F1:  0.9515638310819033\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.76      0.87        17\n",
      "          1       0.91      1.00      0.95        81\n",
      "          2       1.00      0.78      0.88        18\n",
      "\n",
      "avg / total       0.94      0.93      0.93       116\n",
      "\n",
      "[13  4  0  0 81  0  0  4 14]\n",
      "LR Accuracy:  0.9310344827586207\n",
      "LR F1:  0.898202614379085\n",
      "For name:  s_hussein\n",
      "total sample size before apply threshold:  33\n",
      "Counter({'0000-0002-7946-0717': 18, '0000-0002-6305-508X': 9, '0000-0003-3657-7410': 4, '0000-0002-5394-4385': 1, '0000-0002-0139-1483': 1})\n",
      "['0000-0002-7946-0717']\n",
      "Total sample size after apply threshold:  18\n",
      "For name:  z_luo\n",
      "total sample size before apply threshold:  25\n",
      "Counter({'0000-0002-3074-046X': 15, '0000-0002-2719-1025': 5, '0000-0002-8129-333X': 3, '0000-0003-0164-4492': 2})\n",
      "['0000-0002-3074-046X']\n",
      "Total sample size after apply threshold:  15\n",
      "For name:  c_pimentel\n",
      "total sample size before apply threshold:  22\n",
      "Counter({'0000-0002-5158-6414': 16, '0000-0002-1106-8962': 3, '0000-0002-8364-8990': 2, '0000-0002-4932-0174': 1})\n",
      "['0000-0002-5158-6414']\n",
      "Total sample size after apply threshold:  16\n",
      "For name:  s_ito\n",
      "total sample size before apply threshold:  59\n",
      "Counter({'0000-0003-1776-4608': 22, '0000-0001-9310-1852': 18, '0000-0003-1108-1371': 14, '0000-0002-0268-013X': 4, '0000-0002-3635-2580': 1})\n",
      "['0000-0001-9310-1852', '0000-0003-1108-1371', '0000-0003-1776-4608']\n",
      "Total sample size after apply threshold:  54\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(54, 134)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(54, 134)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        18\n",
      "          1       1.00      1.00      1.00        14\n",
      "          2       1.00      1.00      1.00        22\n",
      "\n",
      "avg / total       1.00      1.00      1.00        54\n",
      "\n",
      "[18  0  0  0 14  0  0  0 22]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        18\n",
      "          1       1.00      1.00      1.00        14\n",
      "          2       1.00      1.00      1.00        22\n",
      "\n",
      "avg / total       1.00      1.00      1.00        54\n",
      "\n",
      "[18  0  0  0 14  0  0  0 22]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  f_zhang\n",
      "total sample size before apply threshold:  103\n",
      "Counter({'0000-0001-6035-4829': 27, '0000-0001-7434-7339': 23, '0000-0002-0480-7501': 11, '0000-0001-9542-6634': 10, '0000-0003-1298-9795': 9, '0000-0002-1371-266X': 7, '0000-0002-1957-0543': 5, '0000-0002-2822-2049': 4, '0000-0002-9309-9577': 2, '0000-0003-1709-7788': 2, '0000-0001-7550-9483': 1, '0000-0002-8438-7155': 1, '0000-0003-2829-0735': 1})\n",
      "['0000-0001-7434-7339', '0000-0002-0480-7501', '0000-0001-6035-4829', '0000-0001-9542-6634']\n",
      "Total sample size after apply threshold:  71\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71, 158)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(71, 158)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.96      0.96        23\n",
      "          1       1.00      1.00      1.00        11\n",
      "          2       0.96      0.96      0.96        27\n",
      "          3       1.00      1.00      1.00        10\n",
      "\n",
      "avg / total       0.97      0.97      0.97        71\n",
      "\n",
      "[22  0  1  0  0 11  0  0  1  0 26  0  0  0  0 10]\n",
      "svc Accuracy:  0.971830985915493\n",
      "svc F1:  0.9798711755233495\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98        23\n",
      "          1       1.00      1.00      1.00        11\n",
      "          2       1.00      0.96      0.98        27\n",
      "          3       1.00      1.00      1.00        10\n",
      "\n",
      "avg / total       0.99      0.99      0.99        71\n",
      "\n",
      "[23  0  0  0  0 11  0  0  1  0 26  0  0  0  0 10]\n",
      "LR Accuracy:  0.9859154929577465\n",
      "LR F1:  0.9899638699317543\n",
      "For name:  s_chapman\n",
      "total sample size before apply threshold:  71\n",
      "Counter({'0000-0003-3347-6024': 23, '0000-0003-0053-1584': 23, '0000-0002-4314-9193': 15, '0000-0003-0778-084X': 7, '0000-0003-2342-3383': 3})\n",
      "['0000-0003-3347-6024', '0000-0003-0053-1584', '0000-0002-4314-9193']\n",
      "Total sample size after apply threshold:  61\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(61, 128)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(61, 128)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      1.00      0.92        23\n",
      "          1       1.00      0.96      0.98        23\n",
      "          2       1.00      0.80      0.89        15\n",
      "\n",
      "avg / total       0.94      0.93      0.93        61\n",
      "\n",
      "[23  0  0  1 22  0  3  0 12]\n",
      "svc Accuracy:  0.9344262295081968\n",
      "svc F1:  0.9288888888888889\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      1.00      0.90        23\n",
      "          1       1.00      0.96      0.98        23\n",
      "          2       1.00      0.73      0.85        15\n",
      "\n",
      "avg / total       0.93      0.92      0.92        61\n",
      "\n",
      "[23  0  0  1 22  0  4  0 11]\n",
      "LR Accuracy:  0.9180327868852459\n",
      "LR F1:  0.9086308027484499\n",
      "For name:  j_rosa\n",
      "total sample size before apply threshold:  29\n",
      "Counter({'0000-0003-0857-3746': 15, '0000-0001-7770-5381': 7, '0000-0002-7154-2494': 4, '0000-0001-7947-2681': 2, '0000-0002-0015-6254': 1})\n",
      "['0000-0003-0857-3746']\n",
      "Total sample size after apply threshold:  15\n",
      "For name:  y_yin\n",
      "total sample size before apply threshold:  152\n",
      "Counter({'0000-0003-0218-3042': 127, '0000-0003-1077-810X': 8, '0000-0003-3514-5712': 5, '0000-0003-0963-2672': 5, '0000-0003-0965-4951': 4, '0000-0002-8685-4378': 2, '0000-0001-5821-7497': 1})\n",
      "['0000-0003-0218-3042']\n",
      "Total sample size after apply threshold:  127\n",
      "For name:  p_tavares\n",
      "total sample size before apply threshold:  53\n",
      "Counter({'0000-0002-7398-2661': 29, '0000-0001-7589-1299': 13, '0000-0002-2287-2446': 8, '0000-0001-7832-4134': 3})\n",
      "['0000-0001-7589-1299', '0000-0002-7398-2661']\n",
      "Total sample size after apply threshold:  42\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(42, 144)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(42, 144)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.85      0.92        13\n",
      "          1       0.94      1.00      0.97        29\n",
      "\n",
      "avg / total       0.96      0.95      0.95        42\n",
      "\n",
      "[11  2  0 29]\n",
      "svc Accuracy:  0.9523809523809523\n",
      "svc F1:  0.9416666666666667\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.77      0.83        13\n",
      "          1       0.90      0.97      0.93        29\n",
      "\n",
      "avg / total       0.91      0.90      0.90        42\n",
      "\n",
      "[10  3  1 28]\n",
      "LR Accuracy:  0.9047619047619048\n",
      "LR F1:  0.8833333333333333\n",
      "For name:  a_palma\n",
      "total sample size before apply threshold:  61\n",
      "Counter({'0000-0003-2099-1297': 34, '0000-0002-8530-4913': 13, '0000-0002-5971-3676': 8, '0000-0003-0420-1785': 3, '0000-0002-1682-7032': 2, '0000-0002-7263-4868': 1})\n",
      "['0000-0002-8530-4913', '0000-0003-2099-1297']\n",
      "Total sample size after apply threshold:  47\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(47, 71)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(47, 71)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        13\n",
      "          1       1.00      1.00      1.00        34\n",
      "\n",
      "avg / total       1.00      1.00      1.00        47\n",
      "\n",
      "[13  0  0 34]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        13\n",
      "          1       1.00      1.00      1.00        34\n",
      "\n",
      "avg / total       1.00      1.00      1.00        47\n",
      "\n",
      "[13  0  0 34]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  e_shaw\n",
      "total sample size before apply threshold:  16\n",
      "Counter({'0000-0003-1424-7568': 9, '0000-0002-5653-0145': 4, '0000-0002-4148-3526': 2, '0000-0002-4334-1900': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  m_cameron\n",
      "total sample size before apply threshold:  28\n",
      "Counter({'0000-0001-5788-8790': 17, '0000-0002-2277-7035': 9, '0000-0001-9464-8796': 1, '0000-0002-2508-7718': 1})\n",
      "['0000-0001-5788-8790']\n",
      "Total sample size after apply threshold:  17\n",
      "For name:  a_reid\n",
      "total sample size before apply threshold:  44\n",
      "Counter({'0000-0002-0523-926X': 18, '0000-0003-1752-3302': 18, '0000-0003-4713-2951': 6, '0000-0002-2500-2980': 2})\n",
      "['0000-0002-0523-926X', '0000-0003-1752-3302']\n",
      "Total sample size after apply threshold:  36\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(36, 101)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(36, 101)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.83      0.83        18\n",
      "          1       0.83      0.83      0.83        18\n",
      "\n",
      "avg / total       0.83      0.83      0.83        36\n",
      "\n",
      "[15  3  3 15]\n",
      "svc Accuracy:  0.8333333333333334\n",
      "svc F1:  0.8333333333333334\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.94      0.92        18\n",
      "          1       0.94      0.89      0.91        18\n",
      "\n",
      "avg / total       0.92      0.92      0.92        36\n",
      "\n",
      "[17  1  2 16]\n",
      "LR Accuracy:  0.9166666666666666\n",
      "LR F1:  0.9166023166023166\n",
      "For name:  d_gil\n",
      "total sample size before apply threshold:  60\n",
      "Counter({'0000-0003-3179-1987': 23, '0000-0002-2770-4767': 16, '0000-0003-4241-1302': 16, '0000-0001-8910-2780': 4, '0000-0003-0791-8298': 1})\n",
      "['0000-0002-2770-4767', '0000-0003-3179-1987', '0000-0003-4241-1302']\n",
      "Total sample size after apply threshold:  55\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(55, 176)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(55, 176)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.81      0.90        16\n",
      "          1       0.82      1.00      0.90        23\n",
      "          2       1.00      0.88      0.93        16\n",
      "\n",
      "avg / total       0.93      0.91      0.91        55\n",
      "\n",
      "[13  3  0  0 23  0  0  2 14]\n",
      "svc Accuracy:  0.9090909090909091\n",
      "svc F1:  0.9106152805949966\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.81      0.87        16\n",
      "          1       0.85      1.00      0.92        23\n",
      "          2       1.00      0.88      0.93        16\n",
      "\n",
      "avg / total       0.92      0.91      0.91        55\n",
      "\n",
      "[13  3  0  0 23  0  1  1 14]\n",
      "LR Accuracy:  0.9090909090909091\n",
      "LR F1:  0.9066666666666666\n",
      "For name:  s_morgan\n",
      "total sample size before apply threshold:  83\n",
      "Counter({'0000-0003-4069-3801': 38, '0000-0001-5091-3148': 28, '0000-0002-5340-0652': 7, '0000-0001-9528-8323': 4, '0000-0002-1734-4710': 2, '0000-0002-7529-0028': 2, '0000-0001-7601-3551': 1, '0000-0001-7610-4496': 1})\n",
      "['0000-0003-4069-3801', '0000-0001-5091-3148']\n",
      "Total sample size after apply threshold:  66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(66, 136)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(66, 136)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.95      0.96        38\n",
      "          1       0.93      0.96      0.95        28\n",
      "\n",
      "avg / total       0.96      0.95      0.95        66\n",
      "\n",
      "[36  2  1 27]\n",
      "svc Accuracy:  0.9545454545454546\n",
      "svc F1:  0.9536842105263157\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.95      0.96        38\n",
      "          1       0.93      0.96      0.95        28\n",
      "\n",
      "avg / total       0.96      0.95      0.95        66\n",
      "\n",
      "[36  2  1 27]\n",
      "LR Accuracy:  0.9545454545454546\n",
      "LR F1:  0.9536842105263157\n",
      "For name:  p_ross\n",
      "total sample size before apply threshold:  27\n",
      "Counter({'0000-0001-7105-7117': 14, '0000-0002-5051-5382': 10, '0000-0001-7984-6452': 2, '0000-0001-7645-7523': 1})\n",
      "['0000-0002-5051-5382', '0000-0001-7105-7117']\n",
      "Total sample size after apply threshold:  24\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(24, 92)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(24, 92)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.90      0.95        10\n",
      "          1       0.93      1.00      0.97        14\n",
      "\n",
      "avg / total       0.96      0.96      0.96        24\n",
      "\n",
      "[ 9  1  0 14]\n",
      "svc Accuracy:  0.9583333333333334\n",
      "svc F1:  0.956442831215971\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.90      0.95        10\n",
      "          1       0.93      1.00      0.97        14\n",
      "\n",
      "avg / total       0.96      0.96      0.96        24\n",
      "\n",
      "[ 9  1  0 14]\n",
      "LR Accuracy:  0.9583333333333334\n",
      "LR F1:  0.956442831215971\n",
      "For name:  l_simon\n",
      "total sample size before apply threshold:  14\n",
      "Counter({'0000-0003-4321-8539': 7, '0000-0003-4870-1052': 4, '0000-0002-5010-4778': 2, '0000-0002-0148-4217': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  k_thomas\n",
      "total sample size before apply threshold:  60\n",
      "Counter({'0000-0003-3436-2184': 23, '0000-0003-3355-9583': 23, '0000-0001-8152-9974': 6, '0000-0003-2980-2384': 5, '0000-0001-8836-4631': 3})\n",
      "['0000-0003-3436-2184', '0000-0003-3355-9583']\n",
      "Total sample size after apply threshold:  46\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(46, 66)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(46, 66)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.91      0.95        23\n",
      "          1       0.92      1.00      0.96        23\n",
      "\n",
      "avg / total       0.96      0.96      0.96        46\n",
      "\n",
      "[21  2  0 23]\n",
      "svc Accuracy:  0.9565217391304348\n",
      "svc F1:  0.9564393939393939\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.91      0.95        23\n",
      "          1       0.92      1.00      0.96        23\n",
      "\n",
      "avg / total       0.96      0.96      0.96        46\n",
      "\n",
      "[21  2  0 23]\n",
      "LR Accuracy:  0.9565217391304348\n",
      "LR F1:  0.9564393939393939\n",
      "For name:  l_torres\n",
      "total sample size before apply threshold:  65\n",
      "Counter({'0000-0002-0194-7875': 56, '0000-0002-4598-1899': 7, '0000-0002-2512-1074': 1, '0000-0001-9945-7331': 1})\n",
      "['0000-0002-0194-7875']\n",
      "Total sample size after apply threshold:  56\n",
      "For name:  p_ding\n",
      "total sample size before apply threshold:  17\n",
      "Counter({'0000-0002-3535-6053': 8, '0000-0003-2559-4696': 8, '0000-0002-2613-2496': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  g_morris\n",
      "total sample size before apply threshold:  128\n",
      "Counter({'0000-0003-1731-8405': 50, '0000-0003-2588-6349': 23, '0000-0002-1097-4453': 19, '0000-0001-9893-6648': 16, '0000-0002-3067-3359': 15, '0000-0003-2892-8428': 5})\n",
      "['0000-0001-9893-6648', '0000-0003-2588-6349', '0000-0002-1097-4453', '0000-0002-3067-3359', '0000-0003-1731-8405']\n",
      "Total sample size after apply threshold:  123\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(123, 377)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(123, 377)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.62      0.77        16\n",
      "          1       1.00      0.74      0.85        23\n",
      "          2       0.84      0.84      0.84        19\n",
      "          3       0.91      0.67      0.77        15\n",
      "          4       0.73      0.96      0.83        50\n",
      "\n",
      "avg / total       0.85      0.82      0.82       123\n",
      "\n",
      "[10  0  1  0  5  0 17  0  1  5  0  0 16  0  3  0  0  0 10  5  0  0  2  0\n",
      " 48]\n",
      "svc Accuracy:  0.8211382113821138\n",
      "svc F1:  0.811630601703197\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.62      0.77        16\n",
      "          1       1.00      0.70      0.82        23\n",
      "          2       0.88      0.79      0.83        19\n",
      "          3       1.00      0.60      0.75        15\n",
      "          4       0.70      1.00      0.83        50\n",
      "\n",
      "avg / total       0.86      0.81      0.81       123\n",
      "\n",
      "[10  0  2  0  4  0 16  0  0  7  0  0 15  0  4  0  0  0  9  6  0  0  0  0\n",
      " 50]\n",
      "LR Accuracy:  0.8130081300813008\n",
      "LR F1:  0.7999046408137318\n",
      "For name:  s_andrews\n",
      "total sample size before apply threshold:  60\n",
      "Counter({'0000-0003-4295-2686': 46, '0000-0002-2103-7748': 6, '0000-0002-3851-2197': 3, '0000-0003-0878-1182': 2, '0000-0002-5499-5125': 1, '0000-0003-2174-6728': 1, '0000-0003-4997-3906': 1})\n",
      "['0000-0003-4295-2686']\n",
      "Total sample size after apply threshold:  46\n",
      "For name:  b_yan\n",
      "total sample size before apply threshold:  129\n",
      "Counter({'0000-0001-8802-9606': 93, '0000-0003-4268-4757': 21, '0000-0003-3509-0686': 10, '0000-0001-7235-5554': 4, '0000-0003-2258-2817': 1})\n",
      "['0000-0003-4268-4757', '0000-0001-8802-9606', '0000-0003-3509-0686']\n",
      "Total sample size after apply threshold:  124\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(124, 363)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(124, 363)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.98        21\n",
      "          1       1.00      0.99      0.99        93\n",
      "          2       1.00      1.00      1.00        10\n",
      "\n",
      "avg / total       0.99      0.99      0.99       124\n",
      "\n",
      "[21  0  0  1 92  0  0  0 10]\n",
      "svc Accuracy:  0.9919354838709677\n",
      "svc F1:  0.9904462602137021\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.95      0.95        21\n",
      "          1       0.99      0.99      0.99        93\n",
      "          2       1.00      1.00      1.00        10\n",
      "\n",
      "avg / total       0.98      0.98      0.98       124\n",
      "\n",
      "[20  1  0  1 92  0  0  0 10]\n",
      "LR Accuracy:  0.9838709677419355\n",
      "LR F1:  0.9805427547363031\n",
      "For name:  r_hu\n",
      "total sample size before apply threshold:  128\n",
      "Counter({'0000-0001-6709-031X': 93, '0000-0001-7412-8451': 27, '0000-0001-6893-529X': 4, '0000-0001-5549-3082': 2, '0000-0002-7126-4076': 1, '0000-0001-5921-6891': 1})\n",
      "['0000-0001-6709-031X', '0000-0001-7412-8451']\n",
      "Total sample size after apply threshold:  120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(120, 106)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(120, 106)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99        93\n",
      "          1       1.00      0.93      0.96        27\n",
      "\n",
      "avg / total       0.98      0.98      0.98       120\n",
      "\n",
      "[93  0  2 25]\n",
      "svc Accuracy:  0.9833333333333333\n",
      "svc F1:  0.9754500818330605\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99        93\n",
      "          1       1.00      0.96      0.98        27\n",
      "\n",
      "avg / total       0.99      0.99      0.99       120\n",
      "\n",
      "[93  0  1 26]\n",
      "LR Accuracy:  0.9916666666666667\n",
      "LR F1:  0.9878922409444052\n",
      "For name:  j_braun\n",
      "total sample size before apply threshold:  72\n",
      "Counter({'0000-0002-8886-078X': 37, '0000-0002-4504-6235': 25, '0000-0002-8309-6401': 5, '0000-0002-2491-5788': 5})\n",
      "['0000-0002-8886-078X', '0000-0002-4504-6235']\n",
      "Total sample size after apply threshold:  62\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(62, 157)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(62, 157)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      1.00      0.94        37\n",
      "          1       1.00      0.80      0.89        25\n",
      "\n",
      "avg / total       0.93      0.92      0.92        62\n",
      "\n",
      "[37  0  5 20]\n",
      "svc Accuracy:  0.9193548387096774\n",
      "svc F1:  0.9127988748241913\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      1.00      0.96        37\n",
      "          1       1.00      0.88      0.94        25\n",
      "\n",
      "avg / total       0.96      0.95      0.95        62\n",
      "\n",
      "[37  0  3 22]\n",
      "LR Accuracy:  0.9516129032258065\n",
      "LR F1:  0.9486045869024593\n",
      "For name:  c_he\n",
      "total sample size before apply threshold:  49\n",
      "Counter({'0000-0002-4868-331X': 20, '0000-0002-1918-5186': 13, '0000-0002-0663-275X': 7, '0000-0001-7869-7627': 5, '0000-0001-5426-769X': 2, '0000-0001-9867-9629': 1, '0000-0001-5842-9617': 1})\n",
      "['0000-0002-4868-331X', '0000-0002-1918-5186']\n",
      "Total sample size after apply threshold:  33\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(33, 163)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(33, 163)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.98        20\n",
      "          1       1.00      0.92      0.96        13\n",
      "\n",
      "avg / total       0.97      0.97      0.97        33\n",
      "\n",
      "[20  0  1 12]\n",
      "svc Accuracy:  0.9696969696969697\n",
      "svc F1:  0.9678048780487805\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      1.00      0.95        20\n",
      "          1       1.00      0.85      0.92        13\n",
      "\n",
      "avg / total       0.94      0.94      0.94        33\n",
      "\n",
      "[20  0  2 11]\n",
      "LR Accuracy:  0.9393939393939394\n",
      "LR F1:  0.9345238095238095\n",
      "For name:  w_lu\n",
      "total sample size before apply threshold:  138\n",
      "Counter({'0000-0003-4731-1976': 38, '0000-0001-6722-1527': 33, '0000-0001-5358-305X': 30, '0000-0001-7421-347X': 13, '0000-0002-1405-4806': 6, '0000-0001-9798-8964': 4, '0000-0003-4334-5722': 3, '0000-0002-6570-3044': 3, '0000-0002-5243-5554': 2, '0000-0001-5508-342X': 2, '0000-0002-1398-9933': 1, '0000-0001-6214-4024': 1, '0000-0002-5101-9778': 1, '0000-0002-4528-2246': 1})\n",
      "['0000-0001-5358-305X', '0000-0003-4731-1976', '0000-0001-7421-347X', '0000-0001-6722-1527']\n",
      "Total sample size after apply threshold:  114\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(114, 152)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(114, 152)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.93      0.97        30\n",
      "          1       0.86      0.95      0.90        38\n",
      "          2       0.73      0.85      0.79        13\n",
      "          3       0.97      0.85      0.90        33\n",
      "\n",
      "avg / total       0.91      0.90      0.91       114\n",
      "\n",
      "[28  1  1  0  0 36  1  1  0  2 11  0  0  3  2 28]\n",
      "svc Accuracy:  0.9035087719298246\n",
      "svc F1:  0.8886143333863022\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.93      0.97        30\n",
      "          1       0.86      0.97      0.91        38\n",
      "          2       1.00      0.85      0.92        13\n",
      "          3       0.97      0.94      0.95        33\n",
      "\n",
      "avg / total       0.94      0.94      0.94       114\n",
      "\n",
      "[28  2  0  0  0 37  0  1  0  2 11  0  0  2  0 31]\n",
      "LR Accuracy:  0.9385964912280702\n",
      "LR F1:  0.9374025772014278\n",
      "For name:  r_radhakrishnan\n",
      "total sample size before apply threshold:  62\n",
      "Counter({'0000-0003-0088-4777': 35, '0000-0002-8220-655X': 14, '0000-0001-6616-8525': 7, '0000-0001-7170-699X': 5, '0000-0002-3560-1020': 1})\n",
      "['0000-0003-0088-4777', '0000-0002-8220-655X']\n",
      "Total sample size after apply threshold:  49\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(49, 119)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(49, 119)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.97      0.93        35\n",
      "          1       0.91      0.71      0.80        14\n",
      "\n",
      "avg / total       0.90      0.90      0.89        49\n",
      "\n",
      "[34  1  4 10]\n",
      "svc Accuracy:  0.8979591836734694\n",
      "svc F1:  0.8657534246575342\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.97      0.93        35\n",
      "          1       0.91      0.71      0.80        14\n",
      "\n",
      "avg / total       0.90      0.90      0.89        49\n",
      "\n",
      "[34  1  4 10]\n",
      "LR Accuracy:  0.8979591836734694\n",
      "LR F1:  0.8657534246575342\n",
      "For name:  k_saito\n",
      "total sample size before apply threshold:  61\n",
      "Counter({'0000-0003-4663-1134': 26, '0000-0002-2151-6204': 16, '0000-0002-5726-8775': 11, '0000-0003-2557-1726': 7, '0000-0001-6310-5342': 1})\n",
      "['0000-0002-2151-6204', '0000-0003-4663-1134', '0000-0002-5726-8775']\n",
      "Total sample size after apply threshold:  53\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(53, 189)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(53, 189)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.88      0.93        16\n",
      "          1       0.89      0.92      0.91        26\n",
      "          2       0.67      0.73      0.70        11\n",
      "\n",
      "avg / total       0.88      0.87      0.87        53\n",
      "\n",
      "[14  0  2  0 24  2  0  3  8]\n",
      "svc Accuracy:  0.8679245283018868\n",
      "svc F1:  0.8448819615349558\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.88      0.93        16\n",
      "          1       0.81      1.00      0.90        26\n",
      "          2       1.00      0.64      0.78        11\n",
      "\n",
      "avg / total       0.91      0.89      0.88        53\n",
      "\n",
      "[14  2  0  0 26  0  0  4  7]\n",
      "LR Accuracy:  0.8867924528301887\n",
      "LR F1:  0.8692209450830141\n",
      "For name:  y_wang\n",
      "total sample size before apply threshold:  1689\n",
      "Counter({'0000-0001-8592-0698': 121, '0000-0003-0852-0767': 117, '0000-0002-6227-6112': 69, '0000-0001-5803-5343': 60, '0000-0002-1211-2822': 57, '0000-0002-3063-3066': 55, '0000-0003-2067-382X': 54, '0000-0003-0773-1212': 42, '0000-0002-6574-6706': 40, '0000-0001-9574-2194': 37, '0000-0001-5764-6740': 35, '0000-0001-6046-2934': 31, '0000-0001-8043-5757': 31, '0000-0003-2533-865X': 31, '0000-0001-8619-0455': 30, '0000-0003-0764-2279': 30, '0000-0002-9893-8296': 29, '0000-0001-7076-8312': 29, '0000-0001-5291-9826': 28, '0000-0002-0921-0122': 27, '0000-0003-3557-5085': 26, '0000-0002-0474-4790': 25, '0000-0003-2540-2199': 24, '0000-0003-0513-9039': 22, '0000-0003-3011-1919': 18, '0000-0002-1241-6252': 17, '0000-0002-5845-5150': 17, '0000-0001-9753-5535': 16, '0000-0003-0961-1716': 16, '0000-0001-6321-9542': 15, '0000-0002-0768-1676': 15, '0000-0002-7851-1623': 14, '0000-0003-1360-8931': 14, '0000-0001-7042-9804': 14, '0000-0002-5985-5244': 13, '0000-0001-5716-3183': 13, '0000-0002-7243-441X': 13, '0000-0002-0363-926X': 13, '0000-0001-6790-1311': 12, '0000-0003-0266-0224': 12, '0000-0001-8440-9388': 12, '0000-0002-2110-623X': 11, '0000-0002-2626-478X': 11, '0000-0001-8021-5180': 11, '0000-0001-8697-9165': 11, '0000-0002-1786-5970': 11, '0000-0003-0144-1388': 11, '0000-0002-3002-8069': 10, '0000-0002-6822-4778': 9, '0000-0002-9659-977X': 9, '0000-0002-8601-8302': 9, '0000-0001-9032-9990': 9, '0000-0002-1851-3483': 9, '0000-0002-1255-0937': 9, '0000-0002-7209-585X': 9, '0000-0002-5111-1443': 9, '0000-0002-6295-6492': 8, '0000-0002-4847-6273': 8, '0000-0002-0002-2467': 8, '0000-0002-7389-5066': 8, '0000-0003-2561-1855': 7, '0000-0003-1286-2401': 7, '0000-0002-2900-5126': 7, '0000-0003-3594-2658': 7, '0000-0003-4816-9182': 6, '0000-0001-5580-7766': 6, '0000-0002-0582-0855': 6, '0000-0002-3034-7377': 6, '0000-0002-2188-383X': 6, '0000-0003-1567-3358': 6, '0000-0001-5020-2020': 6, '0000-0001-9997-7636': 5, '0000-0002-6401-7464': 5, '0000-0003-3620-8455': 5, '0000-0002-2532-4832': 5, '0000-0002-3823-2136': 5, '0000-0002-5300-7121': 4, '0000-0002-7986-4500': 4, '0000-0003-3430-2210': 4, '0000-0002-3769-0020': 4, '0000-0001-8925-5277': 4, '0000-0001-6232-0382': 4, '0000-0003-2763-1008': 3, '0000-0001-5231-6283': 3, '0000-0003-3222-0211': 3, '0000-0002-5590-5881': 3, '0000-0002-3729-2743': 3, '0000-0002-1769-1966': 3, '0000-0003-1786-5767': 3, '0000-0003-0708-1950': 2, '0000-0002-1609-2523': 2, '0000-0001-8518-6745': 2, '0000-0001-5495-5839': 2, '0000-0003-1681-9566': 2, '0000-0001-9474-6396': 2, '0000-0001-6108-5157': 2, '0000-0001-5500-1228': 2, '0000-0002-8648-2172': 2, '0000-0002-3184-4201': 2, '0000-0003-3432-0603': 2, '0000-0002-8937-3000': 2, '0000-0002-0676-5886': 2, '0000-0003-1154-820X': 2, '0000-0002-5223-4074': 2, '0000-0001-6264-650X': 2, '0000-0002-6066-2634': 2, '0000-0003-1404-8526': 2, '0000-0003-3928-6926': 2, '0000-0002-5399-2803': 2, '0000-0002-1288-8997': 2, '0000-0001-6085-5615': 2, '0000-0002-3656-4284': 2, '0000-0002-5187-3755': 2, '0000-0002-9628-1382': 2, '0000-0002-2244-1742': 2, '0000-0003-1009-2087': 1, '0000-0001-6823-1225': 1, '0000-0002-5692-3117': 1, '0000-0001-6981-7797': 1, '0000-0001-7956-3102': 1, '0000-0002-2657-7057': 1, '0000-0002-2665-0365': 1, '0000-0002-4336-0474': 1, '0000-0002-7629-4178': 1, '0000-0001-5918-7525': 1, '0000-0002-0891-1517': 1, '0000-0002-9684-1730': 1, '0000-0002-2932-6042': 1, '0000-0001-8538-5998': 1, '0000-0002-4506-4230': 1, '0000-0003-3120-827X': 1, '0000-0002-9640-0871': 1, '0000-0003-3511-0288': 1, '0000-0001-9156-0377': 1, '0000-0002-7281-1908': 1, '0000-0003-2540-5824': 1, '0000-0002-9365-1851': 1, '0000-0002-2333-157X': 1})\n",
      "['0000-0002-0474-4790', '0000-0001-6790-1311', '0000-0002-2110-623X', '0000-0001-5291-9826', '0000-0002-7851-1623', '0000-0002-1241-6252', '0000-0001-5764-6740', '0000-0001-6046-2934', '0000-0002-0921-0122', '0000-0003-0266-0224', '0000-0002-5985-5244', '0000-0003-1360-8931', '0000-0002-9893-8296', '0000-0002-2626-478X', '0000-0001-8021-5180', '0000-0002-5845-5150', '0000-0003-2540-2199', '0000-0001-8619-0455', '0000-0001-6321-9542', '0000-0003-0513-9039', '0000-0002-1211-2822', '0000-0001-8697-9165', '0000-0002-3063-3066', '0000-0001-5716-3183', '0000-0001-8043-5757', '0000-0002-3002-8069', '0000-0001-7076-8312', '0000-0001-9753-5535', '0000-0003-0961-1716', '0000-0001-8592-0698', '0000-0003-0852-0767', '0000-0002-1786-5970', '0000-0003-0764-2279', '0000-0001-5803-5343', '0000-0002-7243-441X', '0000-0003-2067-382X', '0000-0001-7042-9804', '0000-0002-0768-1676', '0000-0002-0363-926X', '0000-0001-9574-2194', '0000-0003-3557-5085', '0000-0002-6574-6706', '0000-0003-0773-1212', '0000-0003-2533-865X', '0000-0003-3011-1919', '0000-0002-6227-6112', '0000-0003-0144-1388', '0000-0001-8440-9388']\n",
      "Total sample size after apply threshold:  1370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(1370, 1958)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(1370, 1958)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.76      0.79        25\n",
      "          1       0.78      0.58      0.67        12\n",
      "          2       0.90      0.82      0.86        11\n",
      "          3       0.70      0.57      0.63        28\n",
      "          4       0.87      0.93      0.90        14\n",
      "          5       1.00      0.94      0.97        17\n",
      "          6       0.83      0.83      0.83        35\n",
      "          7       0.56      0.65      0.60        31\n",
      "          8       0.85      0.81      0.83        27\n",
      "          9       0.38      0.25      0.30        12\n",
      "         10       0.35      0.46      0.40        13\n",
      "         11       1.00      1.00      1.00        14\n",
      "         12       0.76      0.76      0.76        29\n",
      "         13       1.00      0.55      0.71        11\n",
      "         14       0.73      0.73      0.73        11\n",
      "         15       0.89      0.94      0.91        17\n",
      "         16       0.75      0.88      0.81        24\n",
      "         17       0.65      0.67      0.66        30\n",
      "         18       0.71      0.67      0.69        15\n",
      "         19       1.00      0.95      0.98        22\n",
      "         20       0.52      0.56      0.54        57\n",
      "         21       0.50      0.55      0.52        11\n",
      "         22       1.00      0.96      0.98        55\n",
      "         23       1.00      0.54      0.70        13\n",
      "         24       0.45      0.81      0.57        31\n",
      "         25       1.00      0.40      0.57        10\n",
      "         26       1.00      0.86      0.93        29\n",
      "         27       1.00      0.69      0.81        16\n",
      "         28       1.00      0.94      0.97        16\n",
      "         29       0.69      0.69      0.69       121\n",
      "         30       0.47      0.64      0.54       117\n",
      "         31       1.00      0.82      0.90        11\n",
      "         32       0.77      0.90      0.83        30\n",
      "         33       0.93      0.62      0.74        60\n",
      "         34       1.00      0.54      0.70        13\n",
      "         35       0.82      0.78      0.80        54\n",
      "         36       1.00      0.86      0.92        14\n",
      "         37       1.00      0.60      0.75        15\n",
      "         38       0.23      0.54      0.32        13\n",
      "         39       0.94      0.89      0.92        37\n",
      "         40       0.96      1.00      0.98        26\n",
      "         41       0.40      0.42      0.41        40\n",
      "         42       1.00      0.45      0.62        42\n",
      "         43       1.00      0.90      0.95        31\n",
      "         44       0.82      0.78      0.80        18\n",
      "         45       0.79      0.77      0.78        69\n",
      "         46       0.58      0.64      0.61        11\n",
      "         47       1.00      1.00      1.00        12\n",
      "\n",
      "avg / total       0.77      0.73      0.73      1370\n",
      "\n",
      "[19  0  0 ...  0  0 12]\n",
      "svc Accuracy:  0.7255474452554744\n",
      "svc F1:  0.7471865884001243\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.60      0.73        25\n",
      "          1       0.80      0.67      0.73        12\n",
      "          2       1.00      0.91      0.95        11\n",
      "          3       0.88      0.54      0.67        28\n",
      "          4       1.00      0.86      0.92        14\n",
      "          5       1.00      0.94      0.97        17\n",
      "          6       0.86      0.89      0.87        35\n",
      "          7       0.86      0.81      0.83        31\n",
      "          8       0.81      0.78      0.79        27\n",
      "          9       0.67      0.33      0.44        12\n",
      "         10       0.46      0.46      0.46        13\n",
      "         11       1.00      1.00      1.00        14\n",
      "         12       0.79      0.93      0.86        29\n",
      "         13       1.00      0.64      0.78        11\n",
      "         14       0.70      0.64      0.67        11\n",
      "         15       1.00      0.94      0.97        17\n",
      "         16       1.00      0.83      0.91        24\n",
      "         17       0.73      0.80      0.76        30\n",
      "         18       1.00      0.67      0.80        15\n",
      "         19       1.00      0.95      0.98        22\n",
      "         20       0.64      0.65      0.64        57\n",
      "         21       0.86      0.55      0.67        11\n",
      "         22       1.00      0.96      0.98        55\n",
      "         23       1.00      0.54      0.70        13\n",
      "         24       0.50      0.81      0.62        31\n",
      "         25       1.00      0.40      0.57        10\n",
      "         26       1.00      0.83      0.91        29\n",
      "         27       1.00      0.81      0.90        16\n",
      "         28       1.00      0.94      0.97        16\n",
      "         29       0.64      0.89      0.75       121\n",
      "         30       0.60      0.81      0.69       117\n",
      "         31       1.00      0.82      0.90        11\n",
      "         32       0.72      0.93      0.81        30\n",
      "         33       0.93      0.85      0.89        60\n",
      "         34       1.00      0.69      0.82        13\n",
      "         35       0.90      0.87      0.89        54\n",
      "         36       1.00      0.79      0.88        14\n",
      "         37       1.00      0.73      0.85        15\n",
      "         38       0.25      0.08      0.12        13\n",
      "         39       0.87      0.92      0.89        37\n",
      "         40       0.96      0.92      0.94        26\n",
      "         41       0.62      0.50      0.56        40\n",
      "         42       1.00      0.57      0.73        42\n",
      "         43       0.88      0.94      0.91        31\n",
      "         44       0.88      0.83      0.86        18\n",
      "         45       0.76      0.84      0.80        69\n",
      "         46       0.88      0.64      0.74        11\n",
      "         47       1.00      1.00      1.00        12\n",
      "\n",
      "avg / total       0.82      0.79      0.79      1370\n",
      "\n",
      "[15  0  0 ...  0  0 12]\n",
      "LR Accuracy:  0.7927007299270074\n",
      "LR F1:  0.7926508387210277\n",
      "For name:  j_gao\n",
      "total sample size before apply threshold:  222\n",
      "Counter({'0000-0003-3215-7013': 44, '0000-0001-9341-1287': 36, '0000-0001-9778-4312': 26, '0000-0002-6200-4141': 24, '0000-0001-9803-0256': 20, '0000-0001-5732-9905': 14, '0000-0002-4545-1126': 12, '0000-0002-9943-4786': 12, '0000-0002-5739-1781': 11, '0000-0002-3952-208X': 8, '0000-0003-2059-0290': 7, '0000-0002-9959-5600': 2, '0000-0001-6659-5770': 1, '0000-0002-1181-4531': 1, '0000-0003-1160-6553': 1, '0000-0003-2668-6672': 1, '0000-0003-4024-4694': 1, '0000-0002-5977-0021': 1})\n",
      "['0000-0002-6200-4141', '0000-0001-5732-9905', '0000-0001-9341-1287', '0000-0003-3215-7013', '0000-0001-9778-4312', '0000-0002-5739-1781', '0000-0002-4545-1126', '0000-0002-9943-4786', '0000-0001-9803-0256']\n",
      "Total sample size after apply threshold:  199\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(199, 272)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(199, 272)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      0.96      0.82        24\n",
      "          1       0.69      0.64      0.67        14\n",
      "          2       0.74      0.86      0.79        36\n",
      "          3       0.74      0.77      0.76        44\n",
      "          4       0.91      0.77      0.83        26\n",
      "          5       0.71      0.45      0.56        11\n",
      "          6       0.75      0.50      0.60        12\n",
      "          7       1.00      0.92      0.96        12\n",
      "          8       0.72      0.65      0.68        20\n",
      "\n",
      "avg / total       0.77      0.76      0.76       199\n",
      "\n",
      "[23  0  0  1  0  0  0  0  0  0  9  2  1  0  0  1  0  1  3  1 31  0  0  0\n",
      "  1  0  0  1  1  2 34  2  1  0  0  3  1  0  1  4 20  0  0  0  0  1  0  2\n",
      "  3  0  5  0  0  0  2  1  0  2  0  0  6  0  1  0  0  1  0  0  0  0 11  0\n",
      "  1  1  3  1  0  1  0  0 13]\n",
      "svc Accuracy:  0.7638190954773869\n",
      "svc F1:  0.7409048603175223\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.69      0.92      0.79        24\n",
      "          1       0.82      0.64      0.72        14\n",
      "          2       0.79      0.86      0.83        36\n",
      "          3       0.76      0.89      0.82        44\n",
      "          4       0.92      0.92      0.92        26\n",
      "          5       0.75      0.55      0.63        11\n",
      "          6       0.80      0.33      0.47        12\n",
      "          7       1.00      1.00      1.00        12\n",
      "          8       0.87      0.65      0.74        20\n",
      "\n",
      "avg / total       0.81      0.80      0.80       199\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22  1  0  1  0  0  0  0  0  0  9  1  3  0  0  1  0  0  4  1 31  0  0  0\n",
      "  0  0  0  1  0  1 39  2  1  0  0  0  0  0  0  1 24  0  0  0  1  1  0  2\n",
      "  2  0  6  0  0  0  4  0  1  2  0  0  4  0  1  0  0  0  0  0  0  0 12  0\n",
      "  0  0  3  3  0  1  0  0 13]\n",
      "LR Accuracy:  0.8040201005025126\n",
      "LR F1:  0.7690594258396115\n",
      "For name:  d_fernandes\n",
      "total sample size before apply threshold:  40\n",
      "Counter({'0000-0003-0599-3200': 20, '0000-0002-5056-5734': 9, '0000-0001-5263-2737': 5, '0000-0001-6155-6246': 4, '0000-0002-2208-6349': 1, '0000-0003-3466-9450': 1})\n",
      "['0000-0003-0599-3200']\n",
      "Total sample size after apply threshold:  20\n",
      "For name:  c_silva\n",
      "total sample size before apply threshold:  148\n",
      "Counter({'0000-0003-4521-6377': 23, '0000-0001-6348-0505': 16, '0000-0001-6252-8693': 13, '0000-0002-7870-8848': 9, '0000-0002-9310-2457': 9, '0000-0002-1015-5095': 8, '0000-0002-0495-3955': 8, '0000-0002-2357-3405': 7, '0000-0003-1413-8038': 7, '0000-0002-1399-6674': 4, '0000-0002-1439-9214': 3, '0000-0002-9413-4573': 3, '0000-0003-0104-8412': 3, '0000-0003-4331-3755': 3, '0000-0002-5831-2993': 3, '0000-0001-7590-9639': 2, '0000-0002-1196-306X': 2, '0000-0002-1549-6833': 2, '0000-0002-7103-9100': 2, '0000-0002-7092-1169': 2, '0000-0001-6827-8939': 2, '0000-0002-7238-546X': 2, '0000-0002-1771-1517': 2, '0000-0003-1731-7883': 2, '0000-0003-4327-5744': 1, '0000-0002-5656-0061': 1, '0000-0001-6475-6622': 1, '0000-0003-2701-179X': 1, '0000-0001-8172-5860': 1, '0000-0001-9777-8406': 1, '0000-0002-4327-6272': 1, '0000-0002-5077-5176': 1, '0000-0002-7477-1495': 1, '0000-0003-2506-1435': 1, '0000-0002-9148-5458': 1})\n",
      "['0000-0001-6348-0505', '0000-0003-4521-6377', '0000-0001-6252-8693']\n",
      "Total sample size after apply threshold:  52\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(52, 197)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(52, 197)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.88      0.93        16\n",
      "          1       0.88      1.00      0.94        23\n",
      "          2       1.00      0.92      0.96        13\n",
      "\n",
      "avg / total       0.95      0.94      0.94        52\n",
      "\n",
      "[14  2  0  0 23  0  0  1 12]\n",
      "svc Accuracy:  0.9423076923076923\n",
      "svc F1:  0.9440362811791383\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.88      0.93        16\n",
      "          1       0.88      1.00      0.94        23\n",
      "          2       1.00      0.92      0.96        13\n",
      "\n",
      "avg / total       0.95      0.94      0.94        52\n",
      "\n",
      "[14  2  0  0 23  0  0  1 12]\n",
      "LR Accuracy:  0.9423076923076923\n",
      "LR F1:  0.9440362811791383\n",
      "For name:  t_fitzgerald\n",
      "total sample size before apply threshold:  54\n",
      "Counter({'0000-0002-3855-1591': 31, '0000-0002-2370-8496': 21, '0000-0001-9898-1166': 1, '0000-0002-1532-517X': 1})\n",
      "['0000-0002-3855-1591', '0000-0002-2370-8496']\n",
      "Total sample size after apply threshold:  52\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(52, 300)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(52, 300)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.98        31\n",
      "          1       1.00      0.95      0.98        21\n",
      "\n",
      "avg / total       0.98      0.98      0.98        52\n",
      "\n",
      "[31  0  1 20]\n",
      "svc Accuracy:  0.9807692307692307\n",
      "svc F1:  0.9798683701122726\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97        31\n",
      "          1       1.00      0.90      0.95        21\n",
      "\n",
      "avg / total       0.96      0.96      0.96        52\n",
      "\n",
      "[31  0  2 19]\n",
      "LR Accuracy:  0.9615384615384616\n",
      "LR F1:  0.9593750000000001\n",
      "For name:  j_mitchell\n",
      "total sample size before apply threshold:  143\n",
      "Counter({'0000-0002-0379-6097': 57, '0000-0002-8445-0935': 32, '0000-0002-7147-4604': 16, '0000-0002-2361-9805': 14, '0000-0003-4956-1530': 11, '0000-0002-2520-8428': 6, '0000-0001-6785-9352': 3, '0000-0002-0710-5580': 3, '0000-0002-8624-5070': 1})\n",
      "['0000-0002-0379-6097', '0000-0002-8445-0935', '0000-0002-7147-4604', '0000-0003-4956-1530', '0000-0002-2361-9805']\n",
      "Total sample size after apply threshold:  130\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(130, 274)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(130, 274)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      1.00      0.86        57\n",
      "          1       1.00      0.81      0.90        32\n",
      "          2       1.00      0.81      0.90        16\n",
      "          3       1.00      0.55      0.71        11\n",
      "          4       1.00      0.71      0.83        14\n",
      "\n",
      "avg / total       0.89      0.86      0.86       130\n",
      "\n",
      "[57  0  0  0  0  6 26  0  0  0  3  0 13  0  0  5  0  0  6  0  4  0  0  0\n",
      " 10]\n",
      "svc Accuracy:  0.8615384615384616\n",
      "svc F1:  0.8391910996373471\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.69      0.98      0.81        57\n",
      "          1       0.96      0.84      0.90        32\n",
      "          2       1.00      0.69      0.81        16\n",
      "          3       1.00      0.36      0.53        11\n",
      "          4       1.00      0.43      0.60        14\n",
      "\n",
      "avg / total       0.86      0.80      0.79       130\n",
      "\n",
      "[56  1  0  0  0  5 27  0  0  0  5  0 11  0  0  7  0  0  4  0  8  0  0  0\n",
      "  6]\n",
      "LR Accuracy:  0.8\n",
      "LR F1:  0.7319484702093397\n",
      "For name:  a_gomes\n",
      "total sample size before apply threshold:  244\n",
      "Counter({'0000-0002-9819-3036': 44, '0000-0002-0567-064X': 42, '0000-0002-5940-9893': 32, '0000-0001-7883-2446': 20, '0000-0002-8221-6985': 19, '0000-0003-1052-8004': 18, '0000-0002-3348-0448': 16, '0000-0001-9598-1275': 13, '0000-0002-4989-6026': 7, '0000-0003-3976-238X': 6, '0000-0002-6390-9866': 6, '0000-0001-9565-8814': 5, '0000-0003-1998-0291': 5, '0000-0002-1707-9208': 3, '0000-0001-8702-4360': 2, '0000-0002-9793-4816': 1, '0000-0003-0010-2608': 1, '0000-0002-3498-7734': 1, '0000-0001-5466-0272': 1, '0000-0002-3332-834X': 1, '0000-0002-3201-0081': 1})\n",
      "['0000-0002-9819-3036', '0000-0002-5940-9893', '0000-0003-1052-8004', '0000-0002-8221-6985', '0000-0001-9598-1275', '0000-0001-7883-2446', '0000-0002-3348-0448', '0000-0002-0567-064X']\n",
      "Total sample size after apply threshold:  204\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(204, 3913)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(204, 3913)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.69      1.00      0.81        44\n",
      "          1       1.00      1.00      1.00        32\n",
      "          2       0.87      0.72      0.79        18\n",
      "          3       1.00      0.89      0.94        19\n",
      "          4       0.92      0.92      0.92        13\n",
      "          5       1.00      0.90      0.95        20\n",
      "          6       1.00      0.56      0.72        16\n",
      "          7       0.94      0.81      0.87        42\n",
      "\n",
      "avg / total       0.90      0.88      0.88       204\n",
      "\n",
      "[44  0  0  0  0  0  0  0  0 32  0  0  0  0  0  0  4  0 13  0  0  0  0  1\n",
      "  1  0  1 17  0  0  0  0  0  0  1  0 12  0  0  0  2  0  0  0  0 18  0  0\n",
      "  5  0  0  0  1  0  9  1  8  0  0  0  0  0  0 34]\n",
      "svc Accuracy:  0.8774509803921569\n",
      "svc F1:  0.8761722828828091\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      1.00      0.83        44\n",
      "          1       1.00      1.00      1.00        32\n",
      "          2       0.81      0.72      0.76        18\n",
      "          3       1.00      0.89      0.94        19\n",
      "          4       0.93      1.00      0.96        13\n",
      "          5       0.95      0.95      0.95        20\n",
      "          6       1.00      0.44      0.61        16\n",
      "          7       0.97      0.83      0.90        42\n",
      "\n",
      "avg / total       0.91      0.88      0.88       204\n",
      "\n",
      "[44  0  0  0  0  0  0  0  0 32  0  0  0  0  0  0  4  0 13  0  0  1  0  0\n",
      "  1  0  1 17  0  0  0  0  0  0  0  0 13  0  0  0  1  0  0  0  0 19  0  0\n",
      "  5  0  2  0  1  0  7  1  7  0  0  0  0  0  0 35]\n",
      "LR Accuracy:  0.8823529411764706\n",
      "LR F1:  0.8698041898269302\n",
      "For name:  t_weber\n",
      "total sample size before apply threshold:  71\n",
      "Counter({'0000-0002-0494-0484': 29, '0000-0001-8994-1285': 13, '0000-0002-8260-5120': 12, '0000-0003-2931-8963': 10, '0000-0001-8320-361X': 7})\n",
      "['0000-0002-0494-0484', '0000-0001-8994-1285', '0000-0003-2931-8963', '0000-0002-8260-5120']\n",
      "Total sample size after apply threshold:  64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(64, 421)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(64, 421)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      1.00      0.85        29\n",
      "          1       1.00      0.62      0.76        13\n",
      "          2       1.00      0.80      0.89        10\n",
      "          3       1.00      0.75      0.86        12\n",
      "\n",
      "avg / total       0.88      0.84      0.84        64\n",
      "\n",
      "[29  0  0  0  5  8  0  0  2  0  8  0  3  0  0  9]\n",
      "svc Accuracy:  0.84375\n",
      "svc F1:  0.840219421101774\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      1.00      0.85        29\n",
      "          1       1.00      0.62      0.76        13\n",
      "          2       1.00      0.80      0.89        10\n",
      "          3       1.00      0.75      0.86        12\n",
      "\n",
      "avg / total       0.88      0.84      0.84        64\n",
      "\n",
      "[29  0  0  0  5  8  0  0  2  0  8  0  3  0  0  9]\n",
      "LR Accuracy:  0.84375\n",
      "LR F1:  0.840219421101774\n",
      "For name:  j_shim\n",
      "total sample size before apply threshold:  188\n",
      "Counter({'0000-0002-5361-2903': 91, '0000-0003-0167-7307': 36, '0000-0003-1881-8436': 30, '0000-0003-4088-2557': 12, '0000-0002-3974-1290': 6, '0000-0003-4577-1952': 6, '0000-0001-5485-160X': 3, '0000-0003-0101-3076': 2, '0000-0001-9367-2233': 1, '0000-0002-1909-5412': 1})\n",
      "['0000-0003-4088-2557', '0000-0002-5361-2903', '0000-0003-0167-7307', '0000-0003-1881-8436']\n",
      "Total sample size after apply threshold:  169\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(169, 220)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(169, 220)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.92      0.85        12\n",
      "          1       0.98      0.99      0.98        91\n",
      "          2       0.95      1.00      0.97        36\n",
      "          3       1.00      0.83      0.91        30\n",
      "\n",
      "avg / total       0.96      0.96      0.96       169\n",
      "\n",
      "[11  0  1  0  1 90  0  0  0  0 36  0  2  2  1 25]\n",
      "svc Accuracy:  0.9585798816568047\n",
      "svc F1:  0.9279560713986944\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.92      0.96        12\n",
      "          1       0.96      0.99      0.97        91\n",
      "          2       0.94      0.94      0.94        36\n",
      "          3       0.96      0.90      0.93        30\n",
      "\n",
      "avg / total       0.96      0.96      0.96       169\n",
      "\n",
      "[11  0  1  0  0 90  0  1  0  2 34  0  0  2  1 27]\n",
      "LR Accuracy:  0.9585798816568047\n",
      "LR F1:  0.9512434098266181\n",
      "For name:  k_kang\n",
      "total sample size before apply threshold:  128\n",
      "Counter({'0000-0003-2622-9017': 53, '0000-0003-0446-469X': 22, '0000-0002-0457-842X': 12, '0000-0002-8790-9350': 11, '0000-0002-4465-0617': 9, '0000-0001-6374-8356': 8, '0000-0003-3290-1017': 3, '0000-0002-6529-4543': 3, '0000-0002-8428-8288': 3, '0000-0003-1230-3626': 2, '0000-0003-0611-9320': 1, '0000-0001-9135-1890': 1})\n",
      "['0000-0002-0457-842X', '0000-0002-8790-9350', '0000-0003-2622-9017', '0000-0003-0446-469X']\n",
      "Total sample size after apply threshold:  98\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(98, 178)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(98, 178)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.83      0.91        12\n",
      "          1       0.62      0.73      0.67        11\n",
      "          2       0.92      0.92      0.92        53\n",
      "          3       0.64      0.64      0.64        22\n",
      "\n",
      "avg / total       0.83      0.83      0.83        98\n",
      "\n",
      "[10  1  0  1  0  8  0  3  0  0 49  4  0  4  4 14]\n",
      "svc Accuracy:  0.826530612244898\n",
      "svc F1:  0.7841623785020011\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.83      0.91        12\n",
      "          1       0.67      0.73      0.70        11\n",
      "          2       0.84      0.96      0.89        53\n",
      "          3       0.60      0.41      0.49        22\n",
      "\n",
      "avg / total       0.78      0.80      0.78        98\n",
      "\n",
      "[10  1  0  1  0  8  0  3  0  0 51  2  0  3 10  9]\n",
      "LR Accuracy:  0.7959183673469388\n",
      "LR F1:  0.7464916028989256\n",
      "For name:  i_ferreira\n",
      "total sample size before apply threshold:  344\n",
      "Counter({'0000-0003-4910-4882': 166, '0000-0003-1434-0607': 90, '0000-0001-8424-1431': 44, '0000-0001-6552-4479': 19, '0000-0002-8838-0364': 13, '0000-0002-4934-917X': 7, '0000-0002-3164-8227': 3, '0000-0002-5368-9505': 2})\n",
      "['0000-0003-1434-0607', '0000-0001-6552-4479', '0000-0003-4910-4882', '0000-0002-8838-0364', '0000-0001-8424-1431']\n",
      "Total sample size after apply threshold:  332\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(332, 492)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(332, 492)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.93      0.97        90\n",
      "          1       1.00      0.89      0.94        19\n",
      "          2       0.98      0.98      0.98       166\n",
      "          3       1.00      0.85      0.92        13\n",
      "          4       0.78      0.98      0.87        44\n",
      "\n",
      "avg / total       0.96      0.95      0.96       332\n",
      "\n",
      "[ 84   0   0   0   6   0  17   0   0   2   0   0 162   0   4   0   0   2\n",
      "  11   0   0   0   1   0  43]\n",
      "svc Accuracy:  0.9548192771084337\n",
      "svc F1:  0.9348334369847027\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.96      0.98        90\n",
      "          1       1.00      0.95      0.97        19\n",
      "          2       0.92      1.00      0.96       166\n",
      "          3       1.00      0.92      0.96        13\n",
      "          4       0.97      0.80      0.88        44\n",
      "\n",
      "avg / total       0.96      0.95      0.95       332\n",
      "\n",
      "[ 86   0   3   0   1   0  18   1   0   0   0   0 166   0   0   0   0   1\n",
      "  12   0   0   0   9   0  35]\n",
      "LR Accuracy:  0.9548192771084337\n",
      "LR F1:  0.9489566545000072\n",
      "For name:  y_jia\n",
      "total sample size before apply threshold:  46\n",
      "Counter({'0000-0002-2784-1905': 24, '0000-0003-3852-7302': 10, '0000-0002-8852-7557': 3, '0000-0001-9657-0806': 3, '0000-0001-7978-9312': 3, '0000-0001-9395-2139': 2, '0000-0003-4972-1004': 1})\n",
      "['0000-0003-3852-7302', '0000-0002-2784-1905']\n",
      "Total sample size after apply threshold:  34\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(34, 73)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(34, 73)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        10\n",
      "          1       1.00      1.00      1.00        24\n",
      "\n",
      "avg / total       1.00      1.00      1.00        34\n",
      "\n",
      "[10  0  0 24]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        10\n",
      "          1       1.00      1.00      1.00        24\n",
      "\n",
      "avg / total       1.00      1.00      1.00        34\n",
      "\n",
      "[10  0  0 24]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  p_gaspar\n",
      "total sample size before apply threshold:  93\n",
      "Counter({'0000-0003-4217-5717': 87, '0000-0001-5967-0584': 3, '0000-0002-4832-8537': 2, '0000-0003-3388-1724': 1})\n",
      "['0000-0003-4217-5717']\n",
      "Total sample size after apply threshold:  87\n",
      "For name:  r_o'connor\n",
      "total sample size before apply threshold:  82\n",
      "Counter({'0000-0003-4426-2507': 36, '0000-0002-4643-9794': 27, '0000-0002-6869-7954': 13, '0000-0002-3916-3101': 6})\n",
      "['0000-0002-6869-7954', '0000-0003-4426-2507', '0000-0002-4643-9794']\n",
      "Total sample size after apply threshold:  76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(76, 215)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(76, 215)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.85      0.92        13\n",
      "          1       1.00      0.92      0.96        36\n",
      "          2       0.84      1.00      0.92        27\n",
      "\n",
      "avg / total       0.94      0.93      0.94        76\n",
      "\n",
      "[11  0  2  0 33  3  0  0 27]\n",
      "svc Accuracy:  0.9342105263157895\n",
      "svc F1:  0.9294808810284124\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.85      0.92        13\n",
      "          1       1.00      0.94      0.97        36\n",
      "          2       0.87      1.00      0.93        27\n",
      "\n",
      "avg / total       0.95      0.95      0.95        76\n",
      "\n",
      "[11  0  2  0 34  2  0  0 27]\n",
      "LR Accuracy:  0.9473684210526315\n",
      "LR F1:  0.9397099069512862\n",
      "For name:  k_larsen\n",
      "total sample size before apply threshold:  47\n",
      "Counter({'0000-0003-2172-7519': 35, '0000-0002-3918-6645': 6, '0000-0002-1421-6182': 4, '0000-0002-6473-7285': 1, '0000-0003-1182-7727': 1})\n",
      "['0000-0003-2172-7519']\n",
      "Total sample size after apply threshold:  35\n",
      "For name:  s_das\n",
      "total sample size before apply threshold:  197\n",
      "Counter({'0000-0002-1659-2499': 50, '0000-0002-2424-2851': 21, '0000-0002-8394-5303': 17, '0000-0002-5353-0422': 15, '0000-0002-0539-5174': 14, '0000-0003-1185-9366': 13, '0000-0002-2384-3903': 9, '0000-0001-6256-5646': 9, '0000-0002-7066-2128': 6, '0000-0002-8097-6542': 6, '0000-0002-4217-9972': 5, '0000-0001-6470-7302': 4, '0000-0002-5974-7649': 4, '0000-0001-9380-2907': 3, '0000-0002-8628-5128': 2, '0000-0003-0467-0872': 2, '0000-0002-4852-1396': 2, '0000-0003-0745-469X': 2, '0000-0002-9302-7645': 2, '0000-0002-3428-1862': 1, '0000-0001-5339-7708': 1, '0000-0002-0994-8960': 1, '0000-0003-2889-8644': 1, '0000-0003-2161-4784': 1, '0000-0002-0285-8970': 1, '0000-0002-4464-3417': 1, '0000-0002-7336-9568': 1, '0000-0002-3010-6469': 1, '0000-0001-7329-8264': 1, '0000-0002-9896-3520': 1})\n",
      "['0000-0002-0539-5174', '0000-0002-5353-0422', '0000-0002-2424-2851', '0000-0003-1185-9366', '0000-0002-8394-5303', '0000-0002-1659-2499']\n",
      "Total sample size after apply threshold:  130\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(130, 271)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(130, 271)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.93      0.96        14\n",
      "          1       1.00      0.80      0.89        15\n",
      "          2       1.00      0.86      0.92        21\n",
      "          3       1.00      1.00      1.00        13\n",
      "          4       0.94      0.94      0.94        17\n",
      "          5       0.86      0.98      0.92        50\n",
      "\n",
      "avg / total       0.94      0.93      0.93       130\n",
      "\n",
      "[13  0  0  0  0  1  0 12  0  0  0  3  0  0 18  0  0  3  0  0  0 13  0  0\n",
      "  0  0  0  0 16  1  0  0  0  0  1 49]\n",
      "svc Accuracy:  0.9307692307692308\n",
      "svc F1:  0.9386655159973833\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.93      0.96        14\n",
      "          1       1.00      0.80      0.89        15\n",
      "          2       1.00      0.90      0.95        21\n",
      "          3       1.00      1.00      1.00        13\n",
      "          4       0.94      0.94      0.94        17\n",
      "          5       0.88      0.98      0.92        50\n",
      "\n",
      "avg / total       0.94      0.94      0.94       130\n",
      "\n",
      "[13  0  0  0  0  1  0 12  0  0  0  3  0  0 19  0  0  2  0  0  0 13  0  0\n",
      "  0  0  0  0 16  1  0  0  0  0  1 49]\n",
      "LR Accuracy:  0.9384615384615385\n",
      "LR F1:  0.9445927707211467\n",
      "For name:  f_rodriguez\n",
      "total sample size before apply threshold:  6\n",
      "Counter({'0000-0003-4044-8734': 3, '0000-0003-1213-0999': 2, '0000-0003-4053-099X': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  w_peng\n",
      "total sample size before apply threshold:  38\n",
      "Counter({'0000-0001-5093-7115': 14, '0000-0002-4506-0942': 13, '0000-0001-9747-2466': 8, '0000-0003-4917-6851': 3})\n",
      "['0000-0002-4506-0942', '0000-0001-5093-7115']\n",
      "Total sample size after apply threshold:  27\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(27, 101)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(27, 101)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        13\n",
      "          1       1.00      1.00      1.00        14\n",
      "\n",
      "avg / total       1.00      1.00      1.00        27\n",
      "\n",
      "[13  0  0 14]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        13\n",
      "          1       1.00      1.00      1.00        14\n",
      "\n",
      "avg / total       1.00      1.00      1.00        27\n",
      "\n",
      "[13  0  0 14]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  c_torres\n",
      "total sample size before apply threshold:  300\n",
      "Counter({'0000-0003-3709-1690': 237, '0000-0001-8573-0990': 20, '0000-0001-6303-4417': 16, '0000-0001-6786-8769': 15, '0000-0003-3991-0573': 9, '0000-0001-6322-5862': 2, '0000-0002-7908-6884': 1})\n",
      "['0000-0001-8573-0990', '0000-0001-6786-8769', '0000-0003-3709-1690', '0000-0001-6303-4417']\n",
      "Total sample size after apply threshold:  288\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(288, 528)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(288, 528)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.90      0.95        20\n",
      "          1       1.00      0.67      0.80        15\n",
      "          2       0.97      1.00      0.98       237\n",
      "          3       1.00      0.94      0.97        16\n",
      "\n",
      "avg / total       0.97      0.97      0.97       288\n",
      "\n",
      "[ 18   0   2   0   0  10   5   0   0   0 237   0   0   0   1  15]\n",
      "svc Accuracy:  0.9722222222222222\n",
      "svc F1:  0.9246282115407647\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.80      0.89        20\n",
      "          1       1.00      0.67      0.80        15\n",
      "          2       0.95      1.00      0.97       237\n",
      "          3       1.00      0.75      0.86        16\n",
      "\n",
      "avg / total       0.96      0.95      0.95       288\n",
      "\n",
      "[ 16   0   4   0   0  10   5   0   0   0 237   0   0   0   4  12]\n",
      "LR Accuracy:  0.9548611111111112\n",
      "LR F1:  0.8798344252143021\n",
      "For name:  s_rossi\n",
      "total sample size before apply threshold:  199\n",
      "Counter({'0000-0003-3257-8248': 86, '0000-0002-9963-8121': 34, '0000-0002-9919-0494': 25, '0000-0002-8854-7072': 14, '0000-0003-0346-8410': 13, '0000-0002-3278-8993': 10, '0000-0002-2694-9535': 8, '0000-0001-5134-8398': 5, '0000-0001-7048-7158': 1, '0000-0001-8853-0775': 1, '0000-0001-9511-3857': 1, '0000-0001-7479-5756': 1})\n",
      "['0000-0002-8854-7072', '0000-0002-9919-0494', '0000-0003-0346-8410', '0000-0002-9963-8121', '0000-0002-3278-8993', '0000-0003-3257-8248']\n",
      "Total sample size after apply threshold:  182\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(182, 538)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(182, 538)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.86      0.92        14\n",
      "          1       0.92      0.88      0.90        25\n",
      "          2       1.00      0.92      0.96        13\n",
      "          3       0.78      0.94      0.85        34\n",
      "          4       1.00      0.80      0.89        10\n",
      "          5       1.00      0.99      0.99        86\n",
      "\n",
      "avg / total       0.95      0.94      0.94       182\n",
      "\n",
      "[12  0  0  2  0  0  0 22  0  3  0  0  0  0 12  1  0  0  0  2  0 32  0  0\n",
      "  0  0  0  2  8  0  0  0  0  1  0 85]\n",
      "svc Accuracy:  0.9395604395604396\n",
      "svc F1:  0.9195683959593733\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.93      0.96        14\n",
      "          1       1.00      0.88      0.94        25\n",
      "          2       1.00      0.92      0.96        13\n",
      "          3       0.83      1.00      0.91        34\n",
      "          4       1.00      0.90      0.95        10\n",
      "          5       1.00      0.99      0.99        86\n",
      "\n",
      "avg / total       0.97      0.96      0.96       182\n",
      "\n",
      "[13  0  0  1  0  0  0 22  0  3  0  0  0  0 12  1  0  0  0  0  0 34  0  0\n",
      "  0  0  0  1  9  0  0  0  0  1  0 85]\n",
      "LR Accuracy:  0.9615384615384616\n",
      "LR F1:  0.9512200517053074\n",
      "For name:  s_alavi\n",
      "total sample size before apply threshold:  38\n",
      "Counter({'0000-0003-4328-4747': 23, '0000-0003-4009-4921': 14, '0000-0003-1130-3165': 1})\n",
      "['0000-0003-4328-4747', '0000-0003-4009-4921']\n",
      "Total sample size after apply threshold:  37\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(37, 80)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(37, 80)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        23\n",
      "          1       1.00      1.00      1.00        14\n",
      "\n",
      "avg / total       1.00      1.00      1.00        37\n",
      "\n",
      "[23  0  0 14]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        23\n",
      "          1       1.00      1.00      1.00        14\n",
      "\n",
      "avg / total       1.00      1.00      1.00        37\n",
      "\n",
      "[23  0  0 14]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  r_marques\n",
      "total sample size before apply threshold:  41\n",
      "Counter({'0000-0002-6949-0947': 11, '0000-0002-4749-7523': 11, '0000-0002-3125-3911': 8, '0000-0001-6239-5456': 3, '0000-0002-9416-1299': 2, '0000-0001-8261-4409': 1, '0000-0001-6925-041X': 1, '0000-0002-9197-9845': 1, '0000-0002-0672-9260': 1, '0000-0001-8622-9786': 1, '0000-0003-0314-3675': 1})\n",
      "['0000-0002-6949-0947', '0000-0002-4749-7523']\n",
      "Total sample size after apply threshold:  22\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(22, 89)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(22, 89)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      1.00      0.96        11\n",
      "          1       1.00      0.91      0.95        11\n",
      "\n",
      "avg / total       0.96      0.95      0.95        22\n",
      "\n",
      "[11  0  1 10]\n",
      "svc Accuracy:  0.9545454545454546\n",
      "svc F1:  0.9544513457556936\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      1.00      0.96        11\n",
      "          1       1.00      0.91      0.95        11\n",
      "\n",
      "avg / total       0.96      0.95      0.95        22\n",
      "\n",
      "[11  0  1 10]\n",
      "LR Accuracy:  0.9545454545454546\n",
      "LR F1:  0.9544513457556936\n",
      "For name:  m_wheeler\n",
      "total sample size before apply threshold:  163\n",
      "Counter({'0000-0002-7480-7267': 112, '0000-0001-5589-357X': 47, '0000-0002-0319-1987': 3, '0000-0002-7404-7069': 1})\n",
      "['0000-0002-7480-7267', '0000-0001-5589-357X']\n",
      "Total sample size after apply threshold:  159\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(159, 507)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(159, 507)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.96      0.97       112\n",
      "          1       0.92      0.96      0.94        47\n",
      "\n",
      "avg / total       0.96      0.96      0.96       159\n",
      "\n",
      "[108   4   2  45]\n",
      "svc Accuracy:  0.9622641509433962\n",
      "svc F1:  0.9552364864864866\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      1.00      0.96       112\n",
      "          1       1.00      0.79      0.88        47\n",
      "\n",
      "avg / total       0.94      0.94      0.93       159\n",
      "\n",
      "[112   0  10  37]\n",
      "LR Accuracy:  0.9371069182389937\n",
      "LR F1:  0.9191086691086692\n",
      "For name:  l_rasmussen\n",
      "total sample size before apply threshold:  249\n",
      "Counter({'0000-0002-7480-3004': 214, '0000-0002-4497-8049': 24, '0000-0001-6613-2469': 5, '0000-0001-5962-6647': 4, '0000-0001-5795-4794': 1, '0000-0002-7301-3182': 1})\n",
      "['0000-0002-4497-8049', '0000-0002-7480-3004']\n",
      "Total sample size after apply threshold:  238\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(238, 523)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(238, 523)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.79      0.88        24\n",
      "          1       0.98      1.00      0.99       214\n",
      "\n",
      "avg / total       0.98      0.98      0.98       238\n",
      "\n",
      "[ 19   5   0 214]\n",
      "svc Accuracy:  0.9789915966386554\n",
      "svc F1:  0.9360867930608519\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.62      0.77        24\n",
      "          1       0.96      1.00      0.98       214\n",
      "\n",
      "avg / total       0.96      0.96      0.96       238\n",
      "\n",
      "[ 15   9   0 214]\n",
      "LR Accuracy:  0.9621848739495799\n",
      "LR F1:  0.874317901777856\n",
      "For name:  m_saad\n",
      "total sample size before apply threshold:  4\n",
      "Counter({'0000-0003-0458-5942': 1, '0000-0002-8071-2328': 1, '0000-0002-5655-8674': 1, '0000-0003-1291-366X': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  j_carr\n",
      "total sample size before apply threshold:  271\n",
      "Counter({'0000-0002-4398-8237': 179, '0000-0002-6445-2992': 42, '0000-0002-5028-2160': 40, '0000-0002-2729-0920': 6, '0000-0002-9164-4156': 2, '0000-0002-2324-8944': 1, '0000-0002-1080-1472': 1})\n",
      "['0000-0002-6445-2992', '0000-0002-5028-2160', '0000-0002-4398-8237']\n",
      "Total sample size after apply threshold:  261\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(261, 791)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(261, 791)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.93      0.78        42\n",
      "          1       0.94      0.75      0.83        40\n",
      "          2       0.98      0.94      0.96       179\n",
      "\n",
      "avg / total       0.93      0.91      0.91       261\n",
      "\n",
      "[ 39   0   3  10  30   0   9   2 168]\n",
      "svc Accuracy:  0.9080459770114943\n",
      "svc F1:  0.8577777777777778\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.74      0.85        42\n",
      "          1       1.00      0.68      0.81        40\n",
      "          2       0.88      1.00      0.94       179\n",
      "\n",
      "avg / total       0.92      0.91      0.90       261\n",
      "\n",
      "[ 31   0  11   0  27  13   0   0 179]\n",
      "LR Accuracy:  0.9080459770114943\n",
      "LR F1:  0.8641526642053305\n",
      "For name:  j_fraser\n",
      "total sample size before apply threshold:  101\n",
      "Counter({'0000-0002-5080-2859': 38, '0000-0002-6505-1883': 36, '0000-0002-5980-3989': 9, '0000-0003-0111-9137': 6, '0000-0002-8020-2985': 6, '0000-0001-9697-3795': 3, '0000-0003-4941-1997': 3})\n",
      "['0000-0002-6505-1883', '0000-0002-5080-2859']\n",
      "Total sample size after apply threshold:  74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(74, 258)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(74, 258)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.92      0.96        36\n",
      "          1       0.93      1.00      0.96        38\n",
      "\n",
      "avg / total       0.96      0.96      0.96        74\n",
      "\n",
      "[33  3  0 38]\n",
      "svc Accuracy:  0.9594594594594594\n",
      "svc F1:  0.9592735277930655\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.92      0.96        36\n",
      "          1       0.93      1.00      0.96        38\n",
      "\n",
      "avg / total       0.96      0.96      0.96        74\n",
      "\n",
      "[33  3  0 38]\n",
      "LR Accuracy:  0.9594594594594594\n",
      "LR F1:  0.9592735277930655\n",
      "For name:  s_woo\n",
      "total sample size before apply threshold:  25\n",
      "Counter({'0000-0003-3692-7169': 22, '0000-0001-8788-2875': 1, '0000-0001-6765-4322': 1, '0000-0001-6902-0315': 1})\n",
      "['0000-0003-3692-7169']\n",
      "Total sample size after apply threshold:  22\n",
      "For name:  s_bartlett\n",
      "total sample size before apply threshold:  104\n",
      "Counter({'0000-0001-9755-2490': 80, '0000-0003-4387-670X': 18, '0000-0002-7044-4454': 3, '0000-0003-0699-2250': 3})\n",
      "['0000-0003-4387-670X', '0000-0001-9755-2490']\n",
      "Total sample size after apply threshold:  98\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(98, 486)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(98, 486)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.67      0.77        18\n",
      "          1       0.93      0.99      0.96        80\n",
      "\n",
      "avg / total       0.93      0.93      0.92        98\n",
      "\n",
      "[12  6  1 79]\n",
      "svc Accuracy:  0.9285714285714286\n",
      "svc F1:  0.8658846529814272\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.39      0.56        18\n",
      "          1       0.88      1.00      0.94        80\n",
      "\n",
      "avg / total       0.90      0.89      0.87        98\n",
      "\n",
      "[ 7 11  0 80]\n",
      "LR Accuracy:  0.8877551020408163\n",
      "LR F1:  0.7478362573099415\n",
      "For name:  m_lucas\n",
      "total sample size before apply threshold:  75\n",
      "Counter({'0000-0002-3252-0145': 25, '0000-0002-3625-9714': 19, '0000-0001-8672-9940': 15, '0000-0002-5463-0505': 14, '0000-0002-1646-4139': 2})\n",
      "['0000-0002-5463-0505', '0000-0001-8672-9940', '0000-0002-3625-9714', '0000-0002-3252-0145']\n",
      "Total sample size after apply threshold:  73\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(73, 201)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(73, 201)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.93      0.96        14\n",
      "          1       1.00      1.00      1.00        15\n",
      "          2       0.73      1.00      0.84        19\n",
      "          3       1.00      0.76      0.86        25\n",
      "\n",
      "avg / total       0.93      0.90      0.91        73\n",
      "\n",
      "[13  0  1  0  0 15  0  0  0  0 19  0  0  0  6 19]\n",
      "svc Accuracy:  0.9041095890410958\n",
      "svc F1:  0.9177609427609428\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.93      0.96        14\n",
      "          1       1.00      1.00      1.00        15\n",
      "          2       0.83      1.00      0.90        19\n",
      "          3       1.00      0.88      0.94        25\n",
      "\n",
      "avg / total       0.95      0.95      0.95        73\n",
      "\n",
      "[13  0  1  0  0 15  0  0  0  0 19  0  0  0  3 22]\n",
      "LR Accuracy:  0.9452054794520548\n",
      "LR F1:  0.9509737701227063\n",
      "For name:  w_lee\n",
      "total sample size before apply threshold:  590\n",
      "Counter({'0000-0003-3171-7672': 108, '0000-0001-5833-989X': 100, '0000-0003-3231-9764': 82, '0000-0002-1082-7592': 62, '0000-0003-3267-4811': 40, '0000-0001-7805-869X': 36, '0000-0003-2883-0391': 21, '0000-0002-0607-038X': 21, '0000-0002-5461-6770': 16, '0000-0002-3912-6095': 11, '0000-0001-6757-885X': 11, '0000-0001-6408-7668': 10, '0000-0002-9873-1033': 9, '0000-0001-7801-083X': 8, '0000-0001-8430-4797': 7, '0000-0002-2572-7287': 5, '0000-0002-6766-8481': 5, '0000-0001-8706-6026': 4, '0000-0002-0036-2859': 4, '0000-0002-9624-0505': 3, '0000-0002-3413-4029': 3, '0000-0003-1817-8395': 3, '0000-0003-1744-8525': 3, '0000-0001-8052-2420': 2, '0000-0003-0853-8561': 2, '0000-0001-7285-4054': 2, '0000-0001-9645-8179': 2, '0000-0002-4383-756X': 2, '0000-0003-1911-3454': 2, '0000-0003-4333-5444': 1, '0000-0002-7324-5792': 1, '0000-0002-2152-7210': 1, '0000-0003-4040-1100': 1, '0000-0003-0133-9076': 1, '0000-0002-7696-5517': 1})\n",
      "['0000-0001-7805-869X', '0000-0002-3912-6095', '0000-0003-2883-0391', '0000-0001-6408-7668', '0000-0003-3267-4811', '0000-0003-3171-7672', '0000-0003-3231-9764', '0000-0001-5833-989X', '0000-0002-0607-038X', '0000-0002-1082-7592', '0000-0001-6757-885X', '0000-0002-5461-6770']\n",
      "Total sample size after apply threshold:  518\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(518, 550)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(518, 550)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.69      0.75        36\n",
      "          1       0.75      0.27      0.40        11\n",
      "          2       1.00      0.52      0.69        21\n",
      "          3       0.80      0.80      0.80        10\n",
      "          4       0.66      0.72      0.69        40\n",
      "          5       0.61      0.85      0.71       108\n",
      "          6       0.86      0.72      0.78        82\n",
      "          7       0.85      0.93      0.89       100\n",
      "          8       1.00      0.76      0.86        21\n",
      "          9       0.98      0.89      0.93        62\n",
      "         10       0.20      0.09      0.13        11\n",
      "         11       0.91      0.62      0.74        16\n",
      "\n",
      "avg / total       0.80      0.78      0.77       518\n",
      "\n",
      "[25  0  0  0  0  4  2  3  0  0  2  0  0  3  0  0  0  7  0  1  0  0  0  0\n",
      "  0  1 11  0  0  9  0  0  0  0  0  0  0  0  0  8  0  0  0  2  0  0  0  0\n",
      "  0  0  0  0 29 10  1  0  0  0  0  0  0  0  0  0 10 92  6  0  0  0  0  0\n",
      "  0  0  0  0  5 18 59  0  0  0  0  0  4  0  0  2  0  0  0 93  0  1  0  0\n",
      "  0  0  0  0  0  5  0  0 16  0  0  0  0  0  0  0  0  2  0  3  0 55  1  1\n",
      "  0  0  0  0  0  4  1  5  0  0  1  0  2  0  0  0  0  1  0  2  0  0  1 10]\n",
      "svc Accuracy:  0.7760617760617761\n",
      "svc F1:  0.6971796047561218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gao137/intel/intelpython3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/gao137/intel/intelpython3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.56      0.70        36\n",
      "          1       1.00      0.09      0.17        11\n",
      "          2       1.00      0.38      0.55        21\n",
      "          3       0.88      0.70      0.78        10\n",
      "          4       0.70      0.65      0.68        40\n",
      "          5       0.54      0.86      0.66       108\n",
      "          6       0.87      0.71      0.78        82\n",
      "          7       0.86      0.97      0.91       100\n",
      "          8       1.00      0.76      0.86        21\n",
      "          9       0.89      0.90      0.90        62\n",
      "         10       0.00      0.00      0.00        11\n",
      "         11       0.91      0.62      0.74        16\n",
      "\n",
      "avg / total       0.79      0.76      0.74       518\n",
      "\n",
      "[20  0  0  0  0 11  0  3  0  2  0  0  0  1  0  0  0  9  0  1  0  0  0  0\n",
      "  0  0  8  0  0 13  0  0  0  0  0  0  0  0  0  7  0  0  0  2  0  1  0  0\n",
      "  0  0  0  0 26 11  3  0  0  0  0  0  0  0  0  0  9 93  6  0  0  0  0  0\n",
      "  0  0  0  0  2 22 58  0  0  0  0  0  0  0  0  1  0  0  0 97  0  2  0  0\n",
      "  0  0  0  0  0  5  0  0 16  0  0  0  0  0  0  0  0  2  0  3  0 56  0  1\n",
      "  1  0  0  0  0  5  0  5  0  0  0  0  0  0  0  0  0  2  0  2  0  2  0 10]\n",
      "LR Accuracy:  0.7567567567567568\n",
      "LR F1:  0.6438413807878616\n",
      "For name:  j_cheng\n",
      "total sample size before apply threshold:  66\n",
      "Counter({'0000-0003-1786-6188': 19, '0000-0001-8285-3207': 16, '0000-0001-5318-5668': 8, '0000-0002-7004-5138': 6, '0000-0003-3928-1770': 6, '0000-0002-1881-012X': 5, '0000-0002-4364-9657': 3, '0000-0002-1722-2617': 1, '0000-0002-5434-1201': 1, '0000-0001-6065-2682': 1})\n",
      "['0000-0003-1786-6188', '0000-0001-8285-3207']\n",
      "Total sample size after apply threshold:  35\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(35, 76)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(35, 76)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.95      0.97        19\n",
      "          1       0.94      1.00      0.97        16\n",
      "\n",
      "avg / total       0.97      0.97      0.97        35\n",
      "\n",
      "[18  1  0 16]\n",
      "svc Accuracy:  0.9714285714285714\n",
      "svc F1:  0.9713349713349714\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.95      0.97        19\n",
      "          1       0.94      1.00      0.97        16\n",
      "\n",
      "avg / total       0.97      0.97      0.97        35\n",
      "\n",
      "[18  1  0 16]\n",
      "LR Accuracy:  0.9714285714285714\n",
      "LR F1:  0.9713349713349714\n",
      "For name:  g_lewis\n",
      "total sample size before apply threshold:  367\n",
      "Counter({'0000-0001-5205-8245': 343, '0000-0002-2548-8423': 12, '0000-0003-3081-9319': 7, '0000-0003-4112-5048': 5})\n",
      "['0000-0002-2548-8423', '0000-0001-5205-8245']\n",
      "Total sample size after apply threshold:  355\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(355, 707)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(355, 707)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        12\n",
      "          1       1.00      1.00      1.00       343\n",
      "\n",
      "avg / total       1.00      1.00      1.00       355\n",
      "\n",
      "[ 12   0   0 343]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.67      0.80        12\n",
      "          1       0.99      1.00      0.99       343\n",
      "\n",
      "avg / total       0.99      0.99      0.99       355\n",
      "\n",
      "[  8   4   0 343]\n",
      "LR Accuracy:  0.9887323943661972\n",
      "LR F1:  0.8971014492753624\n",
      "For name:  j_albert\n",
      "total sample size before apply threshold:  78\n",
      "Counter({'0000-0002-3420-7371': 40, '0000-0001-6538-9801': 19, '0000-0001-5330-1892': 13, '0000-0002-8256-2650': 6})\n",
      "['0000-0002-3420-7371', '0000-0001-6538-9801', '0000-0001-5330-1892']\n",
      "Total sample size after apply threshold:  72\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(72, 249)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(72, 249)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      1.00      0.96        40\n",
      "          1       1.00      0.89      0.94        19\n",
      "          2       1.00      0.92      0.96        13\n",
      "\n",
      "avg / total       0.96      0.96      0.96        72\n",
      "\n",
      "[40  0  0  2 17  0  1  0 12]\n",
      "svc Accuracy:  0.9583333333333334\n",
      "svc F1:  0.9560999553770638\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.98        40\n",
      "          1       1.00      0.95      0.97        19\n",
      "          2       1.00      0.92      0.96        13\n",
      "\n",
      "avg / total       0.97      0.97      0.97        72\n",
      "\n",
      "[40  0  0  1 18  0  1  0 12]\n",
      "LR Accuracy:  0.9722222222222222\n",
      "LR F1:  0.9695275763568447\n",
      "For name:  k_goh\n",
      "total sample size before apply threshold:  42\n",
      "Counter({'0000-0002-2839-8722': 22, '0000-0002-3623-4891': 5, '0000-0003-0599-9696': 5, '0000-0001-5499-5187': 4, '0000-0002-2367-8303': 3, '0000-0001-5416-9627': 2, '0000-0002-8265-3421': 1})\n",
      "['0000-0002-2839-8722']\n",
      "Total sample size after apply threshold:  22\n",
      "For name:  n_harris\n",
      "total sample size before apply threshold:  14\n",
      "Counter({'0000-0002-1320-282X': 5, '0000-0003-1256-3006': 4, '0000-0002-3443-3643': 2, '0000-0002-1965-6750': 2, '0000-0001-9664-2769': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  s_hill\n",
      "total sample size before apply threshold:  152\n",
      "Counter({'0000-0002-4424-239X': 118, '0000-0002-6474-0214': 12, '0000-0003-3010-8998': 7, '0000-0002-5909-692X': 5, '0000-0002-2995-2596': 4, '0000-0001-8055-860X': 3, '0000-0001-6742-3620': 2, '0000-0002-3305-6954': 1})\n",
      "['0000-0002-4424-239X', '0000-0002-6474-0214']\n",
      "Total sample size after apply threshold:  130\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(130, 182)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(130, 182)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99       118\n",
      "          1       1.00      0.75      0.86        12\n",
      "\n",
      "avg / total       0.98      0.98      0.98       130\n",
      "\n",
      "[118   0   3   9]\n",
      "svc Accuracy:  0.9769230769230769\n",
      "svc F1:  0.9222952779438135\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97       118\n",
      "          1       1.00      0.33      0.50        12\n",
      "\n",
      "avg / total       0.94      0.94      0.92       130\n",
      "\n",
      "[118   0   8   4]\n",
      "LR Accuracy:  0.9384615384615385\n",
      "LR F1:  0.7336065573770492\n",
      "For name:  p_pathak\n",
      "total sample size before apply threshold:  9\n",
      "Counter({'0000-0003-0118-3235': 4, '0000-0002-1157-5550': 3, '0000-0002-9771-6624': 1, '0000-0003-2152-3938': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  h_zeng\n",
      "total sample size before apply threshold:  82\n",
      "Counter({'0000-0002-8246-2000': 42, '0000-0002-0260-1059': 21, '0000-0002-9909-7732': 6, '0000-0002-9150-214X': 6, '0000-0003-0293-7692': 4, '0000-0002-7657-6714': 3})\n",
      "['0000-0002-0260-1059', '0000-0002-8246-2000']\n",
      "Total sample size after apply threshold:  63\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(63, 118)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(63, 118)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.76      0.84        21\n",
      "          1       0.89      0.98      0.93        42\n",
      "\n",
      "avg / total       0.91      0.90      0.90        63\n",
      "\n",
      "[16  5  1 41]\n",
      "svc Accuracy:  0.9047619047619048\n",
      "svc F1:  0.8869617224880382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.76      0.84        21\n",
      "          1       0.89      0.98      0.93        42\n",
      "\n",
      "avg / total       0.91      0.90      0.90        63\n",
      "\n",
      "[16  5  1 41]\n",
      "LR Accuracy:  0.9047619047619048\n",
      "LR F1:  0.8869617224880382\n",
      "For name:  h_liu\n",
      "total sample size before apply threshold:  439\n",
      "Counter({'0000-0001-6715-6366': 100, '0000-0002-0253-647X': 45, '0000-0002-1006-6666': 39, '0000-0001-7639-0904': 39, '0000-0002-7233-1509': 31, '0000-0001-9366-6204': 26, '0000-0002-4723-845X': 18, '0000-0003-3326-2640': 17, '0000-0002-3745-7202': 13, '0000-0003-4837-5373': 11, '0000-0003-3103-6949': 10, '0000-0002-4548-2002': 9, '0000-0003-0266-9472': 9, '0000-0001-7984-6305': 8, '0000-0002-7645-0855': 8, '0000-0003-2394-5421': 7, '0000-0001-5451-6828': 6, '0000-0002-1852-4537': 5, '0000-0003-2183-9609': 3, '0000-0003-1837-1435': 3, '0000-0002-2781-2637': 3, '0000-0001-8959-0315': 3, '0000-0003-1313-4000': 3, '0000-0003-1724-4418': 2, '0000-0003-0345-6647': 2, '0000-0001-8519-3240': 2, '0000-0002-3292-9303': 2, '0000-0003-1679-6560': 2, '0000-0003-4341-672X': 2, '0000-0001-8806-6204': 1, '0000-0003-3125-4399': 1, '0000-0002-5450-5958': 1, '0000-0003-0658-4425': 1, '0000-0002-6370-0704': 1, '0000-0001-6604-5509': 1, '0000-0002-6009-8797': 1, '0000-0003-4566-2107': 1, '0000-0003-3055-5528': 1, '0000-0002-5437-4695': 1, '0000-0003-3607-7176': 1})\n",
      "['0000-0003-4837-5373', '0000-0002-1006-6666', '0000-0003-3326-2640', '0000-0002-3745-7202', '0000-0001-6715-6366', '0000-0001-9366-6204', '0000-0002-0253-647X', '0000-0002-4723-845X', '0000-0001-7639-0904', '0000-0003-3103-6949', '0000-0002-7233-1509']\n",
      "Total sample size after apply threshold:  349\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(349, 633)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(349, 633)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.91      0.95        11\n",
      "          1       0.80      0.72      0.76        39\n",
      "          2       0.88      0.82      0.85        17\n",
      "          3       0.86      0.46      0.60        13\n",
      "          4       0.76      0.97      0.85       100\n",
      "          5       0.92      0.88      0.90        26\n",
      "          6       0.89      0.89      0.89        45\n",
      "          7       0.94      0.83      0.88        18\n",
      "          8       0.97      0.87      0.92        39\n",
      "          9       1.00      0.60      0.75        10\n",
      "         10       0.88      0.74      0.81        31\n",
      "\n",
      "avg / total       0.86      0.85      0.85       349\n",
      "\n",
      "[10  0  0  0  1  0  0  0  0  0  0  0 28  0  0  5  0  2  1  1  0  2  0  0\n",
      " 14  0  3  0  0  0  0  0  0  0  0  1  6  6  0  0  0  0  0  0  0  1  1  1\n",
      " 97  0  0  0  0  0  0  0  0  0  0  3 23  0  0  0  0  0  0  2  0  0  3  0\n",
      " 40  0  0  0  0  0  1  0  0  0  0  1 15  0  0  1  0  1  0  0  3  1  0  0\n",
      " 34  0  0  0  0  0  0  4  0  0  0  0  6  0  0  2  0  0  3  1  2  0  0  0\n",
      " 23]\n",
      "svc Accuracy:  0.8481375358166189\n",
      "svc F1:  0.8325126207056971\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.82      0.90        11\n",
      "          1       0.82      0.72      0.77        39\n",
      "          2       0.93      0.82      0.87        17\n",
      "          3       0.88      0.54      0.67        13\n",
      "          4       0.79      0.99      0.88       100\n",
      "          5       1.00      0.88      0.94        26\n",
      "          6       0.93      0.93      0.93        45\n",
      "          7       0.94      0.83      0.88        18\n",
      "          8       0.92      0.92      0.92        39\n",
      "          9       1.00      0.60      0.75        10\n",
      "         10       0.93      0.84      0.88        31\n",
      "\n",
      "avg / total       0.88      0.87      0.87       349\n",
      "\n",
      "[ 9  0  0  0  2  0  0  0  0  0  0  0 28  0  0  7  0  1  1  1  0  1  0  0\n",
      " 14  1  2  0  0  0  0  0  0  0  0  0  7  6  0  0  0  0  0  0  0  0  1  0\n",
      " 99  0  0  0  0  0  0  0  0  0  0  3 23  0  0  0  0  0  0  1  0  0  1  0\n",
      " 42  0  1  0  0  0  1  0  0  0  0  1 15  0  0  1  0  2  0  0  1  0  0  0\n",
      " 36  0  0  0  0  0  0  4  0  0  0  0  6  0  0  2  0  0  1  0  1  0  1  0\n",
      " 26]\n",
      "LR Accuracy:  0.8739255014326648\n",
      "LR F1:  0.8539809808202148\n",
      "For name:  s_bae\n",
      "total sample size before apply threshold:  83\n",
      "Counter({'0000-0003-0551-7618': 19, '0000-0002-3019-0584': 17, '0000-0002-4995-6543': 17, '0000-0002-8993-8884': 9, '0000-0003-0098-8816': 8, '0000-0003-1926-5466': 6, '0000-0001-7603-7676': 6, '0000-0003-0637-4110': 1})\n",
      "['0000-0003-0551-7618', '0000-0002-3019-0584', '0000-0002-4995-6543']\n",
      "Total sample size after apply threshold:  53\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(53, 92)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(53, 92)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        19\n",
      "          1       0.91      0.59      0.71        17\n",
      "          2       0.70      0.94      0.80        17\n",
      "\n",
      "avg / total       0.87      0.85      0.84        53\n",
      "\n",
      "[19  0  0  0 10  7  0  1 16]\n",
      "svc Accuracy:  0.8490566037735849\n",
      "svc F1:  0.8380952380952381\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        19\n",
      "          1       0.93      0.82      0.87        17\n",
      "          2       0.84      0.94      0.89        17\n",
      "\n",
      "avg / total       0.93      0.92      0.92        53\n",
      "\n",
      "[19  0  0  0 14  3  0  1 16]\n",
      "LR Accuracy:  0.9245283018867925\n",
      "LR F1:  0.9212962962962963\n",
      "For name:  s_fernandes\n",
      "total sample size before apply threshold:  38\n",
      "Counter({'0000-0003-1128-833X': 20, '0000-0002-1295-5010': 6, '0000-0002-9035-793X': 5, '0000-0002-7871-6717': 5, '0000-0002-0790-303X': 2})\n",
      "['0000-0003-1128-833X']\n",
      "Total sample size after apply threshold:  20\n",
      "For name:  a_miller\n",
      "total sample size before apply threshold:  109\n",
      "Counter({'0000-0002-7056-8502': 33, '0000-0001-8474-5090': 28, '0000-0002-7293-764X': 22, '0000-0002-0553-8470': 15, '0000-0001-9735-6609': 5, '0000-0001-8527-1595': 1, '0000-0002-1761-4143': 1, '0000-0002-3099-1648': 1, '0000-0002-0941-1717': 1, '0000-0001-9739-8462': 1, '0000-0003-0924-8443': 1})\n",
      "['0000-0001-8474-5090', '0000-0002-0553-8470', '0000-0002-7056-8502', '0000-0002-7293-764X']\n",
      "Total sample size after apply threshold:  98\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(98, 283)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(98, 283)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.89      0.71        28\n",
      "          1       1.00      0.87      0.93        15\n",
      "          2       0.89      0.73      0.80        33\n",
      "          3       0.94      0.68      0.79        22\n",
      "\n",
      "avg / total       0.83      0.79      0.79        98\n",
      "\n",
      "[25  0  3  0  1 13  0  1  9  0 24  0  7  0  0 15]\n",
      "svc Accuracy:  0.7857142857142857\n",
      "svc F1:  0.8080827067669172\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.79      0.76        28\n",
      "          1       1.00      0.87      0.93        15\n",
      "          2       0.67      0.79      0.72        33\n",
      "          3       0.94      0.68      0.79        22\n",
      "\n",
      "avg / total       0.80      0.78      0.78        98\n",
      "\n",
      "[22  0  6  0  1 13  0  1  7  0 26  0  0  0  7 15]\n",
      "LR Accuracy:  0.7755102040816326\n",
      "LR F1:  0.7997220061648374\n",
      "For name:  a_eklund\n",
      "total sample size before apply threshold:  118\n",
      "Counter({'0000-0002-2031-722X': 73, '0000-0003-0861-1001': 40, '0000-0003-1271-1814': 4, '0000-0002-2162-7537': 1})\n",
      "['0000-0003-0861-1001', '0000-0002-2031-722X']\n",
      "Total sample size after apply threshold:  113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(113, 438)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(113, 438)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.90      0.95        40\n",
      "          1       0.95      1.00      0.97        73\n",
      "\n",
      "avg / total       0.97      0.96      0.96       113\n",
      "\n",
      "[36  4  0 73]\n",
      "svc Accuracy:  0.9646017699115044\n",
      "svc F1:  0.9603508771929825\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.93      0.96        40\n",
      "          1       0.96      1.00      0.98        73\n",
      "\n",
      "avg / total       0.97      0.97      0.97       113\n",
      "\n",
      "[37  3  0 73]\n",
      "LR Accuracy:  0.9734513274336283\n",
      "LR F1:  0.9704523664255208\n",
      "For name:  r_moore\n",
      "total sample size before apply threshold:  221\n",
      "Counter({'0000-0002-0776-5861': 75, '0000-0001-7221-6693': 51, '0000-0003-1072-2755': 45, '0000-0003-2027-2428': 44, '0000-0003-4196-1804': 6})\n",
      "['0000-0003-2027-2428', '0000-0003-1072-2755', '0000-0001-7221-6693', '0000-0002-0776-5861']\n",
      "Total sample size after apply threshold:  215\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(215, 579)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(215, 579)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.84      0.91        44\n",
      "          1       1.00      0.84      0.92        45\n",
      "          2       1.00      0.96      0.98        51\n",
      "          3       0.82      1.00      0.90        75\n",
      "\n",
      "avg / total       0.94      0.93      0.93       215\n",
      "\n",
      "[37  0  0  7  0 38  0  7  0  0 49  2  0  0  0 75]\n",
      "svc Accuracy:  0.9255813953488372\n",
      "svc F1:  0.9282143388368288\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.80      0.89        44\n",
      "          1       1.00      0.87      0.93        45\n",
      "          2       1.00      0.96      0.98        51\n",
      "          3       0.82      1.00      0.90        75\n",
      "\n",
      "avg / total       0.94      0.92      0.92       215\n",
      "\n",
      "[35  0  0  9  0 39  0  6  0  0 49  2  0  0  0 75]\n",
      "LR Accuracy:  0.9209302325581395\n",
      "LR F1:  0.9232127426882222\n",
      "For name:  m_thomsen\n",
      "total sample size before apply threshold:  98\n",
      "Counter({'0000-0002-2469-6458': 37, '0000-0003-2453-5141': 32, '0000-0001-6805-7247': 17, '0000-0003-3081-9220': 7, '0000-0003-3814-1709': 3, '0000-0003-1208-5497': 2})\n",
      "['0000-0003-2453-5141', '0000-0002-2469-6458', '0000-0001-6805-7247']\n",
      "Total sample size after apply threshold:  86\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(86, 213)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(86, 213)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.84      0.90        32\n",
      "          1       0.78      0.97      0.87        37\n",
      "          2       1.00      0.71      0.83        17\n",
      "\n",
      "avg / total       0.89      0.87      0.87        86\n",
      "\n",
      "[27  5  0  1 36  0  0  5 12]\n",
      "svc Accuracy:  0.872093023255814\n",
      "svc F1:  0.8650186954715413\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.94      0.94        32\n",
      "          1       0.85      0.95      0.90        37\n",
      "          2       1.00      0.76      0.87        17\n",
      "\n",
      "avg / total       0.91      0.91      0.91        86\n",
      "\n",
      "[30  2  0  2 35  0  0  4 13]\n",
      "LR Accuracy:  0.9069767441860465\n",
      "LR F1:  0.900534188034188\n",
      "For name:  l_ng\n",
      "total sample size before apply threshold:  44\n",
      "Counter({'0000-0003-1905-3586': 37, '0000-0002-6973-9466': 3, '0000-0001-7500-9403': 1, '0000-0001-5988-008X': 1, '0000-0003-3135-244X': 1, '0000-0002-7189-1272': 1})\n",
      "['0000-0003-1905-3586']\n",
      "Total sample size after apply threshold:  37\n",
      "For name:  a_phillips\n",
      "total sample size before apply threshold:  170\n",
      "Counter({'0000-0002-5461-0598': 98, '0000-0001-6367-9784': 24, '0000-0001-5599-6499': 24, '0000-0003-4883-0022': 9, '0000-0003-4225-0158': 7, '0000-0003-4473-5108': 4, '0000-0001-6618-0145': 3, '0000-0001-6335-9430': 1})\n",
      "['0000-0001-6367-9784', '0000-0001-5599-6499', '0000-0002-5461-0598']\n",
      "Total sample size after apply threshold:  146\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(146, 333)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(146, 333)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.79      0.88        24\n",
      "          1       1.00      0.79      0.88        24\n",
      "          2       0.91      1.00      0.95        98\n",
      "\n",
      "avg / total       0.94      0.93      0.93       146\n",
      "\n",
      "[19  0  5  0 19  5  0  0 98]\n",
      "svc Accuracy:  0.9315068493150684\n",
      "svc F1:  0.906299390381576\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.75      0.86        24\n",
      "          1       1.00      0.83      0.91        24\n",
      "          2       0.91      1.00      0.95        98\n",
      "\n",
      "avg / total       0.94      0.93      0.93       146\n",
      "\n",
      "[18  0  6  0 20  4  0  0 98]\n",
      "LR Accuracy:  0.9315068493150684\n",
      "LR F1:  0.9058966923044593\n",
      "For name:  y_ye\n",
      "total sample size before apply threshold:  85\n",
      "Counter({'0000-0002-7517-1715': 75, '0000-0002-2029-4558': 8, '0000-0003-3962-8463': 1, '0000-0002-9172-6514': 1})\n",
      "['0000-0002-7517-1715']\n",
      "Total sample size after apply threshold:  75\n",
      "For name:  m_guerreiro\n",
      "total sample size before apply threshold:  36\n",
      "Counter({'0000-0002-1948-1516': 23, '0000-0002-5133-8779': 6, '0000-0002-2863-887X': 6, '0000-0001-6774-9348': 1})\n",
      "['0000-0002-1948-1516']\n",
      "Total sample size after apply threshold:  23\n",
      "For name:  g_alves\n",
      "total sample size before apply threshold:  60\n",
      "Counter({'0000-0002-4213-0714': 40, '0000-0003-0630-2870': 12, '0000-0003-3945-9962': 7, '0000-0003-4985-5555': 1})\n",
      "['0000-0003-0630-2870', '0000-0002-4213-0714']\n",
      "Total sample size after apply threshold:  52\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(52, 99)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(52, 99)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        12\n",
      "          1       1.00      1.00      1.00        40\n",
      "\n",
      "avg / total       1.00      1.00      1.00        52\n",
      "\n",
      "[12  0  0 40]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        12\n",
      "          1       1.00      1.00      1.00        40\n",
      "\n",
      "avg / total       1.00      1.00      1.00        52\n",
      "\n",
      "[12  0  0 40]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  m_mohammed\n",
      "total sample size before apply threshold:  6\n",
      "Counter({'0000-0003-3423-0085': 2, '0000-0002-1795-579X': 2, '0000-0002-9695-396X': 1, '0000-0002-7103-0165': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  s_mohammadi\n",
      "total sample size before apply threshold:  59\n",
      "Counter({'0000-0003-1311-9636': 42, '0000-0003-0650-6654': 13, '0000-0003-3450-6424': 1, '0000-0003-1658-9756': 1, '0000-0002-6656-025X': 1, '0000-0002-9209-3034': 1})\n",
      "['0000-0003-0650-6654', '0000-0003-1311-9636']\n",
      "Total sample size after apply threshold:  55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(55, 148)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(55, 148)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.92      0.96        13\n",
      "          1       0.98      1.00      0.99        42\n",
      "\n",
      "avg / total       0.98      0.98      0.98        55\n",
      "\n",
      "[12  1  0 42]\n",
      "svc Accuracy:  0.9818181818181818\n",
      "svc F1:  0.9741176470588235\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.85      0.92        13\n",
      "          1       0.95      1.00      0.98        42\n",
      "\n",
      "avg / total       0.97      0.96      0.96        55\n",
      "\n",
      "[11  2  0 42]\n",
      "LR Accuracy:  0.9636363636363636\n",
      "LR F1:  0.9467054263565892\n",
      "For name:  c_chao\n",
      "total sample size before apply threshold:  155\n",
      "Counter({'0000-0003-2892-7986': 86, '0000-0002-2804-7447': 34, '0000-0001-6499-5789': 19, '0000-0002-8789-7732': 7, '0000-0001-7769-9305': 7, '0000-0003-1215-8588': 1, '0000-0003-4108-2658': 1})\n",
      "['0000-0002-2804-7447', '0000-0003-2892-7986', '0000-0001-6499-5789']\n",
      "Total sample size after apply threshold:  139\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(139, 106)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(139, 106)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.79      0.84        34\n",
      "          1       0.91      0.97      0.94        86\n",
      "          2       0.89      0.84      0.86        19\n",
      "\n",
      "avg / total       0.91      0.91      0.90       139\n",
      "\n",
      "[27  6  1  2 83  1  1  2 16]\n",
      "svc Accuracy:  0.9064748201438849\n",
      "svc F1:  0.8821559907364992\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.76      0.85        34\n",
      "          1       0.88      1.00      0.93        86\n",
      "          2       1.00      0.74      0.85        19\n",
      "\n",
      "avg / total       0.92      0.91      0.90       139\n",
      "\n",
      "[26  8  0  0 86  0  1  4 14]\n",
      "LR Accuracy:  0.9064748201438849\n",
      "LR F1:  0.8785754911913144\n",
      "For name:  s_teixeira\n",
      "total sample size before apply threshold:  36\n",
      "Counter({'0000-0003-0419-2348': 12, '0000-0001-5845-058X': 11, '0000-0002-2462-8535': 3, '0000-0002-9473-0113': 3, '0000-0002-7464-3944': 3, '0000-0002-6603-7936': 3, '0000-0003-3664-2577': 1})\n",
      "['0000-0003-0419-2348', '0000-0001-5845-058X']\n",
      "Total sample size after apply threshold:  23\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(23, 99)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(23, 99)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      1.00      0.89        12\n",
      "          1       1.00      0.73      0.84        11\n",
      "\n",
      "avg / total       0.90      0.87      0.87        23\n",
      "\n",
      "[12  0  3  8]\n",
      "svc Accuracy:  0.8695652173913043\n",
      "svc F1:  0.8654970760233919\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      1.00      0.89        12\n",
      "          1       1.00      0.73      0.84        11\n",
      "\n",
      "avg / total       0.90      0.87      0.87        23\n",
      "\n",
      "[12  0  3  8]\n",
      "LR Accuracy:  0.8695652173913043\n",
      "LR F1:  0.8654970760233919\n",
      "For name:  l_almeida\n",
      "total sample size before apply threshold:  133\n",
      "Counter({'0000-0002-4861-8649': 57, '0000-0002-7769-4712': 43, '0000-0003-1370-961X': 12, '0000-0003-0370-214X': 8, '0000-0002-0651-7014': 5, '0000-0001-9346-7520': 4, '0000-0002-1324-0068': 1, '0000-0002-9544-3028': 1, '0000-0003-4711-4454': 1, '0000-0002-0921-887X': 1})\n",
      "['0000-0003-1370-961X', '0000-0002-4861-8649', '0000-0002-7769-4712']\n",
      "Total sample size after apply threshold:  112\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(112, 197)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(112, 197)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.83      0.87        12\n",
      "          1       1.00      0.98      0.99        57\n",
      "          2       0.96      1.00      0.98        43\n",
      "\n",
      "avg / total       0.97      0.97      0.97       112\n",
      "\n",
      "[10  0  2  1 56  0  0  0 43]\n",
      "svc Accuracy:  0.9732142857142857\n",
      "svc F1:  0.9459961290473027\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.83      0.91        12\n",
      "          1       1.00      0.98      0.99        57\n",
      "          2       0.93      1.00      0.97        43\n",
      "\n",
      "avg / total       0.97      0.97      0.97       112\n",
      "\n",
      "[10  0  2  0 56  1  0  0 43]\n",
      "LR Accuracy:  0.9732142857142857\n",
      "LR F1:  0.9555111621334152\n",
      "For name:  y_tseng\n",
      "total sample size before apply threshold:  61\n",
      "Counter({'0000-0002-8461-6181': 45, '0000-0002-2354-5906': 9, '0000-0001-6917-893X': 2, '0000-0002-3803-7410': 2, '0000-0002-1814-5553': 2, '0000-0002-3511-7191': 1})\n",
      "['0000-0002-8461-6181']\n",
      "Total sample size after apply threshold:  45\n",
      "For name:  a_ferro\n",
      "total sample size before apply threshold:  125\n",
      "Counter({'0000-0002-5486-9145': 91, '0000-0003-2399-3626': 11, '0000-0001-6042-4591': 10, '0000-0003-4470-079X': 7, '0000-0001-8403-9823': 4, '0000-0002-9431-5788': 2})\n",
      "['0000-0001-6042-4591', '0000-0002-5486-9145', '0000-0003-2399-3626']\n",
      "Total sample size after apply threshold:  112\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(112, 289)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(112, 289)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.80      0.89        10\n",
      "          1       0.95      1.00      0.97        91\n",
      "          2       1.00      0.73      0.84        11\n",
      "\n",
      "avg / total       0.96      0.96      0.95       112\n",
      "\n",
      "[ 8  2  0  0 91  0  0  3  8]\n",
      "svc Accuracy:  0.9553571428571429\n",
      "svc F1:  0.9014187280441152\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.80      0.89        10\n",
      "          1       0.94      1.00      0.97        91\n",
      "          2       1.00      0.64      0.78        11\n",
      "\n",
      "avg / total       0.95      0.95      0.94       112\n",
      "\n",
      "[ 8  2  0  0 91  0  0  4  7]\n",
      "LR Accuracy:  0.9464285714285714\n",
      "LR F1:  0.8782505910165485\n",
      "For name:  d_he\n",
      "total sample size before apply threshold:  29\n",
      "Counter({'0000-0003-3253-654X': 17, '0000-0003-2212-1973': 4, '0000-0002-9947-6177': 3, '0000-0002-2446-7436': 2, '0000-0002-4001-826X': 2, '0000-0002-3360-9352': 1})\n",
      "['0000-0003-3253-654X']\n",
      "Total sample size after apply threshold:  17\n",
      "For name:  k_ko\n",
      "total sample size before apply threshold:  168\n",
      "Counter({'0000-0002-0978-1937': 153, '0000-0003-3649-4594': 11, '0000-0002-6412-1026': 2, '0000-0002-0192-0269': 1, '0000-0002-0515-5904': 1})\n",
      "['0000-0003-3649-4594', '0000-0002-0978-1937']\n",
      "Total sample size after apply threshold:  164\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(164, 224)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(164, 224)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.45      0.62        11\n",
      "          1       0.96      1.00      0.98       153\n",
      "\n",
      "avg / total       0.96      0.96      0.96       164\n",
      "\n",
      "[  5   6   0 153]\n",
      "svc Accuracy:  0.9634146341463414\n",
      "svc F1:  0.8028846153846154\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.27      0.43        11\n",
      "          1       0.95      1.00      0.97       153\n",
      "\n",
      "avg / total       0.95      0.95      0.94       164\n",
      "\n",
      "[  3   8   0 153]\n",
      "LR Accuracy:  0.9512195121951219\n",
      "LR F1:  0.7015468607825296\n",
      "For name:  t_mori\n",
      "total sample size before apply threshold:  104\n",
      "Counter({'0000-0003-3918-0873': 92, '0000-0002-0370-1924': 10, '0000-0001-5340-3282': 1, '0000-0001-7096-4161': 1})\n",
      "['0000-0002-0370-1924', '0000-0003-3918-0873']\n",
      "Total sample size after apply threshold:  102\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(102, 157)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(102, 157)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.90      0.95        10\n",
      "          1       0.99      1.00      0.99        92\n",
      "\n",
      "avg / total       0.99      0.99      0.99       102\n",
      "\n",
      "[ 9  1  0 92]\n",
      "svc Accuracy:  0.9901960784313726\n",
      "svc F1:  0.9709815078236131\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.90      0.95        10\n",
      "          1       0.99      1.00      0.99        92\n",
      "\n",
      "avg / total       0.99      0.99      0.99       102\n",
      "\n",
      "[ 9  1  0 92]\n",
      "LR Accuracy:  0.9901960784313726\n",
      "LR F1:  0.9709815078236131\n",
      "For name:  p_lima\n",
      "total sample size before apply threshold:  24\n",
      "Counter({'0000-0002-1252-2565': 8, '0000-0002-9739-0783': 8, '0000-0002-4323-3918': 4, '0000-0003-2081-571X': 2, '0000-0002-8962-8050': 1, '0000-0003-2937-9520': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  d_ferguson\n",
      "total sample size before apply threshold:  217\n",
      "Counter({'0000-0001-5045-819X': 174, '0000-0001-9302-5992': 35, '0000-0001-6448-8701': 4, '0000-0003-0612-6512': 3, '0000-0002-7400-7892': 1})\n",
      "['0000-0001-5045-819X', '0000-0001-9302-5992']\n",
      "Total sample size after apply threshold:  209\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(209, 856)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(209, 856)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99       174\n",
      "          1       1.00      0.91      0.96        35\n",
      "\n",
      "avg / total       0.99      0.99      0.99       209\n",
      "\n",
      "[174   0   3  32]\n",
      "svc Accuracy:  0.9856459330143541\n",
      "svc F1:  0.9733384360250033\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97       174\n",
      "          1       1.00      0.69      0.81        35\n",
      "\n",
      "avg / total       0.95      0.95      0.94       209\n",
      "\n",
      "[174   0  11  24]\n",
      "LR Accuracy:  0.9473684210526315\n",
      "LR F1:  0.8914593267551107\n",
      "For name:  h_moreira\n",
      "total sample size before apply threshold:  28\n",
      "Counter({'0000-0002-1487-0539': 13, '0000-0002-5481-0688': 10, '0000-0002-4674-5417': 3, '0000-0002-4556-5027': 1, '0000-0002-5588-374X': 1})\n",
      "['0000-0002-1487-0539', '0000-0002-5481-0688']\n",
      "Total sample size after apply threshold:  23\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(23, 29)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(23, 29)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      1.00      0.96        13\n",
      "          1       1.00      0.90      0.95        10\n",
      "\n",
      "avg / total       0.96      0.96      0.96        23\n",
      "\n",
      "[13  0  1  9]\n",
      "svc Accuracy:  0.9565217391304348\n",
      "svc F1:  0.9551656920077973\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      1.00      0.96        13\n",
      "          1       1.00      0.90      0.95        10\n",
      "\n",
      "avg / total       0.96      0.96      0.96        23\n",
      "\n",
      "[13  0  1  9]\n",
      "LR Accuracy:  0.9565217391304348\n",
      "LR F1:  0.9551656920077973\n",
      "For name:  s_yi\n",
      "total sample size before apply threshold:  67\n",
      "Counter({'0000-0002-6656-6205': 29, '0000-0001-6333-4399': 19, '0000-0003-2689-8595': 11, '0000-0003-2804-7161': 3, '0000-0003-4932-8237': 3, '0000-0002-9190-5643': 2})\n",
      "['0000-0002-6656-6205', '0000-0003-2689-8595', '0000-0001-6333-4399']\n",
      "Total sample size after apply threshold:  59\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(59, 52)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(59, 52)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.86      0.89        29\n",
      "          1       0.67      0.91      0.77        11\n",
      "          2       0.88      0.79      0.83        19\n",
      "\n",
      "avg / total       0.86      0.85      0.85        59\n",
      "\n",
      "[25  3  1  0 10  1  2  2 15]\n",
      "svc Accuracy:  0.847457627118644\n",
      "svc F1:  0.8318070818070818\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.90      0.91        29\n",
      "          1       0.80      0.73      0.76        11\n",
      "          2       0.90      1.00      0.95        19\n",
      "\n",
      "avg / total       0.90      0.90      0.90        59\n",
      "\n",
      "[26  2  1  2  8  1  0  0 19]\n",
      "LR Accuracy:  0.8983050847457628\n",
      "LR F1:  0.8747284878863827\n",
      "For name:  q_liu\n",
      "total sample size before apply threshold:  264\n",
      "Counter({'0000-0001-8477-6452': 62, '0000-0001-8525-7961': 62, '0000-0002-1179-290X': 47, '0000-0002-8402-029X': 26, '0000-0001-5286-4423': 24, '0000-0003-3533-7140': 18, '0000-0003-4114-5540': 8, '0000-0002-3616-351X': 6, '0000-0002-2199-2999': 2, '0000-0003-1508-7172': 2, '0000-0003-0769-4642': 1, '0000-0001-9746-2938': 1, '0000-0002-6286-941X': 1, '0000-0002-7574-3752': 1, '0000-0002-4678-3333': 1, '0000-0002-8398-1021': 1, '0000-0002-7285-5425': 1})\n",
      "['0000-0002-1179-290X', '0000-0003-3533-7140', '0000-0001-8477-6452', '0000-0001-5286-4423', '0000-0002-8402-029X', '0000-0001-8525-7961']\n",
      "Total sample size after apply threshold:  239\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(239, 352)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(239, 352)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.87      0.92        47\n",
      "          1       0.80      0.67      0.73        18\n",
      "          2       0.93      0.84      0.88        62\n",
      "          3       0.56      0.92      0.70        24\n",
      "          4       0.96      0.96      0.96        26\n",
      "          5       1.00      0.98      0.99        62\n",
      "\n",
      "avg / total       0.91      0.89      0.90       239\n",
      "\n",
      "[41  1  2  2  1  0  1 12  1  4  0  0  0  0 52 10  0  0  0  1  1 22  0  0\n",
      "  0  0  0  1 25  0  0  1  0  0  0 61]\n",
      "svc Accuracy:  0.891213389121339\n",
      "svc F1:  0.8636330087888675\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.91      0.95        47\n",
      "          1       0.92      0.67      0.77        18\n",
      "          2       0.90      0.90      0.90        62\n",
      "          3       0.71      0.83      0.77        24\n",
      "          4       1.00      0.96      0.98        26\n",
      "          5       0.91      0.98      0.95        62\n",
      "\n",
      "avg / total       0.91      0.91      0.91       239\n",
      "\n",
      "[43  0  1  1  0  2  1 12  1  2  0  2  0  0 56  4  0  2  0  0  4 20  0  0\n",
      "  0  0  0  1 25  0  0  1  0  0  0 61]\n",
      "LR Accuracy:  0.9079497907949791\n",
      "LR F1:  0.8863056100159493\n",
      "For name:  m_ibrahim\n",
      "total sample size before apply threshold:  146\n",
      "Counter({'0000-0002-5756-5198': 20, '0000-0002-9698-0837': 19, '0000-0001-6509-2979': 17, '0000-0003-4614-7182': 15, '0000-0003-0257-860X': 14, '0000-0001-6019-5055': 9, '0000-0001-8657-3368': 9, '0000-0002-0116-597X': 6, '0000-0003-1412-2132': 6, '0000-0002-2603-8280': 5, '0000-0002-7762-1580': 5, '0000-0003-0623-5225': 4, '0000-0003-0468-617X': 3, '0000-0002-8854-8198': 3, '0000-0002-7925-4585': 2, '0000-0002-0021-5971': 2, '0000-0002-9288-2359': 2, '0000-0002-5121-7256': 1, '0000-0001-8433-7409': 1, '0000-0003-3407-4983': 1, '0000-0002-3425-600X': 1, '0000-0002-2953-2305': 1})\n",
      "['0000-0001-6509-2979', '0000-0003-4614-7182', '0000-0003-0257-860X', '0000-0002-9698-0837', '0000-0002-5756-5198']\n",
      "Total sample size after apply threshold:  85\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(85, 253)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(85, 253)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.94      0.97        17\n",
      "          1       1.00      0.73      0.85        15\n",
      "          2       1.00      0.71      0.83        14\n",
      "          3       0.68      1.00      0.81        19\n",
      "          4       1.00      1.00      1.00        20\n",
      "\n",
      "avg / total       0.93      0.89      0.90        85\n",
      "\n",
      "[16  0  0  1  0  0 11  0  4  0  0  0 10  4  0  0  0  0 19  0  0  0  0  0\n",
      " 20]\n",
      "svc Accuracy:  0.8941176470588236\n",
      "svc F1:  0.8915389574964042\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.94      0.97        17\n",
      "          1       1.00      0.73      0.85        15\n",
      "          2       1.00      0.71      0.83        14\n",
      "          3       0.68      1.00      0.81        19\n",
      "          4       1.00      1.00      1.00        20\n",
      "\n",
      "avg / total       0.93      0.89      0.90        85\n",
      "\n",
      "[16  0  0  1  0  0 11  0  4  0  0  0 10  4  0  0  0  0 19  0  0  0  0  0\n",
      " 20]\n",
      "LR Accuracy:  0.8941176470588236\n",
      "LR F1:  0.8915389574964042\n",
      "For name:  s_collins\n",
      "total sample size before apply threshold:  163\n",
      "Counter({'0000-0002-0193-2892': 43, '0000-0002-4276-5840': 38, '0000-0002-0648-7433': 24, '0000-0003-0204-5109': 15, '0000-0001-9989-8794': 13, '0000-0002-5245-6611': 10, '0000-0003-1571-7410': 9, '0000-0002-3110-1037': 7, '0000-0001-5503-7386': 2, '0000-0003-4721-0040': 2})\n",
      "['0000-0002-0193-2892', '0000-0002-4276-5840', '0000-0002-5245-6611', '0000-0003-0204-5109', '0000-0002-0648-7433', '0000-0001-9989-8794']\n",
      "Total sample size after apply threshold:  143\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(143, 693)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(143, 693)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.95      0.79        43\n",
      "          1       1.00      0.97      0.99        38\n",
      "          2       1.00      0.40      0.57        10\n",
      "          3       0.91      0.67      0.77        15\n",
      "          4       0.94      0.71      0.81        24\n",
      "          5       1.00      0.92      0.96        13\n",
      "\n",
      "avg / total       0.88      0.85      0.84       143\n",
      "\n",
      "[41  0  0  1  1  0  1 37  0  0  0  0  6  0  4  0  0  0  5  0  0 10  0  0\n",
      "  7  0  0  0 17  0  1  0  0  0  0 12]\n",
      "svc Accuracy:  0.8461538461538461\n",
      "svc F1:  0.8142185592185593\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.93      0.82        43\n",
      "          1       1.00      1.00      1.00        38\n",
      "          2       1.00      0.30      0.46        10\n",
      "          3       0.86      0.80      0.83        15\n",
      "          4       0.95      0.83      0.89        24\n",
      "          5       1.00      0.92      0.96        13\n",
      "\n",
      "avg / total       0.90      0.87      0.87       143\n",
      "\n",
      "[40  0  0  2  1  0  0 38  0  0  0  0  7  0  3  0  0  0  3  0  0 12  0  0\n",
      "  4  0  0  0 20  0  1  0  0  0  0 12]\n",
      "LR Accuracy:  0.8741258741258742\n",
      "LR F1:  0.8257233479893579\n",
      "For name:  d_franco\n",
      "total sample size before apply threshold:  115\n",
      "Counter({'0000-0002-5669-7164': 58, '0000-0002-0093-7042': 40, '0000-0003-3849-4272': 8, '0000-0001-5604-2531': 6, '0000-0002-8653-0488': 2, '0000-0002-2050-7883': 1})\n",
      "['0000-0002-0093-7042', '0000-0002-5669-7164']\n",
      "Total sample size after apply threshold:  98\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(98, 228)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(98, 228)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.90      0.95        40\n",
      "          1       0.94      1.00      0.97        58\n",
      "\n",
      "avg / total       0.96      0.96      0.96        98\n",
      "\n",
      "[36  4  0 58]\n",
      "svc Accuracy:  0.9591836734693877\n",
      "svc F1:  0.9570175438596491\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.95      0.97        40\n",
      "          1       0.97      1.00      0.98        58\n",
      "\n",
      "avg / total       0.98      0.98      0.98        98\n",
      "\n",
      "[38  2  0 58]\n",
      "LR Accuracy:  0.9795918367346939\n",
      "LR F1:  0.9787049109083007\n",
      "For name:  h_brown\n",
      "total sample size before apply threshold:  48\n",
      "Counter({'0000-0001-8578-5510': 17, '0000-0002-0067-991X': 9, '0000-0003-4870-8369': 8, '0000-0001-7418-5536': 6, '0000-0001-6227-5147': 3, '0000-0001-9404-9515': 3, '0000-0003-2292-7766': 2})\n",
      "['0000-0001-8578-5510']\n",
      "Total sample size after apply threshold:  17\n",
      "For name:  s_martins\n",
      "total sample size before apply threshold:  84\n",
      "Counter({'0000-0002-9396-5957': 18, '0000-0002-3720-2920': 15, '0000-0001-7217-6273': 15, '0000-0003-0237-6370': 12, '0000-0002-1812-8913': 8, '0000-0002-1874-0192': 7, '0000-0002-7733-4485': 5, '0000-0003-2122-0670': 3, '0000-0002-3526-3199': 1})\n",
      "['0000-0002-3720-2920', '0000-0003-0237-6370', '0000-0001-7217-6273', '0000-0002-9396-5957']\n",
      "Total sample size after apply threshold:  60\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(60, 159)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(60, 159)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.87      0.84        15\n",
      "          1       0.83      0.83      0.83        12\n",
      "          2       0.94      1.00      0.97        15\n",
      "          3       1.00      0.89      0.94        18\n",
      "\n",
      "avg / total       0.90      0.90      0.90        60\n",
      "\n",
      "[13  2  0  0  1 10  1  0  0  0 15  0  2  0  0 16]\n",
      "svc Accuracy:  0.9\n",
      "svc F1:  0.8952403542061986\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.87      0.90        15\n",
      "          1       0.91      0.83      0.87        12\n",
      "          2       0.94      1.00      0.97        15\n",
      "          3       0.95      1.00      0.97        18\n",
      "\n",
      "avg / total       0.93      0.93      0.93        60\n",
      "\n",
      "[13  1  0  1  1 10  1  0  0  0 15  0  0  0  0 18]\n",
      "LR Accuracy:  0.9333333333333333\n",
      "LR F1:  0.9267079624965198\n",
      "For name:  m_ruiz\n",
      "total sample size before apply threshold:  111\n",
      "Counter({'0000-0003-4174-6688': 40, '0000-0002-2734-2196': 32, '0000-0002-1530-9508': 9, '0000-0002-1337-0110': 5, '0000-0001-8617-667X': 4, '0000-0001-7492-9873': 3, '0000-0003-4419-1649': 3, '0000-0002-2926-702X': 3, '0000-0003-1437-5578': 2, '0000-0002-4670-9037': 2, '0000-0002-4917-1252': 2, '0000-0002-1286-6624': 2, '0000-0002-6799-1537': 1, '0000-0002-1116-206X': 1, '0000-0003-0118-668X': 1, '0000-0002-8527-4734': 1})\n",
      "['0000-0003-4174-6688', '0000-0002-2734-2196']\n",
      "Total sample size after apply threshold:  72\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(72, 157)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(72, 157)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.97      0.96        40\n",
      "          1       0.97      0.94      0.95        32\n",
      "\n",
      "avg / total       0.96      0.96      0.96        72\n",
      "\n",
      "[39  1  2 30]\n",
      "svc Accuracy:  0.9583333333333334\n",
      "svc F1:  0.9576719576719577\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.97      0.96        40\n",
      "          1       0.97      0.94      0.95        32\n",
      "\n",
      "avg / total       0.96      0.96      0.96        72\n",
      "\n",
      "[39  1  2 30]\n",
      "LR Accuracy:  0.9583333333333334\n",
      "LR F1:  0.9576719576719577\n",
      "For name:  a_levy\n",
      "total sample size before apply threshold:  23\n",
      "Counter({'0000-0003-4770-1886': 13, '0000-0002-6709-4190': 6, '0000-0002-5856-8294': 3, '0000-0002-1521-658X': 1})\n",
      "['0000-0003-4770-1886']\n",
      "Total sample size after apply threshold:  13\n",
      "For name:  j_murray\n",
      "total sample size before apply threshold:  213\n",
      "Counter({'0000-0002-2282-3839': 78, '0000-0002-8897-0161': 32, '0000-0002-8992-7317': 23, '0000-0002-6928-2347': 23, '0000-0001-9314-2283': 18, '0000-0001-8224-679X': 13, '0000-0003-1941-9090': 11, '0000-0002-8577-7964': 8, '0000-0003-2994-4155': 3, '0000-0002-8741-4964': 1, '0000-0003-4390-1039': 1, '0000-0001-9721-992X': 1, '0000-0003-3000-9199': 1})\n",
      "['0000-0002-2282-3839', '0000-0001-9314-2283', '0000-0002-8992-7317', '0000-0003-1941-9090', '0000-0002-8897-0161', '0000-0001-8224-679X', '0000-0002-6928-2347']\n",
      "Total sample size after apply threshold:  198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(198, 651)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(198, 651)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.69      1.00      0.82        78\n",
      "          1       1.00      0.67      0.80        18\n",
      "          2       1.00      0.91      0.95        23\n",
      "          3       1.00      0.55      0.71        11\n",
      "          4       1.00      0.75      0.86        32\n",
      "          5       0.82      0.69      0.75        13\n",
      "          6       0.91      0.43      0.59        23\n",
      "\n",
      "avg / total       0.86      0.81      0.80       198\n",
      "\n",
      "[78  0  0  0  0  0  0  5 12  0  0  0  0  1  2  0 21  0  0  0  0  5  0  0\n",
      "  6  0  0  0  8  0  0  0 24  0  0  4  0  0  0  0  9  0 11  0  0  0  0  2\n",
      " 10]\n",
      "svc Accuracy:  0.8080808080808081\n",
      "svc F1:  0.781794269349815\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      1.00      0.78        78\n",
      "          1       1.00      0.56      0.71        18\n",
      "          2       1.00      0.91      0.95        23\n",
      "          3       1.00      0.18      0.31        11\n",
      "          4       1.00      0.72      0.84        32\n",
      "          5       1.00      0.54      0.70        13\n",
      "          6       0.92      0.48      0.63        23\n",
      "\n",
      "avg / total       0.85      0.77      0.75       198\n",
      "\n",
      "[78  0  0  0  0  0  0  7 10  0  0  0  0  1  2  0 21  0  0  0  0  9  0  0\n",
      "  2  0  0  0  9  0  0  0 23  0  0  6  0  0  0  0  7  0 12  0  0  0  0  0\n",
      " 11]\n",
      "LR Accuracy:  0.7676767676767676\n",
      "LR F1:  0.7025111349205166\n",
      "For name:  y_hou\n",
      "total sample size before apply threshold:  162\n",
      "Counter({'0000-0001-6546-2597': 97, '0000-0002-3995-7219': 29, '0000-0002-0420-0726': 14, '0000-0002-8114-166X': 12, '0000-0002-7360-5751': 5, '0000-0002-4978-9829': 4, '0000-0003-3195-7430': 1})\n",
      "['0000-0002-3995-7219', '0000-0002-8114-166X', '0000-0001-6546-2597', '0000-0002-0420-0726']\n",
      "Total sample size after apply threshold:  152\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(152, 320)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(152, 320)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.97      0.98        29\n",
      "          1       1.00      0.92      0.96        12\n",
      "          2       0.95      1.00      0.97        97\n",
      "          3       1.00      0.79      0.88        14\n",
      "\n",
      "avg / total       0.97      0.97      0.97       152\n",
      "\n",
      "[28  0  1  0  0 11  1  0  0  0 97  0  0  0  3 11]\n",
      "svc Accuracy:  0.9671052631578947\n",
      "svc F1:  0.948463062835152\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.97      0.98        29\n",
      "          1       1.00      0.92      0.96        12\n",
      "          2       0.95      1.00      0.97        97\n",
      "          3       1.00      0.79      0.88        14\n",
      "\n",
      "avg / total       0.97      0.97      0.97       152\n",
      "\n",
      "[28  0  1  0  0 11  1  0  0  0 97  0  0  0  3 11]\n",
      "LR Accuracy:  0.9671052631578947\n",
      "LR F1:  0.948463062835152\n",
      "For name:  m_sahin\n",
      "total sample size before apply threshold:  48\n",
      "Counter({'0000-0001-7044-2953': 41, '0000-0002-3490-6009': 3, '0000-0001-6502-2209': 2, '0000-0001-7677-8423': 2})\n",
      "['0000-0001-7044-2953']\n",
      "Total sample size after apply threshold:  41\n",
      "For name:  c_feng\n",
      "total sample size before apply threshold:  88\n",
      "Counter({'0000-0002-1854-356X': 30, '0000-0002-2130-8851': 26, '0000-0003-3267-0968': 12, '0000-0002-7031-4211': 12, '0000-0002-3278-9451': 7, '0000-0003-1085-4395': 1})\n",
      "['0000-0002-1854-356X', '0000-0002-2130-8851', '0000-0003-3267-0968', '0000-0002-7031-4211']\n",
      "Total sample size after apply threshold:  80\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(80, 86)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(80, 86)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.98        30\n",
      "          1       0.92      0.92      0.92        26\n",
      "          2       0.91      0.83      0.87        12\n",
      "          3       1.00      1.00      1.00        12\n",
      "\n",
      "avg / total       0.95      0.95      0.95        80\n",
      "\n",
      "[30  0  0  0  1 24  1  0  0  2 10  0  0  0  0 12]\n",
      "svc Accuracy:  0.95\n",
      "svc F1:  0.9440621744613191\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        30\n",
      "          1       0.93      1.00      0.96        26\n",
      "          2       1.00      0.83      0.91        12\n",
      "          3       1.00      1.00      1.00        12\n",
      "\n",
      "avg / total       0.98      0.97      0.97        80\n",
      "\n",
      "[30  0  0  0  0 26  0  0  0  2 10  0  0  0  0 12]\n",
      "LR Accuracy:  0.975\n",
      "LR F1:  0.968013468013468\n",
      "For name:  j_coutinho\n",
      "total sample size before apply threshold:  129\n",
      "Counter({'0000-0002-3841-743X': 105, '0000-0002-6303-9549': 13, '0000-0002-1562-0099': 8, '0000-0003-0280-366X': 3})\n",
      "['0000-0002-6303-9549', '0000-0002-3841-743X']\n",
      "Total sample size after apply threshold:  118\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(118, 181)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(118, 181)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.62      0.73        13\n",
      "          1       0.95      0.99      0.97       105\n",
      "\n",
      "avg / total       0.95      0.95      0.95       118\n",
      "\n",
      "[  8   5   1 104]\n",
      "svc Accuracy:  0.9491525423728814\n",
      "svc F1:  0.8496176720475787\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.54      0.70        13\n",
      "          1       0.95      1.00      0.97       105\n",
      "\n",
      "avg / total       0.95      0.95      0.94       118\n",
      "\n",
      "[  7   6   0 105]\n",
      "LR Accuracy:  0.9491525423728814\n",
      "LR F1:  0.8361111111111111\n",
      "For name:  s_huber\n",
      "total sample size before apply threshold:  44\n",
      "Counter({'0000-0002-4125-159X': 26, '0000-0003-3558-351X': 12, '0000-0002-8271-7835': 3, '0000-0002-5842-5859': 2, '0000-0001-6303-5188': 1})\n",
      "['0000-0003-3558-351X', '0000-0002-4125-159X']\n",
      "Total sample size after apply threshold:  38\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(38, 93)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(38, 93)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.75      0.75        12\n",
      "          1       0.88      0.88      0.88        26\n",
      "\n",
      "avg / total       0.84      0.84      0.84        38\n",
      "\n",
      "[ 9  3  3 23]\n",
      "svc Accuracy:  0.8421052631578947\n",
      "svc F1:  0.8173076923076923\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.58      0.74        12\n",
      "          1       0.84      1.00      0.91        26\n",
      "\n",
      "avg / total       0.89      0.87      0.86        38\n",
      "\n",
      "[ 7  5  0 26]\n",
      "LR Accuracy:  0.868421052631579\n",
      "LR F1:  0.8245614035087718\n",
      "For name:  a_rocha\n",
      "total sample size before apply threshold:  73\n",
      "Counter({'0000-0003-3218-7001': 26, '0000-0001-9710-9835': 21, '0000-0003-2165-5519': 12, '0000-0002-4094-7982': 3, '0000-0002-5637-1041': 3, '0000-0001-6528-9034': 3, '0000-0003-4940-6522': 2, '0000-0003-0298-8246': 2, '0000-0001-8679-2886': 1})\n",
      "['0000-0001-9710-9835', '0000-0003-2165-5519', '0000-0003-3218-7001']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sample size after apply threshold:  59\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(59, 108)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(59, 108)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.95      0.95        21\n",
      "          1       1.00      0.92      0.96        12\n",
      "          2       0.96      1.00      0.98        26\n",
      "\n",
      "avg / total       0.97      0.97      0.97        59\n",
      "\n",
      "[20  0  1  1 11  0  0  0 26]\n",
      "svc Accuracy:  0.9661016949152542\n",
      "svc F1:  0.963344922327695\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.86      0.90        21\n",
      "          1       1.00      0.92      0.96        12\n",
      "          2       0.90      1.00      0.95        26\n",
      "\n",
      "avg / total       0.94      0.93      0.93        59\n",
      "\n",
      "[18  0  3  1 11  0  0  0 26]\n",
      "LR Accuracy:  0.9322033898305084\n",
      "LR F1:  0.9339920948616601\n",
      "For name:  a_white\n",
      "total sample size before apply threshold:  386\n",
      "Counter({'0000-0002-9668-4632': 108, '0000-0003-1802-9891': 87, '0000-0002-7686-2884': 85, '0000-0001-9639-5200': 41, '0000-0002-5442-6985': 16, '0000-0002-9859-0947': 13, '0000-0002-1539-0158': 9, '0000-0001-5530-742X': 9, '0000-0001-7499-7390': 4, '0000-0002-3904-2019': 4, '0000-0002-7771-3899': 3, '0000-0002-9708-2406': 2, '0000-0002-2783-895X': 2, '0000-0002-7268-5163': 1, '0000-0002-4837-7128': 1, '0000-0002-7106-6440': 1})\n",
      "['0000-0002-7686-2884', '0000-0002-5442-6985', '0000-0002-9668-4632', '0000-0002-9859-0947', '0000-0001-9639-5200', '0000-0003-1802-9891']\n",
      "Total sample size after apply threshold:  350\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(350, 1067)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(350, 1067)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.84      0.89        85\n",
      "          1       1.00      0.81      0.90        16\n",
      "          2       0.72      1.00      0.84       108\n",
      "          3       1.00      0.38      0.56        13\n",
      "          4       1.00      0.76      0.86        41\n",
      "          5       1.00      0.87      0.93        87\n",
      "\n",
      "avg / total       0.90      0.87      0.87       350\n",
      "\n",
      "[ 71   0  14   0   0   0   1  13   2   0   0   0   0   0 108   0   0   0\n",
      "   0   0   8   5   0   0   2   0   8   0  31   0   1   0  10   0   0  76]\n",
      "svc Accuracy:  0.8685714285714285\n",
      "svc F1:  0.8284071717589153\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.92      0.93        85\n",
      "          1       1.00      0.75      0.86        16\n",
      "          2       0.78      1.00      0.88       108\n",
      "          3       1.00      0.23      0.38        13\n",
      "          4       1.00      0.80      0.89        41\n",
      "          5       1.00      0.93      0.96        87\n",
      "\n",
      "avg / total       0.92      0.90      0.89       350\n",
      "\n",
      "[ 78   0   7   0   0   0   2  12   2   0   0   0   0   0 108   0   0   0\n",
      "   0   0  10   3   0   0   2   0   6   0  33   0   1   0   5   0   0  81]\n",
      "LR Accuracy:  0.9\n",
      "LR F1:  0.8158234453966161\n",
      "For name:  j_scott\n",
      "total sample size before apply threshold:  342\n",
      "Counter({'0000-0002-7203-8601': 155, '0000-0002-0744-0688': 60, '0000-0003-0765-9054': 44, '0000-0002-9116-948X': 36, '0000-0002-7513-6768': 21, '0000-0002-9916-6523': 8, '0000-0001-7782-3601': 7, '0000-0002-5073-0832': 6, '0000-0003-2368-8218': 1, '0000-0003-2971-7673': 1, '0000-0002-5616-2688': 1, '0000-0002-4900-0891': 1, '0000-0001-8408-5176': 1})\n",
      "['0000-0002-7203-8601', '0000-0003-0765-9054', '0000-0002-0744-0688', '0000-0002-9116-948X', '0000-0002-7513-6768']\n",
      "Total sample size after apply threshold:  316\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(316, 1124)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(316, 1124)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.99      0.87       155\n",
      "          1       1.00      0.73      0.84        44\n",
      "          2       1.00      0.75      0.86        60\n",
      "          3       0.97      0.78      0.86        36\n",
      "          4       1.00      0.57      0.73        21\n",
      "\n",
      "avg / total       0.89      0.86      0.85       316\n",
      "\n",
      "[154   0   0   1   0  12  32   0   0   0  15   0  45   0   0   8   0   0\n",
      "  28   0   9   0   0   0  12]\n",
      "svc Accuracy:  0.8575949367088608\n",
      "svc F1:  0.8321161111141728\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      1.00      0.84       155\n",
      "          1       1.00      0.59      0.74        44\n",
      "          2       1.00      0.72      0.83        60\n",
      "          3       1.00      0.69      0.82        36\n",
      "          4       1.00      0.48      0.65        21\n",
      "\n",
      "avg / total       0.87      0.82      0.81       316\n",
      "\n",
      "[155   0   0   0   0  18  26   0   0   0  17   0  43   0   0  11   0   0\n",
      "  25   0  11   0   0   0  10]\n",
      "LR Accuracy:  0.819620253164557\n",
      "LR F1:  0.7774657338278612\n",
      "For name:  s_hosseini\n",
      "total sample size before apply threshold:  25\n",
      "Counter({'0000-0001-6222-792X': 8, '0000-0002-5881-6796': 8, '0000-0002-5468-1281': 6, '0000-0002-0211-6248': 1, '0000-0002-0907-9427': 1, '0000-0001-7521-7907': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  d_vieira\n",
      "total sample size before apply threshold:  68\n",
      "Counter({'0000-0002-7366-6765': 60, '0000-0003-4232-9413': 5, '0000-0001-7388-6904': 2, '0000-0001-6288-2086': 1})\n",
      "['0000-0002-7366-6765']\n",
      "Total sample size after apply threshold:  60\n",
      "For name:  j_kang\n",
      "total sample size before apply threshold:  200\n",
      "Counter({'0000-0002-6350-3997': 57, '0000-0001-7311-6053': 42, '0000-0001-8995-5636': 25, '0000-0002-9181-6819': 15, '0000-0002-1412-6179': 11, '0000-0003-4788-0028': 11, '0000-0002-5262-2712': 9, '0000-0002-8467-2503': 8, '0000-0002-9425-847X': 6, '0000-0002-8660-7940': 4, '0000-0003-1610-6742': 3, '0000-0002-1841-5357': 3, '0000-0001-8894-2630': 3, '0000-0001-5013-2683': 1, '0000-0003-4200-1020': 1, '0000-0002-2603-9718': 1})\n",
      "['0000-0002-1412-6179', '0000-0002-6350-3997', '0000-0002-9181-6819', '0000-0001-7311-6053', '0000-0001-8995-5636', '0000-0003-4788-0028']\n",
      "Total sample size after apply threshold:  161\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(161, 216)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(161, 216)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.45      0.56        11\n",
      "          1       0.86      0.88      0.87        57\n",
      "          2       0.50      0.33      0.40        15\n",
      "          3       0.85      0.81      0.83        42\n",
      "          4       0.58      0.84      0.69        25\n",
      "          5       1.00      0.91      0.95        11\n",
      "\n",
      "avg / total       0.78      0.78      0.77       161\n",
      "\n",
      "[ 5  1  3  2  0  0  0 50  0  0  7  0  2  1  5  4  3  0  0  2  2 34  4  0\n",
      "  0  4  0  0 21  0  0  0  0  0  1 10]\n",
      "svc Accuracy:  0.7763975155279503\n",
      "svc F1:  0.715882434695779\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.55      0.63        11\n",
      "          1       0.89      0.95      0.92        57\n",
      "          2       0.83      0.33      0.48        15\n",
      "          3       0.84      0.90      0.87        42\n",
      "          4       0.68      0.84      0.75        25\n",
      "          5       1.00      0.91      0.95        11\n",
      "\n",
      "avg / total       0.84      0.83      0.82       161\n",
      "\n",
      "[ 6  1  1  3  0  0  0 54  0  0  3  0  2  1  5  4  3  0  0  1  0 38  3  0\n",
      "  0  4  0  0 21  0  0  0  0  0  1 10]\n",
      "LR Accuracy:  0.8322981366459627\n",
      "LR F1:  0.7664946386031316\n",
      "For name:  j_jensen\n",
      "total sample size before apply threshold:  388\n",
      "Counter({'0000-0002-4733-1224': 124, '0000-0002-7464-7435': 99, '0000-0003-0657-4032': 43, '0000-0001-6841-1808': 30, '0000-0002-1465-1010': 21, '0000-0003-3291-8468': 18, '0000-0002-2369-8291': 17, '0000-0003-4036-0521': 17, '0000-0001-6228-2988': 12, '0000-0002-7954-8073': 3, '0000-0001-9962-6166': 3, '0000-0003-1873-4531': 1})\n",
      "['0000-0003-0657-4032', '0000-0002-1465-1010', '0000-0002-2369-8291', '0000-0001-6841-1808', '0000-0001-6228-2988', '0000-0002-4733-1224', '0000-0003-4036-0521', '0000-0003-3291-8468', '0000-0002-7464-7435']\n",
      "Total sample size after apply threshold:  381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(381, 984)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(381, 984)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.65      0.78        43\n",
      "          1       0.88      0.71      0.79        21\n",
      "          2       1.00      0.29      0.45        17\n",
      "          3       1.00      0.80      0.89        30\n",
      "          4       1.00      0.92      0.96        12\n",
      "          5       0.64      1.00      0.78       124\n",
      "          6       1.00      0.71      0.83        17\n",
      "          7       0.77      0.56      0.65        18\n",
      "          8       0.92      0.72      0.81        99\n",
      "\n",
      "avg / total       0.84      0.79      0.78       381\n",
      "\n",
      "[ 28   0   0   0   0  14   0   0   1   0  15   0   0   0   6   0   0   0\n",
      "   0   0   5   0   0  12   0   0   0   0   1   0  24   0   4   0   0   1\n",
      "   0   0   0   0  11   1   0   0   0   0   0   0   0   0 124   0   0   0\n",
      "   0   1   0   0   0   3  12   0   1   0   0   0   0   0   5   0  10   3\n",
      "   1   0   0   0   0  24   0   3  71]\n",
      "svc Accuracy:  0.7874015748031497\n",
      "svc F1:  0.7699008453831601\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.65      0.76        43\n",
      "          1       0.94      0.71      0.81        21\n",
      "          2       1.00      0.24      0.38        17\n",
      "          3       1.00      0.83      0.91        30\n",
      "          4       1.00      0.92      0.96        12\n",
      "          5       0.68      0.98      0.81       124\n",
      "          6       0.90      0.53      0.67        17\n",
      "          7       1.00      0.50      0.67        18\n",
      "          8       0.83      0.81      0.82        99\n",
      "\n",
      "avg / total       0.83      0.80      0.79       381\n",
      "\n",
      "[ 28   0   0   0   0  10   0   0   5   0  15   0   0   0   4   0   0   2\n",
      "   0   0   4   0   0  13   0   0   0   0   0   0  25   0   4   0   0   1\n",
      "   0   0   0   0  11   1   0   0   0   0   0   0   0   0 122   0   0   2\n",
      "   0   1   0   0   0   4   9   0   3   3   0   0   0   0   3   0   9   3\n",
      "   0   0   0   0   0  18   1   0  80]\n",
      "LR Accuracy:  0.7952755905511811\n",
      "LR F1:  0.7525843642933612\n",
      "For name:  k_lai\n",
      "total sample size before apply threshold:  52\n",
      "Counter({'0000-0001-8135-6030': 36, '0000-0001-9296-0882': 5, '0000-0002-0037-792X': 4, '0000-0002-3365-3927': 2, '0000-0002-4069-054X': 2, '0000-0001-8203-4252': 1, '0000-0001-7734-0941': 1, '0000-0003-1478-4996': 1})\n",
      "['0000-0001-8135-6030']\n",
      "Total sample size after apply threshold:  36\n",
      "For name:  j_gonzalez\n",
      "total sample size before apply threshold:  39\n",
      "Counter({'0000-0001-5569-0705': 13, '0000-0003-3063-1770': 10, '0000-0002-3448-7393': 6, '0000-0002-9926-3323': 4, '0000-0002-0381-6393': 3, '0000-0002-0389-5263': 2, '0000-0003-3415-5943': 1})\n",
      "['0000-0003-3063-1770', '0000-0001-5569-0705']\n",
      "Total sample size after apply threshold:  23\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(23, 133)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(23, 133)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      1.00      0.95        10\n",
      "          1       1.00      0.92      0.96        13\n",
      "\n",
      "avg / total       0.96      0.96      0.96        23\n",
      "\n",
      "[10  0  1 12]\n",
      "svc Accuracy:  0.9565217391304348\n",
      "svc F1:  0.9561904761904763\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      1.00      0.95        10\n",
      "          1       1.00      0.92      0.96        13\n",
      "\n",
      "avg / total       0.96      0.96      0.96        23\n",
      "\n",
      "[10  0  1 12]\n",
      "LR Accuracy:  0.9565217391304348\n",
      "LR F1:  0.9561904761904763\n",
      "For name:  m_zakaria\n",
      "total sample size before apply threshold:  24\n",
      "Counter({'0000-0002-3694-3460': 10, '0000-0003-2525-0092': 8, '0000-0002-2698-615X': 5, '0000-0003-2456-6415': 1})\n",
      "['0000-0002-3694-3460']\n",
      "Total sample size after apply threshold:  10\n",
      "For name:  c_campos\n",
      "total sample size before apply threshold:  48\n",
      "Counter({'0000-0003-1734-6924': 31, '0000-0002-7616-8518': 10, '0000-0003-1809-8272': 3, '0000-0002-2070-8618': 1, '0000-0002-4978-5449': 1, '0000-0001-6054-4243': 1, '0000-0001-8592-5384': 1})\n",
      "['0000-0002-7616-8518', '0000-0003-1734-6924']\n",
      "Total sample size after apply threshold:  41\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(41, 278)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(41, 278)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.80      0.89        10\n",
      "          1       0.94      1.00      0.97        31\n",
      "\n",
      "avg / total       0.95      0.95      0.95        41\n",
      "\n",
      "[ 8  2  0 31]\n",
      "svc Accuracy:  0.9512195121951219\n",
      "svc F1:  0.9288194444444444\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        10\n",
      "          1       1.00      1.00      1.00        31\n",
      "\n",
      "avg / total       1.00      1.00      1.00        41\n",
      "\n",
      "[10  0  0 31]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  a_gad\n",
      "total sample size before apply threshold:  29\n",
      "Counter({'0000-0002-1098-9129': 17, '0000-0001-9741-2105': 10, '0000-0002-5298-5206': 1, '0000-0002-0762-0953': 1})\n",
      "['0000-0001-9741-2105', '0000-0002-1098-9129']\n",
      "Total sample size after apply threshold:  27\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(27, 111)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(27, 111)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.80      0.89        10\n",
      "          1       0.89      1.00      0.94        17\n",
      "\n",
      "avg / total       0.93      0.93      0.92        27\n",
      "\n",
      "[ 8  2  0 17]\n",
      "svc Accuracy:  0.9259259259259259\n",
      "svc F1:  0.9166666666666667\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.80      0.89        10\n",
      "          1       0.89      1.00      0.94        17\n",
      "\n",
      "avg / total       0.93      0.93      0.92        27\n",
      "\n",
      "[ 8  2  0 17]\n",
      "LR Accuracy:  0.9259259259259259\n",
      "LR F1:  0.9166666666666667\n",
      "For name:  y_zhao\n",
      "total sample size before apply threshold:  338\n",
      "Counter({'0000-0003-1215-2565': 48, '0000-0002-7916-8687': 47, '0000-0001-6783-5182': 20, '0000-0002-9408-9979': 20, '0000-0002-6541-0612': 18, '0000-0002-5455-2586': 17, '0000-0002-2903-4218': 16, '0000-0003-0302-3470': 16, '0000-0002-6184-2530': 15, '0000-0003-1035-2272': 15, '0000-0002-6923-1099': 13, '0000-0002-1442-992X': 12, '0000-0001-6747-1665': 12, '0000-0003-3618-1379': 11, '0000-0002-9231-8360': 11, '0000-0003-1384-6024': 9, '0000-0002-0278-7543': 7, '0000-0001-8541-893X': 5, '0000-0001-8986-9164': 4, '0000-0002-2944-1315': 4, '0000-0001-8970-9398': 3, '0000-0001-8925-9462': 2, '0000-0003-1254-6732': 2, '0000-0002-5866-5932': 2, '0000-0001-8808-9481': 2, '0000-0003-1815-1408': 1, '0000-0002-4148-2603': 1, '0000-0003-1490-0416': 1, '0000-0002-7761-0072': 1, '0000-0002-6806-1593': 1, '0000-0003-4188-5725': 1, '0000-0003-2289-5709': 1})\n",
      "['0000-0002-1442-992X', '0000-0002-6541-0612', '0000-0003-3618-1379', '0000-0001-6783-5182', '0000-0002-6184-2530', '0000-0001-6747-1665', '0000-0002-2903-4218', '0000-0002-5455-2586', '0000-0002-9231-8360', '0000-0002-6923-1099', '0000-0003-1035-2272', '0000-0003-1215-2565', '0000-0002-7916-8687', '0000-0003-0302-3470', '0000-0002-9408-9979']\n",
      "Total sample size after apply threshold:  291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(291, 434)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(291, 434)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.22      0.17      0.19        12\n",
      "          1       0.58      0.61      0.59        18\n",
      "          2       1.00      1.00      1.00        11\n",
      "          3       1.00      1.00      1.00        20\n",
      "          4       0.62      0.67      0.65        15\n",
      "          5       0.57      0.67      0.62        12\n",
      "          6       0.80      1.00      0.89        16\n",
      "          7       1.00      0.88      0.94        17\n",
      "          8       0.82      0.82      0.82        11\n",
      "          9       0.67      0.31      0.42        13\n",
      "         10       0.65      0.73      0.69        15\n",
      "         11       0.67      0.96      0.79        48\n",
      "         12       0.81      0.72      0.76        47\n",
      "         13       0.90      0.56      0.69        16\n",
      "         14       0.83      0.50      0.62        20\n",
      "\n",
      "avg / total       0.75      0.74      0.73       291\n",
      "\n",
      "[ 2  2  0  0  2  1  1  0  1  0  1  1  1  0  0  1 11  0  0  1  0  0  0  0\n",
      "  0  2  3  0  0  0  0  0 11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      " 20  0  0  0  0  0  0  0  0  0  0  0  2  1  0  0 10  0  0  0  0  0  0  1\n",
      "  1  0  0  1  1  0  0  0  8  1  0  0  0  0  1  0  0  0  0  0  0  0  0  0\n",
      " 16  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0 15  0  0  1  0  0  0  0\n",
      "  0  0  0  0  1  0  0  0  9  0  0  0  1  0  0  0  0  0  0  0  1  0  0  1\n",
      "  4  0  4  2  1  0  0  2  0  0  0  1  0  0  0  0 11  0  1  0  0  0  1  0\n",
      "  0  0  0  0  0  0  1  0 46  0  0  0  2  0  0  0  2  2  2  0  0  0  0  5\n",
      " 34  0  0  0  0  0  0  0  1  0  0  0  1  2  1  0  9  2  0  1  0  0  0  0\n",
      "  0  0  0  0  0  7  2  0 10]\n",
      "svc Accuracy:  0.7422680412371134\n",
      "svc F1:  0.7110944967920225\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.33      0.17      0.22        12\n",
      "          1       0.75      0.67      0.71        18\n",
      "          2       1.00      0.73      0.84        11\n",
      "          3       1.00      1.00      1.00        20\n",
      "          4       0.71      0.67      0.69        15\n",
      "          5       0.90      0.75      0.82        12\n",
      "          6       0.89      1.00      0.94        16\n",
      "          7       0.94      0.88      0.91        17\n",
      "          8       1.00      0.82      0.90        11\n",
      "          9       0.50      0.15      0.24        13\n",
      "         10       1.00      0.80      0.89        15\n",
      "         11       0.58      0.96      0.72        48\n",
      "         12       0.71      0.79      0.75        47\n",
      "         13       0.75      0.75      0.75        16\n",
      "         14       1.00      0.55      0.71        20\n",
      "\n",
      "avg / total       0.78      0.76      0.75       291\n",
      "\n",
      "[ 2  0  0  0  2  1  1  0  0  0  0  3  2  1  0  1 12  0  0  0  0  0  0  0\n",
      "  0  0  5  0  0  0  0  0  8  0  0  0  0  0  0  0  0  3  0  0  0  0  0  0\n",
      " 20  0  0  0  0  0  0  0  0  0  0  0  1  1  0  0 10  0  0  0  0  0  0  0\n",
      "  2  1  0  0  0  0  0  0  9  0  1  0  1  0  1  0  0  0  0  0  0  0  0  0\n",
      " 16  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0 15  0  0  0  0  1  0  0\n",
      "  0  0  0  0  1  0  0  0  9  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0\n",
      "  2  0  6  3  2  0  0  1  0  0  0  0  0  0  0  0 12  1  1  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0 46  2  0  0  2  0  0  0  1  0  1  0  0  0  0  6\n",
      " 37  0  0  0  0  0  0  0  0  0  0  0  1  0  1  2 12  0  0  1  0  0  0  0\n",
      "  0  0  0  0  0  7  1  0 11]\n",
      "LR Accuracy:  0.7594501718213058\n",
      "LR F1:  0.7389372553853655\n",
      "For name:  s_hussain\n",
      "total sample size before apply threshold:  52\n",
      "Counter({'0000-0001-7894-2485': 18, '0000-0002-3298-6260': 11, '0000-0002-9765-0565': 9, '0000-0001-8564-9113': 5, '0000-0001-6835-7207': 2, '0000-0001-6687-7591': 2, '0000-0001-8537-7322': 1, '0000-0002-0529-7451': 1, '0000-0002-7164-3076': 1, '0000-0003-1342-143X': 1, '0000-0001-8475-9791': 1})\n",
      "['0000-0002-3298-6260', '0000-0001-7894-2485']\n",
      "Total sample size after apply threshold:  29\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(29, 66)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(29, 66)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.91      0.87        11\n",
      "          1       0.94      0.89      0.91        18\n",
      "\n",
      "avg / total       0.90      0.90      0.90        29\n",
      "\n",
      "[10  1  2 16]\n",
      "svc Accuracy:  0.896551724137931\n",
      "svc F1:  0.8919254658385093\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.91      0.91        11\n",
      "          1       0.94      0.94      0.94        18\n",
      "\n",
      "avg / total       0.93      0.93      0.93        29\n",
      "\n",
      "[10  1  1 17]\n",
      "LR Accuracy:  0.9310344827586207\n",
      "LR F1:  0.9267676767676767\n",
      "For name:  k_scott\n",
      "total sample size before apply threshold:  16\n",
      "Counter({'0000-0001-7263-6778': 11, '0000-0001-7952-0348': 3, '0000-0002-7066-887X': 1, '0000-0003-0345-5417': 1})\n",
      "['0000-0001-7263-6778']\n",
      "Total sample size after apply threshold:  11\n",
      "For name:  a_martinez\n",
      "total sample size before apply threshold:  180\n",
      "Counter({'0000-0003-1643-6506': 64, '0000-0002-2707-8110': 56, '0000-0002-4804-6687': 20, '0000-0003-4882-4044': 17, '0000-0003-0710-2336': 10, '0000-0002-4395-0511': 7, '0000-0001-5448-0140': 4, '0000-0001-9076-6197': 2})\n",
      "['0000-0003-0710-2336', '0000-0003-4882-4044', '0000-0003-1643-6506', '0000-0002-2707-8110', '0000-0002-4804-6687']\n",
      "Total sample size after apply threshold:  167\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(167, 542)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(167, 542)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        10\n",
      "          1       1.00      0.29      0.45        17\n",
      "          2       0.78      0.97      0.87        64\n",
      "          3       0.86      0.86      0.86        56\n",
      "          4       1.00      0.85      0.92        20\n",
      "\n",
      "avg / total       0.87      0.85      0.84       167\n",
      "\n",
      "[10  0  0  0  0  0  5  7  5  0  0  0 62  2  0  0  0  8 48  0  0  0  2  1\n",
      " 17]\n",
      "svc Accuracy:  0.8502994011976048\n",
      "svc F1:  0.8195480195480196\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        10\n",
      "          1       1.00      0.18      0.30        17\n",
      "          2       0.76      1.00      0.86        64\n",
      "          3       0.94      0.88      0.91        56\n",
      "          4       1.00      0.90      0.95        20\n",
      "\n",
      "avg / total       0.89      0.86      0.84       167\n",
      "\n",
      "[10  0  0  0  0  0  3 12  2  0  0  0 64  0  0  0  0  7 49  0  0  0  1  1\n",
      " 18]\n",
      "LR Accuracy:  0.8622754491017964\n",
      "LR F1:  0.8039281386649808\n",
      "For name:  r_luz\n",
      "total sample size before apply threshold:  20\n",
      "Counter({'0000-0002-3999-4298': 9, '0000-0002-1021-5772': 6, '0000-0003-0045-6959': 3, '0000-0003-3051-5710': 2})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  p_tran\n",
      "total sample size before apply threshold:  18\n",
      "Counter({'0000-0003-3283-1450': 6, '0000-0003-2405-2086': 5, '0000-0003-1735-6903': 2, '0000-0002-7319-872X': 2, '0000-0003-3438-5829': 1, '0000-0001-9788-3433': 1, '0000-0003-4619-4376': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  e_romero\n",
      "total sample size before apply threshold:  23\n",
      "Counter({'0000-0001-9087-2790': 20, '0000-0003-1430-8412': 1, '0000-0003-1858-1264': 1, '0000-0003-3115-7572': 1})\n",
      "['0000-0001-9087-2790']\n",
      "Total sample size after apply threshold:  20\n",
      "For name:  j_stevens\n",
      "total sample size before apply threshold:  161\n",
      "Counter({'0000-0002-6348-9347': 75, '0000-0002-9867-7209': 41, '0000-0002-1013-8447': 13, '0000-0003-2375-1360': 11, '0000-0003-1601-9008': 9, '0000-0003-4674-0314': 4, '0000-0003-0182-3829': 4, '0000-0002-4661-3481': 3, '0000-0002-2234-1960': 1})\n",
      "['0000-0003-2375-1360', '0000-0002-1013-8447', '0000-0002-9867-7209', '0000-0002-6348-9347']\n",
      "Total sample size after apply threshold:  140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(140, 349)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(140, 349)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.18      0.31        11\n",
      "          1       1.00      0.92      0.96        13\n",
      "          2       1.00      0.80      0.89        41\n",
      "          3       0.81      1.00      0.89        75\n",
      "\n",
      "avg / total       0.90      0.87      0.85       140\n",
      "\n",
      "[ 2  0  0  9  0 12  0  1  0  0 33  8  0  0  0 75]\n",
      "svc Accuracy:  0.8714285714285714\n",
      "svc F1:  0.7631103356103356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gao137/intel/intelpython3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/gao137/intel/intelpython3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00        11\n",
      "          1       1.00      0.92      0.96        13\n",
      "          2       1.00      0.83      0.91        41\n",
      "          3       0.80      1.00      0.89        75\n",
      "\n",
      "avg / total       0.81      0.86      0.83       140\n",
      "\n",
      "[ 0  0  0 11  0 12  0  1  0  0 34  7  0  0  0 75]\n",
      "LR Accuracy:  0.8642857142857143\n",
      "LR F1:  0.688560157790927\n",
      "For name:  l_you\n",
      "total sample size before apply threshold:  32\n",
      "Counter({'0000-0001-7304-0474': 12, '0000-0003-3058-2884': 12, '0000-0003-1162-0064': 7, '0000-0002-4741-0715': 1})\n",
      "['0000-0001-7304-0474', '0000-0003-3058-2884']\n",
      "Total sample size after apply threshold:  24\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(24, 63)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(24, 63)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.75      0.78        12\n",
      "          1       0.77      0.83      0.80        12\n",
      "\n",
      "avg / total       0.79      0.79      0.79        24\n",
      "\n",
      "[ 9  3  2 10]\n",
      "svc Accuracy:  0.7916666666666666\n",
      "svc F1:  0.7913043478260869\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.83      0.83        12\n",
      "          1       0.83      0.83      0.83        12\n",
      "\n",
      "avg / total       0.83      0.83      0.83        24\n",
      "\n",
      "[10  2  2 10]\n",
      "LR Accuracy:  0.8333333333333334\n",
      "LR F1:  0.8333333333333334\n",
      "For name:  p_stevenson\n",
      "total sample size before apply threshold:  122\n",
      "Counter({'0000-0002-3520-5060': 86, '0000-0001-6780-6859': 33, '0000-0002-3232-5155': 2, '0000-0002-6616-0328': 1})\n",
      "['0000-0002-3520-5060', '0000-0001-6780-6859']\n",
      "Total sample size after apply threshold:  119\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(119, 135)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(119, 135)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99        86\n",
      "          1       1.00      0.97      0.98        33\n",
      "\n",
      "avg / total       0.99      0.99      0.99       119\n",
      "\n",
      "[86  0  1 32]\n",
      "svc Accuracy:  0.9915966386554622\n",
      "svc F1:  0.9894175188972878\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99        86\n",
      "          1       1.00      0.97      0.98        33\n",
      "\n",
      "avg / total       0.99      0.99      0.99       119\n",
      "\n",
      "[86  0  1 32]\n",
      "LR Accuracy:  0.9915966386554622\n",
      "LR F1:  0.9894175188972878\n",
      "For name:  t_kang\n",
      "total sample size before apply threshold:  26\n",
      "Counter({'0000-0001-5939-6247': 16, '0000-0002-4589-9772': 8, '0000-0002-8444-9889': 1, '0000-0001-6570-2570': 1})\n",
      "['0000-0001-5939-6247']\n",
      "Total sample size after apply threshold:  16\n",
      "For name:  s_mohanty\n",
      "total sample size before apply threshold:  67\n",
      "Counter({'0000-0001-6601-944X': 43, '0000-0001-9822-427X': 12, '0000-0002-2142-5572': 9, '0000-0003-4464-8434': 2, '0000-0002-1378-3775': 1})\n",
      "['0000-0001-9822-427X', '0000-0001-6601-944X']\n",
      "Total sample size after apply threshold:  55\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(55, 168)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(55, 168)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.92      0.92        12\n",
      "          1       0.98      0.98      0.98        43\n",
      "\n",
      "avg / total       0.96      0.96      0.96        55\n",
      "\n",
      "[11  1  1 42]\n",
      "svc Accuracy:  0.9636363636363636\n",
      "svc F1:  0.946705426356589\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.92      0.92        12\n",
      "          1       0.98      0.98      0.98        43\n",
      "\n",
      "avg / total       0.96      0.96      0.96        55\n",
      "\n",
      "[11  1  1 42]\n",
      "LR Accuracy:  0.9636363636363636\n",
      "LR F1:  0.946705426356589\n",
      "For name:  m_amorim\n",
      "total sample size before apply threshold:  95\n",
      "Counter({'0000-0001-8137-3295': 55, '0000-0002-4159-4023': 20, '0000-0002-4129-6659': 13, '0000-0002-3831-9602': 4, '0000-0002-6872-6671': 1, '0000-0002-0901-0614': 1, '0000-0003-1516-8407': 1})\n",
      "['0000-0002-4159-4023', '0000-0002-4129-6659', '0000-0001-8137-3295']\n",
      "Total sample size after apply threshold:  88\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(88, 141)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(88, 141)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.80      0.89        20\n",
      "          1       0.91      0.77      0.83        13\n",
      "          2       0.90      1.00      0.95        55\n",
      "\n",
      "avg / total       0.93      0.92      0.92        88\n",
      "\n",
      "[16  1  3  0 10  3  0  0 55]\n",
      "svc Accuracy:  0.9204545454545454\n",
      "svc F1:  0.8901660280970626\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.85      0.92        20\n",
      "          1       1.00      0.62      0.76        13\n",
      "          2       0.87      1.00      0.93        55\n",
      "\n",
      "avg / total       0.92      0.91      0.90        88\n",
      "\n",
      "[17  0  3  0  8  5  0  0 55]\n",
      "LR Accuracy:  0.9090909090909091\n",
      "LR F1:  0.8710090235513963\n",
      "For name:  y_kamiya\n",
      "total sample size before apply threshold:  161\n",
      "Counter({'0000-0003-4415-520X': 113, '0000-0001-9790-9867': 42, '0000-0001-8716-2536': 5, '0000-0002-0758-0234': 1})\n",
      "['0000-0003-4415-520X', '0000-0001-9790-9867']\n",
      "Total sample size after apply threshold:  155\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(155, 609)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(155, 609)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.99       113\n",
      "          1       1.00      0.93      0.96        42\n",
      "\n",
      "avg / total       0.98      0.98      0.98       155\n",
      "\n",
      "[113   0   3  39]\n",
      "svc Accuracy:  0.9806451612903225\n",
      "svc F1:  0.9749312631408702\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      1.00      0.97       113\n",
      "          1       1.00      0.81      0.89        42\n",
      "\n",
      "avg / total       0.95      0.95      0.95       155\n",
      "\n",
      "[113   0   8  34]\n",
      "LR Accuracy:  0.9483870967741935\n",
      "LR F1:  0.9302744039586145\n",
      "For name:  w_he\n",
      "total sample size before apply threshold:  48\n",
      "Counter({'0000-0003-3254-1242': 20, '0000-0003-3137-8420': 16, '0000-0003-0161-3274': 7, '0000-0003-1236-3047': 5})\n",
      "['0000-0003-3254-1242', '0000-0003-3137-8420']\n",
      "Total sample size after apply threshold:  36\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(36, 72)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(36, 72)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        20\n",
      "          1       1.00      1.00      1.00        16\n",
      "\n",
      "avg / total       1.00      1.00      1.00        36\n",
      "\n",
      "[20  0  0 16]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.95      0.97        20\n",
      "          1       0.94      1.00      0.97        16\n",
      "\n",
      "avg / total       0.97      0.97      0.97        36\n",
      "\n",
      "[19  1  0 16]\n",
      "LR Accuracy:  0.9722222222222222\n",
      "LR F1:  0.9720279720279721\n",
      "For name:  t_kato\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total sample size before apply threshold:  249\n",
      "Counter({'0000-0001-7856-3952': 218, '0000-0002-0827-0051': 18, '0000-0003-1061-5888': 3, '0000-0002-9312-6272': 3, '0000-0002-1469-0685': 2, '0000-0002-7095-5676': 1, '0000-0003-3757-3243': 1, '0000-0003-4063-0042': 1, '0000-0003-4473-1131': 1, '0000-0002-6009-7962': 1})\n",
      "['0000-0002-0827-0051', '0000-0001-7856-3952']\n",
      "Total sample size after apply threshold:  236\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(236, 546)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(236, 546)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00        18\n",
      "          1       0.92      0.99      0.95       218\n",
      "\n",
      "avg / total       0.85      0.91      0.88       236\n",
      "\n",
      "[  0  18   3 215]\n",
      "svc Accuracy:  0.9110169491525424\n",
      "svc F1:  0.47671840354767187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gao137/intel/intelpython3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/gao137/intel/intelpython3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00        18\n",
      "          1       0.92      1.00      0.96       218\n",
      "\n",
      "avg / total       0.85      0.92      0.89       236\n",
      "\n",
      "[  0  18   0 218]\n",
      "LR Accuracy:  0.923728813559322\n",
      "LR F1:  0.4801762114537445\n",
      "For name:  a_ward\n",
      "total sample size before apply threshold:  164\n",
      "Counter({'0000-0001-7945-7975': 92, '0000-0003-4102-8694': 40, '0000-0002-7000-2453': 10, '0000-0001-6948-4814': 9, '0000-0002-6376-0061': 6, '0000-0003-0038-9426': 4, '0000-0002-9774-8677': 2, '0000-0003-1321-3358': 1})\n",
      "['0000-0001-7945-7975', '0000-0002-7000-2453', '0000-0003-4102-8694']\n",
      "Total sample size after apply threshold:  142\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(142, 376)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(142, 376)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      1.00      0.96        92\n",
      "          1       1.00      0.90      0.95        10\n",
      "          2       1.00      0.82      0.90        40\n",
      "\n",
      "avg / total       0.95      0.94      0.94       142\n",
      "\n",
      "[92  0  0  1  9  0  7  0 33]\n",
      "svc Accuracy:  0.9436619718309859\n",
      "svc F1:  0.9366037811423537\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      1.00      0.93        92\n",
      "          1       1.00      0.40      0.57        10\n",
      "          2       1.00      0.80      0.89        40\n",
      "\n",
      "avg / total       0.91      0.90      0.89       142\n",
      "\n",
      "[92  0  0  6  4  0  8  0 32]\n",
      "LR Accuracy:  0.9014084507042254\n",
      "LR F1:  0.7965367965367965\n",
      "For name:  j_chen\n",
      "total sample size before apply threshold:  1139\n",
      "Counter({'0000-0001-5077-4483': 92, '0000-0002-5756-3336': 87, '0000-0001-7858-8236': 73, '0000-0002-1752-4201': 61, '0000-0002-9220-8436': 55, '0000-0001-5859-3070': 51, '0000-0003-2996-5781': 49, '0000-0001-8807-3607': 43, '0000-0001-6527-4801': 41, '0000-0001-6879-5936': 35, '0000-0002-7253-2722': 33, '0000-0001-7336-8808': 31, '0000-0001-8634-1145': 29, '0000-0001-6491-6577': 28, '0000-0002-0662-782X': 19, '0000-0001-7381-0918': 18, '0000-0002-4429-283X': 17, '0000-0001-5168-7074': 16, '0000-0002-1591-9744': 14, '0000-0002-7409-7859': 14, '0000-0002-8021-7458': 13, '0000-0002-7530-4215': 12, '0000-0002-4114-3046': 12, '0000-0001-9970-4582': 12, '0000-0002-7000-1469': 11, '0000-0002-3850-4875': 11, '0000-0001-5648-9202': 11, '0000-0002-1038-4162': 11, '0000-0002-3671-553X': 10, '0000-0002-3329-6384': 9, '0000-0001-6661-1734': 9, '0000-0003-0326-8304': 9, '0000-0001-9202-404X': 8, '0000-0001-6321-0505': 8, '0000-0002-5323-1801': 8, '0000-0001-7942-0187': 8, '0000-0003-0339-5880': 8, '0000-0002-0708-6498': 8, '0000-0003-4599-3600': 8, '0000-0002-1071-2234': 7, '0000-0003-3158-9471': 7, '0000-0001-8446-1821': 6, '0000-0002-5684-6692': 6, '0000-0003-1403-1708': 6, '0000-0001-5637-1829': 6, '0000-0001-5507-235X': 6, '0000-0002-6497-4141': 6, '0000-0001-6109-8433': 6, '0000-0002-4801-5397': 5, '0000-0002-6664-2597': 5, '0000-0003-3987-4816': 5, '0000-0002-2424-3969': 4, '0000-0002-7107-2867': 4, '0000-0002-2454-0058': 4, '0000-0003-4188-6189': 3, '0000-0002-9964-293X': 3, '0000-0002-2926-1090': 3, '0000-0001-7547-6423': 3, '0000-0001-5205-923X': 3, '0000-0002-3124-5452': 3, '0000-0002-5042-6179': 3, '0000-0002-0963-3520': 2, '0000-0002-6981-3363': 2, '0000-0002-5258-9035': 2, '0000-0002-2481-8814': 2, '0000-0002-6989-9048': 2, '0000-0003-0447-7466': 2, '0000-0001-8714-2543': 2, '0000-0003-3767-9486': 2, '0000-0003-0320-8707': 2, '0000-0002-3764-1149': 2, '0000-0001-9100-4784': 1, '0000-0002-7226-1678': 1, '0000-0002-4923-5076': 1, '0000-0002-2315-6070': 1, '0000-0002-4833-6756': 1, '0000-0001-6234-1001': 1, '0000-0003-2844-6947': 1, '0000-0002-2745-442X': 1, '0000-0001-6075-4806': 1, '0000-0002-6387-6814': 1, '0000-0002-5433-5178': 1, '0000-0003-4612-3279': 1, '0000-0001-7299-0355': 1, '0000-0001-7962-0840': 1, '0000-0003-4693-5234': 1, '0000-0002-1758-4634': 1, '0000-0001-7360-2238': 1, '0000-0001-5636-985X': 1, '0000-0002-9320-6774': 1, '0000-0002-7808-2670': 1, '0000-0001-5302-2463': 1, '0000-0002-0934-8519': 1, '0000-0002-0024-4561': 1})\n",
      "['0000-0001-7336-8808', '0000-0001-7858-8236', '0000-0002-7253-2722', '0000-0002-9220-8436', '0000-0001-6491-6577', '0000-0002-1591-9744', '0000-0002-4429-283X', '0000-0001-5077-4483', '0000-0002-7000-1469', '0000-0002-5756-3336', '0000-0001-5168-7074', '0000-0003-2996-5781', '0000-0002-7530-4215', '0000-0002-3671-553X', '0000-0002-4114-3046', '0000-0001-8807-3607', '0000-0001-7381-0918', '0000-0002-0662-782X', '0000-0002-1752-4201', '0000-0001-6527-4801', '0000-0001-9970-4582', '0000-0002-3850-4875', '0000-0001-5648-9202', '0000-0002-1038-4162', '0000-0002-7409-7859', '0000-0002-8021-7458', '0000-0001-5859-3070', '0000-0001-6879-5936', '0000-0001-8634-1145']\n",
      "Total sample size after apply threshold:  909\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(909, 1178)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(909, 1178)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      1.00      0.85        31\n",
      "          1       0.68      0.86      0.76        73\n",
      "          2       1.00      0.70      0.82        33\n",
      "          3       0.51      0.87      0.64        55\n",
      "          4       0.96      0.96      0.96        28\n",
      "          5       0.82      0.64      0.72        14\n",
      "          6       0.93      0.82      0.87        17\n",
      "          7       0.78      0.80      0.79        92\n",
      "          8       0.50      0.18      0.27        11\n",
      "          9       0.81      0.91      0.86        87\n",
      "         10       1.00      0.88      0.93        16\n",
      "         11       0.66      0.43      0.52        49\n",
      "         12       0.24      0.42      0.30        12\n",
      "         13       1.00      0.80      0.89        10\n",
      "         14       0.86      0.50      0.63        12\n",
      "         15       0.91      0.49      0.64        43\n",
      "         16       1.00      0.89      0.94        18\n",
      "         17       0.92      0.63      0.75        19\n",
      "         18       0.76      0.82      0.79        61\n",
      "         19       0.51      0.59      0.55        41\n",
      "         20       1.00      1.00      1.00        12\n",
      "         21       1.00      0.82      0.90        11\n",
      "         22       0.43      0.55      0.48        11\n",
      "         23       1.00      0.55      0.71        11\n",
      "         24       1.00      0.79      0.88        14\n",
      "         25       0.56      0.69      0.62        13\n",
      "         26       0.73      0.73      0.73        51\n",
      "         27       1.00      0.40      0.57        35\n",
      "         28       1.00      0.93      0.96        29\n",
      "\n",
      "avg / total       0.78      0.75      0.74       909\n",
      "\n",
      "[31  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0 63  0  1  0  0  0  3  1  0  0  0  4  0  0  0  0  0  1\n",
      "  0  0  0  0  0  0  0  0  0  0  1  0 23  5  0  1  0  0  0  3  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  2  0 48  0  0  0  1  1\n",
      "  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  1  0  0  0  0  0  1\n",
      " 27  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  1  0  0  0  9  0  0  0  0  0  0  0  0  1  0  0  0  1  1  0  0  0\n",
      "  0  0  1  0  0  0  0  0  0  0  0  0 14  1  0  0  0  0  0  0  0  0  0  0\n",
      "  0  1  0  0  1  0  0  0  0  0  0  1  5  0  2  0  0  0 74  0  1  0  0  2\n",
      "  0  0  0  0  0  1  4  0  0  2  0  0  0  0  0  0  0  1  0  6  0  0  0  0\n",
      "  2  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0\n",
      "  1  0  0  0  0  0 79  0  0  1  0  0  0  0  0  3  0  0  0  0  0  0  1  2\n",
      "  0  0  0  0  0  0  0  0  0  0  0  1 14  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  1  0  0  0  2  6  0  1  0  0  0  3  0  0  0 21  1  0  0  2  0\n",
      "  0  2  6  0  0  3  0  0  0  2  0  0  0  3  0  0  0  0  0  3  0  0  0  0\n",
      "  5  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  8  0  0  0  0  1  0  0  0  0  0  0  0  1  0  0  1  0\n",
      "  0  1  0  0  0  0  0  1  0  1  0  0  6  0  0  0  0  0  0  0  0  0  0  0\n",
      "  2  0  0  0  3  0  6  0  0  0  2  0  3  0  1  2  0  0 21  0  0  3  2  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  1  0  0  0  0  0  0\n",
      " 16  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  1  0  0  0  2  0  0  0\n",
      "  0  0  0  0  0  0 12  0  3  0  0  0  0  0  0  0  0  0  0  1  0  3  0  0\n",
      "  0  1  0  1  0  1  2  0  0  0  0  0 50  0  0  0  0  0  0  0  2  0  0  1\n",
      "  4  0  1  0  1  0  1  0  0  0  5  1  0  0  0  0  1  0 24  0  0  1  0  0\n",
      "  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      " 12  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  1  0  0  0  0  0\n",
      "  0  0  0  0  0  0  9  0  0  0  0  0  0  0  0  1  0  0  0  0  0  2  0  0\n",
      "  0  1  1  0  0  0  0  0  0  0  0  0  6  0  0  0  0  0  0  0  1  0  3  0\n",
      "  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  6  0  0  0  0  0\n",
      "  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      " 11  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0  1\n",
      "  0  0  0  0  0  0  9  1  0  0  3  0  0  1  1  0  0  2  0  2  0  0  0  0\n",
      "  0  0  0  0  0  3  0  0  0  0  0  2 37  0  0  1  1  0  8  0  0  0  0  0\n",
      "  2  0  0  2  0  0  0  0  0  2  2  0  0  0  0  0  2  1 14  0  0  0  0  0\n",
      "  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0\n",
      " 27]\n",
      "svc Accuracy:  0.7458745874587459\n",
      "svc F1:  0.7356444716607748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      1.00      0.91        31\n",
      "          1       0.69      0.85      0.76        73\n",
      "          2       1.00      0.85      0.92        33\n",
      "          3       0.60      0.91      0.72        55\n",
      "          4       1.00      0.96      0.98        28\n",
      "          5       1.00      0.57      0.73        14\n",
      "          6       0.88      0.82      0.85        17\n",
      "          7       0.70      0.85      0.77        92\n",
      "          8       1.00      0.18      0.31        11\n",
      "          9       0.74      0.92      0.82        87\n",
      "         10       1.00      0.81      0.90        16\n",
      "         11       0.80      0.57      0.67        49\n",
      "         12       0.27      0.25      0.26        12\n",
      "         13       1.00      0.80      0.89        10\n",
      "         14       1.00      0.50      0.67        12\n",
      "         15       0.90      0.63      0.74        43\n",
      "         16       1.00      0.89      0.94        18\n",
      "         17       0.93      0.74      0.82        19\n",
      "         18       0.80      0.85      0.83        61\n",
      "         19       0.54      0.61      0.57        41\n",
      "         20       1.00      1.00      1.00        12\n",
      "         21       0.80      0.73      0.76        11\n",
      "         22       0.00      0.00      0.00        11\n",
      "         23       1.00      0.45      0.62        11\n",
      "         24       1.00      0.86      0.92        14\n",
      "         25       0.90      0.69      0.78        13\n",
      "         26       0.59      0.65      0.62        51\n",
      "         27       0.89      0.49      0.63        35\n",
      "         28       1.00      0.93      0.96        29\n",
      "\n",
      "avg / total       0.78      0.76      0.76       909\n",
      "\n",
      "[31  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0 62  0  0  0  0  0  6  0  0  0  1  2  0  0  0  0  0  1\n",
      "  1  0  0  0  0  0  0  0  0  0  1  1 28  1  0  0  0  0  0  2  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0 50  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  2  1  0  0  0  0  0  0  1  0  0  0  0  0  1\n",
      " 27  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  2  0  0  0  8  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0\n",
      "  0  0  1  2  0  0  0  0  0  0  0  0 14  2  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  1  0  0  4  0  1  0  0  0 78  0  1  0  2  1\n",
      "  0  0  0  0  1  1  2  0  0  1  0  0  0  0  0  0  0  0  0  6  0  0  0  0\n",
      "  2  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  2  0  0  0  0  0\n",
      "  1  0  0  0  0  0 80  0  1  1  0  0  0  0  0  2  0  0  0  0  0  0  0  2\n",
      "  0  0  0  0  0  0  0  0  0  0  0  3 13  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  1  5  0  1  0  0  1  3  0  0  0 28  0  0  0  2  0\n",
      "  0  1  4  0  0  0  0  0  0  3  0  0  0  1  0  2  0  0  0  3  0  0  0  1\n",
      "  3  0  0  0  0  0  0  0  0  0  1  0  0  0  0  1  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  8  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  1  0  0  0  0  0  2  0  0  0  0  6  0  0  0  0  0  0  0  0  0  0  0\n",
      "  3  0  0  0  3  0  4  0  0  0  2  0  4  0  0  1  0  0 27  0  0  0  0  0\n",
      "  0  1  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0\n",
      " 16  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0  0  0\n",
      "  0  0  0  0  0  0 14  0  3  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0\n",
      "  1  0  0  5  0  0  0  0  0  1  0  0 52  0  0  0  0  0  0  0  1  0  0  0\n",
      "  6  0  1  0  0  0  4  0  1  0  2  0  0  0  0  0  0  0 25  0  0  0  0  0\n",
      "  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      " 12  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  1  0  0  0  0  0\n",
      "  0  0  0  0  0  0  8  0  0  0  0  1  0  0  0  2  0  0  0  0  0  7  0  1\n",
      "  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  2  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  5  0  0  1  0  0\n",
      "  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      " 12  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0\n",
      "  0  0  1  0  0  0  9  2  0  0  3  1  0  1  0  0  0  2  0  5  0  0  0  0\n",
      "  0  0  0  0  1  4  0  1  0  0  0  0 33  0  0  1  1  0  7  0  0  0  0  0\n",
      "  1  0  0  2  0  0  0  0  0  2  2  0  0  0  0  0  0  2 17  0  0  0  0  0\n",
      "  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      " 27]\n",
      "LR Accuracy:  0.7645764576457645\n",
      "LR F1:  0.7364461450789428\n",
      "For name:  m_tseng\n",
      "total sample size before apply threshold:  141\n",
      "Counter({'0000-0002-9969-9055': 85, '0000-0002-0114-102X': 24, '0000-0001-8354-7586': 15, '0000-0003-3763-9548': 10, '0000-0002-2702-3590': 5, '0000-0001-6310-4390': 1, '0000-0001-8641-587X': 1})\n",
      "['0000-0003-3763-9548', '0000-0001-8354-7586', '0000-0002-0114-102X', '0000-0002-9969-9055']\n",
      "Total sample size after apply threshold:  134\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(134, 152)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(134, 152)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.90      0.95        10\n",
      "          1       0.82      0.60      0.69        15\n",
      "          2       0.83      0.83      0.83        24\n",
      "          3       0.93      0.99      0.96        85\n",
      "\n",
      "avg / total       0.91      0.91      0.91       134\n",
      "\n",
      "[ 9  1  0  0  0  9  3  3  0  1 20  3  0  0  1 84]\n",
      "svc Accuracy:  0.9104477611940298\n",
      "svc F1:  0.8582523616734143\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.90      0.95        10\n",
      "          1       0.82      0.60      0.69        15\n",
      "          2       0.80      0.67      0.73        24\n",
      "          3       0.89      0.99      0.94        85\n",
      "\n",
      "avg / total       0.88      0.88      0.87       134\n",
      "\n",
      "[ 9  1  0  0  0  9  3  3  0  1 16  7  0  0  1 84]\n",
      "LR Accuracy:  0.8805970149253731\n",
      "LR F1:  0.8263740816666426\n",
      "For name:  c_henderson\n",
      "total sample size before apply threshold:  107\n",
      "Counter({'0000-0002-4764-639X': 97, '0000-0002-9936-3279': 6, '0000-0001-6954-7328': 2, '0000-0002-4020-0854': 2})\n",
      "['0000-0002-4764-639X']\n",
      "Total sample size after apply threshold:  97\n",
      "For name:  j_mcdonald\n",
      "total sample size before apply threshold:  21\n",
      "Counter({'0000-0003-1955-6052': 7, '0000-0002-7494-1466': 7, '0000-0002-8317-0069': 4, '0000-0002-7953-1458': 1, '0000-0003-4115-7875': 1, '0000-0002-6328-3752': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  m_ismail\n",
      "total sample size before apply threshold:  24\n",
      "Counter({'0000-0003-3111-3588': 10, '0000-0002-5509-5735': 7, '0000-0003-1688-5096': 1, '0000-0002-0264-6476': 1, '0000-0002-1946-9007': 1, '0000-0003-2747-054X': 1, '0000-0002-7019-2146': 1, '0000-0002-1695-3119': 1, '0000-0001-7608-7884': 1})\n",
      "['0000-0003-3111-3588']\n",
      "Total sample size after apply threshold:  10\n",
      "For name:  x_xu\n",
      "total sample size before apply threshold:  408\n",
      "Counter({'0000-0002-7229-0081': 107, '0000-0002-4459-2082': 38, '0000-0002-4567-7117': 34, '0000-0002-7426-272X': 32, '0000-0003-4132-5322': 28, '0000-0002-2009-0483': 19, '0000-0003-2220-0890': 18, '0000-0001-6088-976X': 17, '0000-0002-3863-9593': 17, '0000-0002-5695-8213': 15, '0000-0001-6435-8181': 13, '0000-0003-0847-5871': 8, '0000-0003-3695-4845': 7, '0000-0003-1672-0830': 7, '0000-0002-5042-9505': 7, '0000-0002-4876-5710': 7, '0000-0002-3265-7678': 6, '0000-0003-0205-6342': 6, '0000-0003-1157-8660': 5, '0000-0002-1982-7062': 4, '0000-0001-6909-0743': 4, '0000-0002-6315-6083': 2, '0000-0003-2094-3164': 1, '0000-0003-1405-8089': 1, '0000-0001-6501-7442': 1, '0000-0002-9662-7582': 1, '0000-0003-3950-3425': 1, '0000-0001-9769-7323': 1, '0000-0003-2047-7298': 1})\n",
      "['0000-0001-6088-976X', '0000-0003-4132-5322', '0000-0001-6435-8181', '0000-0002-7229-0081', '0000-0002-3863-9593', '0000-0002-4567-7117', '0000-0002-5695-8213', '0000-0002-2009-0483', '0000-0002-7426-272X', '0000-0002-4459-2082', '0000-0003-2220-0890']\n",
      "Total sample size after apply threshold:  338\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(338, 539)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(338, 539)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.88      0.71        17\n",
      "          1       0.92      0.86      0.89        28\n",
      "          2       0.69      0.69      0.69        13\n",
      "          3       0.68      0.90      0.77       107\n",
      "          4       0.71      0.71      0.71        17\n",
      "          5       0.89      0.74      0.81        34\n",
      "          6       1.00      0.53      0.70        15\n",
      "          7       0.76      1.00      0.86        19\n",
      "          8       1.00      0.75      0.86        32\n",
      "          9       0.88      0.58      0.70        38\n",
      "         10       0.50      0.17      0.25        18\n",
      "\n",
      "avg / total       0.78      0.76      0.75       338\n",
      "\n",
      "[15  0  0  1  0  1  0  0  0  0  0  0 24  1  0  0  1  0  0  0  0  2  0  0\n",
      "  9  3  0  0  0  0  0  1  0  5  0  2 96  1  0  0  2  0  1  0  2  0  0  3\n",
      " 12  0  0  0  0  0  0  1  1  0  6  0 25  0  1  0  0  0  0  0  0  6  1  0\n",
      "  8  0  0  0  0  0  0  0  0  0  0  0 19  0  0  0  0  1  0  6  1  0  0  0\n",
      " 24  0  0  1  0  1  9  2  0  0  2  0 22  1  1  0  0 11  0  1  0  1  0  1\n",
      "  3]\n",
      "svc Accuracy:  0.7603550295857988\n",
      "svc F1:  0.7224412638926143\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.71      0.69        17\n",
      "          1       0.96      0.86      0.91        28\n",
      "          2       0.90      0.69      0.78        13\n",
      "          3       0.66      0.96      0.79       107\n",
      "          4       0.87      0.76      0.81        17\n",
      "          5       0.96      0.74      0.83        34\n",
      "          6       1.00      0.47      0.64        15\n",
      "          7       0.94      0.89      0.92        19\n",
      "          8       1.00      0.81      0.90        32\n",
      "          9       0.85      0.76      0.81        38\n",
      "         10       1.00      0.22      0.36        18\n",
      "\n",
      "avg / total       0.84      0.80      0.79       338\n",
      "\n",
      "[ 12   0   0   3   0   1   0   0   0   1   0   0  24   0   2   0   0   0\n",
      "   0   0   2   0   0   0   9   3   0   0   0   0   0   1   0   2   0   0\n",
      " 103   1   0   0   0   0   1   0   1   0   0   3  13   0   0   0   0   0\n",
      "   0   1   1   0   6   0  25   0   1   0   0   0   0   0   0   8   0   0\n",
      "   7   0   0   0   0   0   0   0   2   0   0   0  17   0   0   0   0   0\n",
      "   0   6   0   0   0   0  26   0   0   1   0   1   6   1   0   0   0   0\n",
      "  29   0   1   0   0  13   0   0   0   0   0   0   4]\n",
      "LR Accuracy:  0.7958579881656804\n",
      "LR F1:  0.7661002211504928\n",
      "For name:  f_liu\n",
      "total sample size before apply threshold:  185\n",
      "Counter({'0000-0003-3228-0943': 31, '0000-0001-6224-5167': 30, '0000-0001-9241-8161': 22, '0000-0003-3028-5927': 17, '0000-0002-1934-3674': 16, '0000-0002-5006-8965': 16, '0000-0002-2261-6899': 13, '0000-0002-8325-1213': 11, '0000-0001-6693-1981': 5, '0000-0002-8371-6316': 4, '0000-0002-1074-2601': 3, '0000-0001-7029-0312': 3, '0000-0002-7776-0222': 3, '0000-0002-1467-8328': 2, '0000-0001-8032-6681': 2, '0000-0003-2644-2416': 2, '0000-0002-2769-5012': 1, '0000-0003-1322-4997': 1, '0000-0001-8701-2984': 1, '0000-0002-6572-251X': 1, '0000-0001-5625-2969': 1})\n",
      "['0000-0003-3028-5927', '0000-0002-1934-3674', '0000-0002-8325-1213', '0000-0002-5006-8965', '0000-0002-2261-6899', '0000-0001-9241-8161', '0000-0001-6224-5167', '0000-0003-3228-0943']\n",
      "Total sample size after apply threshold:  156\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(156, 628)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(156, 628)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.65      0.71        17\n",
      "          1       0.93      0.81      0.87        16\n",
      "          2       0.80      0.73      0.76        11\n",
      "          3       0.87      0.81      0.84        16\n",
      "          4       1.00      1.00      1.00        13\n",
      "          5       1.00      0.91      0.95        22\n",
      "          6       1.00      0.87      0.93        30\n",
      "          7       0.61      0.87      0.72        31\n",
      "\n",
      "avg / total       0.86      0.84      0.85       156\n",
      "\n",
      "[11  0  0  0  0  0  0  6  0 13  0  0  0  0  0  3  0  0  8  1  0  0  0  2\n",
      "  0  0  0 13  0  0  0  3  0  0  0  0 13  0  0  0  0  0  0  0  0 20  0  2\n",
      "  3  0  0  0  0  0 26  1  0  1  2  1  0  0  0 27]\n",
      "svc Accuracy:  0.8397435897435898\n",
      "svc F1:  0.8472388632872504\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.71      0.83        17\n",
      "          1       0.81      0.81      0.81        16\n",
      "          2       0.80      0.73      0.76        11\n",
      "          3       0.93      0.81      0.87        16\n",
      "          4       1.00      1.00      1.00        13\n",
      "          5       1.00      1.00      1.00        22\n",
      "          6       1.00      1.00      1.00        30\n",
      "          7       0.69      0.87      0.77        31\n",
      "\n",
      "avg / total       0.90      0.88      0.89       156\n",
      "\n",
      "[12  0  0  0  0  0  0  5  0 13  0  0  0  0  0  3  0  0  8  1  0  0  0  2\n",
      "  0  1  0 13  0  0  0  2  0  0  0  0 13  0  0  0  0  0  0  0  0 22  0  0\n",
      "  0  0  0  0  0  0 30  0  0  2  2  0  0  0  0 27]\n",
      "LR Accuracy:  0.8846153846153846\n",
      "LR F1:  0.880010775862069\n",
      "For name:  a_rego\n",
      "total sample size before apply threshold:  78\n",
      "Counter({'0000-0003-0700-3776': 70, '0000-0002-3131-4219': 5, '0000-0003-0883-0511': 2, '0000-0002-4596-3703': 1})\n",
      "['0000-0003-0700-3776']\n",
      "Total sample size after apply threshold:  70\n",
      "For name:  s_hammad\n",
      "total sample size before apply threshold:  32\n",
      "Counter({'0000-0003-0571-4208': 26, '0000-0003-2102-1081': 3, '0000-0002-1313-2542': 1, '0000-0003-3280-564X': 1, '0000-0001-6061-9962': 1})\n",
      "['0000-0003-0571-4208']\n",
      "Total sample size after apply threshold:  26\n",
      "For name:  k_johansson\n",
      "total sample size before apply threshold:  26\n",
      "Counter({'0000-0003-3735-3611': 13, '0000-0002-3749-998X': 9, '0000-0001-9940-5929': 3, '0000-0002-1571-1775': 1})\n",
      "['0000-0003-3735-3611']\n",
      "Total sample size after apply threshold:  13\n",
      "For name:  m_barreto\n",
      "total sample size before apply threshold:  201\n",
      "Counter({'0000-0002-0215-4930': 181, '0000-0002-6973-7233': 9, '0000-0001-6464-548X': 7, '0000-0001-8377-616X': 3, '0000-0001-5797-8913': 1})\n",
      "['0000-0002-0215-4930']\n",
      "Total sample size after apply threshold:  181\n",
      "For name:  j_moore\n",
      "total sample size before apply threshold:  154\n",
      "Counter({'0000-0001-8451-9421': 63, '0000-0002-5486-0407': 22, '0000-0003-4750-1550': 19, '0000-0003-4059-0538': 13, '0000-0002-6028-2084': 13, '0000-0002-0053-3347': 6, '0000-0003-4028-811X': 6, '0000-0002-5496-752X': 5, '0000-0001-8245-9306': 2, '0000-0002-7273-974X': 1, '0000-0001-8503-4880': 1, '0000-0001-9039-1014': 1, '0000-0002-8698-6143': 1, '0000-0001-5682-6897': 1})\n",
      "['0000-0002-5486-0407', '0000-0003-4059-0538', '0000-0001-8451-9421', '0000-0003-4750-1550', '0000-0002-6028-2084']\n",
      "Total sample size after apply threshold:  130\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(130, 463)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(130, 463)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.50      0.67        22\n",
      "          1       0.25      0.62      0.36        13\n",
      "          2       0.97      0.90      0.93        63\n",
      "          3       0.60      0.63      0.62        19\n",
      "          4       1.00      0.62      0.76        13\n",
      "\n",
      "avg / total       0.85      0.74      0.77       130\n",
      "\n",
      "[11  8  2  1  0  0  8  0  5  0  0  5 57  1  0  0  7  0 12  0  0  4  0  1\n",
      "  8]\n",
      "svc Accuracy:  0.7384615384615385\n",
      "svc F1:  0.6667875658039593\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.68      0.81        22\n",
      "          1       1.00      0.15      0.27        13\n",
      "          2       0.69      1.00      0.82        63\n",
      "          3       1.00      0.53      0.69        19\n",
      "          4       1.00      0.92      0.96        13\n",
      "\n",
      "avg / total       0.85      0.78      0.76       130\n",
      "\n",
      "[15  0  7  0  0  0  2 11  0  0  0  0 63  0  0  0  0  9 10  0  0  0  1  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12]\n",
      "LR Accuracy:  0.7846153846153846\n",
      "LR F1:  0.7090628936146178\n",
      "For name:  a_gray\n",
      "total sample size before apply threshold:  121\n",
      "Counter({'0000-0003-4299-2194': 107, '0000-0003-1062-7942': 5, '0000-0002-6273-0637': 5, '0000-0002-5711-4872': 3, '0000-0003-0239-7278': 1})\n",
      "['0000-0003-4299-2194']\n",
      "Total sample size after apply threshold:  107\n",
      "For name:  v_martins\n",
      "total sample size before apply threshold:  104\n",
      "Counter({'0000-0002-2909-8502': 71, '0000-0001-7611-861X': 18, '0000-0001-7565-9641': 6, '0000-0003-2465-5880': 5, '0000-0002-8824-7328': 3, '0000-0002-0327-538X': 1})\n",
      "['0000-0002-2909-8502', '0000-0001-7611-861X']\n",
      "Total sample size after apply threshold:  89\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(89, 349)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(89, 349)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.97        71\n",
      "          1       1.00      0.78      0.88        18\n",
      "\n",
      "avg / total       0.96      0.96      0.95        89\n",
      "\n",
      "[71  0  4 14]\n",
      "svc Accuracy:  0.9550561797752809\n",
      "svc F1:  0.9238013698630136\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      1.00      0.96        71\n",
      "          1       1.00      0.67      0.80        18\n",
      "\n",
      "avg / total       0.94      0.93      0.93        89\n",
      "\n",
      "[71  0  6 12]\n",
      "LR Accuracy:  0.9325842696629213\n",
      "LR F1:  0.8797297297297297\n",
      "For name:  t_zhou\n",
      "total sample size before apply threshold:  76\n",
      "Counter({'0000-0002-3935-4637': 55, '0000-0002-7858-0047': 12, '0000-0002-8744-9083': 3, '0000-0001-7416-5594': 2, '0000-0002-5829-7279': 2, '0000-0003-2219-6385': 2})\n",
      "['0000-0002-7858-0047', '0000-0002-3935-4637']\n",
      "Total sample size after apply threshold:  67\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(67, 314)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(67, 314)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.92      0.88        12\n",
      "          1       0.98      0.96      0.97        55\n",
      "\n",
      "avg / total       0.96      0.96      0.96        67\n",
      "\n",
      "[11  1  2 53]\n",
      "svc Accuracy:  0.9552238805970149\n",
      "svc F1:  0.9262385321100917\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.92      0.92        12\n",
      "          1       0.98      0.98      0.98        55\n",
      "\n",
      "avg / total       0.97      0.97      0.97        67\n",
      "\n",
      "[11  1  1 54]\n",
      "LR Accuracy:  0.9701492537313433\n",
      "LR F1:  0.9492424242424242\n",
      "For name:  s_howell\n",
      "total sample size before apply threshold:  31\n",
      "Counter({'0000-0001-8141-6515': 28, '0000-0001-8184-0324': 1, '0000-0001-5311-6996': 1, '0000-0002-5126-3228': 1})\n",
      "['0000-0001-8141-6515']\n",
      "Total sample size after apply threshold:  28\n",
      "For name:  m_larsson\n",
      "total sample size before apply threshold:  61\n",
      "Counter({'0000-0003-3584-7829': 16, '0000-0003-4164-6513': 15, '0000-0002-5795-9867': 13, '0000-0002-6755-8418': 9, '0000-0002-3226-7397': 6, '0000-0001-7368-953X': 2})\n",
      "['0000-0003-3584-7829', '0000-0002-5795-9867', '0000-0003-4164-6513']\n",
      "Total sample size after apply threshold:  44\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(44, 106)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(44, 106)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.75      0.86        16\n",
      "          1       1.00      0.77      0.87        13\n",
      "          2       0.68      1.00      0.81        15\n",
      "\n",
      "avg / total       0.89      0.84      0.85        44\n",
      "\n",
      "[12  0  4  0 10  3  0  0 15]\n",
      "svc Accuracy:  0.8409090909090909\n",
      "svc F1:  0.8458396284483242\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.75      0.86        16\n",
      "          1       1.00      0.77      0.87        13\n",
      "          2       0.68      1.00      0.81        15\n",
      "\n",
      "avg / total       0.89      0.84      0.85        44\n",
      "\n",
      "[12  0  4  0 10  3  0  0 15]\n",
      "LR Accuracy:  0.8409090909090909\n",
      "LR F1:  0.8458396284483242\n",
      "For name:  s_morris\n",
      "total sample size before apply threshold:  33\n",
      "Counter({'0000-0003-2551-9717': 14, '0000-0002-5334-5809': 11, '0000-0002-7023-8634': 4, '0000-0002-8056-0934': 2, '0000-0003-4866-110X': 2})\n",
      "['0000-0002-5334-5809', '0000-0003-2551-9717']\n",
      "Total sample size after apply threshold:  25\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(25, 58)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(25, 58)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.82      0.82        11\n",
      "          1       0.86      0.86      0.86        14\n",
      "\n",
      "avg / total       0.84      0.84      0.84        25\n",
      "\n",
      "[ 9  2  2 12]\n",
      "svc Accuracy:  0.84\n",
      "svc F1:  0.8376623376623377\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.82      0.82        11\n",
      "          1       0.86      0.86      0.86        14\n",
      "\n",
      "avg / total       0.84      0.84      0.84        25\n",
      "\n",
      "[ 9  2  2 12]\n",
      "LR Accuracy:  0.84\n",
      "LR F1:  0.8376623376623377\n",
      "For name:  s_biswas\n",
      "total sample size before apply threshold:  37\n",
      "Counter({'0000-0001-5067-0174': 10, '0000-0002-0700-7286': 9, '0000-0002-1348-2358': 6, '0000-0003-3144-0060': 5, '0000-0001-9250-5556': 2, '0000-0003-3844-980X': 1, '0000-0001-6448-4487': 1, '0000-0001-7103-9939': 1, '0000-0002-4343-6926': 1, '0000-0001-5979-3605': 1})\n",
      "['0000-0001-5067-0174']\n",
      "Total sample size after apply threshold:  10\n",
      "For name:  s_patel\n",
      "total sample size before apply threshold:  416\n",
      "Counter({'0000-0002-9142-5172': 117, '0000-0001-7247-2013': 81, '0000-0003-0046-5513': 57, '0000-0003-0614-6951': 48, '0000-0002-2386-8940': 28, '0000-0002-0626-1899': 21, '0000-0001-6969-490X': 14, '0000-0002-1235-3458': 10, '0000-0002-4471-2996': 7, '0000-0002-5448-5926': 6, '0000-0002-6136-3556': 6, '0000-0003-1200-0254': 5, '0000-0002-4969-3317': 3, '0000-0003-1674-711X': 3, '0000-0001-9540-9957': 3, '0000-0001-9472-0188': 2, '0000-0002-2177-5038': 1, '0000-0002-8455-6545': 1, '0000-0001-7803-8920': 1, '0000-0002-3444-2179': 1, '0000-0002-6058-8382': 1})\n",
      "['0000-0002-2386-8940', '0000-0003-0046-5513', '0000-0002-1235-3458', '0000-0001-6969-490X', '0000-0002-0626-1899', '0000-0001-7247-2013', '0000-0003-0614-6951', '0000-0002-9142-5172']\n",
      "Total sample size after apply threshold:  376\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(376, 1096)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(376, 1096)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.75      0.86        28\n",
      "          1       0.86      0.56      0.68        57\n",
      "          2       0.88      0.70      0.78        10\n",
      "          3       1.00      0.21      0.35        14\n",
      "          4       1.00      0.95      0.98        21\n",
      "          5       0.97      0.81      0.89        81\n",
      "          6       1.00      0.79      0.88        48\n",
      "          7       0.62      0.96      0.75       117\n",
      "\n",
      "avg / total       0.85      0.80      0.79       376\n",
      "\n",
      "[ 21   0   0   0   0   0   0   7   0  32   0   0   0   1   0  24   0   1\n",
      "   7   0   0   0   0   2   0   1   0   3   0   0   0  10   0   0   0   0\n",
      "  20   0   0   1   0   0   0   0   0  66   0  15   0   0   0   0   0   0\n",
      "  38  10   0   3   1   0   0   1   0 112]\n",
      "svc Accuracy:  0.7952127659574468\n",
      "svc F1:  0.7707034317710724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.82      0.90        28\n",
      "          1       0.85      0.68      0.76        57\n",
      "          2       1.00      0.70      0.82        10\n",
      "          3       1.00      0.29      0.44        14\n",
      "          4       1.00      0.95      0.98        21\n",
      "          5       0.97      0.88      0.92        81\n",
      "          6       1.00      0.81      0.90        48\n",
      "          7       0.68      0.96      0.80       117\n",
      "\n",
      "avg / total       0.87      0.84      0.84       376\n",
      "\n",
      "[ 23   0   0   0   0   0   0   5   0  39   0   0   0   1   0  17   0   1\n",
      "   7   0   0   0   0   2   0   1   0   4   0   0   0   9   0   0   0   0\n",
      "  20   0   0   1   0   0   0   0   0  71   0  10   0   1   0   0   0   0\n",
      "  39   8   0   4   0   0   0   1   0 112]\n",
      "LR Accuracy:  0.8377659574468085\n",
      "LR F1:  0.8148260776431725\n",
      "For name:  m_white\n",
      "total sample size before apply threshold:  292\n",
      "Counter({'0000-0003-1543-9342': 115, '0000-0002-3617-3232': 71, '0000-0002-9826-3962': 47, '0000-0001-9472-7806': 14, '0000-0002-7399-8348': 11, '0000-0001-6719-790X': 6, '0000-0002-7655-8145': 6, '0000-0002-2760-9057': 4, '0000-0001-6817-7126': 4, '0000-0003-3271-1221': 4, '0000-0002-8611-9525': 3, '0000-0002-0238-8913': 2, '0000-0001-9912-5070': 2, '0000-0002-1861-6757': 2, '0000-0001-5022-5505': 1})\n",
      "['0000-0002-9826-3962', '0000-0002-7399-8348', '0000-0003-1543-9342', '0000-0002-3617-3232', '0000-0001-9472-7806']\n",
      "Total sample size after apply threshold:  258\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(258, 768)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(258, 768)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.62      0.76        47\n",
      "          1       1.00      0.64      0.78        11\n",
      "          2       0.78      1.00      0.87       115\n",
      "          3       0.98      0.85      0.91        71\n",
      "          4       1.00      0.93      0.96        14\n",
      "\n",
      "avg / total       0.90      0.87      0.86       258\n",
      "\n",
      "[ 29   0  17   1   0   0   7   4   0   0   0   0 115   0   0   0   0  11\n",
      "  60   0   0   0   1   0  13]\n",
      "svc Accuracy:  0.8682170542635659\n",
      "svc F1:  0.8575028518794777\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.53      0.69        47\n",
      "          1       1.00      0.27      0.43        11\n",
      "          2       0.72      1.00      0.84       115\n",
      "          3       0.98      0.83      0.90        71\n",
      "          4       1.00      0.79      0.88        14\n",
      "\n",
      "avg / total       0.87      0.83      0.81       258\n",
      "\n",
      "[ 25   0  21   1   0   0   3   8   0   0   0   0 115   0   0   0   0  12\n",
      "  59   0   0   0   3   0  11]\n",
      "LR Accuracy:  0.8255813953488372\n",
      "LR F1:  0.7486390580377319\n",
      "For name:  s_sherman\n",
      "total sample size before apply threshold:  125\n",
      "Counter({'0000-0002-3079-5153': 107, '0000-0001-6708-3398': 8, '0000-0003-3667-9898': 6, '0000-0003-4903-0422': 4})\n",
      "['0000-0002-3079-5153']\n",
      "Total sample size after apply threshold:  107\n",
      "For name:  j_dai\n",
      "total sample size before apply threshold:  31\n",
      "Counter({'0000-0002-7720-8032': 15, '0000-0002-1185-5165': 11, '0000-0002-0111-9009': 4, '0000-0002-7414-3659': 1})\n",
      "['0000-0002-7720-8032', '0000-0002-1185-5165']\n",
      "Total sample size after apply threshold:  26\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(26, 45)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(26, 45)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.80      0.89        15\n",
      "          1       0.79      1.00      0.88        11\n",
      "\n",
      "avg / total       0.91      0.88      0.89        26\n",
      "\n",
      "[12  3  0 11]\n",
      "svc Accuracy:  0.8846153846153846\n",
      "svc F1:  0.8844444444444445\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.93      0.93        15\n",
      "          1       0.91      0.91      0.91        11\n",
      "\n",
      "avg / total       0.92      0.92      0.92        26\n",
      "\n",
      "[14  1  1 10]\n",
      "LR Accuracy:  0.9230769230769231\n",
      "LR F1:  0.9212121212121211\n",
      "For name:  m_fischer\n",
      "total sample size before apply threshold:  48\n",
      "Counter({'0000-0002-3429-1876': 10, '0000-0001-5133-1537': 9, '0000-0002-9429-0859': 8, '0000-0002-4014-3626': 8, '0000-0002-1888-1809': 7, '0000-0002-1885-0535': 3, '0000-0002-7826-9726': 2, '0000-0003-0810-6064': 1})\n",
      "['0000-0002-3429-1876']\n",
      "Total sample size after apply threshold:  10\n",
      "For name:  y_zeng\n",
      "total sample size before apply threshold:  26\n",
      "Counter({'0000-0001-7483-5017': 20, '0000-0002-5310-0473': 3, '0000-0002-6164-5502': 1, '0000-0003-1193-3335': 1, '0000-0002-4237-6669': 1})\n",
      "['0000-0001-7483-5017']\n",
      "Total sample size after apply threshold:  20\n",
      "For name:  j_turner\n",
      "total sample size before apply threshold:  178\n",
      "Counter({'0000-0003-0076-8434': 78, '0000-0002-2760-1071': 26, '0000-0003-2427-1430': 23, '0000-0002-7258-1639': 17, '0000-0003-4106-6295': 14, '0000-0002-0023-4275': 13, '0000-0001-7311-0313': 4, '0000-0003-0286-8949': 1, '0000-0002-4327-9385': 1, '0000-0003-0793-4159': 1})\n",
      "['0000-0002-7258-1639', '0000-0003-2427-1430', '0000-0003-0076-8434', '0000-0003-4106-6295', '0000-0002-0023-4275', '0000-0002-2760-1071']\n",
      "Total sample size after apply threshold:  171\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(171, 795)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(171, 795)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.82      0.90        17\n",
      "          1       1.00      0.65      0.79        23\n",
      "          2       0.82      0.99      0.90        78\n",
      "          3       1.00      1.00      1.00        14\n",
      "          4       0.90      0.69      0.78        13\n",
      "          5       0.96      0.88      0.92        26\n",
      "\n",
      "avg / total       0.90      0.89      0.89       171\n",
      "\n",
      "[14  0  3  0  0  0  0 15  7  0  0  1  0  0 77  0  1  0  0  0  0 14  0  0\n",
      "  0  0  4  0  9  0  0  0  3  0  0 23]\n",
      "svc Accuracy:  0.8888888888888888\n",
      "svc F1:  0.8817761705872692\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.82      0.90        17\n",
      "          1       1.00      0.70      0.82        23\n",
      "          2       0.84      1.00      0.91        78\n",
      "          3       1.00      1.00      1.00        14\n",
      "          4       1.00      0.69      0.82        13\n",
      "          5       0.96      0.92      0.94        26\n",
      "\n",
      "avg / total       0.92      0.91      0.90       171\n",
      "\n",
      "[14  0  3  0  0  0  0 16  6  0  0  1  0  0 78  0  0  0  0  0  0 14  0  0\n",
      "  0  0  4  0  9  0  0  0  2  0  0 24]\n",
      "LR Accuracy:  0.9064327485380117\n",
      "LR F1:  0.8992296029148122\n",
      "For name:  c_cai\n",
      "total sample size before apply threshold:  57\n",
      "Counter({'0000-0002-8701-2586': 28, '0000-0001-9008-6327': 19, '0000-0002-0167-1397': 7, '0000-0002-7213-621X': 2, '0000-0002-5047-0815': 1})\n",
      "['0000-0001-9008-6327', '0000-0002-8701-2586']\n",
      "Total sample size after apply threshold:  47\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(47, 100)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(47, 100)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        19\n",
      "          1       1.00      1.00      1.00        28\n",
      "\n",
      "avg / total       1.00      1.00      1.00        47\n",
      "\n",
      "[19  0  0 28]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        19\n",
      "          1       1.00      1.00      1.00        28\n",
      "\n",
      "avg / total       1.00      1.00      1.00        47\n",
      "\n",
      "[19  0  0 28]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  f_pereira\n",
      "total sample size before apply threshold:  86\n",
      "Counter({'0000-0001-8950-1036': 28, '0000-0002-9381-3320': 21, '0000-0003-3317-8756': 13, '0000-0003-4392-4644': 7, '0000-0003-3421-7833': 4, '0000-0003-2100-0280': 4, '0000-0002-1937-6548': 4, '0000-0002-9602-2452': 3, '0000-0001-9718-3867': 1, '0000-0002-8132-0625': 1})\n",
      "['0000-0003-3317-8756', '0000-0002-9381-3320', '0000-0001-8950-1036']\n",
      "Total sample size after apply threshold:  62\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(62, 222)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(62, 222)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.77      0.87        13\n",
      "          1       1.00      0.95      0.98        21\n",
      "          2       0.88      1.00      0.93        28\n",
      "\n",
      "avg / total       0.94      0.94      0.93        62\n",
      "\n",
      "[10  0  3  0 20  1  0  0 28]\n",
      "svc Accuracy:  0.9354838709677419\n",
      "svc F1:  0.9261694356073996\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.77      0.87        13\n",
      "          1       1.00      0.90      0.95        21\n",
      "          2       0.85      1.00      0.92        28\n",
      "\n",
      "avg / total       0.93      0.92      0.92        62\n",
      "\n",
      "[10  0  3  0 19  2  0  0 28]\n",
      "LR Accuracy:  0.9193548387096774\n",
      "LR F1:  0.9125326680921834\n",
      "For name:  a_vitale\n",
      "total sample size before apply threshold:  56\n",
      "Counter({'0000-0001-5586-2255': 43, '0000-0002-8682-3125': 7, '0000-0002-7339-4034': 4, '0000-0003-4980-5574': 2})\n",
      "['0000-0001-5586-2255']\n",
      "Total sample size after apply threshold:  43\n",
      "For name:  q_yang\n",
      "total sample size before apply threshold:  102\n",
      "Counter({'0000-0002-3510-8906': 18, '0000-0001-9849-6996': 17, '0000-0003-4205-1909': 17, '0000-0001-6628-5393': 15, '0000-0002-4378-2335': 10, '0000-0003-4038-2464': 8, '0000-0002-6788-8775': 7, '0000-0003-0279-8784': 5, '0000-0001-6720-8795': 2, '0000-0001-8253-2278': 1, '0000-0002-1437-4498': 1, '0000-0003-2067-5999': 1})\n",
      "['0000-0001-9849-6996', '0000-0001-6628-5393', '0000-0002-4378-2335', '0000-0002-3510-8906', '0000-0003-4205-1909']\n",
      "Total sample size after apply threshold:  77\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(77, 131)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(77, 131)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.82      0.85        17\n",
      "          1       1.00      0.80      0.89        15\n",
      "          2       0.62      1.00      0.77        10\n",
      "          3       0.88      0.83      0.86        18\n",
      "          4       1.00      0.94      0.97        17\n",
      "\n",
      "avg / total       0.90      0.87      0.87        77\n",
      "\n",
      "[14  0  2  1  0  0 12  2  1  0  0  0 10  0  0  2  0  1 15  0  0  0  1  0\n",
      " 16]\n",
      "svc Accuracy:  0.8701298701298701\n",
      "svc F1:  0.8666888666888667\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.88      0.88        17\n",
      "          1       0.93      0.93      0.93        15\n",
      "          2       0.78      0.70      0.74        10\n",
      "          3       0.90      1.00      0.95        18\n",
      "          4       1.00      0.94      0.97        17\n",
      "\n",
      "avg / total       0.91      0.91      0.91        77\n",
      "\n",
      "[15  0  1  1  0  0 14  0  1  0  2  1  7  0  0  0  0  0 18  0  0  0  1  0\n",
      " 16]\n",
      "LR Accuracy:  0.9090909090909091\n",
      "LR F1:  0.8939187541045126\n",
      "For name:  d_xue\n",
      "total sample size before apply threshold:  111\n",
      "Counter({'0000-0002-0748-0962': 47, '0000-0002-8429-8136': 45, '0000-0001-9904-7615': 10, '0000-0001-6132-1236': 5, '0000-0003-1938-9055': 2, '0000-0001-5285-8867': 2})\n",
      "['0000-0002-0748-0962', '0000-0002-8429-8136', '0000-0001-9904-7615']\n",
      "Total sample size after apply threshold:  102\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(102, 134)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(102, 134)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.96      0.83        47\n",
      "          1       0.94      0.76      0.84        45\n",
      "          2       0.80      0.40      0.53        10\n",
      "\n",
      "avg / total       0.84      0.81      0.81       102\n",
      "\n",
      "[45  1  1 11 34  0  5  1  4]\n",
      "svc Accuracy:  0.8137254901960784\n",
      "svc F1:  0.7353909465020577\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.96      0.84        47\n",
      "          1       0.90      0.80      0.85        45\n",
      "          2       1.00      0.20      0.33        10\n",
      "\n",
      "avg / total       0.84      0.81      0.79       102\n",
      "\n",
      "[45  2  0  9 36  0  6  2  2]\n",
      "LR Accuracy:  0.8137254901960784\n",
      "LR F1:  0.6738378840632827\n",
      "For name:  m_sadeghi\n",
      "total sample size before apply threshold:  98\n",
      "Counter({'0000-0003-0401-3493': 54, '0000-0001-5055-4544': 37, '0000-0002-3586-3012': 3, '0000-0002-7698-5630': 3, '0000-0002-0751-1255': 1})\n",
      "['0000-0003-0401-3493', '0000-0001-5055-4544']\n",
      "Total sample size after apply threshold:  91\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(91, 203)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(91, 203)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.98      0.88        54\n",
      "          1       0.96      0.65      0.77        37\n",
      "\n",
      "avg / total       0.87      0.85      0.84        91\n",
      "\n",
      "[53  1 13 24]\n",
      "svc Accuracy:  0.8461538461538461\n",
      "svc F1:  0.8287634408602149\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      1.00      0.92        54\n",
      "          1       1.00      0.73      0.84        37\n",
      "\n",
      "avg / total       0.91      0.89      0.89        91\n",
      "\n",
      "[54  0 10 27]\n",
      "LR Accuracy:  0.8901098901098901\n",
      "LR F1:  0.8795021186440677\n",
      "For name:  h_chang\n",
      "total sample size before apply threshold:  182\n",
      "Counter({'0000-0001-5411-6680': 55, '0000-0002-7997-4822': 39, '0000-0002-8417-8847': 22, '0000-0001-8877-1886': 18, '0000-0001-5810-7562': 11, '0000-0001-5577-2356': 9, '0000-0002-9812-8015': 6, '0000-0002-5248-3433': 5, '0000-0003-4987-5943': 4, '0000-0003-1832-8509': 4, '0000-0002-9405-2121': 2, '0000-0001-7378-8212': 2, '0000-0003-4843-1259': 1, '0000-0002-5605-6500': 1, '0000-0002-5162-103X': 1, '0000-0003-0400-3658': 1, '0000-0002-7494-0033': 1})\n",
      "['0000-0002-8417-8847', '0000-0002-7997-4822', '0000-0001-5810-7562', '0000-0001-5411-6680', '0000-0001-8877-1886']\n",
      "Total sample size after apply threshold:  145\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(145, 223)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(145, 223)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.86      0.88        22\n",
      "          1       0.86      0.97      0.92        39\n",
      "          2       1.00      1.00      1.00        11\n",
      "          3       0.93      0.95      0.94        55\n",
      "          4       1.00      0.72      0.84        18\n",
      "\n",
      "avg / total       0.92      0.92      0.92       145\n",
      "\n",
      "[19  0  0  3  0  0 38  0  1  0  0  0 11  0  0  1  2  0 52  0  1  4  0  0\n",
      " 13]\n",
      "svc Accuracy:  0.9172413793103448\n",
      "svc F1:  0.915006039038252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.77      0.87        22\n",
      "          1       0.95      0.97      0.96        39\n",
      "          2       1.00      1.00      1.00        11\n",
      "          3       0.90      1.00      0.95        55\n",
      "          4       1.00      0.89      0.94        18\n",
      "\n",
      "avg / total       0.95      0.94      0.94       145\n",
      "\n",
      "[17  0  0  5  0  0 38  0  1  0  0  0 11  0  0  0  0  0 55  0  0  2  0  0\n",
      " 16]\n",
      "LR Accuracy:  0.9448275862068966\n",
      "LR F1:  0.9446545041815538\n",
      "For name:  a_lombardi\n",
      "total sample size before apply threshold:  90\n",
      "Counter({'0000-0002-2013-3009': 49, '0000-0001-5421-9970': 21, '0000-0002-7875-2697': 15, '0000-0002-4520-0183': 4, '0000-0002-0383-9579': 1})\n",
      "['0000-0001-5421-9970', '0000-0002-2013-3009', '0000-0002-7875-2697']\n",
      "Total sample size after apply threshold:  85\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(85, 278)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(85, 278)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.95      0.98        21\n",
      "          1       1.00      0.98      0.99        49\n",
      "          2       0.88      1.00      0.94        15\n",
      "\n",
      "avg / total       0.98      0.98      0.98        85\n",
      "\n",
      "[20  0  1  0 48  1  0  0 15]\n",
      "svc Accuracy:  0.9764705882352941\n",
      "svc F1:  0.9676001592490152\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.95      0.98        21\n",
      "          1       0.98      1.00      0.99        49\n",
      "          2       1.00      1.00      1.00        15\n",
      "\n",
      "avg / total       0.99      0.99      0.99        85\n",
      "\n",
      "[20  1  0  0 49  0  0  0 15]\n",
      "LR Accuracy:  0.9882352941176471\n",
      "LR F1:  0.9885029153321837\n",
      "For name:  c_correia\n",
      "total sample size before apply threshold:  55\n",
      "Counter({'0000-0001-5564-6675': 20, '0000-0001-5481-2010': 13, '0000-0002-4979-3254': 9, '0000-0002-6996-0734': 6, '0000-0003-2482-7873': 5, '0000-0002-0527-3206': 2})\n",
      "['0000-0001-5564-6675', '0000-0001-5481-2010']\n",
      "Total sample size after apply threshold:  33\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(33, 274)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(33, 274)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.95      0.95        20\n",
      "          1       0.92      0.92      0.92        13\n",
      "\n",
      "avg / total       0.94      0.94      0.94        33\n",
      "\n",
      "[19  1  1 12]\n",
      "svc Accuracy:  0.9393939393939394\n",
      "svc F1:  0.9365384615384615\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.98        20\n",
      "          1       1.00      0.92      0.96        13\n",
      "\n",
      "avg / total       0.97      0.97      0.97        33\n",
      "\n",
      "[20  0  1 12]\n",
      "LR Accuracy:  0.9696969696969697\n",
      "LR F1:  0.9678048780487805\n",
      "For name:  j_you\n",
      "total sample size before apply threshold:  239\n",
      "Counter({'0000-0002-2074-6745': 75, '0000-0002-4006-8339': 71, '0000-0002-5763-7403': 56, '0000-0002-4651-9081': 28, '0000-0001-8927-1015': 9})\n",
      "['0000-0002-5763-7403', '0000-0002-4651-9081', '0000-0002-4006-8339', '0000-0002-2074-6745']\n",
      "Total sample size after apply threshold:  230\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(230, 238)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(230, 238)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.93      0.89        56\n",
      "          1       0.96      0.96      0.96        28\n",
      "          2       0.97      0.93      0.95        71\n",
      "          3       0.96      0.93      0.95        75\n",
      "\n",
      "avg / total       0.94      0.93      0.94       230\n",
      "\n",
      "[52  0  1  3  0 27  1  0  4  1 66  0  5  0  0 70]\n",
      "svc Accuracy:  0.9347826086956522\n",
      "svc F1:  0.9371902092225832\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.95      0.94        56\n",
      "          1       0.96      0.93      0.95        28\n",
      "          2       0.96      0.94      0.95        71\n",
      "          3       0.97      0.99      0.98        75\n",
      "\n",
      "avg / total       0.96      0.96      0.96       230\n",
      "\n",
      "[53  0  1  2  0 26  2  0  3  1 67  0  1  0  0 74]\n",
      "LR Accuracy:  0.9565217391304348\n",
      "LR F1:  0.9534986757649705\n",
      "For name:  c_lopez\n",
      "total sample size before apply threshold:  53\n",
      "Counter({'0000-0001-9298-2969': 34, '0000-0003-3668-7468': 12, '0000-0001-6160-632X': 3, '0000-0001-5635-4463': 2, '0000-0002-7669-6572': 1, '0000-0002-3445-4284': 1})\n",
      "['0000-0003-3668-7468', '0000-0001-9298-2969']\n",
      "Total sample size after apply threshold:  46\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(46, 103)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(46, 103)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.50      0.67        12\n",
      "          1       0.85      1.00      0.92        34\n",
      "\n",
      "avg / total       0.89      0.87      0.85        46\n",
      "\n",
      "[ 6  6  0 34]\n",
      "svc Accuracy:  0.8695652173913043\n",
      "svc F1:  0.7927927927927927\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.42      0.59        12\n",
      "          1       0.83      1.00      0.91        34\n",
      "\n",
      "avg / total       0.87      0.85      0.82        46\n",
      "\n",
      "[ 5  7  0 34]\n",
      "LR Accuracy:  0.8478260869565217\n",
      "LR F1:  0.747450980392157\n",
      "For name:  y_oh\n",
      "total sample size before apply threshold:  91\n",
      "Counter({'0000-0002-4438-8890': 55, '0000-0002-9636-3329': 17, '0000-0001-8233-1898': 14, '0000-0002-9055-6250': 2, '0000-0002-3832-6108': 1, '0000-0003-4936-7287': 1, '0000-0003-2761-7820': 1})\n",
      "['0000-0002-4438-8890', '0000-0002-9636-3329', '0000-0001-8233-1898']\n",
      "Total sample size after apply threshold:  86\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(86, 130)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(86, 130)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      1.00      0.93        55\n",
      "          1       1.00      0.94      0.97        17\n",
      "          2       1.00      0.50      0.67        14\n",
      "\n",
      "avg / total       0.92      0.91      0.90        86\n",
      "\n",
      "[55  0  0  1 16  0  7  0  7]\n",
      "svc Accuracy:  0.9069767441860465\n",
      "svc F1:  0.8561890087313816\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      1.00      0.92        55\n",
      "          1       1.00      0.94      0.97        17\n",
      "          2       1.00      0.36      0.53        14\n",
      "\n",
      "avg / total       0.90      0.88      0.86        86\n",
      "\n",
      "[55  0  0  1 16  0  9  0  5]\n",
      "LR Accuracy:  0.8837209302325582\n",
      "LR F1:  0.8042264752791067\n",
      "For name:  s_yoon\n",
      "total sample size before apply threshold:  73\n",
      "Counter({'0000-0002-8556-423X': 27, '0000-0003-3487-6863': 16, '0000-0001-8904-0292': 15, '0000-0003-1787-7282': 8, '0000-0003-1868-1054': 1, '0000-0001-7263-8036': 1, '0000-0001-8323-6462': 1, '0000-0002-5330-8784': 1, '0000-0002-8361-9815': 1, '0000-0003-2695-9589': 1, '0000-0003-1384-3405': 1})\n",
      "['0000-0001-8904-0292', '0000-0002-8556-423X', '0000-0003-3487-6863']\n",
      "Total sample size after apply threshold:  58\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(58, 69)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(58, 69)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.93      0.85        15\n",
      "          1       0.91      0.78      0.84        27\n",
      "          2       0.71      0.75      0.73        16\n",
      "\n",
      "avg / total       0.82      0.81      0.81        58\n",
      "\n",
      "[14  0  1  2 21  4  2  2 12]\n",
      "svc Accuracy:  0.8103448275862069\n",
      "svc F1:  0.8052525252525252\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      0.87      0.79        15\n",
      "          1       0.81      0.81      0.81        27\n",
      "          2       0.77      0.62      0.69        16\n",
      "\n",
      "avg / total       0.78      0.78      0.77        58\n",
      "\n",
      "[13  1  1  3 22  2  2  4 10]\n",
      "LR Accuracy:  0.7758620689655172\n",
      "LR F1:  0.7641162583691319\n",
      "For name:  a_lima\n",
      "total sample size before apply threshold:  85\n",
      "Counter({'0000-0002-1507-2264': 16, '0000-0002-9779-0584': 12, '0000-0002-3582-2640': 10, '0000-0002-2396-9880': 9, '0000-0003-2261-2801': 8, '0000-0001-6980-6553': 8, '0000-0001-8251-6286': 7, '0000-0002-3714-9904': 5, '0000-0002-1055-0554': 5, '0000-0002-9083-3377': 2, '0000-0002-4473-5311': 2, '0000-0002-4568-9126': 1})\n",
      "['0000-0002-1507-2264', '0000-0002-9779-0584', '0000-0002-3582-2640']\n",
      "Total sample size after apply threshold:  38\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(38, 80)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(38, 80)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.94      0.94        16\n",
      "          1       1.00      0.92      0.96        12\n",
      "          2       0.91      1.00      0.95        10\n",
      "\n",
      "avg / total       0.95      0.95      0.95        38\n",
      "\n",
      "[15  0  1  1 11  0  0  0 10]\n",
      "svc Accuracy:  0.9473684210526315\n",
      "svc F1:  0.9488008971704623\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.94      0.91        16\n",
      "          1       1.00      1.00      1.00        12\n",
      "          2       0.89      0.80      0.84        10\n",
      "\n",
      "avg / total       0.92      0.92      0.92        38\n",
      "\n",
      "[15  0  1  0 12  0  2  0  8]\n",
      "LR Accuracy:  0.9210526315789473\n",
      "LR F1:  0.9170653907496013\n",
      "For name:  h_singh\n",
      "total sample size before apply threshold:  45\n",
      "Counter({'0000-0002-8686-2802': 23, '0000-0002-9586-8544': 7, '0000-0002-6135-1897': 7, '0000-0001-9198-8779': 3, '0000-0002-2354-6474': 2, '0000-0002-1989-9066': 1, '0000-0002-9713-6048': 1, '0000-0003-1653-232X': 1})\n",
      "['0000-0002-8686-2802']\n",
      "Total sample size after apply threshold:  23\n",
      "For name:  s_scott\n",
      "total sample size before apply threshold:  143\n",
      "Counter({'0000-0001-7510-6297': 101, '0000-0002-8290-0461': 27, '0000-0002-4597-9094': 8, '0000-0002-8480-1547': 4, '0000-0002-5830-8943': 2, '0000-0002-4754-246X': 1})\n",
      "['0000-0002-8290-0461', '0000-0001-7510-6297']\n",
      "Total sample size after apply threshold:  128\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(128, 274)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(128, 274)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.70      0.83        27\n",
      "          1       0.93      1.00      0.96       101\n",
      "\n",
      "avg / total       0.94      0.94      0.93       128\n",
      "\n",
      "[ 19   8   0 101]\n",
      "svc Accuracy:  0.9375\n",
      "svc F1:  0.8939958592132505\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.67      0.80        27\n",
      "          1       0.92      1.00      0.96       101\n",
      "\n",
      "avg / total       0.94      0.93      0.92       128\n",
      "\n",
      "[ 18   9   0 101]\n",
      "LR Accuracy:  0.9296875\n",
      "LR F1:  0.8786729857819906\n",
      "For name:  z_he\n",
      "total sample size before apply threshold:  160\n",
      "Counter({'0000-0002-6098-7893': 46, '0000-0001-6302-6556': 40, '0000-0001-9526-8816': 18, '0000-0003-3507-5013': 18, '0000-0001-6496-3971': 12, '0000-0003-1505-8750': 7, '0000-0003-3608-0244': 6, '0000-0002-3265-7539': 6, '0000-0001-8569-6008': 5, '0000-0003-3947-4011': 1, '0000-0002-8431-5274': 1})\n",
      "['0000-0001-6302-6556', '0000-0001-9526-8816', '0000-0002-6098-7893', '0000-0001-6496-3971', '0000-0003-3507-5013']\n",
      "Total sample size after apply threshold:  134\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(134, 196)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(134, 196)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.97      0.85        40\n",
      "          1       0.94      0.83      0.88        18\n",
      "          2       0.95      0.83      0.88        46\n",
      "          3       1.00      1.00      1.00        12\n",
      "          4       1.00      0.78      0.88        18\n",
      "\n",
      "avg / total       0.90      0.88      0.88       134\n",
      "\n",
      "[39  0  1  0  0  3 15  0  0  0  7  1 38  0  0  0  0  0 12  0  3  0  1  0\n",
      " 14]\n",
      "svc Accuracy:  0.8805970149253731\n",
      "svc F1:  0.8977799916731101\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.93      0.90        40\n",
      "          1       0.94      0.83      0.88        18\n",
      "          2       0.88      0.93      0.91        46\n",
      "          3       1.00      1.00      1.00        12\n",
      "          4       1.00      0.83      0.91        18\n",
      "\n",
      "avg / total       0.91      0.91      0.91       134\n",
      "\n",
      "[37  0  3  0  0  2 15  1  0  0  2  1 43  0  0  0  0  0 12  0  1  0  2  0\n",
      " 15]\n",
      "LR Accuracy:  0.9104477611940298\n",
      "LR F1:  0.9198292065104721\n",
      "For name:  s_mukherjee\n",
      "total sample size before apply threshold:  125\n",
      "Counter({'0000-0002-6715-3920': 38, '0000-0003-2522-2884': 20, '0000-0002-3295-9668': 17, '0000-0001-6031-2557': 13, '0000-0002-2932-2834': 8, '0000-0002-2449-2826': 5, '0000-0002-5479-3750': 5, '0000-0002-3417-4530': 3, '0000-0001-8371-4014': 3, '0000-0002-8445-0492': 3, '0000-0003-4794-137X': 2, '0000-0001-8743-7050': 2, '0000-0003-1668-0461': 2, '0000-0003-4176-3496': 1, '0000-0001-9651-6228': 1, '0000-0001-7299-0304': 1, '0000-0002-6157-1224': 1})\n",
      "['0000-0001-6031-2557', '0000-0002-3295-9668', '0000-0002-6715-3920', '0000-0003-2522-2884']\n",
      "Total sample size after apply threshold:  88\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(88, 435)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(88, 435)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.77      0.87        13\n",
      "          1       1.00      0.94      0.97        17\n",
      "          2       0.81      1.00      0.89        38\n",
      "          3       1.00      0.75      0.86        20\n",
      "\n",
      "avg / total       0.92      0.90      0.90        88\n",
      "\n",
      "[10  0  3  0  0 16  1  0  0  0 38  0  0  0  5 15]\n",
      "svc Accuracy:  0.8977272727272727\n",
      "svc F1:  0.8976306728224887\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.62      0.76        13\n",
      "          1       1.00      0.94      0.97        17\n",
      "          2       0.83      1.00      0.90        38\n",
      "          3       1.00      0.90      0.95        20\n",
      "\n",
      "avg / total       0.92      0.91      0.91        88\n",
      "\n",
      "[ 8  0  5  0  0 16  1  0  0  0 38  0  0  0  2 18]\n",
      "LR Accuracy:  0.9090909090909091\n",
      "LR F1:  0.8959330143540669\n",
      "For name:  j_yue\n",
      "total sample size before apply threshold:  62\n",
      "Counter({'0000-0001-9694-7722': 25, '0000-0001-6384-5447': 24, '0000-0002-2122-9221': 9, '0000-0002-2549-9261': 2, '0000-0003-4043-0737': 2})\n",
      "['0000-0001-6384-5447', '0000-0001-9694-7722']\n",
      "Total sample size after apply threshold:  49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(49, 142)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(49, 142)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      1.00      0.96        24\n",
      "          1       1.00      0.92      0.96        25\n",
      "\n",
      "avg / total       0.96      0.96      0.96        49\n",
      "\n",
      "[24  0  2 23]\n",
      "svc Accuracy:  0.9591836734693877\n",
      "svc F1:  0.9591666666666667\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98        24\n",
      "          1       1.00      0.96      0.98        25\n",
      "\n",
      "avg / total       0.98      0.98      0.98        49\n",
      "\n",
      "[24  0  1 24]\n",
      "LR Accuracy:  0.9795918367346939\n",
      "LR F1:  0.9795918367346939\n",
      "For name:  f_dias\n",
      "total sample size before apply threshold:  68\n",
      "Counter({'0000-0001-9841-863X': 25, '0000-0001-9945-9185': 21, '0000-0002-5123-4929': 15, '0000-0002-4993-4467': 7})\n",
      "['0000-0001-9841-863X', '0000-0001-9945-9185', '0000-0002-5123-4929']\n",
      "Total sample size after apply threshold:  61\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(61, 168)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(61, 168)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.88      0.94        25\n",
      "          1       1.00      0.95      0.98        21\n",
      "          2       0.79      1.00      0.88        15\n",
      "\n",
      "avg / total       0.95      0.93      0.94        61\n",
      "\n",
      "[22  0  3  0 20  1  0  0 15]\n",
      "svc Accuracy:  0.9344262295081968\n",
      "svc F1:  0.9313776366799963\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.96      0.96        25\n",
      "          1       0.95      1.00      0.98        21\n",
      "          2       1.00      0.93      0.97        15\n",
      "\n",
      "avg / total       0.97      0.97      0.97        61\n",
      "\n",
      "[24  1  0  0 21  0  1  0 14]\n",
      "LR Accuracy:  0.9672131147540983\n",
      "LR F1:  0.9674204758086073\n",
      "For name:  r_walker\n",
      "total sample size before apply threshold:  87\n",
      "Counter({'0000-0002-5936-1068': 25, '0000-0003-0348-2407': 16, '0000-0001-7383-7846': 15, '0000-0002-6089-8225': 11, '0000-0003-0032-9925': 10, '0000-0001-9736-3497': 7, '0000-0002-2064-4546': 2, '0000-0002-5332-3562': 1})\n",
      "['0000-0001-7383-7846', '0000-0003-0348-2407', '0000-0002-5936-1068', '0000-0003-0032-9925', '0000-0002-6089-8225']\n",
      "Total sample size after apply threshold:  77\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(77, 508)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(77, 508)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.73      0.85        15\n",
      "          1       0.76      0.81      0.79        16\n",
      "          2       0.85      0.92      0.88        25\n",
      "          3       0.82      0.90      0.86        10\n",
      "          4       1.00      1.00      1.00        11\n",
      "\n",
      "avg / total       0.88      0.87      0.87        77\n",
      "\n",
      "[11  1  1  2  0  0 13  3  0  0  0  2 23  0  0  0  1  0  9  0  0  0  0  0\n",
      " 11]\n",
      "svc Accuracy:  0.8701298701298701\n",
      "svc F1:  0.8751581751581752\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.73      0.81        15\n",
      "          1       1.00      0.62      0.77        16\n",
      "          2       0.76      1.00      0.86        25\n",
      "          3       0.82      0.90      0.86        10\n",
      "          4       1.00      1.00      1.00        11\n",
      "\n",
      "avg / total       0.88      0.86      0.85        77\n",
      "\n",
      "[11  0  2  2  0  1 10  5  0  0  0  0 25  0  0  0  0  1  9  0  0  0  0  0\n",
      " 11]\n",
      "LR Accuracy:  0.8571428571428571\n",
      "LR F1:  0.8606514813411366\n",
      "For name:  l_campos\n",
      "total sample size before apply threshold:  12\n",
      "Counter({'0000-0001-5414-8746': 10, '0000-0003-2431-3274': 1, '0000-0002-1610-7617': 1})\n",
      "['0000-0001-5414-8746']\n",
      "Total sample size after apply threshold:  10\n",
      "For name:  m_iqbal\n",
      "total sample size before apply threshold:  8\n",
      "Counter({'0000-0001-5891-9798': 3, '0000-0003-4790-3584': 2, '0000-0001-6241-7547': 2, '0000-0002-5535-0839': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  s_lim\n",
      "total sample size before apply threshold:  136\n",
      "Counter({'0000-0002-5475-4153': 27, '0000-0002-0360-6361': 23, '0000-0002-5192-0486': 20, '0000-0001-7589-5150': 15, '0000-0003-3807-4163': 13, '0000-0001-8471-5684': 6, '0000-0003-4528-8514': 6, '0000-0001-9086-5101': 5, '0000-0003-0312-9937': 5, '0000-0002-4890-0396': 4, '0000-0003-0377-9032': 3, '0000-0003-4246-6223': 3, '0000-0002-9783-9050': 1, '0000-0002-9907-0628': 1, '0000-0003-0845-9994': 1, '0000-0003-0598-4574': 1, '0000-0002-9460-5136': 1, '0000-0003-0204-4990': 1})\n",
      "['0000-0003-3807-4163', '0000-0002-5475-4153', '0000-0001-7589-5150', '0000-0002-0360-6361', '0000-0002-5192-0486']\n",
      "Total sample size after apply threshold:  98\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(98, 128)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(98, 128)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.69      0.64        13\n",
      "          1       0.89      0.89      0.89        27\n",
      "          2       1.00      0.80      0.89        15\n",
      "          3       0.83      0.87      0.85        23\n",
      "          4       0.95      0.95      0.95        20\n",
      "\n",
      "avg / total       0.87      0.86      0.86        98\n",
      "\n",
      "[ 9  2  0  2  0  2 24  0  0  1  2  0 12  1  0  2  1  0 20  0  0  0  0  1\n",
      " 19]\n",
      "svc Accuracy:  0.8571428571428571\n",
      "svc F1:  0.844339750084431\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.69      0.78        13\n",
      "          1       0.84      0.96      0.90        27\n",
      "          2       1.00      0.80      0.89        15\n",
      "          3       0.81      0.91      0.86        23\n",
      "          4       0.95      0.90      0.92        20\n",
      "\n",
      "avg / total       0.89      0.88      0.88        98\n",
      "\n",
      "[ 9  3  0  1  0  0 26  0  0  1  0  0 12  3  0  1  1  0 21  0  0  1  0  1\n",
      " 18]\n",
      "LR Accuracy:  0.8775510204081632\n",
      "LR F1:  0.8696538177797548\n",
      "For name:  p_li\n",
      "total sample size before apply threshold:  118\n",
      "Counter({'0000-0002-5715-548X': 20, '0000-0001-9602-9550': 18, '0000-0001-9098-7598': 14, '0000-0002-5876-2177': 9, '0000-0001-5836-1069': 9, '0000-0002-2572-5935': 7, '0000-0001-9339-3111': 7, '0000-0002-4273-4577': 7, '0000-0002-4684-4909': 6, '0000-0001-8771-3369': 5, '0000-0001-7960-1025': 4, '0000-0002-5192-8509': 4, '0000-0001-5761-9435': 3, '0000-0001-7603-7852': 2, '0000-0002-9330-5713': 1, '0000-0002-7112-9974': 1, '0000-0002-9445-506X': 1})\n",
      "['0000-0001-9098-7598', '0000-0001-9602-9550', '0000-0002-5715-548X']\n",
      "Total sample size after apply threshold:  52\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(52, 200)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(52, 200)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.86      0.73        14\n",
      "          1       0.93      0.72      0.81        18\n",
      "          2       0.95      0.90      0.92        20\n",
      "\n",
      "avg / total       0.86      0.83      0.83        52\n",
      "\n",
      "[12  1  1  5 13  0  2  0 18]\n",
      "svc Accuracy:  0.8269230769230769\n",
      "svc F1:  0.8209498834498836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.86      0.80        14\n",
      "          1       0.93      0.78      0.85        18\n",
      "          2       0.95      1.00      0.98        20\n",
      "\n",
      "avg / total       0.89      0.88      0.88        52\n",
      "\n",
      "[12  1  1  4 14  0  0  0 20]\n",
      "LR Accuracy:  0.8846153846153846\n",
      "LR F1:  0.8746982015274698\n",
      "For name:  f_andrade\n",
      "total sample size before apply threshold:  37\n",
      "Counter({'0000-0002-3856-3816': 12, '0000-0003-1199-2837': 12, '0000-0002-4947-2346': 11, '0000-0001-6257-1712': 2})\n",
      "['0000-0002-4947-2346', '0000-0002-3856-3816', '0000-0003-1199-2837']\n",
      "Total sample size after apply threshold:  35\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(35, 79)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(35, 79)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        11\n",
      "          1       0.75      1.00      0.86        12\n",
      "          2       1.00      0.67      0.80        12\n",
      "\n",
      "avg / total       0.91      0.89      0.88        35\n",
      "\n",
      "[11  0  0  0 12  0  0  4  8]\n",
      "svc Accuracy:  0.8857142857142857\n",
      "svc F1:  0.8857142857142858\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        11\n",
      "          1       1.00      1.00      1.00        12\n",
      "          2       1.00      1.00      1.00        12\n",
      "\n",
      "avg / total       1.00      1.00      1.00        35\n",
      "\n",
      "[11  0  0  0 12  0  0  0 12]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  c_schmitt\n",
      "total sample size before apply threshold:  13\n",
      "Counter({'0000-0003-2143-9226': 9, '0000-0002-7646-4739': 2, '0000-0003-3829-6970': 1, '0000-0002-8527-9682': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  t_tan\n",
      "total sample size before apply threshold:  73\n",
      "Counter({'0000-0003-1228-9449': 40, '0000-0001-6624-1593': 18, '0000-0002-7589-602X': 11, '0000-0002-8547-6328': 3, '0000-0001-5329-7192': 1})\n",
      "['0000-0002-7589-602X', '0000-0001-6624-1593', '0000-0003-1228-9449']\n",
      "Total sample size after apply threshold:  69\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(69, 224)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(69, 224)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        11\n",
      "          1       0.90      0.50      0.64        18\n",
      "          2       0.81      0.97      0.89        40\n",
      "\n",
      "avg / total       0.87      0.86      0.84        69\n",
      "\n",
      "[11  0  0  0  9  9  0  1 39]\n",
      "svc Accuracy:  0.855072463768116\n",
      "svc F1:  0.843073593073593\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.91      0.91        11\n",
      "          1       0.90      0.50      0.64        18\n",
      "          2       0.79      0.95      0.86        40\n",
      "\n",
      "avg / total       0.84      0.83      0.81        69\n",
      "\n",
      "[10  0  1  0  9  9  1  1 38]\n",
      "LR Accuracy:  0.8260869565217391\n",
      "LR F1:  0.8051948051948052\n",
      "For name:  h_gomes\n",
      "total sample size before apply threshold:  11\n",
      "Counter({'0000-0003-1131-7604': 7, '0000-0001-6898-2408': 2, '0000-0003-3664-4740': 1, '0000-0002-6222-9180': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  m_matos\n",
      "total sample size before apply threshold:  82\n",
      "Counter({'0000-0001-6998-5133': 27, '0000-0002-9584-6636': 21, '0000-0003-1358-1054': 15, '0000-0001-7320-7107': 14, '0000-0002-5240-8070': 3, '0000-0003-4076-2459': 1, '0000-0003-1692-2205': 1})\n",
      "['0000-0003-1358-1054', '0000-0002-9584-6636', '0000-0001-7320-7107', '0000-0001-6998-5133']\n",
      "Total sample size after apply threshold:  77\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(77, 109)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(77, 109)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        15\n",
      "          1       1.00      0.86      0.92        21\n",
      "          2       1.00      1.00      1.00        14\n",
      "          3       0.90      1.00      0.95        27\n",
      "\n",
      "avg / total       0.96      0.96      0.96        77\n",
      "\n",
      "[15  0  0  0  0 18  0  3  0  0 14  0  0  0  0 27]\n",
      "svc Accuracy:  0.961038961038961\n",
      "svc F1:  0.9676113360323886\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        15\n",
      "          1       1.00      0.90      0.95        21\n",
      "          2       1.00      1.00      1.00        14\n",
      "          3       0.93      1.00      0.96        27\n",
      "\n",
      "avg / total       0.98      0.97      0.97        77\n",
      "\n",
      "[15  0  0  0  0 19  0  2  0  0 14  0  0  0  0 27]\n",
      "LR Accuracy:  0.974025974025974\n",
      "LR F1:  0.9785714285714286\n",
      "For name:  k_ryan\n",
      "total sample size before apply threshold:  182\n",
      "Counter({'0000-0002-1059-9681': 79, '0000-0003-3670-8505': 36, '0000-0001-5304-2026': 23, '0000-0003-4563-3744': 22, '0000-0001-9149-260X': 11, '0000-0002-0582-3693': 7, '0000-0002-9454-8768': 3, '0000-0002-6057-452X': 1})\n",
      "['0000-0001-9149-260X', '0000-0003-3670-8505', '0000-0001-5304-2026', '0000-0003-4563-3744', '0000-0002-1059-9681']\n",
      "Total sample size after apply threshold:  171\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(171, 1543)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(171, 1543)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.64      0.78        11\n",
      "          1       0.94      0.81      0.87        36\n",
      "          2       0.88      0.61      0.72        23\n",
      "          3       1.00      1.00      1.00        22\n",
      "          4       0.80      0.96      0.87        79\n",
      "\n",
      "avg / total       0.88      0.87      0.86       171\n",
      "\n",
      "[ 7  0  0  0  4  0 29  1  0  6  0  0 14  0  9  0  0  0 22  0  0  2  1  0\n",
      " 76]\n",
      "svc Accuracy:  0.8654970760233918\n",
      "svc F1:  0.8469922711816691\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.64      0.78        11\n",
      "          1       0.96      0.72      0.83        36\n",
      "          2       1.00      0.57      0.72        23\n",
      "          3       1.00      0.95      0.98        22\n",
      "          4       0.76      0.99      0.86        79\n",
      "\n",
      "avg / total       0.88      0.85      0.84       171\n",
      "\n",
      "[ 7  0  0  0  4  0 26  0  0 10  0  0 13  0 10  0  0  0 21  1  0  1  0  0\n",
      " 78]\n",
      "LR Accuracy:  0.847953216374269\n",
      "LR F1:  0.8318567737172389\n",
      "For name:  w_zheng\n",
      "total sample size before apply threshold:  93\n",
      "Counter({'0000-0002-6236-9765': 48, '0000-0003-1034-0757': 24, '0000-0003-0021-6672': 9, '0000-0003-0799-3474': 7, '0000-0002-9915-6982': 3, '0000-0002-1750-4999': 2})\n",
      "['0000-0002-6236-9765', '0000-0003-1034-0757']\n",
      "Total sample size after apply threshold:  72\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(72, 125)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(72, 125)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      1.00      0.94        48\n",
      "          1       1.00      0.75      0.86        24\n",
      "\n",
      "avg / total       0.93      0.92      0.91        72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[48  0  6 18]\n",
      "svc Accuracy:  0.9166666666666666\n",
      "svc F1:  0.8991596638655461\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      1.00      0.94        48\n",
      "          1       1.00      0.75      0.86        24\n",
      "\n",
      "avg / total       0.93      0.92      0.91        72\n",
      "\n",
      "[48  0  6 18]\n",
      "LR Accuracy:  0.9166666666666666\n",
      "LR F1:  0.8991596638655461\n",
      "For name:  j_franco\n",
      "total sample size before apply threshold:  85\n",
      "Counter({'0000-0002-3874-8618': 46, '0000-0001-9255-8084': 16, '0000-0002-0898-3510': 13, '0000-0002-3165-394X': 9, '0000-0002-8249-5224': 1})\n",
      "['0000-0001-9255-8084', '0000-0002-3874-8618', '0000-0002-0898-3510']\n",
      "Total sample size after apply threshold:  75\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(75, 269)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(75, 269)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97        16\n",
      "          1       1.00      1.00      1.00        46\n",
      "          2       1.00      0.92      0.96        13\n",
      "\n",
      "avg / total       0.99      0.99      0.99        75\n",
      "\n",
      "[16  0  0  0 46  0  1  0 12]\n",
      "svc Accuracy:  0.9866666666666667\n",
      "svc F1:  0.9765656565656565\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.94      0.94        16\n",
      "          1       0.98      1.00      0.99        46\n",
      "          2       1.00      0.92      0.96        13\n",
      "\n",
      "avg / total       0.97      0.97      0.97        75\n",
      "\n",
      "[15  1  0  0 46  0  1  0 12]\n",
      "LR Accuracy:  0.9733333333333334\n",
      "LR F1:  0.9622491039426523\n",
      "For name:  l_walker\n",
      "total sample size before apply threshold:  194\n",
      "Counter({'0000-0001-9166-3261': 107, '0000-0001-5986-5015': 42, '0000-0003-2556-8076': 29, '0000-0001-5865-7257': 12, '0000-0002-6939-9721': 2, '0000-0001-9726-1853': 1, '0000-0001-8375-8041': 1})\n",
      "['0000-0003-2556-8076', '0000-0001-9166-3261', '0000-0001-5865-7257', '0000-0001-5986-5015']\n",
      "Total sample size after apply threshold:  190\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(190, 579)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(190, 579)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.72      0.84        29\n",
      "          1       0.86      1.00      0.92       107\n",
      "          2       1.00      0.92      0.96        12\n",
      "          3       0.97      0.76      0.85        42\n",
      "\n",
      "avg / total       0.91      0.90      0.90       190\n",
      "\n",
      "[ 21   7   0   1   0 107   0   0   0   1  11   0   0  10   0  32]\n",
      "svc Accuracy:  0.9\n",
      "svc F1:  0.8930672163918041\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.76      0.86        29\n",
      "          1       0.85      1.00      0.92       107\n",
      "          2       1.00      0.92      0.96        12\n",
      "          3       1.00      0.74      0.85        42\n",
      "\n",
      "avg / total       0.92      0.90      0.90       190\n",
      "\n",
      "[ 22   7   0   0   0 107   0   0   0   1  11   0   0  11   0  31]\n",
      "LR Accuracy:  0.9\n",
      "LR F1:  0.8967592103212796\n",
      "For name:  a_gordon\n",
      "total sample size before apply threshold:  126\n",
      "Counter({'0000-0003-1676-9853': 36, '0000-0002-0419-547X': 29, '0000-0002-9352-7877': 27, '0000-0002-1807-4644': 25, '0000-0002-0648-0346': 6, '0000-0002-5731-7215': 1, '0000-0003-2643-5419': 1, '0000-0001-6480-6095': 1})\n",
      "['0000-0003-1676-9853', '0000-0002-1807-4644', '0000-0002-0419-547X', '0000-0002-9352-7877']\n",
      "Total sample size after apply threshold:  117\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(117, 492)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(117, 492)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.86      0.84        36\n",
      "          1       0.68      0.84      0.75        25\n",
      "          2       1.00      0.90      0.95        29\n",
      "          3       1.00      0.81      0.90        27\n",
      "\n",
      "avg / total       0.87      0.85      0.86       117\n",
      "\n",
      "[31  5  0  0  4 21  0  0  1  2 26  0  2  3  0 22]\n",
      "svc Accuracy:  0.8547008547008547\n",
      "svc F1:  0.8578128917414631\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.86      0.85        36\n",
      "          1       0.67      0.80      0.73        25\n",
      "          2       1.00      0.90      0.95        29\n",
      "          3       1.00      0.89      0.94        27\n",
      "\n",
      "avg / total       0.88      0.86      0.87       117\n",
      "\n",
      "[31  5  0  0  5 20  0  0  1  2 26  0  0  3  0 24]\n",
      "LR Accuracy:  0.8632478632478633\n",
      "LR F1:  0.8658047029521647\n",
      "For name:  z_yin\n",
      "total sample size before apply threshold:  52\n",
      "Counter({'0000-0003-1752-644X': 12, '0000-0002-3547-0606': 12, '0000-0002-7189-895X': 6, '0000-0002-4545-1783': 6, '0000-0002-1252-7809': 5, '0000-0003-4396-0215': 4, '0000-0001-8679-5251': 3, '0000-0001-5141-1967': 1, '0000-0002-7567-9084': 1, '0000-0003-0255-4421': 1, '0000-0002-7572-1748': 1})\n",
      "['0000-0003-1752-644X', '0000-0002-3547-0606']\n",
      "Total sample size after apply threshold:  24\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(24, 89)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(24, 89)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.92      0.96        12\n",
      "          1       0.92      1.00      0.96        12\n",
      "\n",
      "avg / total       0.96      0.96      0.96        24\n",
      "\n",
      "[11  1  0 12]\n",
      "svc Accuracy:  0.9583333333333334\n",
      "svc F1:  0.9582608695652175\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        12\n",
      "          1       1.00      1.00      1.00        12\n",
      "\n",
      "avg / total       1.00      1.00      1.00        24\n",
      "\n",
      "[12  0  0 12]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  c_gu\n",
      "total sample size before apply threshold:  65\n",
      "Counter({'0000-0002-8527-8145': 57, '0000-0002-8152-1400': 4, '0000-0002-9294-314X': 3, '0000-0002-3571-4658': 1})\n",
      "['0000-0002-8527-8145']\n",
      "Total sample size after apply threshold:  57\n",
      "For name:  a_soto\n",
      "total sample size before apply threshold:  32\n",
      "Counter({'0000-0002-0144-1399': 17, '0000-0001-9672-9004': 5, '0000-0002-7265-0956': 4, '0000-0002-2641-9032': 3, '0000-0001-8648-8032': 3})\n",
      "['0000-0002-0144-1399']\n",
      "Total sample size after apply threshold:  17\n",
      "For name:  h_hsieh\n",
      "total sample size before apply threshold:  70\n",
      "Counter({'0000-0001-8302-2472': 53, '0000-0003-3201-3677': 13, '0000-0002-2583-7670': 3, '0000-0002-4483-1768': 1})\n",
      "['0000-0003-3201-3677', '0000-0001-8302-2472']\n",
      "Total sample size after apply threshold:  66\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(66, 52)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(66, 52)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.92      0.89        13\n",
      "          1       0.98      0.96      0.97        53\n",
      "\n",
      "avg / total       0.96      0.95      0.96        66\n",
      "\n",
      "[12  1  2 51]\n",
      "svc Accuracy:  0.9545454545454546\n",
      "svc F1:  0.9301587301587302\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.69      0.75        13\n",
      "          1       0.93      0.96      0.94        53\n",
      "\n",
      "avg / total       0.91      0.91      0.91        66\n",
      "\n",
      "[ 9  4  2 51]\n",
      "LR Accuracy:  0.9090909090909091\n",
      "LR F1:  0.8472222222222223\n",
      "For name:  m_crespo\n",
      "total sample size before apply threshold:  49\n",
      "Counter({'0000-0002-7732-7808': 20, '0000-0002-1852-2259': 12, '0000-0001-8762-7874': 9, '0000-0002-7086-9751': 8})\n",
      "['0000-0002-7732-7808', '0000-0002-1852-2259']\n",
      "Total sample size after apply threshold:  32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(32, 165)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(32, 165)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.98        20\n",
      "          1       1.00      0.92      0.96        12\n",
      "\n",
      "avg / total       0.97      0.97      0.97        32\n",
      "\n",
      "[20  0  1 11]\n",
      "svc Accuracy:  0.96875\n",
      "svc F1:  0.9660657476139979\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.98        20\n",
      "          1       1.00      0.92      0.96        12\n",
      "\n",
      "avg / total       0.97      0.97      0.97        32\n",
      "\n",
      "[20  0  1 11]\n",
      "LR Accuracy:  0.96875\n",
      "LR F1:  0.9660657476139979\n",
      "For name:  s_phillips\n",
      "total sample size before apply threshold:  183\n",
      "Counter({'0000-0002-1956-4098': 138, '0000-0002-5694-0670': 20, '0000-0002-2549-8111': 11, '0000-0001-7157-4122': 6, '0000-0002-3720-6470': 5, '0000-0002-4230-4454': 2, '0000-0003-0858-4701': 1})\n",
      "['0000-0002-1956-4098', '0000-0002-5694-0670', '0000-0002-2549-8111']\n",
      "Total sample size after apply threshold:  169\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(169, 282)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(169, 282)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99       138\n",
      "          1       1.00      0.95      0.97        20\n",
      "          2       1.00      0.91      0.95        11\n",
      "\n",
      "avg / total       0.99      0.99      0.99       169\n",
      "\n",
      "[138   0   0   1  19   0   1   0  10]\n",
      "svc Accuracy:  0.9881656804733728\n",
      "svc F1:  0.9731818940452035\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      1.00      0.97       138\n",
      "          1       1.00      0.55      0.71        20\n",
      "          2       1.00      0.91      0.95        11\n",
      "\n",
      "avg / total       0.94      0.94      0.93       169\n",
      "\n",
      "[138   0   0   9  11   0   1   0  10]\n",
      "LR Accuracy:  0.9408284023668639\n",
      "LR F1:  0.8756977789235854\n",
      "For name:  r_rodrigues\n",
      "total sample size before apply threshold:  74\n",
      "Counter({'0000-0002-7631-743X': 30, '0000-0001-8592-850X': 22, '0000-0002-7557-1815': 10, '0000-0002-5894-2506': 2, '0000-0003-4493-2654': 2, '0000-0002-0437-2798': 2, '0000-0002-7589-7807': 1, '0000-0002-4261-1147': 1, '0000-0002-5115-6991': 1, '0000-0001-5631-0970': 1, '0000-0003-3522-9844': 1, '0000-0002-9952-3834': 1})\n",
      "['0000-0002-7557-1815', '0000-0002-7631-743X', '0000-0001-8592-850X']\n",
      "Total sample size after apply threshold:  62\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(62, 173)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(62, 173)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.90      0.86        10\n",
      "          1       0.97      0.93      0.95        30\n",
      "          2       1.00      1.00      1.00        22\n",
      "\n",
      "avg / total       0.95      0.95      0.95        62\n",
      "\n",
      "[ 9  1  0  2 28  0  0  0 22]\n",
      "svc Accuracy:  0.9516129032258065\n",
      "svc F1:  0.9354317998385796\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.70      0.82        10\n",
      "          1       0.91      1.00      0.95        30\n",
      "          2       1.00      1.00      1.00        22\n",
      "\n",
      "avg / total       0.96      0.95      0.95        62\n",
      "\n",
      "[ 7  3  0  0 30  0  0  0 22]\n",
      "LR Accuracy:  0.9516129032258065\n",
      "LR F1:  0.9253034547152194\n",
      "For name:  a_mansour\n",
      "total sample size before apply threshold:  15\n",
      "Counter({'0000-0003-2544-2705': 7, '0000-0002-7543-575X': 4, '0000-0001-7312-4299': 3, '0000-0001-5886-0650': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  a_lau\n",
      "total sample size before apply threshold:  35\n",
      "Counter({'0000-0002-5933-9290': 21, '0000-0002-6489-204X': 8, '0000-0003-3802-828X': 4, '0000-0002-7338-7176': 2})\n",
      "['0000-0002-5933-9290']\n",
      "Total sample size after apply threshold:  21\n",
      "For name:  j_berg\n",
      "total sample size before apply threshold:  171\n",
      "Counter({'0000-0003-0157-5888': 86, '0000-0003-3022-0963': 66, '0000-0003-2360-2664': 11, '0000-0003-2126-6476': 4, '0000-0001-8583-6349': 2, '0000-0001-7947-5073': 2})\n",
      "['0000-0003-3022-0963', '0000-0003-2360-2664', '0000-0003-0157-5888']\n",
      "Total sample size after apply threshold:  163\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(163, 406)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(163, 406)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.69      0.98      0.81        66\n",
      "          1       1.00      0.18      0.31        11\n",
      "          2       0.99      0.77      0.86        86\n",
      "\n",
      "avg / total       0.87      0.82      0.80       163\n",
      "\n",
      "[65  0  1  9  2  0 20  0 66]\n",
      "svc Accuracy:  0.8159509202453987\n",
      "svc F1:  0.6609791352438412\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.98      0.84        66\n",
      "          1       1.00      0.18      0.31        11\n",
      "          2       0.99      0.84      0.91        86\n",
      "\n",
      "avg / total       0.89      0.85      0.84       163\n",
      "\n",
      "[65  0  1  9  2  0 14  0 72]\n",
      "LR Accuracy:  0.852760736196319\n",
      "LR F1:  0.6858361764022142\n",
      "For name:  l_wilson\n",
      "total sample size before apply threshold:  59\n",
      "Counter({'0000-0001-8709-8968': 18, '0000-0003-4175-7125': 11, '0000-0001-6659-6001': 11, '0000-0002-3779-8277': 11, '0000-0002-3532-0309': 5, '0000-0002-8333-5660': 3})\n",
      "['0000-0001-8709-8968', '0000-0003-4175-7125', '0000-0001-6659-6001', '0000-0002-3779-8277']\n",
      "Total sample size after apply threshold:  51\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(51, 93)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(51, 93)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.83      0.91        18\n",
      "          1       1.00      0.73      0.84        11\n",
      "          2       1.00      1.00      1.00        11\n",
      "          3       0.65      1.00      0.79        11\n",
      "\n",
      "avg / total       0.92      0.88      0.89        51\n",
      "\n",
      "[15  0  0  3  0  8  0  3  0  0 11  0  0  0  0 11]\n",
      "svc Accuracy:  0.8823529411764706\n",
      "svc F1:  0.8842276144907724\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.94      0.97        18\n",
      "          1       1.00      0.73      0.84        11\n",
      "          2       0.92      1.00      0.96        11\n",
      "          3       0.71      0.91      0.80        11\n",
      "\n",
      "avg / total       0.92      0.90      0.90        51\n",
      "\n",
      "[17  0  0  1  0  8  0  3  0  0 11  0  0  0  1 10]\n",
      "LR Accuracy:  0.9019607843137255\n",
      "LR F1:  0.8925138934292252\n",
      "For name:  c_park\n",
      "total sample size before apply threshold:  360\n",
      "Counter({'0000-0003-4083-8791': 106, '0000-0002-2350-9876': 69, '0000-0003-1906-1308': 45, '0000-0002-9732-361X': 40, '0000-0002-3363-5788': 35, '0000-0002-7618-9028': 16, '0000-0003-1584-6896': 14, '0000-0002-1788-045X': 11, '0000-0001-9008-1964': 10, '0000-0003-4734-214X': 9, '0000-0002-0776-3188': 2, '0000-0003-0409-8132': 1, '0000-0003-0721-272X': 1, '0000-0002-5935-8264': 1})\n",
      "['0000-0002-3363-5788', '0000-0001-9008-1964', '0000-0002-7618-9028', '0000-0002-1788-045X', '0000-0002-2350-9876', '0000-0003-4083-8791', '0000-0002-9732-361X', '0000-0003-1584-6896', '0000-0003-1906-1308']\n",
      "Total sample size after apply threshold:  346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(346, 469)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(346, 469)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.91      0.90        35\n",
      "          1       1.00      0.90      0.95        10\n",
      "          2       0.77      0.62      0.69        16\n",
      "          3       0.75      0.55      0.63        11\n",
      "          4       0.84      0.84      0.84        69\n",
      "          5       0.75      0.90      0.82       106\n",
      "          6       0.85      0.82      0.84        40\n",
      "          7       1.00      0.79      0.88        14\n",
      "          8       0.94      0.71      0.81        45\n",
      "\n",
      "avg / total       0.84      0.83      0.83       346\n",
      "\n",
      "[32  0  1  0  0  2  0  0  0  0  9  0  0  1  0  0  0  0  0  0 10  1  0  1\n",
      "  3  0  1  0  0  0  6  0  4  1  0  0  1  0  0  0 58 10  0  0  0  0  0  0\n",
      "  1  7 95  2  0  1  0  0  1  0  1  5 33  0  0  0  0  0  0  0  3  0 11  0\n",
      "  3  0  1  0  2  7  0  0 32]\n",
      "svc Accuracy:  0.8265895953757225\n",
      "svc F1:  0.8168456628571097\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.97      0.91        35\n",
      "          1       1.00      0.80      0.89        10\n",
      "          2       0.83      0.62      0.71        16\n",
      "          3       1.00      0.27      0.43        11\n",
      "          4       0.87      0.87      0.87        69\n",
      "          5       0.76      0.92      0.83       106\n",
      "          6       0.85      0.82      0.84        40\n",
      "          7       1.00      0.79      0.88        14\n",
      "          8       0.97      0.80      0.88        45\n",
      "\n",
      "avg / total       0.86      0.84      0.84       346\n",
      "\n",
      "[34  0  1  0  0  0  0  0  0  0  8  0  0  0  2  0  0  0  0  0 10  0  0  3\n",
      "  2  0  1  0  0  0  3  2  5  1  0  0  0  0  0  0 60  9  0  0  0  2  0  0\n",
      "  0  5 97  2  0  0  0  0  0  0  0  7 33  0  0  0  0  0  0  0  3  0 11  0\n",
      "  4  0  1  0  2  1  1  0 36]\n",
      "LR Accuracy:  0.8439306358381503\n",
      "LR F1:  0.8037875288908404\n",
      "For name:  r_thomas\n",
      "total sample size before apply threshold:  368\n",
      "Counter({'0000-0002-0518-8386': 95, '0000-0002-2340-0301': 95, '0000-0003-1448-7182': 74, '0000-0003-2062-8623': 46, '0000-0001-9251-5543': 13, '0000-0002-2970-6352': 10, '0000-0002-2165-5917': 8, '0000-0003-1282-7825': 5, '0000-0003-3588-2317': 5, '0000-0002-7286-2764': 4, '0000-0001-8784-1707': 2, '0000-0001-5256-3313': 2, '0000-0002-2069-1799': 2, '0000-0002-8745-7462': 2, '0000-0001-5296-3114': 1, '0000-0002-8872-7866': 1, '0000-0003-3473-2579': 1, '0000-0002-5362-4816': 1, '0000-0001-7194-3653': 1})\n",
      "['0000-0002-0518-8386', '0000-0003-2062-8623', '0000-0001-9251-5543', '0000-0003-1448-7182', '0000-0002-2340-0301', '0000-0002-2970-6352']\n",
      "Total sample size after apply threshold:  333\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(333, 1474)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(333, 1474)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.95      0.75        95\n",
      "          1       0.81      0.74      0.77        46\n",
      "          2       1.00      0.62      0.76        13\n",
      "          3       1.00      0.74      0.85        74\n",
      "          4       1.00      0.78      0.88        95\n",
      "          5       1.00      1.00      1.00        10\n",
      "\n",
      "avg / total       0.87      0.81      0.82       333\n",
      "\n",
      "[90  5  0  0  0  0 12 34  0  0  0  0  5  0  8  0  0  0 19  0  0 55  0  0\n",
      " 18  3  0  0 74  0  0  0  0  0  0 10]\n",
      "svc Accuracy:  0.8138138138138138\n",
      "svc F1:  0.8360371555351384\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.69      0.97      0.80        95\n",
      "          1       0.92      0.72      0.80        46\n",
      "          2       1.00      0.54      0.70        13\n",
      "          3       1.00      0.81      0.90        74\n",
      "          4       0.99      0.89      0.94        95\n",
      "          5       1.00      1.00      1.00        10\n",
      "\n",
      "avg / total       0.90      0.86      0.86       333\n",
      "\n",
      "[92  2  0  0  1  0 13 33  0  0  0  0  6  0  7  0  0  0 14  0  0 60  0  0\n",
      "  9  1  0  0 85  0  0  0  0  0  0 10]\n",
      "LR Accuracy:  0.8618618618618619\n",
      "LR F1:  0.8571867343264775\n",
      "For name:  j_fonseca\n",
      "total sample size before apply threshold:  170\n",
      "Counter({'0000-0003-1432-3671': 87, '0000-0002-0887-8796': 55, '0000-0001-6477-7028': 8, '0000-0001-6703-3278': 7, '0000-0001-7173-7374': 5, '0000-0003-1206-7969': 3, '0000-0003-2549-5823': 2, '0000-0002-3679-0337': 1, '0000-0002-6073-1791': 1, '0000-0002-2136-1011': 1})\n",
      "['0000-0003-1432-3671', '0000-0002-0887-8796']\n",
      "Total sample size after apply threshold:  142\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(142, 813)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(142, 813)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.97      0.94        87\n",
      "          1       0.94      0.85      0.90        55\n",
      "\n",
      "avg / total       0.92      0.92      0.92       142\n",
      "\n",
      "[84  3  8 47]\n",
      "svc Accuracy:  0.9225352112676056\n",
      "svc F1:  0.9168927906358073\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.98      0.95        87\n",
      "          1       0.96      0.87      0.91        55\n",
      "\n",
      "avg / total       0.94      0.94      0.94       142\n",
      "\n",
      "[85  2  7 48]\n",
      "LR Accuracy:  0.9366197183098591\n",
      "LR F1:  0.9320031923383878\n",
      "For name:  s_henderson\n",
      "total sample size before apply threshold:  82\n",
      "Counter({'0000-0002-1076-3867': 52, '0000-0002-9032-3828': 25, '0000-0003-3019-1891': 4, '0000-0001-6389-4927': 1})\n",
      "['0000-0002-1076-3867', '0000-0002-9032-3828']\n",
      "Total sample size after apply threshold:  77\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(77, 421)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(77, 421)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      1.00      0.95        52\n",
      "          1       1.00      0.80      0.89        25\n",
      "\n",
      "avg / total       0.94      0.94      0.93        77\n",
      "\n",
      "[52  0  5 20]\n",
      "svc Accuracy:  0.935064935064935\n",
      "svc F1:  0.9215086646279307\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      1.00      0.95        52\n",
      "          1       1.00      0.80      0.89        25\n",
      "\n",
      "avg / total       0.94      0.94      0.93        77\n",
      "\n",
      "[52  0  5 20]\n",
      "LR Accuracy:  0.935064935064935\n",
      "LR F1:  0.9215086646279307\n",
      "For name:  m_coelho\n",
      "total sample size before apply threshold:  93\n",
      "Counter({'0000-0003-3288-1693': 44, '0000-0002-7312-3429': 15, '0000-0002-5716-0561': 11, '0000-0002-6542-175X': 5, '0000-0003-3312-191X': 5, '0000-0002-7429-4967': 4, '0000-0002-0392-1118': 3, '0000-0002-9169-7776': 3, '0000-0002-0197-8081': 3})\n",
      "['0000-0003-3288-1693', '0000-0002-7312-3429', '0000-0002-5716-0561']\n",
      "Total sample size after apply threshold:  70\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(70, 223)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(70, 223)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      1.00      0.95        44\n",
      "          1       1.00      0.73      0.85        15\n",
      "          2       1.00      0.91      0.95        11\n",
      "\n",
      "avg / total       0.94      0.93      0.93        70\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44  0  0  4 11  0  1  0 10]\n",
      "svc Accuracy:  0.9285714285714286\n",
      "svc F1:  0.9149237858915278\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      1.00      0.95        44\n",
      "          1       1.00      0.73      0.85        15\n",
      "          2       1.00      0.91      0.95        11\n",
      "\n",
      "avg / total       0.94      0.93      0.93        70\n",
      "\n",
      "[44  0  0  4 11  0  1  0 10]\n",
      "LR Accuracy:  0.9285714285714286\n",
      "LR F1:  0.9149237858915278\n",
      "For name:  j_pearson\n",
      "total sample size before apply threshold:  119\n",
      "Counter({'0000-0002-3318-5406': 65, '0000-0001-5607-4517': 41, '0000-0002-2867-2269': 6, '0000-0002-3777-1453': 4, '0000-0002-9876-7837': 2, '0000-0002-1400-5932': 1})\n",
      "['0000-0001-5607-4517', '0000-0002-3318-5406']\n",
      "Total sample size after apply threshold:  106\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(106, 364)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(106, 364)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.85      0.92        41\n",
      "          1       0.92      1.00      0.96        65\n",
      "\n",
      "avg / total       0.95      0.94      0.94       106\n",
      "\n",
      "[35  6  0 65]\n",
      "svc Accuracy:  0.9433962264150944\n",
      "svc F1:  0.938467492260062\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.95      0.95        41\n",
      "          1       0.97      0.97      0.97        65\n",
      "\n",
      "avg / total       0.96      0.96      0.96       106\n",
      "\n",
      "[39  2  2 63]\n",
      "LR Accuracy:  0.9622641509433962\n",
      "LR F1:  0.9602251407129456\n",
      "For name:  z_xie\n",
      "total sample size before apply threshold:  99\n",
      "Counter({'0000-0003-2974-1825': 48, '0000-0001-5816-6159': 17, '0000-0002-8348-4455': 16, '0000-0002-1539-5100': 8, '0000-0002-4526-9746': 6, '0000-0003-0308-5233': 1, '0000-0002-3137-561X': 1, '0000-0003-2492-0592': 1, '0000-0002-6600-8190': 1})\n",
      "['0000-0003-2974-1825', '0000-0002-8348-4455', '0000-0001-5816-6159']\n",
      "Total sample size after apply threshold:  81\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(81, 1270)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(81, 1270)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.92      0.87        48\n",
      "          1       1.00      0.81      0.90        16\n",
      "          2       0.73      0.65      0.69        17\n",
      "\n",
      "avg / total       0.84      0.84      0.84        81\n",
      "\n",
      "[44  0  4  3 13  0  6  0 11]\n",
      "svc Accuracy:  0.8395061728395061\n",
      "svc F1:  0.8184462842836008\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.98      0.90        48\n",
      "          1       1.00      0.81      0.90        16\n",
      "          2       0.92      0.65      0.76        17\n",
      "\n",
      "avg / total       0.89      0.88      0.87        81\n",
      "\n",
      "[47  0  1  3 13  0  6  0 11]\n",
      "LR Accuracy:  0.8765432098765432\n",
      "LR F1:  0.8530061892130858\n",
      "For name:  m_wright\n",
      "total sample size before apply threshold:  379\n",
      "Counter({'0000-0001-7133-4970': 213, '0000-0002-0541-7556': 87, '0000-0002-2650-2426': 25, '0000-0001-8036-1161': 17, '0000-0003-2731-4707': 15, '0000-0002-9348-8740': 13, '0000-0001-7121-504X': 6, '0000-0001-5522-7796': 2, '0000-0002-5731-2692': 1})\n",
      "['0000-0001-8036-1161', '0000-0001-7133-4970', '0000-0002-2650-2426', '0000-0002-9348-8740', '0000-0002-0541-7556', '0000-0003-2731-4707']\n",
      "Total sample size after apply threshold:  370\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(370, 2203)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(370, 2203)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.65      0.76        17\n",
      "          1       1.00      1.00      1.00       213\n",
      "          2       1.00      0.60      0.75        25\n",
      "          3       0.90      0.69      0.78        13\n",
      "          4       0.82      1.00      0.90        87\n",
      "          5       1.00      0.93      0.97        15\n",
      "\n",
      "avg / total       0.95      0.94      0.94       370\n",
      "\n",
      "[ 11   0   0   1   5   0   0 212   0   0   1   0   1   1  15   0   8   0\n",
      "   0   0   0   9   4   0   0   0   0   0  87   0   0   0   0   0   1  14]\n",
      "svc Accuracy:  0.9405405405405406\n",
      "svc F1:  0.8589343658584973\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.59      0.74        17\n",
      "          1       0.92      1.00      0.96       213\n",
      "          2       1.00      0.60      0.75        25\n",
      "          3       0.91      0.77      0.83        13\n",
      "          4       0.96      0.99      0.97        87\n",
      "          5       1.00      0.80      0.89        15\n",
      "\n",
      "avg / total       0.94      0.94      0.93       370\n",
      "\n",
      "[ 10   5   0   1   1   0   0 213   0   0   0   0   0   7  15   0   3   0\n",
      "   0   3   0  10   0   0   0   1   0   0  86   0   0   3   0   0   0  12]\n",
      "LR Accuracy:  0.9351351351351351\n",
      "LR F1:  0.8570029576964765\n",
      "For name:  j_song\n",
      "total sample size before apply threshold:  248\n",
      "Counter({'0000-0003-0224-6322': 70, '0000-0003-2434-7511': 33, '0000-0001-6303-3801': 26, '0000-0003-0420-2374': 23, '0000-0002-4379-0909': 19, '0000-0002-2736-4037': 13, '0000-0001-6623-4369': 11, '0000-0002-9971-0541': 10, '0000-0001-9223-8590': 9, '0000-0003-3053-0929': 8, '0000-0001-7886-1765': 5, '0000-0003-1265-0337': 3, '0000-0001-7350-9578': 3, '0000-0003-4262-1895': 3, '0000-0002-3863-1719': 2, '0000-0002-3463-0196': 2, '0000-0002-9252-0331': 2, '0000-0002-6631-3232': 2, '0000-0003-3160-4643': 1, '0000-0002-2932-440X': 1, '0000-0003-3497-2513': 1, '0000-0002-7357-2136': 1})\n",
      "['0000-0003-0420-2374', '0000-0001-6303-3801', '0000-0003-0224-6322', '0000-0003-2434-7511', '0000-0001-6623-4369', '0000-0002-2736-4037', '0000-0002-4379-0909', '0000-0002-9971-0541']\n",
      "Total sample size after apply threshold:  205\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(205, 264)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(205, 264)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.87      0.80        23\n",
      "          1       1.00      0.96      0.98        26\n",
      "          2       0.86      0.94      0.90        70\n",
      "          3       0.93      0.76      0.83        33\n",
      "          4       0.50      0.45      0.48        11\n",
      "          5       0.67      0.62      0.64        13\n",
      "          6       0.94      0.89      0.92        19\n",
      "          7       1.00      0.90      0.95        10\n",
      "\n",
      "avg / total       0.86      0.85      0.85       205\n",
      "\n",
      "[20  0  1  1  1  0  0  0  0 25  1  0  0  0  0  0  0  0 66  0  0  3  1  0\n",
      "  3  0  1 25  3  1  0  0  4  0  1  1  5  0  0  0  0  0  5  0  0  8  0  0\n",
      "  0  0  2  0  0  0 17  0  0  0  0  0  1  0  0  9]\n",
      "svc Accuracy:  0.8536585365853658\n",
      "svc F1:  0.8117703112539468\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.68      0.83      0.75        23\n",
      "          1       1.00      0.96      0.98        26\n",
      "          2       0.83      0.97      0.89        70\n",
      "          3       0.87      0.82      0.84        33\n",
      "          4       0.71      0.45      0.56        11\n",
      "          5       0.71      0.38      0.50        13\n",
      "          6       1.00      0.84      0.91        19\n",
      "          7       1.00      0.90      0.95        10\n",
      "\n",
      "avg / total       0.85      0.85      0.84       205\n",
      "\n",
      "[19  0  1  2  1  0  0  0  0 25  1  0  0  0  0  0  0  0 68  0  0  2  0  0\n",
      "  4  0  1 27  1  0  0  0  4  0  0  2  5  0  0  0  0  0  8  0  0  5  0  0\n",
      "  0  0  3  0  0  0 16  0  1  0  0  0  0  0  0  9]\n",
      "LR Accuracy:  0.848780487804878\n",
      "LR F1:  0.7976483411346995\n",
      "For name:  k_becker\n",
      "total sample size before apply threshold:  394\n",
      "Counter({'0000-0002-6794-6656': 180, '0000-0002-6391-1341': 112, '0000-0002-6801-4498': 80, '0000-0003-4231-2590': 19, '0000-0001-6317-1884': 3})\n",
      "['0000-0002-6391-1341', '0000-0002-6801-4498', '0000-0002-6794-6656', '0000-0003-4231-2590']\n",
      "Total sample size after apply threshold:  391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(391, 1502)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(391, 1502)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.81      0.90       112\n",
      "          1       0.98      0.75      0.85        80\n",
      "          2       0.81      0.99      0.90       180\n",
      "          3       1.00      1.00      1.00        19\n",
      "\n",
      "avg / total       0.91      0.89      0.89       391\n",
      "\n",
      "[ 91   0  21   0   0  60  20   0   0   1 179   0   0   0   0  19]\n",
      "svc Accuracy:  0.8925831202046036\n",
      "svc F1:  0.9106538884812913\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.82      0.90       112\n",
      "          1       1.00      0.78      0.87        80\n",
      "          2       0.83      1.00      0.90       180\n",
      "          3       1.00      1.00      1.00        19\n",
      "\n",
      "avg / total       0.92      0.90      0.90       391\n",
      "\n",
      "[ 92   0  20   0   0  62  18   0   0   0 180   0   0   0   0  19]\n",
      "LR Accuracy:  0.9028132992327366\n",
      "LR F1:  0.9199307084996926\n",
      "For name:  r_sinha\n",
      "total sample size before apply threshold:  27\n",
      "Counter({'0000-0001-5497-5055': 11, '0000-0001-8488-6280': 5, '0000-0003-4645-3243': 4, '0000-0003-4185-5198': 3, '0000-0002-7231-1356': 2, '0000-0001-8918-7585': 2})\n",
      "['0000-0001-5497-5055']\n",
      "Total sample size after apply threshold:  11\n",
      "For name:  c_turner\n",
      "total sample size before apply threshold:  88\n",
      "Counter({'0000-0002-2687-932X': 33, '0000-0001-9466-1149': 32, '0000-0002-4458-9748': 14, '0000-0001-7409-4386': 5, '0000-0002-1245-0741': 4})\n",
      "['0000-0001-9466-1149', '0000-0002-2687-932X', '0000-0002-4458-9748']\n",
      "Total sample size after apply threshold:  79\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(79, 183)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(79, 183)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      1.00      0.96        32\n",
      "          1       1.00      0.91      0.95        33\n",
      "          2       1.00      1.00      1.00        14\n",
      "\n",
      "avg / total       0.97      0.96      0.96        79\n",
      "\n",
      "[32  0  0  3 30  0  0  0 14]\n",
      "svc Accuracy:  0.9620253164556962\n",
      "svc F1:  0.9692016109926559\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      1.00      0.96        32\n",
      "          1       1.00      0.91      0.95        33\n",
      "          2       1.00      1.00      1.00        14\n",
      "\n",
      "avg / total       0.97      0.96      0.96        79\n",
      "\n",
      "[32  0  0  3 30  0  0  0 14]\n",
      "LR Accuracy:  0.9620253164556962\n",
      "LR F1:  0.9692016109926559\n",
      "For name:  y_su\n",
      "total sample size before apply threshold:  190\n",
      "Counter({'0000-0002-1771-9017': 83, '0000-0002-5390-4113': 24, '0000-0003-3537-6246': 23, '0000-0003-3398-6294': 17, '0000-0001-8434-1758': 15, '0000-0003-2660-9183': 10, '0000-0003-2193-5473': 5, '0000-0002-4293-5037': 2, '0000-0001-7557-8518': 2, '0000-0002-4643-917X': 1, '0000-0001-8528-0694': 1, '0000-0003-0790-5905': 1, '0000-0003-0355-3981': 1, '0000-0002-8201-1592': 1, '0000-0002-4172-7981': 1, '0000-0001-6544-126X': 1, '0000-0001-7622-7269': 1, '0000-0002-8466-0043': 1})\n",
      "['0000-0001-8434-1758', '0000-0003-3398-6294', '0000-0002-5390-4113', '0000-0003-3537-6246', '0000-0002-1771-9017', '0000-0003-2660-9183']\n",
      "Total sample size after apply threshold:  172\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(172, 281)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(172, 281)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.13      0.24        15\n",
      "          1       1.00      1.00      1.00        17\n",
      "          2       0.74      0.71      0.72        24\n",
      "          3       1.00      0.83      0.90        23\n",
      "          4       0.93      0.92      0.92        83\n",
      "          5       0.34      1.00      0.51        10\n",
      "\n",
      "avg / total       0.89      0.82      0.82       172\n",
      "\n",
      "[ 2  0  1  0  1 11  0 17  0  0  0  0  0  0 17  0  5  2  0  0  0 19  0  4\n",
      "  0  0  5  0 76  2  0  0  0  0  0 10]\n",
      "svc Accuracy:  0.8197674418604651\n",
      "svc F1:  0.7162488186267911\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.40      0.57        15\n",
      "          1       1.00      1.00      1.00        17\n",
      "          2       0.73      0.67      0.70        24\n",
      "          3       1.00      0.91      0.95        23\n",
      "          4       0.86      0.93      0.89        83\n",
      "          5       0.56      0.90      0.69        10\n",
      "\n",
      "avg / total       0.87      0.85      0.84       172\n",
      "\n",
      "[ 6  0  0  0  4  5  0 17  0  0  0  0  0  0 16  0  7  1  0  0  0 21  1  1\n",
      "  0  0  6  0 77  0  0  0  0  0  1  9]\n",
      "LR Accuracy:  0.8488372093023255\n",
      "LR F1:  0.8006845504332311\n",
      "For name:  a_popov\n",
      "total sample size before apply threshold:  135\n",
      "Counter({'0000-0002-7596-0378': 99, '0000-0002-4678-6307': 15, '0000-0001-5024-5311': 13, '0000-0003-3881-7369': 3, '0000-0003-4602-5708': 3, '0000-0002-0889-6986': 1, '0000-0003-2643-4846': 1})\n",
      "['0000-0001-5024-5311', '0000-0002-4678-6307', '0000-0002-7596-0378']\n",
      "Total sample size after apply threshold:  127\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(127, 211)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(127, 211)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.85      0.92        13\n",
      "          1       1.00      0.80      0.89        15\n",
      "          2       0.95      1.00      0.98        99\n",
      "\n",
      "avg / total       0.96      0.96      0.96       127\n",
      "\n",
      "[11  0  2  0 12  3  0  0 99]\n",
      "svc Accuracy:  0.9606299212598425\n",
      "svc F1:  0.9269750045612115\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.85      0.92        13\n",
      "          1       1.00      0.60      0.75        15\n",
      "          2       0.93      1.00      0.96        99\n",
      "\n",
      "avg / total       0.94      0.94      0.93       127\n",
      "\n",
      "[11  0  2  0  9  6  0  0 99]\n",
      "LR Accuracy:  0.937007874015748\n",
      "LR F1:  0.8759439050701187\n",
      "For name:  w_liao\n",
      "total sample size before apply threshold:  79\n",
      "Counter({'0000-0001-5362-6953': 29, '0000-0001-6383-3470': 25, '0000-0002-5619-4997': 16, '0000-0002-9768-0959': 5, '0000-0001-7221-5906': 3, '0000-0002-5333-2717': 1})\n",
      "['0000-0001-5362-6953', '0000-0002-5619-4997', '0000-0001-6383-3470']\n",
      "Total sample size after apply threshold:  70\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(70, 117)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(70, 117)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.72      0.75        29\n",
      "          1       0.79      0.94      0.86        16\n",
      "          2       0.79      0.76      0.78        25\n",
      "\n",
      "avg / total       0.79      0.79      0.78        70\n",
      "\n",
      "[21  3  5  1 15  0  5  1 19]\n",
      "svc Accuracy:  0.7857142857142857\n",
      "svc F1:  0.79421768707483\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.86      0.88        29\n",
      "          1       0.93      0.88      0.90        16\n",
      "          2       0.81      0.88      0.85        25\n",
      "\n",
      "avg / total       0.87      0.87      0.87        70\n",
      "\n",
      "[25  1  3  0 14  2  3  0 22]\n",
      "LR Accuracy:  0.8714285714285714\n",
      "LR F1:  0.8755242116871997\n",
      "For name:  j_zhong\n",
      "total sample size before apply threshold:  280\n",
      "Counter({'0000-0002-2265-9338': 115, '0000-0002-1494-6396': 70, '0000-0003-3148-4143': 37, '0000-0002-3534-7480': 21, '0000-0003-1801-9642': 19, '0000-0001-7157-603X': 8, '0000-0002-8815-4105': 4, '0000-0002-8945-4599': 3, '0000-0002-0556-2964': 1, '0000-0003-2750-9782': 1, '0000-0001-8785-1729': 1})\n",
      "['0000-0003-3148-4143', '0000-0003-1801-9642', '0000-0002-3534-7480', '0000-0002-2265-9338', '0000-0002-1494-6396']\n",
      "Total sample size after apply threshold:  262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(262, 508)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(262, 508)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.92      0.96        37\n",
      "          1       0.93      0.74      0.82        19\n",
      "          2       0.89      0.81      0.85        21\n",
      "          3       0.84      0.93      0.88       115\n",
      "          4       0.85      0.81      0.83        70\n",
      "\n",
      "avg / total       0.88      0.87      0.87       262\n",
      "\n",
      "[ 34   0   0   3   0   0  14   0   4   1   0   1  17   2   1   0   0   0\n",
      " 107   8   0   0   2  11  57]\n",
      "svc Accuracy:  0.8740458015267175\n",
      "svc F1:  0.8695380399240541\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.89      0.94        37\n",
      "          1       1.00      0.58      0.73        19\n",
      "          2       1.00      0.71      0.83        21\n",
      "          3       0.82      0.97      0.88       115\n",
      "          4       0.90      0.86      0.88        70\n",
      "\n",
      "avg / total       0.89      0.88      0.88       262\n",
      "\n",
      "[ 33   0   0   4   0   0  11   0   5   3   0   0  15   6   0   0   0   0\n",
      " 111   4   0   0   0  10  60]\n",
      "LR Accuracy:  0.8778625954198473\n",
      "LR F1:  0.8539796739354711\n",
      "For name:  a_wheeler\n",
      "total sample size before apply threshold:  138\n",
      "Counter({'0000-0001-5230-7475': 72, '0000-0001-9288-8163': 43, '0000-0001-8617-827X': 15, '0000-0002-9926-1301': 5, '0000-0002-1120-3618': 2, '0000-0001-9755-674X': 1})\n",
      "['0000-0001-5230-7475', '0000-0001-8617-827X', '0000-0001-9288-8163']\n",
      "Total sample size after apply threshold:  130\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(130, 334)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(130, 334)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      1.00      0.89        72\n",
      "          1       1.00      0.40      0.57        15\n",
      "          2       1.00      0.81      0.90        43\n",
      "\n",
      "avg / total       0.89      0.87      0.86       130\n",
      "\n",
      "[72  0  0  9  6  0  8  0 35]\n",
      "svc Accuracy:  0.8692307692307693\n",
      "svc F1:  0.7877581355842226\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      1.00      0.87        72\n",
      "          1       0.00      0.00      0.00        15\n",
      "          2       0.97      0.81      0.89        43\n",
      "\n",
      "avg / total       0.75      0.82      0.77       130\n",
      "\n",
      "[72  0  0 14  0  1  8  0 35]\n",
      "LR Accuracy:  0.823076923076923\n",
      "LR F1:  0.5845152762950536\n",
      "For name:  m_walsh\n",
      "total sample size before apply threshold:  37\n",
      "Counter({'0000-0001-5683-1151': 30, '0000-0001-8920-7419': 3, '0000-0002-1770-3314': 2, '0000-0003-0982-4105': 2})\n",
      "['0000-0001-5683-1151']\n",
      "Total sample size after apply threshold:  30\n",
      "For name:  r_figueiredo\n",
      "total sample size before apply threshold:  48\n",
      "Counter({'0000-0002-2122-6530': 29, '0000-0001-5806-0944': 17, '0000-0002-4304-6434': 1, '0000-0002-0933-4854': 1})\n",
      "['0000-0001-5806-0944', '0000-0002-2122-6530']\n",
      "Total sample size after apply threshold:  46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gao137/intel/intelpython3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/gao137/intel/intelpython3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(46, 162)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(46, 162)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97        17\n",
      "          1       1.00      0.97      0.98        29\n",
      "\n",
      "avg / total       0.98      0.98      0.98        46\n",
      "\n",
      "[17  0  1 28]\n",
      "svc Accuracy:  0.9782608695652174\n",
      "svc F1:  0.9769423558897243\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97        17\n",
      "          1       1.00      0.97      0.98        29\n",
      "\n",
      "avg / total       0.98      0.98      0.98        46\n",
      "\n",
      "[17  0  1 28]\n",
      "LR Accuracy:  0.9782608695652174\n",
      "LR F1:  0.9769423558897243\n",
      "For name:  y_lin\n",
      "total sample size before apply threshold:  785\n",
      "Counter({'0000-0003-3791-7587': 146, '0000-0001-8153-1441': 115, '0000-0003-1224-6561': 64, '0000-0002-4192-3165': 49, '0000-0002-2499-8632': 39, '0000-0001-8667-0811': 33, '0000-0002-5887-0880': 24, '0000-0001-5227-2663': 23, '0000-0002-4350-7755': 23, '0000-0003-4913-8003': 22, '0000-0001-6460-2877': 21, '0000-0003-1954-334X': 20, '0000-0001-8572-649X': 20, '0000-0001-5574-7062': 15, '0000-0002-0352-2694': 15, '0000-0002-9390-795X': 13, '0000-0001-8904-1287': 13, '0000-0003-3410-3588': 12, '0000-0003-4384-8354': 9, '0000-0001-6833-8276': 9, '0000-0002-8746-3387': 9, '0000-0002-0796-0130': 8, '0000-0002-0435-7694': 8, '0000-0001-6454-0901': 7, '0000-0002-0123-9836': 6, '0000-0001-7120-4690': 6, '0000-0001-5100-6072': 6, '0000-0003-3913-5298': 6, '0000-0003-3177-5186': 5, '0000-0003-1240-7011': 5, '0000-0003-1470-4159': 5, '0000-0001-7910-1223': 4, '0000-0003-4289-894X': 4, '0000-0002-7289-5347': 4, '0000-0003-1328-1641': 2, '0000-0002-2229-6354': 2, '0000-0002-6835-7116': 2, '0000-0001-6819-1235': 2, '0000-0003-2656-3613': 1, '0000-0002-5379-5359': 1, '0000-0003-4327-7432': 1, '0000-0001-7923-0789': 1, '0000-0002-7492-9985': 1, '0000-0002-7639-9594': 1, '0000-0002-2502-2412': 1, '0000-0001-7243-0980': 1, '0000-0002-8287-1429': 1})\n",
      "['0000-0002-9390-795X', '0000-0001-8904-1287', '0000-0002-2499-8632', '0000-0003-1954-334X', '0000-0001-8667-0811', '0000-0002-4192-3165', '0000-0001-8572-649X', '0000-0003-3791-7587', '0000-0001-5227-2663', '0000-0003-3410-3588', '0000-0002-5887-0880', '0000-0001-5574-7062', '0000-0001-8153-1441', '0000-0003-1224-6561', '0000-0002-0352-2694', '0000-0003-4913-8003', '0000-0001-6460-2877', '0000-0002-4350-7755']\n",
      "Total sample size after apply threshold:  667\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(667, 786)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(667, 786)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.85      0.88        13\n",
      "          1       1.00      0.77      0.87        13\n",
      "          2       0.73      0.62      0.67        39\n",
      "          3       0.48      0.65      0.55        20\n",
      "          4       0.63      0.79      0.70        33\n",
      "          5       0.96      0.90      0.93        49\n",
      "          6       0.82      0.70      0.76        20\n",
      "          7       0.77      0.95      0.85       146\n",
      "          8       0.95      0.78      0.86        23\n",
      "          9       0.86      0.50      0.63        12\n",
      "         10       0.84      0.67      0.74        24\n",
      "         11       0.75      0.40      0.52        15\n",
      "         12       0.82      0.84      0.83       115\n",
      "         13       0.95      0.83      0.88        64\n",
      "         14       1.00      1.00      1.00        15\n",
      "         15       0.35      0.41      0.38        22\n",
      "         16       1.00      0.76      0.86        21\n",
      "         17       0.88      0.65      0.75        23\n",
      "\n",
      "avg / total       0.81      0.80      0.80       667\n",
      "\n",
      "[ 11   0   0   0   0   0   0   1   0   0   1   0   0   0   0   0   0   0\n",
      "   0  10   0   0   0   0   0   2   0   0   0   0   0   0   0   1   0   0\n",
      "   1   0  24   0   1   0   1   2   1   0   0   0   9   0   0   0   0   0\n",
      "   0   0   0  13   3   1   0   1   0   0   0   0   1   0   0   1   0   0\n",
      "   0   0   0   1  26   0   0   3   0   0   0   0   2   0   0   1   0   0\n",
      "   0   0   0   2   2  44   0   0   0   0   0   0   1   0   0   0   0   0\n",
      "   0   0   0   1   0   0  14   2   0   0   1   1   0   0   0   1   0   0\n",
      "   0   0   0   4   0   0   1 138   0   0   0   0   0   1   0   2   0   0\n",
      "   0   0   0   0   0   0   0   4  18   0   0   0   0   0   0   1   0   0\n",
      "   0   0   0   0   0   0   0   3   0   6   0   0   0   2   0   1   0   0\n",
      "   0   0   0   1   3   0   1   1   0   0  16   0   2   0   0   0   0   0\n",
      "   0   0   1   1   2   0   0   1   0   0   0   6   2   0   0   2   0   0\n",
      "   0   0   6   2   1   0   0   2   0   0   1   1  97   0   0   3   0   2\n",
      "   0   0   0   1   1   0   0   8   0   1   0   0   0  53   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0  15   0   0   0\n",
      "   0   0   0   1   2   1   0   8   0   0   0   0   1   0   0   9   0   0\n",
      "   0   0   0   0   0   0   0   2   0   0   0   0   0   0   0   3  16   0\n",
      "   0   0   2   0   0   0   0   1   0   0   0   0   4   0   0   1   0  15]\n",
      "svc Accuracy:  0.7961019490254873\n",
      "svc F1:  0.7589630222388501\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.85      0.92        13\n",
      "          1       1.00      0.69      0.82        13\n",
      "          2       0.76      0.49      0.59        39\n",
      "          3       0.55      0.30      0.39        20\n",
      "          4       0.68      0.76      0.71        33\n",
      "          5       0.81      0.96      0.88        49\n",
      "          6       0.86      0.60      0.71        20\n",
      "          7       0.74      0.97      0.84       146\n",
      "          8       0.85      0.74      0.79        23\n",
      "          9       0.88      0.58      0.70        12\n",
      "         10       0.61      0.58      0.60        24\n",
      "         11       0.62      0.33      0.43        15\n",
      "         12       0.74      0.87      0.80       115\n",
      "         13       0.95      0.86      0.90        64\n",
      "         14       1.00      1.00      1.00        15\n",
      "         15       0.40      0.09      0.15        22\n",
      "         16       0.94      0.81      0.87        21\n",
      "         17       0.89      0.74      0.81        23\n",
      "\n",
      "avg / total       0.78      0.78      0.76       667\n",
      "\n",
      "[ 11   0   0   0   0   0   0   1   0   0   1   0   0   0   0   0   0   0\n",
      "   0   9   1   0   0   0   0   3   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0  19   0   1   0   1   2   1   0   1   0  13   0   0   0   1   0\n",
      "   0   0   0   6   6   2   0   3   0   0   0   0   3   0   0   0   0   0\n",
      "   0   0   0   0  25   2   0   2   0   0   0   0   3   0   0   1   0   0\n",
      "   0   0   0   0   1  47   0   0   0   0   0   0   1   0   0   0   0   0\n",
      "   0   0   0   0   0   0  12   5   0   0   2   1   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0 142   0   0   1   0   1   2   0   0   0   0\n",
      "   0   0   1   0   0   1   0   4  17   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   4   0   7   0   0   0   1   0   0   0   0\n",
      "   0   0   0   0   1   0   1   5   0   0  14   1   2   0   0   0   0   0\n",
      "   0   0   1   0   1   2   0   2   0   0   0   5   4   0   0   0   0   0\n",
      "   0   0   3   0   1   1   0   1   1   1   2   1 100   0   0   2   0   2\n",
      "   0   0   0   0   0   1   0   6   0   0   1   0   1  55   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0  15   0   0   0\n",
      "   0   0   0   2   1   2   0  10   1   0   0   0   4   0   0   2   0   0\n",
      "   0   0   0   3   0   0   0   1   0   0   0   0   0   0   0   0  17   0\n",
      "   0   0   0   0   0   0   0   1   0   0   1   0   4   0   0   0   0  17]\n",
      "LR Accuracy:  0.7796101949025487\n",
      "LR F1:  0.7168749206808379\n",
      "For name:  k_sato\n",
      "total sample size before apply threshold:  65\n",
      "Counter({'0000-0001-6110-9171': 35, '0000-0001-5768-2442': 9, '0000-0001-6706-2175': 7, '0000-0002-3998-7012': 7, '0000-0001-9078-2541': 6, '0000-0003-4045-7796': 1})\n",
      "['0000-0001-6110-9171']\n",
      "Total sample size after apply threshold:  35\n",
      "For name:  f_ahmed\n",
      "total sample size before apply threshold:  25\n",
      "Counter({'0000-0002-7464-7726': 9, '0000-0003-1294-5274': 9, '0000-0002-9839-7039': 4, '0000-0002-8444-2038': 1, '0000-0003-4100-1571': 1, '0000-0001-5256-4666': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  y_watanabe\n",
      "total sample size before apply threshold:  98\n",
      "Counter({'0000-0002-7139-4903': 80, '0000-0001-7740-1361': 11, '0000-0002-7013-3613': 2, '0000-0002-7120-3097': 2, '0000-0001-9999-0486': 1, '0000-0002-6281-9295': 1, '0000-0002-9668-3592': 1})\n",
      "['0000-0002-7139-4903', '0000-0001-7740-1361']\n",
      "Total sample size after apply threshold:  91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(91, 194)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(91, 194)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.98        80\n",
      "          1       0.90      0.82      0.86        11\n",
      "\n",
      "avg / total       0.97      0.97      0.97        91\n",
      "\n",
      "[79  1  2  9]\n",
      "svc Accuracy:  0.967032967032967\n",
      "svc F1:  0.9192546583850931\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98        80\n",
      "          1       1.00      0.73      0.84        11\n",
      "\n",
      "avg / total       0.97      0.97      0.96        91\n",
      "\n",
      "[80  0  3  8]\n",
      "LR Accuracy:  0.967032967032967\n",
      "LR F1:  0.9118501775912173\n",
      "For name:  k_singh\n",
      "total sample size before apply threshold:  25\n",
      "Counter({'0000-0003-3152-1119': 10, '0000-0002-9375-3233': 6, '0000-0001-8352-5897': 3, '0000-0002-4126-9618': 3, '0000-0001-7187-4782': 1, '0000-0002-5199-9331': 1, '0000-0002-2913-6644': 1})\n",
      "['0000-0003-3152-1119']\n",
      "Total sample size after apply threshold:  10\n",
      "For name:  j_mcevoy\n",
      "total sample size before apply threshold:  65\n",
      "Counter({'0000-0001-6530-5479': 53, '0000-0003-0382-1288': 8, '0000-0001-6843-5551': 2, '0000-0001-9591-1824': 2})\n",
      "['0000-0001-6530-5479']\n",
      "Total sample size after apply threshold:  53\n",
      "For name:  g_singh\n",
      "total sample size before apply threshold:  36\n",
      "Counter({'0000-0003-1525-3898': 13, '0000-0001-9700-3344': 11, '0000-0002-1641-5070': 6, '0000-0002-4354-269X': 2, '0000-0002-2113-4230': 1, '0000-0002-9579-6093': 1, '0000-0001-5496-6992': 1, '0000-0002-2670-9814': 1})\n",
      "['0000-0001-9700-3344', '0000-0003-1525-3898']\n",
      "Total sample size after apply threshold:  24\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(24, 60)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(24, 60)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        11\n",
      "          1       1.00      1.00      1.00        13\n",
      "\n",
      "avg / total       1.00      1.00      1.00        24\n",
      "\n",
      "[11  0  0 13]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        11\n",
      "          1       1.00      1.00      1.00        13\n",
      "\n",
      "avg / total       1.00      1.00      1.00        24\n",
      "\n",
      "[11  0  0 13]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  e_ford\n",
      "total sample size before apply threshold:  54\n",
      "Counter({'0000-0002-7885-0019': 34, '0000-0001-5613-8509': 14, '0000-0001-7358-798X': 4, '0000-0003-0952-3660': 2})\n",
      "['0000-0002-7885-0019', '0000-0001-5613-8509']\n",
      "Total sample size after apply threshold:  48\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(48, 81)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(48, 81)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      1.00      0.94        34\n",
      "          1       1.00      0.71      0.83        14\n",
      "\n",
      "avg / total       0.93      0.92      0.91        48\n",
      "\n",
      "[34  0  4 10]\n",
      "svc Accuracy:  0.9166666666666666\n",
      "svc F1:  0.8888888888888888\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      1.00      0.94        34\n",
      "          1       1.00      0.71      0.83        14\n",
      "\n",
      "avg / total       0.93      0.92      0.91        48\n",
      "\n",
      "[34  0  4 10]\n",
      "LR Accuracy:  0.9166666666666666\n",
      "LR F1:  0.8888888888888888\n",
      "For name:  s_chou\n",
      "total sample size before apply threshold:  39\n",
      "Counter({'0000-0001-9237-4517': 16, '0000-0003-1155-6082': 8, '0000-0003-0787-0044': 6, '0000-0001-8081-1679': 4, '0000-0001-5512-9977': 2, '0000-0002-4121-019X': 2, '0000-0001-8163-7430': 1})\n",
      "['0000-0001-9237-4517']\n",
      "Total sample size after apply threshold:  16\n",
      "For name:  s_hughes\n",
      "total sample size before apply threshold:  106\n",
      "Counter({'0000-0001-8227-9225': 74, '0000-0002-9409-9405': 12, '0000-0001-8360-929X': 6, '0000-0002-2264-8479': 5, '0000-0002-9778-140X': 3, '0000-0001-6340-2646': 3, '0000-0001-7689-4272': 1, '0000-0002-8187-4871': 1, '0000-0003-4542-1821': 1})\n",
      "['0000-0002-9409-9405', '0000-0001-8227-9225']\n",
      "Total sample size after apply threshold:  86\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(86, 281)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(86, 281)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.67      0.80        12\n",
      "          1       0.95      1.00      0.97        74\n",
      "\n",
      "avg / total       0.96      0.95      0.95        86\n",
      "\n",
      "[ 8  4  0 74]\n",
      "svc Accuracy:  0.9534883720930233\n",
      "svc F1:  0.8868421052631579\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.50      0.67        12\n",
      "          1       0.93      1.00      0.96        74\n",
      "\n",
      "avg / total       0.94      0.93      0.92        86\n",
      "\n",
      "[ 6  6  0 74]\n",
      "LR Accuracy:  0.9302325581395349\n",
      "LR F1:  0.8138528138528138\n",
      "For name:  m_thomas\n",
      "total sample size before apply threshold:  225\n",
      "Counter({'0000-0002-2452-981X': 69, '0000-0001-5939-1155': 52, '0000-0001-6394-8710': 24, '0000-0003-4374-1039': 18, '0000-0002-4951-9925': 14, '0000-0003-2360-255X': 13, '0000-0002-3042-0669': 10, '0000-0002-5553-5825': 8, '0000-0002-5089-5610': 4, '0000-0003-0354-8779': 4, '0000-0003-2982-0291': 3, '0000-0001-8093-4919': 3, '0000-0002-7569-6896': 1, '0000-0003-2288-1104': 1, '0000-0001-6291-6426': 1})\n",
      "['0000-0001-6394-8710', '0000-0002-3042-0669', '0000-0002-4951-9925', '0000-0003-2360-255X', '0000-0001-5939-1155', '0000-0002-2452-981X', '0000-0003-4374-1039']\n",
      "Total sample size after apply threshold:  200\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(200, 837)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(200, 837)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.67      0.80        24\n",
      "          1       1.00      0.50      0.67        10\n",
      "          2       1.00      0.64      0.78        14\n",
      "          3       0.83      0.77      0.80        13\n",
      "          4       0.58      0.96      0.72        52\n",
      "          5       1.00      0.80      0.89        69\n",
      "          6       0.94      0.89      0.91        18\n",
      "\n",
      "avg / total       0.88      0.81      0.81       200\n",
      "\n",
      "[16  0  0  0  8  0  0  0  5  0  0  5  0  0  0  0  9  0  5  0  0  0  0  0\n",
      " 10  3  0  0  0  0  0  1 50  0  1  0  0  0  1 13 55  0  0  0  0  0  2  0\n",
      " 16]\n",
      "svc Accuracy:  0.805\n",
      "svc F1:  0.7964707902796463\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.75      0.86        24\n",
      "          1       1.00      0.50      0.67        10\n",
      "          2       1.00      0.71      0.83        14\n",
      "          3       0.91      0.77      0.83        13\n",
      "          4       0.70      0.87      0.78        52\n",
      "          5       0.84      0.93      0.88        69\n",
      "          6       1.00      0.89      0.94        18\n",
      "\n",
      "avg / total       0.86      0.84      0.84       200\n",
      "\n",
      "[18  0  0  0  5  1  0  0  5  0  0  3  2  0  0  0 10  0  2  2  0  0  0  0\n",
      " 10  2  1  0  0  0  0  1 45  6  0  0  0  0  0  5 64  0  0  0  0  0  2  0\n",
      " 16]\n",
      "LR Accuracy:  0.84\n",
      "LR F1:  0.8271819072456568\n",
      "For name:  j_liang\n",
      "total sample size before apply threshold:  105\n",
      "Counter({'0000-0002-0264-7735': 84, '0000-0002-4532-0118': 8, '0000-0001-6055-0918': 4, '0000-0001-9439-9320': 3, '0000-0002-2773-6427': 2, '0000-0001-8252-5502': 2, '0000-0003-3994-5709': 1, '0000-0002-8210-0210': 1})\n",
      "['0000-0002-0264-7735']\n",
      "Total sample size after apply threshold:  84\n",
      "For name:  t_wu\n",
      "total sample size before apply threshold:  168\n",
      "Counter({'0000-0003-0845-4827': 68, '0000-0001-8235-5929': 34, '0000-0002-0244-3046': 11, '0000-0003-1845-1769': 11, '0000-0002-2663-2001': 11, '0000-0002-9859-4534': 10, '0000-0002-0060-4408': 7, '0000-0001-5155-6189': 7, '0000-0002-6519-469X': 3, '0000-0002-3560-8898': 2, '0000-0001-6444-598X': 2, '0000-0001-6469-9613': 1, '0000-0002-8775-597X': 1})\n",
      "['0000-0002-0244-3046', '0000-0003-0845-4827', '0000-0003-1845-1769', '0000-0001-8235-5929', '0000-0002-2663-2001', '0000-0002-9859-4534']\n",
      "Total sample size after apply threshold:  145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(145, 363)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(145, 363)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.73      0.73        11\n",
      "          1       0.83      0.94      0.88        68\n",
      "          2       1.00      0.82      0.90        11\n",
      "          3       1.00      0.91      0.95        34\n",
      "          4       0.67      0.55      0.60        11\n",
      "          5       1.00      0.80      0.89        10\n",
      "\n",
      "avg / total       0.87      0.87      0.87       145\n",
      "\n",
      "[ 8  3  0  0  0  0  2 64  0  0  2  0  0  1  9  0  1  0  1  2  0 31  0  0\n",
      "  0  5  0  0  6  0  0  2  0  0  0  8]\n",
      "svc Accuracy:  0.8689655172413793\n",
      "svc F1:  0.8254610651162376\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.82      0.86        11\n",
      "          1       0.83      0.99      0.90        68\n",
      "          2       1.00      0.73      0.84        11\n",
      "          3       1.00      0.94      0.97        34\n",
      "          4       1.00      0.45      0.62        11\n",
      "          5       0.89      0.80      0.84        10\n",
      "\n",
      "avg / total       0.90      0.89      0.88       145\n",
      "\n",
      "[ 9  2  0  0  0  0  1 67  0  0  0  0  0  3  8  0  0  0  0  2  0 32  0  0\n",
      "  0  5  0  0  5  1  0  2  0  0  0  8]\n",
      "LR Accuracy:  0.8896551724137931\n",
      "LR F1:  0.8392298687026698\n",
      "For name:  b_ahmed\n",
      "total sample size before apply threshold:  23\n",
      "Counter({'0000-0001-6021-1414': 10, '0000-0001-9110-4136': 6, '0000-0002-6707-822X': 4, '0000-0002-4840-6945': 3})\n",
      "['0000-0001-6021-1414']\n",
      "Total sample size after apply threshold:  10\n",
      "For name:  m_takahashi\n",
      "total sample size before apply threshold:  54\n",
      "Counter({'0000-0002-3799-2256': 41, '0000-0001-7273-1660': 11, '0000-0003-3233-6783': 1, '0000-0001-6141-0554': 1})\n",
      "['0000-0002-3799-2256', '0000-0001-7273-1660']\n",
      "Total sample size after apply threshold:  52\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(52, 154)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(52, 154)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98        41\n",
      "          1       0.91      0.91      0.91        11\n",
      "\n",
      "avg / total       0.96      0.96      0.96        52\n",
      "\n",
      "[40  1  1 10]\n",
      "svc Accuracy:  0.9615384615384616\n",
      "svc F1:  0.9423503325942351\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.98        41\n",
      "          1       1.00      0.82      0.90        11\n",
      "\n",
      "avg / total       0.96      0.96      0.96        52\n",
      "\n",
      "[41  0  2  9]\n",
      "LR Accuracy:  0.9615384615384616\n",
      "LR F1:  0.9380952380952381\n",
      "For name:  i_lee\n",
      "total sample size before apply threshold:  73\n",
      "Counter({'0000-0002-2588-1444': 41, '0000-0002-0098-392X': 16, '0000-0003-0520-4435': 7, '0000-0001-6057-7176': 5, '0000-0003-1923-0917': 1, '0000-0003-3760-4257': 1, '0000-0001-8167-7168': 1, '0000-0002-9103-0955': 1})\n",
      "['0000-0002-0098-392X', '0000-0002-2588-1444']\n",
      "Total sample size after apply threshold:  57\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(57, 82)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(57, 82)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.69      0.71        16\n",
      "          1       0.88      0.90      0.89        41\n",
      "\n",
      "avg / total       0.84      0.84      0.84        57\n",
      "\n",
      "[11  5  4 37]\n",
      "svc Accuracy:  0.8421052631578947\n",
      "svc F1:  0.8006218422075398\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.69      0.76        16\n",
      "          1       0.89      0.95      0.92        41\n",
      "\n",
      "avg / total       0.88      0.88      0.87        57\n",
      "\n",
      "[11  5  2 39]\n",
      "LR Accuracy:  0.8771929824561403\n",
      "LR F1:  0.8381338742393509\n",
      "For name:  a_figueiredo\n",
      "total sample size before apply threshold:  150\n",
      "Counter({'0000-0002-9105-9619': 79, '0000-0002-3239-3190': 19, '0000-0001-6956-0514': 16, '0000-0001-8156-7700': 14, '0000-0001-8386-8216': 9, '0000-0001-7039-5341': 6, '0000-0003-2329-2854': 3, '0000-0003-0487-8956': 3, '0000-0002-8555-8649': 1})\n",
      "['0000-0001-6956-0514', '0000-0001-8156-7700', '0000-0002-9105-9619', '0000-0002-3239-3190']\n",
      "Total sample size after apply threshold:  128\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(128, 232)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(128, 232)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.94      0.97        16\n",
      "          1       1.00      0.71      0.83        14\n",
      "          2       0.93      1.00      0.96        79\n",
      "          3       1.00      0.95      0.97        19\n",
      "\n",
      "avg / total       0.96      0.95      0.95       128\n",
      "\n",
      "[15  0  1  0  0 10  4  0  0  0 79  0  0  0  1 18]\n",
      "svc Accuracy:  0.953125\n",
      "svc F1:  0.9343657189841297\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.94      0.97        16\n",
      "          1       1.00      0.86      0.92        14\n",
      "          2       0.95      1.00      0.98        79\n",
      "          3       1.00      0.95      0.97        19\n",
      "\n",
      "avg / total       0.97      0.97      0.97       128\n",
      "\n",
      "[15  0  1  0  0 12  2  0  0  0 79  0  0  0  1 18]\n",
      "LR Accuracy:  0.96875\n",
      "LR F1:  0.9597751183772689\n",
      "For name:  s_clark\n",
      "total sample size before apply threshold:  39\n",
      "Counter({'0000-0001-5907-9671': 12, '0000-0002-7488-3438': 9, '0000-0001-7328-0726': 8, '0000-0002-6183-491X': 4, '0000-0001-8394-8355': 3, '0000-0003-4090-6002': 1, '0000-0002-2072-7499': 1, '0000-0002-7633-3376': 1})\n",
      "['0000-0001-5907-9671']\n",
      "Total sample size after apply threshold:  12\n",
      "For name:  a_schmid\n",
      "total sample size before apply threshold:  61\n",
      "Counter({'0000-0002-5196-151X': 28, '0000-0001-7759-0211': 19, '0000-0001-6483-8759': 10, '0000-0002-0141-0971': 4})\n",
      "['0000-0001-7759-0211', '0000-0001-6483-8759', '0000-0002-5196-151X']\n",
      "Total sample size after apply threshold:  57\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(57, 195)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(57, 195)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.97        19\n",
      "          1       1.00      1.00      1.00        10\n",
      "          2       1.00      0.96      0.98        28\n",
      "\n",
      "avg / total       0.98      0.98      0.98        57\n",
      "\n",
      "[19  0  0  0 10  0  1  0 27]\n",
      "svc Accuracy:  0.9824561403508771\n",
      "svc F1:  0.9853923853923855\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        19\n",
      "          1       1.00      1.00      1.00        10\n",
      "          2       1.00      1.00      1.00        28\n",
      "\n",
      "avg / total       1.00      1.00      1.00        57\n",
      "\n",
      "[19  0  0  0 10  0  0  0 28]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  k_cheung\n",
      "total sample size before apply threshold:  16\n",
      "Counter({'0000-0002-6759-4961': 9, '0000-0002-8348-1561': 4, '0000-0003-4107-7840': 2, '0000-0001-7648-4556': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  s_ma\n",
      "total sample size before apply threshold:  136\n",
      "Counter({'0000-0002-1897-7069': 69, '0000-0002-2029-7943': 42, '0000-0002-1810-8357': 9, '0000-0002-0232-8590': 6, '0000-0001-8581-2216': 3, '0000-0002-2704-3540': 2, '0000-0001-8087-0249': 1, '0000-0001-6361-9706': 1, '0000-0002-7995-2041': 1, '0000-0003-4846-9513': 1, '0000-0002-8992-1177': 1})\n",
      "['0000-0002-2029-7943', '0000-0002-1897-7069']\n",
      "Total sample size after apply threshold:  111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(111, 212)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(111, 212)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.93      0.95        42\n",
      "          1       0.96      0.99      0.97        69\n",
      "\n",
      "avg / total       0.96      0.96      0.96       111\n",
      "\n",
      "[39  3  1 68]\n",
      "svc Accuracy:  0.963963963963964\n",
      "svc F1:  0.9613240418118467\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.93      0.96        42\n",
      "          1       0.96      1.00      0.98        69\n",
      "\n",
      "avg / total       0.97      0.97      0.97       111\n",
      "\n",
      "[39  3  0 69]\n",
      "LR Accuracy:  0.972972972972973\n",
      "LR F1:  0.9708431836091411\n",
      "For name:  m_marino\n",
      "total sample size before apply threshold:  69\n",
      "Counter({'0000-0001-9155-6378': 14, '0000-0002-0045-0234': 11, '0000-0002-7470-7493': 11, '0000-0001-7443-3472': 9, '0000-0003-2031-1191': 8, '0000-0003-1226-6036': 7, '0000-0002-4323-3061': 5, '0000-0002-8672-0310': 3, '0000-0003-4651-6128': 1})\n",
      "['0000-0001-9155-6378', '0000-0002-0045-0234', '0000-0002-7470-7493']\n",
      "Total sample size after apply threshold:  36\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(36, 158)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(36, 158)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        14\n",
      "          1       0.73      1.00      0.85        11\n",
      "          2       1.00      0.64      0.78        11\n",
      "\n",
      "avg / total       0.92      0.89      0.89        36\n",
      "\n",
      "[14  0  0  0 11  0  0  4  7]\n",
      "svc Accuracy:  0.8888888888888888\n",
      "svc F1:  0.8746438746438746\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        14\n",
      "          1       0.92      1.00      0.96        11\n",
      "          2       1.00      0.91      0.95        11\n",
      "\n",
      "avg / total       0.97      0.97      0.97        36\n",
      "\n",
      "[14  0  0  0 11  0  0  1 10]\n",
      "LR Accuracy:  0.9722222222222222\n",
      "LR F1:  0.9696342305037957\n",
      "For name:  a_kirby\n",
      "total sample size before apply threshold:  64\n",
      "Counter({'0000-0002-2440-9316': 26, '0000-0001-5663-2961': 25, '0000-0002-6928-668X': 9, '0000-0003-0395-6684': 4})\n",
      "['0000-0002-2440-9316', '0000-0001-5663-2961']\n",
      "Total sample size after apply threshold:  51\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(51, 127)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(51, 127)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98        26\n",
      "          1       1.00      0.96      0.98        25\n",
      "\n",
      "avg / total       0.98      0.98      0.98        51\n",
      "\n",
      "[26  0  1 24]\n",
      "svc Accuracy:  0.9803921568627451\n",
      "svc F1:  0.980361956103196\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.96      0.98        26\n",
      "          1       0.96      1.00      0.98        25\n",
      "\n",
      "avg / total       0.98      0.98      0.98        51\n",
      "\n",
      "[25  1  0 25]\n",
      "LR Accuracy:  0.9803921568627451\n",
      "LR F1:  0.9803921568627451\n",
      "For name:  d_roberts\n",
      "total sample size before apply threshold:  105\n",
      "Counter({'0000-0002-2481-2981': 47, '0000-0001-6111-6291': 20, '0000-0003-0261-691X': 14, '0000-0002-0668-2001': 12, '0000-0001-7175-7754': 10, '0000-0003-0264-921X': 1, '0000-0002-0780-7056': 1})\n",
      "['0000-0001-7175-7754', '0000-0001-6111-6291', '0000-0003-0261-691X', '0000-0002-2481-2981', '0000-0002-0668-2001']\n",
      "Total sample size after apply threshold:  103\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(103, 305)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(103, 305)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.90      0.95        10\n",
      "          1       1.00      0.90      0.95        20\n",
      "          2       1.00      0.86      0.92        14\n",
      "          3       0.85      1.00      0.92        47\n",
      "          4       1.00      0.75      0.86        12\n",
      "\n",
      "avg / total       0.93      0.92      0.92       103\n",
      "\n",
      "[ 9  0  0  1  0  0 18  0  2  0  0  0 12  2  0  0  0  0 47  0  0  0  0  3\n",
      "  9]\n",
      "svc Accuracy:  0.9223300970873787\n",
      "svc F1:  0.9193050499552047\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.90      0.95        10\n",
      "          1       1.00      0.90      0.95        20\n",
      "          2       1.00      0.86      0.92        14\n",
      "          3       0.84      1.00      0.91        47\n",
      "          4       1.00      0.67      0.80        12\n",
      "\n",
      "avg / total       0.93      0.91      0.91       103\n",
      "\n",
      "[ 9  0  0  1  0  0 18  0  2  0  0  0 12  2  0  0  0  0 47  0  0  0  0  4\n",
      "  8]\n",
      "LR Accuracy:  0.912621359223301\n",
      "LR F1:  0.9060870248810975\n",
      "For name:  b_thompson\n",
      "total sample size before apply threshold:  83\n",
      "Counter({'0000-0002-5885-0652': 65, '0000-0002-5358-0796': 8, '0000-0002-2302-0886': 7, '0000-0002-3845-824X': 3})\n",
      "['0000-0002-5885-0652']\n",
      "Total sample size after apply threshold:  65\n",
      "For name:  j_blanco\n",
      "total sample size before apply threshold:  362\n",
      "Counter({'0000-0003-0264-4136': 102, '0000-0002-2225-0217': 91, '0000-0001-8142-0450': 74, '0000-0003-3765-0640': 41, '0000-0003-0647-3856': 40, '0000-0002-5071-4760': 7, '0000-0002-6524-4335': 5, '0000-0002-7351-5342': 1, '0000-0003-0191-2063': 1})\n",
      "['0000-0003-0264-4136', '0000-0003-0647-3856', '0000-0002-2225-0217', '0000-0003-3765-0640', '0000-0001-8142-0450']\n",
      "Total sample size after apply threshold:  348\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(348, 922)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(348, 922)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.95      0.95       102\n",
      "          1       0.93      0.95      0.94        40\n",
      "          2       1.00      0.92      0.96        91\n",
      "          3       0.85      0.85      0.85        41\n",
      "          4       0.90      0.97      0.94        74\n",
      "\n",
      "avg / total       0.94      0.94      0.94       348\n",
      "\n",
      "[97  1  0  3  1  1 38  0  1  0  2  0 84  1  4  1  2  0 35  3  1  0  0  1\n",
      " 72]\n",
      "svc Accuracy:  0.9367816091954023\n",
      "svc F1:  0.9275950937490871\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.94      0.96       102\n",
      "          1       0.95      0.95      0.95        40\n",
      "          2       0.99      0.93      0.96        91\n",
      "          3       1.00      0.90      0.95        41\n",
      "          4       0.84      0.97      0.90        74\n",
      "\n",
      "avg / total       0.95      0.94      0.94       348\n",
      "\n",
      "[96  0  0  0  6  0 38  1  0  1  1  0 85  0  5  0  2  0 37  2  2  0  0  0\n",
      " 72]\n",
      "LR Accuracy:  0.9425287356321839\n",
      "LR F1:  0.9428787613432188\n",
      "For name:  x_cai\n",
      "total sample size before apply threshold:  79\n",
      "Counter({'0000-0001-8933-7133': 32, '0000-0002-5654-7414': 31, '0000-0003-4907-154X': 9, '0000-0003-3706-4414': 4, '0000-0003-0222-553X': 2, '0000-0001-5238-6193': 1})\n",
      "['0000-0002-5654-7414', '0000-0001-8933-7133']\n",
      "Total sample size after apply threshold:  63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(63, 93)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(63, 93)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.97      0.98        31\n",
      "          1       0.97      1.00      0.98        32\n",
      "\n",
      "avg / total       0.98      0.98      0.98        63\n",
      "\n",
      "[30  1  0 32]\n",
      "svc Accuracy:  0.9841269841269841\n",
      "svc F1:  0.9841109709962169\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        31\n",
      "          1       1.00      1.00      1.00        32\n",
      "\n",
      "avg / total       1.00      1.00      1.00        63\n",
      "\n",
      "[31  0  0 32]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  r_menezes\n",
      "total sample size before apply threshold:  29\n",
      "Counter({'0000-0003-0552-8480': 15, '0000-0002-6612-3543': 6, '0000-0003-3109-9683': 5, '0000-0003-4316-2168': 2, '0000-0002-4842-641X': 1})\n",
      "['0000-0003-0552-8480']\n",
      "Total sample size after apply threshold:  15\n",
      "For name:  s_tsang\n",
      "total sample size before apply threshold:  20\n",
      "Counter({'0000-0003-0788-4905': 6, '0000-0002-9862-8503': 5, '0000-0001-6099-6696': 5, '0000-0002-2232-9814': 4})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  c_king\n",
      "total sample size before apply threshold:  218\n",
      "Counter({'0000-0001-8349-9270': 145, '0000-0002-6078-2601': 65, '0000-0002-0892-1301': 4, '0000-0003-1157-5734': 2, '0000-0002-6641-237X': 2})\n",
      "['0000-0002-6078-2601', '0000-0001-8349-9270']\n",
      "Total sample size after apply threshold:  210\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(210, 774)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(210, 774)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.89      0.93        65\n",
      "          1       0.95      0.99      0.97       145\n",
      "\n",
      "avg / total       0.96      0.96      0.96       210\n",
      "\n",
      "[ 58   7   2 143]\n",
      "svc Accuracy:  0.9571428571428572\n",
      "svc F1:  0.9487457627118645\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.91      0.95        65\n",
      "          1       0.96      1.00      0.98       145\n",
      "\n",
      "avg / total       0.97      0.97      0.97       210\n",
      "\n",
      "[ 59   6   0 145]\n",
      "LR Accuracy:  0.9714285714285714\n",
      "LR F1:  0.9656713164777682\n",
      "For name:  h_kobayashi\n",
      "total sample size before apply threshold:  28\n",
      "Counter({'0000-0003-4965-0883': 16, '0000-0002-8460-587X': 9, '0000-0001-9091-3521': 2, '0000-0002-8956-0375': 1})\n",
      "['0000-0003-4965-0883']\n",
      "Total sample size after apply threshold:  16\n",
      "For name:  k_yang\n",
      "total sample size before apply threshold:  70\n",
      "Counter({'0000-0002-9128-9166': 19, '0000-0002-3162-7709': 14, '0000-0001-5968-6738': 12, '0000-0003-0047-2238': 10, '0000-0002-9691-0636': 6, '0000-0002-0587-3201': 5, '0000-0002-0809-2371': 1, '0000-0002-3398-9332': 1, '0000-0001-7963-4337': 1, '0000-0002-8224-5161': 1})\n",
      "['0000-0002-3162-7709', '0000-0002-9128-9166', '0000-0001-5968-6738', '0000-0003-0047-2238']\n",
      "Total sample size after apply threshold:  55\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(55, 117)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(55, 117)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.86      0.83        14\n",
      "          1       0.86      0.95      0.90        19\n",
      "          2       1.00      0.92      0.96        12\n",
      "          3       1.00      0.80      0.89        10\n",
      "\n",
      "avg / total       0.90      0.89      0.89        55\n",
      "\n",
      "[12  2  0  0  1 18  0  0  0  1 11  0  2  0  0  8]\n",
      "svc Accuracy:  0.8909090909090909\n",
      "svc F1:  0.8932492087289688\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.86      0.83        14\n",
      "          1       0.86      0.95      0.90        19\n",
      "          2       1.00      0.92      0.96        12\n",
      "          3       1.00      0.80      0.89        10\n",
      "\n",
      "avg / total       0.90      0.89      0.89        55\n",
      "\n",
      "[12  2  0  0  1 18  0  0  0  1 11  0  2  0  0  8]\n",
      "LR Accuracy:  0.8909090909090909\n",
      "LR F1:  0.8932492087289688\n",
      "For name:  b_zheng\n",
      "total sample size before apply threshold:  90\n",
      "Counter({'0000-0002-7682-6648': 82, '0000-0002-3272-843X': 5, '0000-0003-1551-0970': 2, '0000-0002-2044-2848': 1})\n",
      "['0000-0002-7682-6648']\n",
      "Total sample size after apply threshold:  82\n",
      "For name:  f_xu\n",
      "total sample size before apply threshold:  94\n",
      "Counter({'0000-0003-4351-0222': 29, '0000-0002-8465-5834': 22, '0000-0001-5239-4572': 19, '0000-0001-7958-3787': 12, '0000-0002-0245-057X': 5, '0000-0002-8166-0275': 4, '0000-0003-1600-6346': 2, '0000-0002-2598-2528': 1})\n",
      "['0000-0001-7958-3787', '0000-0001-5239-4572', '0000-0003-4351-0222', '0000-0002-8465-5834']\n",
      "Total sample size after apply threshold:  82\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(82, 133)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(82, 133)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.92      0.92        12\n",
      "          1       0.81      0.89      0.85        19\n",
      "          2       0.89      0.83      0.86        29\n",
      "          3       1.00      1.00      1.00        22\n",
      "\n",
      "avg / total       0.90      0.90      0.90        82\n",
      "\n",
      "[11  0  1  0  0 17  2  0  1  4 24  0  0  0  0 22]\n",
      "svc Accuracy:  0.9024390243902439\n",
      "svc F1:  0.905952380952381\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.92      0.92        12\n",
      "          1       0.89      0.84      0.86        19\n",
      "          2       0.87      0.90      0.88        29\n",
      "          3       1.00      1.00      1.00        22\n",
      "\n",
      "avg / total       0.91      0.91      0.91        82\n",
      "\n",
      "[11  0  1  0  0 16  3  0  1  2 26  0  0  0  0 22]\n",
      "LR Accuracy:  0.9146341463414634\n",
      "LR F1:  0.9157218659337303\n",
      "For name:  r_day\n",
      "total sample size before apply threshold:  202\n",
      "Counter({'0000-0002-6045-6937': 149, '0000-0003-3442-2298': 39, '0000-0003-1766-4068': 6, '0000-0001-5913-2292': 5, '0000-0002-6155-5910': 2, '0000-0003-1467-3196': 1})\n",
      "['0000-0002-6045-6937', '0000-0003-3442-2298']\n",
      "Total sample size after apply threshold:  188\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(188, 374)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(188, 374)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.99      0.95       149\n",
      "          1       0.96      0.62      0.75        39\n",
      "\n",
      "avg / total       0.92      0.91      0.91       188\n",
      "\n",
      "[148   1  15  24]\n",
      "svc Accuracy:  0.9148936170212766\n",
      "svc F1:  0.8493589743589745\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      1.00      0.95       149\n",
      "          1       1.00      0.59      0.74        39\n",
      "\n",
      "avg / total       0.92      0.91      0.91       188\n",
      "\n",
      "[149   0  16  23]\n",
      "LR Accuracy:  0.9148936170212766\n",
      "LR F1:  0.8454900349291145\n",
      "For name:  j_young\n",
      "total sample size before apply threshold:  267\n",
      "Counter({'0000-0002-1514-1522': 124, '0000-0003-4182-341X': 40, '0000-0003-3849-3392': 30, '0000-0002-1294-942X': 23, '0000-0002-2711-9701': 17, '0000-0001-7219-7824': 16, '0000-0003-4886-9517': 10, '0000-0003-1745-2401': 4, '0000-0001-9791-2513': 2, '0000-0001-6583-7643': 1})\n",
      "['0000-0003-3849-3392', '0000-0002-1294-942X', '0000-0001-7219-7824', '0000-0003-4182-341X', '0000-0002-2711-9701', '0000-0003-4886-9517', '0000-0002-1514-1522']\n",
      "Total sample size after apply threshold:  260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(260, 977)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(260, 977)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.67      0.80        30\n",
      "          1       0.33      1.00      0.49        23\n",
      "          2       1.00      0.81      0.90        16\n",
      "          3       1.00      0.55      0.71        40\n",
      "          4       1.00      0.76      0.87        17\n",
      "          5       1.00      0.80      0.89        10\n",
      "          6       0.99      0.91      0.95       124\n",
      "\n",
      "avg / total       0.94      0.82      0.84       260\n",
      "\n",
      "[ 20  10   0   0   0   0   0   0  23   0   0   0   0   0   0   3  13   0\n",
      "   0   0   0   0  17   0  22   0   0   1   0   4   0   0  13   0   0   0\n",
      "   2   0   0   0   8   0   0  11   0   0   0   0 113]\n",
      "svc Accuracy:  0.8153846153846154\n",
      "svc F1:  0.8008554552707253\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.83      0.89        30\n",
      "          1       0.58      0.91      0.71        23\n",
      "          2       1.00      0.88      0.93        16\n",
      "          3       0.97      0.78      0.86        40\n",
      "          4       1.00      0.82      0.90        17\n",
      "          5       1.00      0.80      0.89        10\n",
      "          6       0.94      0.98      0.96       124\n",
      "\n",
      "avg / total       0.92      0.90      0.91       260\n",
      "\n",
      "[ 25   3   0   0   0   0   2   0  21   0   0   0   0   2   1   1  14   0\n",
      "   0   0   0   0   6   0  31   0   0   3   0   2   0   0  14   0   1   0\n",
      "   1   0   1   0   8   0   0   2   0   0   0   0 122]\n",
      "LR Accuracy:  0.9038461538461539\n",
      "LR F1:  0.8788443729545133\n",
      "For name:  c_black\n",
      "total sample size before apply threshold:  41\n",
      "Counter({'0000-0002-0424-4593': 29, '0000-0003-2022-0337': 4, '0000-0003-2934-108X': 4, '0000-0002-1541-106X': 2, '0000-0001-8382-298X': 2})\n",
      "['0000-0002-0424-4593']\n",
      "Total sample size after apply threshold:  29\n",
      "For name:  s_joseph\n",
      "total sample size before apply threshold:  20\n",
      "Counter({'0000-0003-4596-1270': 13, '0000-0002-4741-7183': 5, '0000-0003-1023-0718': 1, '0000-0002-9163-3027': 1})\n",
      "['0000-0003-4596-1270']\n",
      "Total sample size after apply threshold:  13\n",
      "For name:  z_fan\n",
      "total sample size before apply threshold:  38\n",
      "Counter({'0000-0002-9312-2271': 13, '0000-0001-5385-9626': 13, '0000-0001-9492-5722': 6, '0000-0003-4623-6783': 4, '0000-0002-7818-153X': 1, '0000-0002-2145-2458': 1})\n",
      "['0000-0002-9312-2271', '0000-0001-5385-9626']\n",
      "Total sample size after apply threshold:  26\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(26, 112)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(26, 112)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        13\n",
      "          1       1.00      1.00      1.00        13\n",
      "\n",
      "avg / total       1.00      1.00      1.00        26\n",
      "\n",
      "[13  0  0 13]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        13\n",
      "          1       1.00      1.00      1.00        13\n",
      "\n",
      "avg / total       1.00      1.00      1.00        26\n",
      "\n",
      "[13  0  0 13]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  j_matos\n",
      "total sample size before apply threshold:  25\n",
      "Counter({'0000-0002-3754-3709': 12, '0000-0002-0505-8282': 5, '0000-0001-9917-6126': 4, '0000-0003-1335-0635': 3, '0000-0003-0570-7913': 1})\n",
      "['0000-0002-3754-3709']\n",
      "Total sample size after apply threshold:  12\n",
      "For name:  l_santos\n",
      "total sample size before apply threshold:  172\n",
      "Counter({'0000-0003-3040-0358': 55, '0000-0002-2712-0622': 32, '0000-0002-7013-8852': 15, '0000-0002-1915-6780': 13, '0000-0001-5166-530X': 11, '0000-0003-0986-9880': 10, '0000-0002-0694-733X': 9, '0000-0001-8366-1557': 5, '0000-0001-8906-9976': 5, '0000-0002-4453-5766': 4, '0000-0001-7551-5605': 3, '0000-0003-0458-427X': 3, '0000-0001-5915-1186': 2, '0000-0001-9172-6429': 1, '0000-0003-0568-917X': 1, '0000-0002-2221-6692': 1, '0000-0002-7992-7487': 1, '0000-0003-4466-1129': 1})\n",
      "['0000-0003-3040-0358', '0000-0002-1915-6780', '0000-0002-7013-8852', '0000-0003-0986-9880', '0000-0001-5166-530X', '0000-0002-2712-0622']\n",
      "Total sample size after apply threshold:  136\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(136, 206)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(136, 206)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.95      0.94        55\n",
      "          1       0.89      0.62      0.73        13\n",
      "          2       0.87      0.87      0.87        15\n",
      "          3       1.00      0.40      0.57        10\n",
      "          4       1.00      0.91      0.95        11\n",
      "          5       0.74      0.97      0.84        32\n",
      "\n",
      "avg / total       0.88      0.87      0.86       136\n",
      "\n",
      "[52  0  1  0  0  2  2  8  0  0  0  3  0  1 13  0  0  1  1  0  1  4  0  4\n",
      "  0  0  0  0 10  1  1  0  0  0  0 31]\n",
      "svc Accuracy:  0.8676470588235294\n",
      "svc F1:  0.8154206154206154\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.98      0.87        55\n",
      "          1       1.00      0.62      0.76        13\n",
      "          2       0.81      0.87      0.84        15\n",
      "          3       1.00      0.50      0.67        10\n",
      "          4       1.00      0.82      0.90        11\n",
      "          5       0.97      0.88      0.92        32\n",
      "\n",
      "avg / total       0.88      0.86      0.86       136\n",
      "\n",
      "[54  0  1  0  0  0  5  8  0  0  0  0  2  0 13  0  0  0  3  0  1  5  0  1\n",
      "  2  0  0  0  9  0  3  0  1  0  0 28]\n",
      "LR Accuracy:  0.8602941176470589\n",
      "LR F1:  0.8260469391352522\n",
      "For name:  g_taylor\n",
      "total sample size before apply threshold:  44\n",
      "Counter({'0000-0002-0817-2785': 22, '0000-0002-6925-7571': 12, '0000-0002-0988-7168': 4, '0000-0003-4787-9844': 2, '0000-0002-3611-5286': 2, '0000-0002-3773-2390': 1, '0000-0002-2916-4645': 1})\n",
      "['0000-0002-6925-7571', '0000-0002-0817-2785']\n",
      "Total sample size after apply threshold:  34\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(34, 131)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(34, 131)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.83      0.91        12\n",
      "          1       0.92      1.00      0.96        22\n",
      "\n",
      "avg / total       0.95      0.94      0.94        34\n",
      "\n",
      "[10  2  0 22]\n",
      "svc Accuracy:  0.9411764705882353\n",
      "svc F1:  0.9328063241106719\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.83      0.91        12\n",
      "          1       0.92      1.00      0.96        22\n",
      "\n",
      "avg / total       0.95      0.94      0.94        34\n",
      "\n",
      "[10  2  0 22]\n",
      "LR Accuracy:  0.9411764705882353\n",
      "LR F1:  0.9328063241106719\n",
      "For name:  x_yang\n",
      "total sample size before apply threshold:  164\n",
      "Counter({'0000-0001-5207-4210': 40, '0000-0002-2036-1220': 32, '0000-0003-3454-3604': 13, '0000-0003-0437-2015': 12, '0000-0002-1142-3100': 10, '0000-0002-7398-4229': 7, '0000-0002-5118-7755': 6, '0000-0002-5083-1799': 6, '0000-0002-4862-7422': 6, '0000-0003-2642-4963': 4, '0000-0002-1375-4800': 4, '0000-0001-8231-5556': 3, '0000-0002-5095-6735': 3, '0000-0003-0219-0023': 3, '0000-0002-2686-745X': 2, '0000-0002-9462-7992': 2, '0000-0002-5871-7894': 1, '0000-0002-5948-2353': 1, '0000-0001-6136-3575': 1, '0000-0003-4097-6318': 1, '0000-0002-1689-2002': 1, '0000-0003-0081-0938': 1, '0000-0003-0073-0823': 1, '0000-0001-7501-1378': 1, '0000-0002-5583-4032': 1, '0000-0002-4617-0713': 1, '0000-0001-6710-0012': 1})\n",
      "['0000-0002-2036-1220', '0000-0002-1142-3100', '0000-0003-0437-2015', '0000-0003-3454-3604', '0000-0001-5207-4210']\n",
      "Total sample size after apply threshold:  107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(107, 379)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(107, 379)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      1.00      0.90        32\n",
      "          1       0.64      0.90      0.75        10\n",
      "          2       1.00      1.00      1.00        12\n",
      "          3       1.00      0.77      0.87        13\n",
      "          4       1.00      0.80      0.89        40\n",
      "\n",
      "avg / total       0.91      0.89      0.89       107\n",
      "\n",
      "[32  0  0  0  0  1  9  0  0  0  0  0 12  0  0  2  1  0 10  0  4  4  0  0\n",
      " 32]\n",
      "svc Accuracy:  0.8878504672897196\n",
      "svc F1:  0.8819725113968838\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      1.00      0.93        32\n",
      "          1       0.73      0.80      0.76        10\n",
      "          2       1.00      1.00      1.00        12\n",
      "          3       1.00      0.85      0.92        13\n",
      "          4       0.97      0.88      0.92        40\n",
      "\n",
      "avg / total       0.92      0.92      0.92       107\n",
      "\n",
      "[32  0  0  0  0  1  8  0  0  1  0  0 12  0  0  2  0  0 11  0  2  3  0  0\n",
      " 35]\n",
      "LR Accuracy:  0.9158878504672897\n",
      "LR F1:  0.9054320584068869\n",
      "For name:  s_bianchi\n",
      "total sample size before apply threshold:  45\n",
      "Counter({'0000-0002-1365-9408': 19, '0000-0001-7290-8489': 10, '0000-0001-7673-3030': 6, '0000-0003-3731-5463': 5, '0000-0003-2292-4303': 2, '0000-0002-4622-4240': 1, '0000-0002-9384-846X': 1, '0000-0002-6979-3622': 1})\n",
      "['0000-0002-1365-9408', '0000-0001-7290-8489']\n",
      "Total sample size after apply threshold:  29\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(29, 103)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(29, 103)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.95      0.97        19\n",
      "          1       0.91      1.00      0.95        10\n",
      "\n",
      "avg / total       0.97      0.97      0.97        29\n",
      "\n",
      "[18  1  0 10]\n",
      "svc Accuracy:  0.9655172413793104\n",
      "svc F1:  0.9626769626769627\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.95      0.97        19\n",
      "          1       0.91      1.00      0.95        10\n",
      "\n",
      "avg / total       0.97      0.97      0.97        29\n",
      "\n",
      "[18  1  0 10]\n",
      "LR Accuracy:  0.9655172413793104\n",
      "LR F1:  0.9626769626769627\n",
      "For name:  a_morales\n",
      "total sample size before apply threshold:  77\n",
      "Counter({'0000-0001-8702-2269': 47, '0000-0002-1526-3327': 19, '0000-0002-9518-3166': 9, '0000-0003-2081-6018': 1, '0000-0003-3656-2497': 1})\n",
      "['0000-0001-8702-2269', '0000-0002-1526-3327']\n",
      "Total sample size after apply threshold:  66\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(66, 157)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(66, 157)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.96      0.98        47\n",
      "          1       0.90      1.00      0.95        19\n",
      "\n",
      "avg / total       0.97      0.97      0.97        66\n",
      "\n",
      "[45  2  0 19]\n",
      "svc Accuracy:  0.9696969696969697\n",
      "svc F1:  0.9641304347826087\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.98      0.99        47\n",
      "          1       0.95      1.00      0.97        19\n",
      "\n",
      "avg / total       0.99      0.98      0.98        66\n",
      "\n",
      "[46  1  0 19]\n",
      "LR Accuracy:  0.9848484848484849\n",
      "LR F1:  0.9818031430934657\n",
      "For name:  p_wong\n",
      "total sample size before apply threshold:  36\n",
      "Counter({'0000-0002-6360-849X': 13, '0000-0003-1592-4823': 8, '0000-0001-7935-7245': 7, '0000-0003-4982-8127': 3, '0000-0003-4645-0384': 3, '0000-0003-3804-3041': 1, '0000-0002-8171-3242': 1})\n",
      "['0000-0002-6360-849X']\n",
      "Total sample size after apply threshold:  13\n",
      "For name:  a_cooper\n",
      "total sample size before apply threshold:  265\n",
      "Counter({'0000-0001-6709-7343': 112, '0000-0001-6050-3863': 72, '0000-0002-5897-2107': 23, '0000-0003-1025-0268': 16, '0000-0003-3975-3897': 15, '0000-0003-4588-2513': 12, '0000-0003-4097-5569': 4, '0000-0002-0815-0084': 4, '0000-0001-6027-8272': 3, '0000-0002-8305-8587': 2, '0000-0002-7328-4361': 1, '0000-0001-8763-8530': 1})\n",
      "['0000-0002-5897-2107', '0000-0001-6709-7343', '0000-0003-3975-3897', '0000-0003-4588-2513', '0000-0001-6050-3863', '0000-0003-1025-0268']\n",
      "Total sample size after apply threshold:  250\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(250, 883)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(250, 883)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.70      0.82        23\n",
      "          1       0.73      0.99      0.84       112\n",
      "          2       1.00      0.67      0.80        15\n",
      "          3       1.00      0.50      0.67        12\n",
      "          4       0.98      0.69      0.81        72\n",
      "          5       1.00      0.94      0.97        16\n",
      "\n",
      "avg / total       0.87      0.83      0.83       250\n",
      "\n",
      "[ 16   7   0   0   0   0   0 111   0   0   1   0   0   5  10   0   0   0\n",
      "   0   6   0   6   0   0   0  22   0   0  50   0   0   1   0   0   0  15]\n",
      "svc Accuracy:  0.832\n",
      "svc F1:  0.8181397739422916\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.61      0.76        23\n",
      "          1       0.74      1.00      0.85       112\n",
      "          2       1.00      0.60      0.75        15\n",
      "          3       1.00      0.50      0.67        12\n",
      "          4       1.00      0.75      0.86        72\n",
      "          5       1.00      0.94      0.97        16\n",
      "\n",
      "avg / total       0.88      0.84      0.84       250\n",
      "\n",
      "[ 14   9   0   0   0   0   0 112   0   0   0   0   0   6   9   0   0   0\n",
      "   0   6   0   6   0   0   0  18   0   0  54   0   0   1   0   0   0  15]\n",
      "LR Accuracy:  0.84\n",
      "LR F1:  0.8077988440891666\n",
      "For name:  j_nguyen\n",
      "total sample size before apply threshold:  27\n",
      "Counter({'0000-0002-8578-7396': 20, '0000-0002-4747-5383': 2, '0000-0003-3574-6278': 1, '0000-0003-0778-3776': 1, '0000-0003-3394-7412': 1, '0000-0002-8410-7395': 1, '0000-0001-5755-5814': 1})\n",
      "['0000-0002-8578-7396']\n",
      "Total sample size after apply threshold:  20\n",
      "For name:  t_lang\n",
      "total sample size before apply threshold:  107\n",
      "Counter({'0000-0002-3720-8038': 83, '0000-0002-7482-7727': 11, '0000-0003-4206-8743': 7, '0000-0001-9619-6762': 6})\n",
      "['0000-0002-3720-8038', '0000-0002-7482-7727']\n",
      "Total sample size after apply threshold:  94\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(94, 327)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(94, 327)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.90      0.93        83\n",
      "          1       0.50      0.73      0.59        11\n",
      "\n",
      "avg / total       0.91      0.88      0.89        94\n",
      "\n",
      "[75  8  3  8]\n",
      "svc Accuracy:  0.8829787234042553\n",
      "svc F1:  0.7621348056130665\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      1.00      0.94        83\n",
      "          1       0.00      0.00      0.00        11\n",
      "\n",
      "avg / total       0.78      0.88      0.83        94\n",
      "\n",
      "[83  0 11  0]\n",
      "LR Accuracy:  0.8829787234042553\n",
      "LR F1:  0.4689265536723164\n",
      "For name:  s_russo\n",
      "total sample size before apply threshold:  45\n",
      "Counter({'0000-0003-3589-3040': 33, '0000-0002-9699-4681': 10, '0000-0001-9137-9391': 1, '0000-0002-5490-3155': 1})\n",
      "['0000-0003-3589-3040', '0000-0002-9699-4681']\n",
      "Total sample size after apply threshold:  43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gao137/intel/intelpython3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/gao137/intel/intelpython3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(43, 80)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(43, 80)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        33\n",
      "          1       1.00      1.00      1.00        10\n",
      "\n",
      "avg / total       1.00      1.00      1.00        43\n",
      "\n",
      "[33  0  0 10]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        33\n",
      "          1       1.00      1.00      1.00        10\n",
      "\n",
      "avg / total       1.00      1.00      1.00        43\n",
      "\n",
      "[33  0  0 10]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  r_arora\n",
      "total sample size before apply threshold:  64\n",
      "Counter({'0000-0002-5799-3619': 41, '0000-0002-2613-4539': 9, '0000-0002-4549-3860': 8, '0000-0001-6447-5628': 6})\n",
      "['0000-0002-5799-3619']\n",
      "Total sample size after apply threshold:  41\n",
      "For name:  c_porter\n",
      "total sample size before apply threshold:  157\n",
      "Counter({'0000-0003-3474-7551': 131, '0000-0001-8636-4515': 19, '0000-0001-8774-0180': 6, '0000-0002-4541-064X': 1})\n",
      "['0000-0001-8636-4515', '0000-0003-3474-7551']\n",
      "Total sample size after apply threshold:  150\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(150, 302)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(150, 302)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.63      0.77        19\n",
      "          1       0.95      1.00      0.97       131\n",
      "\n",
      "avg / total       0.96      0.95      0.95       150\n",
      "\n",
      "[ 12   7   0 131]\n",
      "svc Accuracy:  0.9533333333333334\n",
      "svc F1:  0.8740856217771915\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.47      0.64        19\n",
      "          1       0.93      1.00      0.96       131\n",
      "\n",
      "avg / total       0.94      0.93      0.92       150\n",
      "\n",
      "[  9  10   0 131]\n",
      "LR Accuracy:  0.9333333333333333\n",
      "LR F1:  0.803046218487395\n",
      "For name:  m_moore\n",
      "total sample size before apply threshold:  112\n",
      "Counter({'0000-0002-5127-4509': 45, '0000-0003-3074-6631': 38, '0000-0002-7853-5756': 18, '0000-0003-4768-5329': 7, '0000-0002-7914-0166': 4})\n",
      "['0000-0002-7853-5756', '0000-0003-3074-6631', '0000-0002-5127-4509']\n",
      "Total sample size after apply threshold:  101\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(101, 410)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(101, 410)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.78      0.88        18\n",
      "          1       1.00      0.74      0.85        38\n",
      "          2       0.76      1.00      0.87        45\n",
      "\n",
      "avg / total       0.89      0.86      0.86       101\n",
      "\n",
      "[14  0  4  0 28 10  0  0 45]\n",
      "svc Accuracy:  0.8613861386138614\n",
      "svc F1:  0.862956487956488\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.78      0.88        18\n",
      "          1       1.00      0.89      0.94        38\n",
      "          2       0.85      1.00      0.92        45\n",
      "\n",
      "avg / total       0.93      0.92      0.92       101\n",
      "\n",
      "[14  0  4  0 34  4  0  0 45]\n",
      "LR Accuracy:  0.9207920792079208\n",
      "LR F1:  0.9126039304610734\n",
      "For name:  c_johnson\n",
      "total sample size before apply threshold:  300\n",
      "Counter({'0000-0002-6864-6604': 114, '0000-0002-9719-3771': 47, '0000-0001-9616-6205': 44, '0000-0002-9511-905X': 21, '0000-0001-9190-8441': 18, '0000-0003-3892-7082': 16, '0000-0003-4428-3594': 14, '0000-0002-2298-7462': 12, '0000-0001-9079-813X': 5, '0000-0001-9616-5755': 2, '0000-0003-1954-5142': 2, '0000-0003-2192-3616': 1, '0000-0002-7390-9720': 1, '0000-0002-6679-833X': 1, '0000-0002-6616-4441': 1, '0000-0003-2733-3326': 1})\n",
      "['0000-0002-9511-905X', '0000-0001-9616-6205', '0000-0003-3892-7082', '0000-0002-2298-7462', '0000-0002-6864-6604', '0000-0001-9190-8441', '0000-0002-9719-3771', '0000-0003-4428-3594']\n",
      "Total sample size after apply threshold:  286\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(286, 747)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(286, 747)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.29      0.44        21\n",
      "          1       0.40      1.00      0.57        44\n",
      "          2       1.00      0.50      0.67        16\n",
      "          3       1.00      0.92      0.96        12\n",
      "          4       1.00      0.87      0.93       114\n",
      "          5       1.00      0.61      0.76        18\n",
      "          6       0.91      0.68      0.78        47\n",
      "          7       1.00      0.43      0.60        14\n",
      "\n",
      "avg / total       0.89      0.76      0.77       286\n",
      "\n",
      "[ 6 14  0  0  0  0  1  0  0 44  0  0  0  0  0  0  0  7  8  0  0  0  1  0\n",
      "  0  1  0 11  0  0  0  0  0 14  0  0 99  0  1  0  0  7  0  0  0 11  0  0\n",
      "  0 15  0  0  0  0 32  0  0  8  0  0  0  0  0  6]\n",
      "svc Accuracy:  0.7587412587412588\n",
      "svc F1:  0.7134684226240089\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.43      0.60        21\n",
      "          1       0.49      1.00      0.66        44\n",
      "          2       1.00      0.50      0.67        16\n",
      "          3       1.00      0.92      0.96        12\n",
      "          4       0.98      0.94      0.96       114\n",
      "          5       1.00      0.67      0.80        18\n",
      "          6       0.83      0.74      0.79        47\n",
      "          7       1.00      0.43      0.60        14\n",
      "\n",
      "avg / total       0.89      0.81      0.81       286\n",
      "\n",
      "[  9  11   0   0   0   0   1   0   0  44   0   0   0   0   0   0   0   3\n",
      "   8   0   1   0   4   0   0   1   0  11   0   0   0   0   0   6   0   0\n",
      " 107   0   1   0   0   5   0   0   1  12   0   0   0  12   0   0   0   0\n",
      "  35   0   0   7   0   0   0   0   1   6]\n",
      "LR Accuracy:  0.8111888111888111\n",
      "LR F1:  0.7538750813341766\n",
      "For name:  e_henry\n",
      "total sample size before apply threshold:  31\n",
      "Counter({'0000-0002-5648-8696': 18, '0000-0002-2547-3467': 10, '0000-0002-3884-2612': 2, '0000-0003-3178-2749': 1})\n",
      "['0000-0002-5648-8696', '0000-0002-2547-3467']\n",
      "Total sample size after apply threshold:  28\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(28, 103)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(28, 103)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.97        18\n",
      "          1       1.00      0.90      0.95        10\n",
      "\n",
      "avg / total       0.97      0.96      0.96        28\n",
      "\n",
      "[18  0  1  9]\n",
      "svc Accuracy:  0.9642857142857143\n",
      "svc F1:  0.9601706970128023\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.97        18\n",
      "          1       1.00      0.90      0.95        10\n",
      "\n",
      "avg / total       0.97      0.96      0.96        28\n",
      "\n",
      "[18  0  1  9]\n",
      "LR Accuracy:  0.9642857142857143\n",
      "LR F1:  0.9601706970128023\n",
      "For name:  x_xie\n",
      "total sample size before apply threshold:  24\n",
      "Counter({'0000-0002-2701-8660': 13, '0000-0002-1964-4370': 6, '0000-0003-2988-3065': 2, '0000-0002-6796-8521': 1, '0000-0002-3103-3724': 1, '0000-0002-7970-2974': 1})\n",
      "['0000-0002-2701-8660']\n",
      "Total sample size after apply threshold:  13\n",
      "For name:  x_jin\n",
      "total sample size before apply threshold:  62\n",
      "Counter({'0000-0002-1550-2199': 27, '0000-0003-2454-1621': 11, '0000-0002-2809-7882': 11, '0000-0003-4293-8665': 9, '0000-0001-7339-2920': 2, '0000-0001-6742-1799': 1, '0000-0003-3033-758X': 1})\n",
      "['0000-0002-1550-2199', '0000-0003-2454-1621', '0000-0002-2809-7882']\n",
      "Total sample size after apply threshold:  49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(49, 112)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(49, 112)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.96      0.98        27\n",
      "          1       1.00      1.00      1.00        11\n",
      "          2       0.92      1.00      0.96        11\n",
      "\n",
      "avg / total       0.98      0.98      0.98        49\n",
      "\n",
      "[26  0  1  0 11  0  0  0 11]\n",
      "svc Accuracy:  0.9795918367346939\n",
      "svc F1:  0.979217938200711\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.96      0.98        27\n",
      "          1       1.00      1.00      1.00        11\n",
      "          2       0.92      1.00      0.96        11\n",
      "\n",
      "avg / total       0.98      0.98      0.98        49\n",
      "\n",
      "[26  0  1  0 11  0  0  0 11]\n",
      "LR Accuracy:  0.9795918367346939\n",
      "LR F1:  0.979217938200711\n",
      "For name:  s_singh\n",
      "total sample size before apply threshold:  344\n",
      "Counter({'0000-0003-0912-941X': 70, '0000-0003-3454-2089': 61, '0000-0001-6545-583X': 40, '0000-0003-1033-2546': 19, '0000-0001-9115-3296': 18, '0000-0002-3656-2596': 15, '0000-0002-3482-7001': 14, '0000-0002-9391-0155': 13, '0000-0001-9984-5385': 11, '0000-0002-1028-6255': 9, '0000-0001-6820-9896': 9, '0000-0002-0900-8370': 7, '0000-0003-4404-6089': 6, '0000-0002-8730-524X': 5, '0000-0002-8114-1539': 5, '0000-0001-6521-0998': 5, '0000-0002-8524-0809': 4, '0000-0001-5482-9744': 3, '0000-0001-9505-4842': 3, '0000-0002-5154-3318': 2, '0000-0002-4038-5924': 2, '0000-0001-5545-7831': 2, '0000-0002-1878-8516': 2, '0000-0003-1914-4955': 2, '0000-0001-5361-4303': 2, '0000-0002-4897-8812': 2, '0000-0001-5935-3829': 2, '0000-0003-3562-6807': 1, '0000-0001-9826-2508': 1, '0000-0001-5985-5781': 1, '0000-0002-0193-9349': 1, '0000-0001-9754-1724': 1, '0000-0003-4805-7383': 1, '0000-0002-0022-6240': 1, '0000-0001-7509-3115': 1, '0000-0001-5412-2888': 1, '0000-0001-9669-3531': 1, '0000-0001-9338-1209': 1})\n",
      "['0000-0002-9391-0155', '0000-0002-3656-2596', '0000-0001-9115-3296', '0000-0002-3482-7001', '0000-0003-3454-2089', '0000-0003-0912-941X', '0000-0001-9984-5385', '0000-0001-6545-583X', '0000-0003-1033-2546']\n",
      "Total sample size after apply threshold:  261\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(261, 509)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(261, 509)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.31      0.47        13\n",
      "          1       0.86      0.80      0.83        15\n",
      "          2       1.00      0.78      0.88        18\n",
      "          3       1.00      0.93      0.96        14\n",
      "          4       0.97      0.61      0.75        61\n",
      "          5       0.56      1.00      0.72        70\n",
      "          6       1.00      0.73      0.84        11\n",
      "          7       0.97      0.70      0.81        40\n",
      "          8       0.94      0.79      0.86        19\n",
      "\n",
      "avg / total       0.86      0.77      0.77       261\n",
      "\n",
      "[ 4  0  0  0  0  9  0  0  0  0 12  0  0  0  2  0  1  0  0  0 14  0  0  4\n",
      "  0  0  0  0  0  0 13  0  1  0  0  0  0  0  0  0 37 24  0  0  0  0  0  0\n",
      "  0  0 70  0  0  0  0  0  0  0  0  3  8  0  0  0  2  0  0  1  8  0 28  1\n",
      "  0  0  0  0  0  4  0  0 15]\n",
      "svc Accuracy:  0.7701149425287356\n",
      "svc F1:  0.7902670215307113\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.23      0.38        13\n",
      "          1       0.86      0.80      0.83        15\n",
      "          2       1.00      0.83      0.91        18\n",
      "          3       0.92      0.86      0.89        14\n",
      "          4       1.00      0.77      0.87        61\n",
      "          5       0.61      1.00      0.76        70\n",
      "          6       1.00      0.73      0.84        11\n",
      "          7       0.93      0.70      0.80        40\n",
      "          8       0.94      0.84      0.89        19\n",
      "\n",
      "avg / total       0.87      0.81      0.81       261\n",
      "\n",
      "[ 3  0  0  0  0 10  0  0  0  0 12  0  1  0  1  0  1  0  0  0 15  0  0  3\n",
      "  0  0  0  0  0  0 12  0  2  0  0  0  0  0  0  0 47 13  0  1  0  0  0  0\n",
      "  0  0 70  0  0  0  0  0  0  0  0  3  8  0  0  0  2  0  0  0  9  0 28  1\n",
      "  0  0  0  0  0  3  0  0 16]\n",
      "LR Accuracy:  0.8084291187739464\n",
      "LR F1:  0.795866676945655\n",
      "For name:  m_reid\n",
      "total sample size before apply threshold:  62\n",
      "Counter({'0000-0003-4005-9384': 55, '0000-0002-4101-0921': 5, '0000-0002-0397-2556': 1, '0000-0002-3948-9347': 1})\n",
      "['0000-0003-4005-9384']\n",
      "Total sample size after apply threshold:  55\n",
      "For name:  m_wallace\n",
      "total sample size before apply threshold:  144\n",
      "Counter({'0000-0002-0166-906X': 57, '0000-0001-6894-4903': 52, '0000-0002-5692-8313': 30, '0000-0001-5407-8653': 3, '0000-0002-8318-7952': 2})\n",
      "['0000-0002-0166-906X', '0000-0002-5692-8313', '0000-0001-6894-4903']\n",
      "Total sample size after apply threshold:  139\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(139, 201)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(139, 201)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.91      0.95        57\n",
      "          1       1.00      0.47      0.64        30\n",
      "          2       0.71      1.00      0.83        52\n",
      "\n",
      "avg / total       0.89      0.85      0.84       139\n",
      "\n",
      "[52  0  5  0 14 16  0  0 52]\n",
      "svc Accuracy:  0.8489208633093526\n",
      "svc F1:  0.8074973589102029\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.93      0.91        57\n",
      "          1       1.00      0.60      0.75        30\n",
      "          2       0.79      0.94      0.86        52\n",
      "\n",
      "avg / total       0.88      0.86      0.86       139\n",
      "\n",
      "[53  0  4  3 18  9  3  0 49]\n",
      "LR Accuracy:  0.8633093525179856\n",
      "LR F1:  0.8411474087517644\n",
      "For name:  y_zhang\n",
      "total sample size before apply threshold:  1244\n",
      "Counter({'0000-0001-8642-4071': 104, '0000-0002-3254-8965': 64, '0000-0001-7307-9408': 56, '0000-0002-9956-3879': 48, '0000-0003-2932-4159': 48, '0000-0003-2317-2190': 45, '0000-0003-2753-7601': 37, '0000-0001-6118-6695': 31, '0000-0002-6460-6779': 29, '0000-0002-1079-4137': 28, '0000-0001-8357-5544': 28, '0000-0002-2663-1279': 27, '0000-0002-8457-5922': 27, '0000-0002-1239-3441': 26, '0000-0003-1148-949X': 26, '0000-0001-7436-9757': 23, '0000-0002-0045-0808': 22, '0000-0002-3859-3839': 22, '0000-0001-6777-3487': 22, '0000-0002-8448-3059': 21, '0000-0003-0592-9153': 21, '0000-0003-4698-5645': 20, '0000-0002-2832-2277': 20, '0000-0003-4082-5026': 20, '0000-0001-7433-1820': 17, '0000-0002-7339-8342': 16, '0000-0002-7926-9904': 14, '0000-0002-8270-1067': 14, '0000-0002-9548-0021': 14, '0000-0001-6577-5235': 14, '0000-0002-6035-8536': 14, '0000-0003-2212-1527': 13, '0000-0003-2560-3927': 13, '0000-0001-6751-9294': 12, '0000-0003-2351-5579': 11, '0000-0002-4405-4268': 9, '0000-0001-7882-5692': 9, '0000-0002-4477-7570': 8, '0000-0002-2559-3741': 8, '0000-0001-8759-0194': 7, '0000-0003-2968-0081': 7, '0000-0001-5562-9090': 7, '0000-0002-2614-5975': 7, '0000-0001-5734-0709': 7, '0000-0002-9263-6262': 7, '0000-0003-0022-1201': 6, '0000-0002-1990-9439': 6, '0000-0002-5765-0923': 6, '0000-0002-5715-2182': 5, '0000-0002-4762-6639': 5, '0000-0003-1204-8717': 5, '0000-0003-0614-2096': 5, '0000-0001-8938-1927': 5, '0000-0002-6764-3567': 5, '0000-0003-0522-6300': 5, '0000-0002-6201-7970': 5, '0000-0001-8286-300X': 5, '0000-0001-9983-5451': 5, '0000-0001-9321-9228': 4, '0000-0002-7422-8206': 4, '0000-0001-9157-5544': 4, '0000-0001-8702-909X': 4, '0000-0001-8915-1769': 4, '0000-0003-0919-2224': 4, '0000-0002-6893-2053': 4, '0000-0001-8537-8181': 4, '0000-0002-7468-2409': 4, '0000-0003-3531-0052': 4, '0000-0001-5996-5438': 4, '0000-0002-0814-2965': 4, '0000-0003-1011-3001': 4, '0000-0003-4355-9755': 4, '0000-0002-4870-1493': 4, '0000-0002-9731-5943': 3, '0000-0002-3562-2323': 3, '0000-0003-0757-1837': 3, '0000-0003-4353-593X': 3, '0000-0003-4638-0056': 3, '0000-0003-1608-4467': 3, '0000-0003-1620-3825': 2, '0000-0002-9738-5343': 2, '0000-0001-9126-4922': 2, '0000-0002-1634-5017': 2, '0000-0001-9934-7925': 2, '0000-0003-3709-7144': 2, '0000-0001-7636-7368': 2, '0000-0002-8663-5001': 2, '0000-0002-8754-8938': 2, '0000-0002-1084-9994': 2, '0000-0001-8474-5947': 2, '0000-0002-1483-9021': 2, '0000-0002-8121-3678': 2, '0000-0002-0238-0719': 2, '0000-0003-1174-6599': 2, '0000-0003-0182-4215': 1, '0000-0002-9318-0324': 1, '0000-0003-3770-0046': 1, '0000-0001-7783-8336': 1, '0000-0003-4267-0144': 1, '0000-0003-2179-3698': 1, '0000-0002-7484-8800': 1, '0000-0002-5254-6764': 1, '0000-0003-0859-9735': 1, '0000-0002-4087-420X': 1, '0000-0003-2158-6541': 1, '0000-0002-2684-7395': 1, '0000-0002-8123-7805': 1, '0000-0002-5901-4242': 1, '0000-0003-0397-7143': 1, '0000-0003-1614-1943': 1, '0000-0001-9588-2314': 1, '0000-0002-1259-020X': 1, '0000-0002-7175-6150': 1, '0000-0002-7627-488X': 1, '0000-0002-3873-4574': 1, '0000-0003-2903-2287': 1, '0000-0003-4114-7183': 1, '0000-0001-8738-1851': 1, '0000-0002-1566-098X': 1})\n",
      "['0000-0002-0045-0808', '0000-0002-7926-9904', '0000-0002-8448-3059', '0000-0003-4698-5645', '0000-0003-2212-1527', '0000-0002-3859-3839', '0000-0002-3254-8965', '0000-0001-7307-9408', '0000-0001-6777-3487', '0000-0002-1239-3441', '0000-0002-1079-4137', '0000-0002-9956-3879', '0000-0002-8270-1067', '0000-0001-6751-9294', '0000-0003-2932-4159', '0000-0002-2663-1279', '0000-0003-1148-949X', '0000-0003-2317-2190', '0000-0001-7436-9757', '0000-0001-8357-5544', '0000-0001-8642-4071', '0000-0002-9548-0021', '0000-0001-7433-1820', '0000-0003-2351-5579', '0000-0002-7339-8342', '0000-0001-6118-6695', '0000-0002-8457-5922', '0000-0001-6577-5235', '0000-0003-2560-3927', '0000-0002-2832-2277', '0000-0003-0592-9153', '0000-0002-6460-6779', '0000-0003-2753-7601', '0000-0002-6035-8536', '0000-0003-4082-5026']\n",
      "Total sample size after apply threshold:  967\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(967, 1103)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(967, 1103)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      1.00      0.90        22\n",
      "          1       0.92      0.79      0.85        14\n",
      "          2       0.72      0.62      0.67        21\n",
      "          3       0.39      0.35      0.37        20\n",
      "          4       1.00      1.00      1.00        13\n",
      "          5       0.50      0.45      0.48        22\n",
      "          6       0.60      0.72      0.65        64\n",
      "          7       0.58      0.70      0.63        56\n",
      "          8       0.87      0.59      0.70        22\n",
      "          9       0.50      0.88      0.64        26\n",
      "         10       0.92      0.86      0.89        28\n",
      "         11       0.93      0.85      0.89        48\n",
      "         12       0.73      0.57      0.64        14\n",
      "         13       0.62      0.42      0.50        12\n",
      "         14       0.79      0.77      0.78        48\n",
      "         15       0.91      0.74      0.82        27\n",
      "         16       0.71      0.85      0.77        26\n",
      "         17       0.32      0.53      0.40        45\n",
      "         18       1.00      0.70      0.82        23\n",
      "         19       0.80      0.86      0.83        28\n",
      "         20       0.85      0.79      0.82       104\n",
      "         21       0.86      0.43      0.57        14\n",
      "         22       0.70      0.41      0.52        17\n",
      "         23       0.71      0.45      0.56        11\n",
      "         24       1.00      0.69      0.81        16\n",
      "         25       0.63      0.84      0.72        31\n",
      "         26       1.00      0.74      0.85        27\n",
      "         27       0.92      0.86      0.89        14\n",
      "         28       0.83      0.77      0.80        13\n",
      "         29       1.00      0.90      0.95        20\n",
      "         30       1.00      0.81      0.89        21\n",
      "         31       0.96      0.86      0.91        29\n",
      "         32       0.59      0.54      0.56        37\n",
      "         33       1.00      1.00      1.00        14\n",
      "         34       0.94      0.85      0.89        20\n",
      "\n",
      "avg / total       0.76      0.73      0.74       967\n",
      "\n",
      "[22  0  0 ...  0  0 17]\n",
      "svc Accuracy:  0.7321613236814891\n",
      "svc F1:  0.7420055672971834\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      1.00      0.94        22\n",
      "          1       0.92      0.86      0.89        14\n",
      "          2       0.89      0.76      0.82        21\n",
      "          3       0.40      0.20      0.27        20\n",
      "          4       1.00      1.00      1.00        13\n",
      "          5       0.86      0.27      0.41        22\n",
      "          6       0.58      0.78      0.67        64\n",
      "          7       0.63      0.70      0.66        56\n",
      "          8       1.00      0.77      0.87        22\n",
      "          9       0.62      0.88      0.73        26\n",
      "         10       0.96      0.86      0.91        28\n",
      "         11       0.88      0.90      0.89        48\n",
      "         12       0.69      0.64      0.67        14\n",
      "         13       0.88      0.58      0.70        12\n",
      "         14       0.67      0.83      0.74        48\n",
      "         15       0.88      0.78      0.82        27\n",
      "         16       0.69      0.77      0.73        26\n",
      "         17       0.49      0.60      0.54        45\n",
      "         18       1.00      0.70      0.82        23\n",
      "         19       0.96      0.79      0.86        28\n",
      "         20       0.70      0.87      0.78       104\n",
      "         21       1.00      0.36      0.53        14\n",
      "         22       0.89      0.47      0.62        17\n",
      "         23       0.71      0.45      0.56        11\n",
      "         24       0.92      0.75      0.83        16\n",
      "         25       0.62      0.90      0.74        31\n",
      "         26       1.00      0.85      0.92        27\n",
      "         27       0.77      0.71      0.74        14\n",
      "         28       0.92      0.85      0.88        13\n",
      "         29       1.00      0.90      0.95        20\n",
      "         30       1.00      0.95      0.98        21\n",
      "         31       1.00      0.83      0.91        29\n",
      "         32       0.67      0.49      0.56        37\n",
      "         33       1.00      0.93      0.96        14\n",
      "         34       0.95      0.95      0.95        20\n",
      "\n",
      "avg / total       0.78      0.76      0.76       967\n",
      "\n",
      "[22  0  0 ...  0  0 19]\n",
      "LR Accuracy:  0.7600827300930714\n",
      "LR F1:  0.7660509511516167\n",
      "For name:  m_young\n",
      "total sample size before apply threshold:  101\n",
      "Counter({'0000-0003-0450-5375': 51, '0000-0002-9615-9002': 27, '0000-0002-7263-6505': 9, '0000-0001-5168-9416': 8, '0000-0001-8479-9910': 4, '0000-0002-1262-5935': 2})\n",
      "['0000-0002-9615-9002', '0000-0003-0450-5375']\n",
      "Total sample size after apply threshold:  78\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(78, 197)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(78, 197)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.74      0.85        27\n",
      "          1       0.88      1.00      0.94        51\n",
      "\n",
      "avg / total       0.92      0.91      0.91        78\n",
      "\n",
      "[20  7  0 51]\n",
      "svc Accuracy:  0.9102564102564102\n",
      "svc F1:  0.8934218231504978\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.74      0.85        27\n",
      "          1       0.88      1.00      0.94        51\n",
      "\n",
      "avg / total       0.92      0.91      0.91        78\n",
      "\n",
      "[20  7  0 51]\n",
      "LR Accuracy:  0.9102564102564102\n",
      "LR F1:  0.8934218231504978\n",
      "For name:  s_saraf\n",
      "total sample size before apply threshold:  54\n",
      "Counter({'0000-0002-8384-9370': 38, '0000-0002-0569-1213': 13, '0000-0003-3905-0542': 2, '0000-0002-4180-0931': 1})\n",
      "['0000-0002-0569-1213', '0000-0002-8384-9370']\n",
      "Total sample size after apply threshold:  51\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(51, 39)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(51, 39)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.69      0.78        13\n",
      "          1       0.90      0.97      0.94        38\n",
      "\n",
      "avg / total       0.90      0.90      0.90        51\n",
      "\n",
      "[ 9  4  1 37]\n",
      "svc Accuracy:  0.9019607843137255\n",
      "svc F1:  0.8596587782058338\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.69      0.78        13\n",
      "          1       0.90      0.97      0.94        38\n",
      "\n",
      "avg / total       0.90      0.90      0.90        51\n",
      "\n",
      "[ 9  4  1 37]\n",
      "LR Accuracy:  0.9019607843137255\n",
      "LR F1:  0.8596587782058338\n",
      "For name:  r_pinto\n",
      "total sample size before apply threshold:  85\n",
      "Counter({'0000-0002-1667-7871': 36, '0000-0002-2775-860X': 21, '0000-0001-5600-2396': 8, '0000-0002-6429-2087': 8, '0000-0003-0058-8652': 6, '0000-0002-4068-7391': 2, '0000-0001-9402-5775': 2, '0000-0002-1251-5007': 1, '0000-0002-4512-5566': 1})\n",
      "['0000-0002-1667-7871', '0000-0002-2775-860X']\n",
      "Total sample size after apply threshold:  57\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(57, 174)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(57, 174)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.97      0.99        36\n",
      "          1       0.95      1.00      0.98        21\n",
      "\n",
      "avg / total       0.98      0.98      0.98        57\n",
      "\n",
      "[35  1  0 21]\n",
      "svc Accuracy:  0.9824561403508771\n",
      "svc F1:  0.981329839502129\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      1.00      0.96        36\n",
      "          1       1.00      0.86      0.92        21\n",
      "\n",
      "avg / total       0.95      0.95      0.95        57\n",
      "\n",
      "[36  0  3 18]\n",
      "LR Accuracy:  0.9473684210526315\n",
      "LR F1:  0.9415384615384615\n",
      "For name:  m_brito\n",
      "total sample size before apply threshold:  86\n",
      "Counter({'0000-0002-8493-4649': 51, '0000-0001-6394-658X': 31, '0000-0002-8973-104X': 2, '0000-0001-9689-7040': 1, '0000-0002-1779-4535': 1})\n",
      "['0000-0002-8493-4649', '0000-0001-6394-658X']\n",
      "Total sample size after apply threshold:  82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(82, 169)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(82, 169)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.94      0.97        51\n",
      "          1       0.91      1.00      0.95        31\n",
      "\n",
      "avg / total       0.97      0.96      0.96        82\n",
      "\n",
      "[48  3  0 31]\n",
      "svc Accuracy:  0.9634146341463414\n",
      "svc F1:  0.9617715617715619\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.92      0.95        51\n",
      "          1       0.88      0.97      0.92        31\n",
      "\n",
      "avg / total       0.94      0.94      0.94        82\n",
      "\n",
      "[47  4  1 30]\n",
      "LR Accuracy:  0.9390243902439024\n",
      "LR F1:  0.9362859362859361\n",
      "For name:  s_goel\n",
      "total sample size before apply threshold:  31\n",
      "Counter({'0000-0001-7485-5392': 16, '0000-0003-2866-790X': 7, '0000-0001-7886-9441': 4, '0000-0002-8694-332X': 3, '0000-0002-9739-4178': 1})\n",
      "['0000-0001-7485-5392']\n",
      "Total sample size after apply threshold:  16\n",
      "For name:  y_park\n",
      "total sample size before apply threshold:  627\n",
      "Counter({'0000-0002-6281-489X': 171, '0000-0002-5879-6879': 78, '0000-0002-3671-6364': 67, '0000-0002-9553-8561': 56, '0000-0003-1191-7335': 35, '0000-0002-8288-9450': 32, '0000-0002-8808-4530': 28, '0000-0002-1310-148X': 28, '0000-0002-5466-2339': 22, '0000-0001-8336-8051': 20, '0000-0003-3652-591X': 16, '0000-0001-8583-4335': 15, '0000-0001-8495-9224': 14, '0000-0001-7025-8945': 13, '0000-0002-1959-0843': 9, '0000-0002-7574-4165': 7, '0000-0003-1997-6444': 6, '0000-0002-8536-0835': 3, '0000-0001-6587-6562': 3, '0000-0002-1702-0986': 1, '0000-0002-2801-2674': 1, '0000-0001-5110-5716': 1, '0000-0002-3019-5748': 1})\n",
      "['0000-0001-7025-8945', '0000-0002-5466-2339', '0000-0003-3652-591X', '0000-0002-3671-6364', '0000-0002-8808-4530', '0000-0003-1191-7335', '0000-0002-5879-6879', '0000-0001-8495-9224', '0000-0002-8288-9450', '0000-0001-8336-8051', '0000-0001-8583-4335', '0000-0002-1310-148X', '0000-0002-6281-489X', '0000-0002-9553-8561']\n",
      "Total sample size after apply threshold:  595\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(595, 1039)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(595, 1039)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.77      0.87        13\n",
      "          1       0.33      0.64      0.43        22\n",
      "          2       1.00      1.00      1.00        16\n",
      "          3       0.89      0.82      0.85        67\n",
      "          4       0.39      0.39      0.39        28\n",
      "          5       0.76      0.83      0.79        35\n",
      "          6       0.86      0.83      0.84        78\n",
      "          7       0.50      0.64      0.56        14\n",
      "          8       0.69      0.78      0.74        32\n",
      "          9       1.00      0.35      0.52        20\n",
      "         10       1.00      0.67      0.80        15\n",
      "         11       1.00      0.93      0.96        28\n",
      "         12       0.98      0.96      0.97       171\n",
      "         13       0.62      0.64      0.63        56\n",
      "\n",
      "avg / total       0.83      0.80      0.81       595\n",
      "\n",
      "[ 10   1   0   0   1   1   0   0   0   0   0   0   0   0   0  14   0   0\n",
      "   1   1   0   1   0   0   0   0   2   3   0   0  16   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0  55   1   0   3   2   3   0   0   0\n",
      "   0   3   0   2   0   0  11   1   4   1   0   0   0   0   0   9   0   4\n",
      "   0   0   0  29   0   0   1   0   0   0   1   0   0   2   0   2   4   1\n",
      "  65   1   0   0   0   0   0   3   0   0   0   0   3   0   1   9   1   0\n",
      "   0   0   0   0   0   1   0   1   1   0   0   2  25   0   0   0   0   2\n",
      "   0   5   0   0   3   1   1   0   1   7   0   0   0   2   0   3   0   0\n",
      "   0   2   0   0   0   0  10   0   0   0   0   1   0   0   0   0   0   1\n",
      "   0   0   0  26   0   0   0   4   0   0   0   2   0   1   0   0   0   0\n",
      " 164   0   0   6   0   4   3   0   2   0   5   0   0   0   0  36]\n",
      "svc Accuracy:  0.8016806722689076\n",
      "svc F1:  0.7404178506495497\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.69      0.82        13\n",
      "          1       0.67      0.36      0.47        22\n",
      "          2       0.93      0.88      0.90        16\n",
      "          3       0.76      0.87      0.81        67\n",
      "          4       0.62      0.29      0.39        28\n",
      "          5       0.91      0.83      0.87        35\n",
      "          6       0.79      0.91      0.85        78\n",
      "          7       0.67      0.29      0.40        14\n",
      "          8       0.71      0.78      0.75        32\n",
      "          9       1.00      0.60      0.75        20\n",
      "         10       1.00      0.60      0.75        15\n",
      "         11       1.00      0.96      0.98        28\n",
      "         12       0.91      0.98      0.94       171\n",
      "         13       0.59      0.79      0.67        56\n",
      "\n",
      "avg / total       0.82      0.82      0.80       595\n",
      "\n",
      "[  9   1   0   0   2   0   0   0   0   0   0   0   1   0   0   8   0   2\n",
      "   1   0   1   0   0   0   0   0   4   6   0   0  14   1   0   0   0   0\n",
      "   0   0   0   0   0   1   0   0   0  58   1   0   2   1   1   0   0   0\n",
      "   0   4   0   0   1   0   8   1   6   1   0   0   0   0   2   9   0   1\n",
      "   0   0   0  29   1   0   0   0   0   0   3   1   0   0   0   4   1   0\n",
      "  71   0   0   0   0   0   1   1   0   0   0   1   0   0   3   4   4   0\n",
      "   0   0   0   2   0   0   0   3   0   0   1   0  25   0   0   0   0   3\n",
      "   0   0   0   2   0   0   1   0   0  12   0   0   1   4   0   1   0   0\n",
      "   0   0   0   0   0   0   9   0   5   0   0   0   0   1   0   0   0   0\n",
      "   0   0   0  27   0   0   0   1   0   0   0   2   1   0   0   0   0   0\n",
      " 167   0   0   0   0   4   0   0   3   0   5   0   0   0   0  44]\n",
      "LR Accuracy:  0.8151260504201681\n",
      "LR F1:  0.7389304246237499\n",
      "For name:  p_melo\n",
      "total sample size before apply threshold:  28\n",
      "Counter({'0000-0003-0590-0684': 14, '0000-0002-4486-0200': 6, '0000-0002-3892-4140': 5, '0000-0002-4117-239X': 3})\n",
      "['0000-0003-0590-0684']\n",
      "Total sample size after apply threshold:  14\n",
      "For name:  c_lemos\n",
      "total sample size before apply threshold:  52\n",
      "Counter({'0000-0001-9803-9584': 21, '0000-0003-3182-6289': 14, '0000-0001-8273-489X': 9, '0000-0002-3372-6719': 4, '0000-0003-3468-4191': 4})\n",
      "['0000-0003-3182-6289', '0000-0001-9803-9584']\n",
      "Total sample size after apply threshold:  35\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(35, 124)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(35, 124)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.93      0.96        14\n",
      "          1       0.95      1.00      0.98        21\n",
      "\n",
      "avg / total       0.97      0.97      0.97        35\n",
      "\n",
      "[13  1  0 21]\n",
      "svc Accuracy:  0.9714285714285714\n",
      "svc F1:  0.9698535745047374\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        14\n",
      "          1       1.00      1.00      1.00        21\n",
      "\n",
      "avg / total       1.00      1.00      1.00        35\n",
      "\n",
      "[14  0  0 21]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  b_liu\n",
      "total sample size before apply threshold:  298\n",
      "Counter({'0000-0002-0956-2777': 97, '0000-0002-8662-0512': 24, '0000-0002-0787-5825': 18, '0000-0001-6052-8411': 15, '0000-0001-9992-9319': 14, '0000-0002-5836-2333': 14, '0000-0002-8676-4794': 12, '0000-0003-0122-3866': 11, '0000-0002-4948-2835': 10, '0000-0003-2211-5557': 10, '0000-0003-3060-3120': 9, '0000-0001-8211-6303': 8, '0000-0002-9318-1335': 8, '0000-0001-6655-1866': 7, '0000-0002-4511-6926': 6, '0000-0002-7257-2441': 5, '0000-0001-8806-820X': 5, '0000-0002-8550-1722': 3, '0000-0002-7347-1941': 3, '0000-0001-6221-1047': 3, '0000-0003-4532-3658': 3, '0000-0002-9539-2005': 2, '0000-0003-2529-0123': 2, '0000-0002-8318-9667': 2, '0000-0002-6825-3536': 2, '0000-0002-9495-6809': 2, '0000-0002-1677-2772': 1, '0000-0002-5272-3425': 1, '0000-0002-7543-1054': 1})\n",
      "['0000-0003-0122-3866', '0000-0001-6052-8411', '0000-0002-0787-5825', '0000-0002-8662-0512', '0000-0001-9992-9319', '0000-0002-4948-2835', '0000-0002-0956-2777', '0000-0003-2211-5557', '0000-0002-5836-2333', '0000-0002-8676-4794']\n",
      "Total sample size after apply threshold:  225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(225, 465)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(225, 465)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.55      0.67        11\n",
      "          1       1.00      0.80      0.89        15\n",
      "          2       1.00      0.72      0.84        18\n",
      "          3       0.84      0.88      0.86        24\n",
      "          4       0.79      0.79      0.79        14\n",
      "          5       1.00      0.80      0.89        10\n",
      "          6       0.82      0.97      0.89        97\n",
      "          7       1.00      0.90      0.95        10\n",
      "          8       0.92      0.79      0.85        14\n",
      "          9       0.90      0.75      0.82        12\n",
      "\n",
      "avg / total       0.87      0.86      0.86       225\n",
      "\n",
      "[ 6  0  0  0  0  0  5  0  0  0  0 12  0  0  2  0  1  0  0  0  0  0 13  0\n",
      "  0  0  5  0  0  0  0  0  0 21  0  0  2  0  1  0  0  0  0  0 11  0  3  0\n",
      "  0  0  0  0  0  1  0  8  1  0  0  0  0  0  0  2  1  0 94  0  0  0  1  0\n",
      "  0  0  0  0  0  9  0  0  0  0  0  0  0  0  2  0 11  1  0  0  0  1  0  0\n",
      "  2  0  0  9]\n",
      "svc Accuracy:  0.8622222222222222\n",
      "svc F1:  0.8424507802939425\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.27      0.43        11\n",
      "          1       1.00      0.80      0.89        15\n",
      "          2       1.00      0.67      0.80        18\n",
      "          3       0.95      0.88      0.91        24\n",
      "          4       0.82      0.64      0.72        14\n",
      "          5       1.00      0.80      0.89        10\n",
      "          6       0.74      0.99      0.85        97\n",
      "          7       1.00      0.60      0.75        10\n",
      "          8       0.92      0.79      0.85        14\n",
      "          9       0.89      0.67      0.76        12\n",
      "\n",
      "avg / total       0.86      0.83      0.82       225\n",
      "\n",
      "[ 3  0  0  0  0  0  8  0  0  0  0 12  0  0  1  0  2  0  0  0  0  0 12  0\n",
      "  0  0  6  0  0  0  0  0  0 21  0  0  2  0  1  0  0  0  0  0  9  0  5  0\n",
      "  0  0  0  0  0  0  0  8  2  0  0  0  0  0  0  1  0  0 96  0  0  0  0  0\n",
      "  0  0  0  0  4  6  0  0  0  0  0  0  0  0  2  0 11  1  0  0  0  0  1  0\n",
      "  3  0  0  8]\n",
      "LR Accuracy:  0.8266666666666667\n",
      "LR F1:  0.7843266270642252\n",
      "For name:  k_turner\n",
      "total sample size before apply threshold:  62\n",
      "Counter({'0000-0002-8152-6017': 42, '0000-0003-3714-5118': 11, '0000-0002-3867-2684': 5, '0000-0001-8982-0301': 3, '0000-0002-1163-2201': 1})\n",
      "['0000-0002-8152-6017', '0000-0003-3714-5118']\n",
      "Total sample size after apply threshold:  53\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(53, 156)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(53, 156)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99        42\n",
      "          1       1.00      0.91      0.95        11\n",
      "\n",
      "avg / total       0.98      0.98      0.98        53\n",
      "\n",
      "[42  0  1 10]\n",
      "svc Accuracy:  0.9811320754716981\n",
      "svc F1:  0.9703081232492996\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99        42\n",
      "          1       1.00      0.91      0.95        11\n",
      "\n",
      "avg / total       0.98      0.98      0.98        53\n",
      "\n",
      "[42  0  1 10]\n",
      "LR Accuracy:  0.9811320754716981\n",
      "LR F1:  0.9703081232492996\n",
      "For name:  r_rao\n",
      "total sample size before apply threshold:  94\n",
      "Counter({'0000-0002-5776-8366': 52, '0000-0002-0262-5122': 14, '0000-0002-2285-6788': 12, '0000-0002-1475-3893': 9, '0000-0002-6415-0185': 7})\n",
      "['0000-0002-2285-6788', '0000-0002-5776-8366', '0000-0002-0262-5122']\n",
      "Total sample size after apply threshold:  78\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(78, 130)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(78, 130)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.75      0.86        12\n",
      "          1       0.80      1.00      0.89        52\n",
      "          2       1.00      0.29      0.44        14\n",
      "\n",
      "avg / total       0.87      0.83      0.80        78\n",
      "\n",
      "[ 9  3  0  0 52  0  0 10  4]\n",
      "svc Accuracy:  0.8333333333333334\n",
      "svc F1:  0.7301587301587302\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.75      0.86        12\n",
      "          1       0.80      1.00      0.89        52\n",
      "          2       1.00      0.29      0.44        14\n",
      "\n",
      "avg / total       0.87      0.83      0.80        78\n",
      "\n",
      "[ 9  3  0  0 52  0  0 10  4]\n",
      "LR Accuracy:  0.8333333333333334\n",
      "LR F1:  0.7301587301587302\n",
      "For name:  b_barker\n",
      "total sample size before apply threshold:  35\n",
      "Counter({'0000-0002-3439-4517': 21, '0000-0001-6932-479X': 8, '0000-0001-9327-7057': 5, '0000-0001-5732-9550': 1})\n",
      "['0000-0002-3439-4517']\n",
      "Total sample size after apply threshold:  21\n",
      "For name:  a_wright\n",
      "total sample size before apply threshold:  149\n",
      "Counter({'0000-0002-2369-0601': 67, '0000-0002-3172-5253': 31, '0000-0002-4866-5699': 21, '0000-0002-8718-8143': 13, '0000-0002-0373-5219': 11, '0000-0003-0721-7854': 4, '0000-0001-6442-5583': 1, '0000-0001-8428-890X': 1})\n",
      "['0000-0002-2369-0601', '0000-0002-8718-8143', '0000-0002-4866-5699', '0000-0002-3172-5253', '0000-0002-0373-5219']\n",
      "Total sample size after apply threshold:  143\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(143, 358)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(143, 358)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      1.00      0.89        67\n",
      "          1       0.78      0.54      0.64        13\n",
      "          2       1.00      0.86      0.92        21\n",
      "          3       1.00      0.81      0.89        31\n",
      "          4       1.00      0.73      0.84        11\n",
      "\n",
      "avg / total       0.89      0.87      0.87       143\n",
      "\n",
      "[67  0  0  0  0  6  7  0  0  0  3  0 18  0  0  5  1  0 25  0  2  1  0  0\n",
      "  8]\n",
      "svc Accuracy:  0.8741258741258742\n",
      "svc F1:  0.8375472597577861\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      1.00      0.91        67\n",
      "          1       0.88      0.54      0.67        13\n",
      "          2       1.00      0.90      0.95        21\n",
      "          3       0.93      0.90      0.92        31\n",
      "          4       1.00      0.55      0.71        11\n",
      "\n",
      "avg / total       0.90      0.89      0.88       143\n",
      "\n",
      "[67  0  0  0  0  5  7  0  1  0  1  0 19  1  0  3  0  0 28  0  4  1  0  0\n",
      "  6]\n",
      "LR Accuracy:  0.8881118881118881\n",
      "LR F1:  0.830429286468686\n",
      "For name:  z_ma\n",
      "total sample size before apply threshold:  111\n",
      "Counter({'0000-0002-5426-0997': 36, '0000-0002-2391-4943': 18, '0000-0002-4429-5213': 15, '0000-0003-1186-9396': 12, '0000-0003-0257-5695': 11, '0000-0002-1629-7764': 9, '0000-0002-3164-6117': 5, '0000-0002-7276-2229': 4, '0000-0002-7120-9106': 1})\n",
      "['0000-0002-5426-0997', '0000-0002-4429-5213', '0000-0002-2391-4943', '0000-0003-0257-5695', '0000-0003-1186-9396']\n",
      "Total sample size after apply threshold:  92\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(92, 145)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(92, 145)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.97      0.93        36\n",
      "          1       0.87      0.87      0.87        15\n",
      "          2       0.94      0.89      0.91        18\n",
      "          3       1.00      0.82      0.90        11\n",
      "          4       1.00      1.00      1.00        12\n",
      "\n",
      "avg / total       0.93      0.92      0.92        92\n",
      "\n",
      "[35  1  0  0  0  1 13  1  0  0  2  0 16  0  0  1  1  0  9  0  0  0  0  0\n",
      " 12]\n",
      "svc Accuracy:  0.9239130434782609\n",
      "svc F1:  0.9228571428571428\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      1.00      0.92        36\n",
      "          1       1.00      0.87      0.93        15\n",
      "          2       1.00      0.89      0.94        18\n",
      "          3       1.00      0.82      0.90        11\n",
      "          4       1.00      1.00      1.00        12\n",
      "\n",
      "avg / total       0.94      0.93      0.93        92\n",
      "\n",
      "[36  0  0  0  0  2 13  0  0  0  2  0 16  0  0  2  0  0  9  0  0  0  0  0\n",
      " 12]\n",
      "LR Accuracy:  0.9347826086956522\n",
      "LR F1:  0.9385649644473173\n",
      "For name:  s_bose\n",
      "total sample size before apply threshold:  28\n",
      "Counter({'0000-0001-7310-9881': 16, '0000-0003-2397-4740': 6, '0000-0002-6569-4643': 5, '0000-0003-0137-4322': 1})\n",
      "['0000-0001-7310-9881']\n",
      "Total sample size after apply threshold:  16\n",
      "For name:  j_dyer\n",
      "total sample size before apply threshold:  61\n",
      "Counter({'0000-0002-7220-6062': 44, '0000-0002-3275-8612': 13, '0000-0002-7570-9941': 3, '0000-0001-6215-0053': 1})\n",
      "['0000-0002-3275-8612', '0000-0002-7220-6062']\n",
      "Total sample size after apply threshold:  57\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(57, 134)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(57, 134)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.92      0.96        13\n",
      "          1       0.98      1.00      0.99        44\n",
      "\n",
      "avg / total       0.98      0.98      0.98        57\n",
      "\n",
      "[12  1  0 44]\n",
      "svc Accuracy:  0.9824561403508771\n",
      "svc F1:  0.9743820224719102\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.92      0.96        13\n",
      "          1       0.98      1.00      0.99        44\n",
      "\n",
      "avg / total       0.98      0.98      0.98        57\n",
      "\n",
      "[12  1  0 44]\n",
      "LR Accuracy:  0.9824561403508771\n",
      "LR F1:  0.9743820224719102\n",
      "For name:  f_blanco\n",
      "total sample size before apply threshold:  128\n",
      "Counter({'0000-0003-2545-4319': 55, '0000-0003-4332-434X': 38, '0000-0002-9929-6707': 16, '0000-0002-8380-8472': 14, '0000-0003-1283-8313': 5})\n",
      "['0000-0003-4332-434X', '0000-0002-8380-8472', '0000-0002-9929-6707', '0000-0003-2545-4319']\n",
      "Total sample size after apply threshold:  123\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(123, 1030)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(123, 1030)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.95      0.96        38\n",
      "          1       1.00      1.00      1.00        14\n",
      "          2       1.00      0.81      0.90        16\n",
      "          3       0.93      1.00      0.96        55\n",
      "\n",
      "avg / total       0.96      0.96      0.96       123\n",
      "\n",
      "[36  0  0  2  0 14  0  0  1  0 13  2  0  0  0 55]\n",
      "svc Accuracy:  0.959349593495935\n",
      "svc F1:  0.9553660012099213\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.99        38\n",
      "          1       1.00      0.86      0.92        14\n",
      "          2       1.00      0.81      0.90        16\n",
      "          3       0.93      1.00      0.96        55\n",
      "\n",
      "avg / total       0.96      0.96      0.96       123\n",
      "\n",
      "[38  0  0  0  0 12  0  2  1  0 13  2  0  0  0 55]\n",
      "LR Accuracy:  0.959349593495935\n",
      "LR F1:  0.9428884787323988\n",
      "For name:  s_ferreira\n",
      "total sample size before apply threshold:  70\n",
      "Counter({'0000-0001-7159-2769': 20, '0000-0001-8308-2862': 17, '0000-0001-7486-5056': 10, '0000-0001-6475-5742': 6, '0000-0001-8174-0200': 6, '0000-0002-9969-2507': 2, '0000-0002-2519-8979': 2, '0000-0002-9209-7772': 2, '0000-0001-7469-3186': 2, '0000-0001-7698-6599': 2, '0000-0002-3527-1623': 1})\n",
      "['0000-0001-8308-2862', '0000-0001-7486-5056', '0000-0001-7159-2769']\n",
      "Total sample size after apply threshold:  47\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(47, 93)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(47, 93)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.94      0.97        17\n",
      "          1       1.00      1.00      1.00        10\n",
      "          2       0.95      1.00      0.98        20\n",
      "\n",
      "avg / total       0.98      0.98      0.98        47\n",
      "\n",
      "[16  0  1  0 10  0  0  0 20]\n",
      "svc Accuracy:  0.9787234042553191\n",
      "svc F1:  0.9817689085981769\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        17\n",
      "          1       1.00      1.00      1.00        10\n",
      "          2       1.00      1.00      1.00        20\n",
      "\n",
      "avg / total       1.00      1.00      1.00        47\n",
      "\n",
      "[17  0  0  0 10  0  0  0 20]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  j_ren\n",
      "total sample size before apply threshold:  102\n",
      "Counter({'0000-0002-7978-8093': 38, '0000-0002-4161-1292': 30, '0000-0003-2806-7226': 17, '0000-0001-6116-3194': 6, '0000-0001-7461-0491': 6, '0000-0003-2711-2048': 4, '0000-0002-6905-2824': 1})\n",
      "['0000-0002-7978-8093', '0000-0003-2806-7226', '0000-0002-4161-1292']\n",
      "Total sample size after apply threshold:  85\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(85, 80)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(85, 80)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.89      0.89        38\n",
      "          1       0.71      0.88      0.79        17\n",
      "          2       1.00      0.87      0.93        30\n",
      "\n",
      "avg / total       0.90      0.88      0.89        85\n",
      "\n",
      "[34  4  0  2 15  0  2  2 26]\n",
      "svc Accuracy:  0.8823529411764706\n",
      "svc F1:  0.8709273182957394\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.92      0.86        38\n",
      "          1       0.67      0.59      0.62        17\n",
      "          2       0.96      0.87      0.91        30\n",
      "\n",
      "avg / total       0.84      0.84      0.83        85\n",
      "\n",
      "[35  3  0  6 10  1  2  2 26]\n",
      "LR Accuracy:  0.8352941176470589\n",
      "LR F1:  0.8004927442061945\n",
      "For name:  j_muller\n",
      "total sample size before apply threshold:  113\n",
      "Counter({'0000-0001-6009-7471': 58, '0000-0002-7682-559X': 42, '0000-0002-1046-2968': 12, '0000-0002-0855-3852': 1})\n",
      "['0000-0002-7682-559X', '0000-0002-1046-2968', '0000-0001-6009-7471']\n",
      "Total sample size after apply threshold:  112\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(112, 775)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(112, 775)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.95      0.98        42\n",
      "          1       1.00      0.75      0.86        12\n",
      "          2       0.92      1.00      0.96        58\n",
      "\n",
      "avg / total       0.96      0.96      0.95       112\n",
      "\n",
      "[40  0  2  0  9  3  0  0 58]\n",
      "svc Accuracy:  0.9553571428571429\n",
      "svc F1:  0.9304767663969438\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.95      0.98        42\n",
      "          1       1.00      0.67      0.80        12\n",
      "          2       0.91      1.00      0.95        58\n",
      "\n",
      "avg / total       0.95      0.95      0.94       112\n",
      "\n",
      "[40  0  2  0  8  4  0  0 58]\n",
      "LR Accuracy:  0.9464285714285714\n",
      "LR F1:  0.9088098094095695\n",
      "For name:  h_tanaka\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total sample size before apply threshold:  28\n",
      "Counter({'0000-0002-4378-5747': 21, '0000-0003-1511-8557': 4, '0000-0002-3153-8802': 1, '0000-0002-1760-691X': 1, '0000-0001-8622-7422': 1})\n",
      "['0000-0002-4378-5747']\n",
      "Total sample size after apply threshold:  21\n",
      "For name:  j_pierce\n",
      "total sample size before apply threshold:  39\n",
      "Counter({'0000-0002-7107-4766': 24, '0000-0002-2861-0519': 10, '0000-0002-4241-838X': 3, '0000-0002-4241-993X': 1, '0000-0002-2558-8184': 1})\n",
      "['0000-0002-7107-4766', '0000-0002-2861-0519']\n",
      "Total sample size after apply threshold:  34\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(34, 51)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(34, 51)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        24\n",
      "          1       1.00      1.00      1.00        10\n",
      "\n",
      "avg / total       1.00      1.00      1.00        34\n",
      "\n",
      "[24  0  0 10]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      1.00      0.92        24\n",
      "          1       1.00      0.60      0.75        10\n",
      "\n",
      "avg / total       0.90      0.88      0.87        34\n",
      "\n",
      "[24  0  4  6]\n",
      "LR Accuracy:  0.8823529411764706\n",
      "LR F1:  0.8365384615384615\n",
      "For name:  j_guerrero\n",
      "total sample size before apply threshold:  15\n",
      "Counter({'0000-0001-6729-585X': 10, '0000-0001-5209-2267': 2, '0000-0001-5236-4592': 2, '0000-0003-1442-9302': 1})\n",
      "['0000-0001-6729-585X']\n",
      "Total sample size after apply threshold:  10\n",
      "For name:  r_coelho\n",
      "total sample size before apply threshold:  26\n",
      "Counter({'0000-0001-5790-8616': 12, '0000-0002-1127-1661': 7, '0000-0002-9340-3612': 4, '0000-0003-3813-5157': 3})\n",
      "['0000-0001-5790-8616']\n",
      "Total sample size after apply threshold:  12\n",
      "For name:  a_masi\n",
      "total sample size before apply threshold:  39\n",
      "Counter({'0000-0002-0361-0950': 20, '0000-0002-7163-3978': 17, '0000-0001-9822-9767': 1, '0000-0002-9695-6634': 1})\n",
      "['0000-0002-0361-0950', '0000-0002-7163-3978']\n",
      "Total sample size after apply threshold:  37\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(37, 182)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(37, 182)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        20\n",
      "          1       1.00      1.00      1.00        17\n",
      "\n",
      "avg / total       1.00      1.00      1.00        37\n",
      "\n",
      "[20  0  0 17]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        20\n",
      "          1       1.00      1.00      1.00        17\n",
      "\n",
      "avg / total       1.00      1.00      1.00        37\n",
      "\n",
      "[20  0  0 17]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  b_jackson\n",
      "total sample size before apply threshold:  29\n",
      "Counter({'0000-0002-4917-1199': 14, '0000-0001-6313-0812': 10, '0000-0002-7127-1735': 4, '0000-0001-6405-8111': 1})\n",
      "['0000-0001-6313-0812', '0000-0002-4917-1199']\n",
      "Total sample size after apply threshold:  24\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(24, 70)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(24, 70)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        10\n",
      "          1       1.00      1.00      1.00        14\n",
      "\n",
      "avg / total       1.00      1.00      1.00        24\n",
      "\n",
      "[10  0  0 14]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        10\n",
      "          1       1.00      1.00      1.00        14\n",
      "\n",
      "avg / total       1.00      1.00      1.00        24\n",
      "\n",
      "[10  0  0 14]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  a_jha\n",
      "total sample size before apply threshold:  39\n",
      "Counter({'0000-0002-8061-5085': 11, '0000-0001-9185-2249': 10, '0000-0002-6305-0721': 9, '0000-0002-6852-1641': 8, '0000-0001-9660-4308': 1})\n",
      "['0000-0001-9185-2249', '0000-0002-8061-5085']\n",
      "Total sample size after apply threshold:  21\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(21, 37)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(21, 37)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        10\n",
      "          1       1.00      1.00      1.00        11\n",
      "\n",
      "avg / total       1.00      1.00      1.00        21\n",
      "\n",
      "[10  0  0 11]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        10\n",
      "          1       1.00      1.00      1.00        11\n",
      "\n",
      "avg / total       1.00      1.00      1.00        21\n",
      "\n",
      "[10  0  0 11]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  m_mosquera\n",
      "total sample size before apply threshold:  60\n",
      "Counter({'0000-0003-2248-3050': 25, '0000-0002-5528-0535': 17, '0000-0003-4823-6154': 12, '0000-0002-4632-0195': 6})\n",
      "['0000-0003-4823-6154', '0000-0003-2248-3050', '0000-0002-5528-0535']\n",
      "Total sample size after apply threshold:  54\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(54, 188)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(54, 188)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.67      0.80        12\n",
      "          1       0.83      0.96      0.89        25\n",
      "          2       0.94      0.94      0.94        17\n",
      "\n",
      "avg / total       0.90      0.89      0.89        54\n",
      "\n",
      "[ 8  4  0  0 24  1  0  1 16]\n",
      "svc Accuracy:  0.8888888888888888\n",
      "svc F1:  0.8766884531590415\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.92      0.96        12\n",
      "          1       0.92      0.96      0.94        25\n",
      "          2       0.94      0.94      0.94        17\n",
      "\n",
      "avg / total       0.95      0.94      0.94        54\n",
      "\n",
      "[11  1  0  0 24  1  0  1 16]\n",
      "LR Accuracy:  0.9444444444444444\n",
      "LR F1:  0.9462915601023019\n",
      "For name:  a_silva\n",
      "total sample size before apply threshold:  786\n",
      "Counter({'0000-0003-2861-8286': 158, '0000-0001-5525-0494': 156, '0000-0002-8984-8600': 74, '0000-0001-5790-5116': 41, '0000-0002-7524-9914': 39, '0000-0002-7802-8690': 39, '0000-0003-4968-5138': 30, '0000-0002-7713-1813': 22, '0000-0002-9968-3707': 18, '0000-0002-6332-5182': 16, '0000-0002-5668-7134': 16, '0000-0001-5554-7714': 14, '0000-0002-4839-8279': 14, '0000-0002-1112-1209': 11, '0000-0003-0423-2514': 10, '0000-0002-4386-5851': 10, '0000-0002-9679-8357': 10, '0000-0003-3786-2889': 10, '0000-0002-1673-2164': 10, '0000-0001-7604-792X': 8, '0000-0002-1840-1473': 8, '0000-0003-0393-1655': 7, '0000-0003-4212-5955': 7, '0000-0002-0067-0288': 5, '0000-0002-0634-0546': 5, '0000-0003-2002-4774': 4, '0000-0001-5470-9523': 4, '0000-0002-4364-4979': 4, '0000-0002-5388-1732': 3, '0000-0001-5203-5908': 3, '0000-0001-7231-7021': 3, '0000-0002-5334-0047': 3, '0000-0002-1718-0744': 2, '0000-0003-0384-4447': 2, '0000-0002-2100-7223': 2, '0000-0003-4504-0607': 2, '0000-0003-3576-9023': 2, '0000-0002-3403-5792': 2, '0000-0003-2092-801X': 1, '0000-0002-9595-0038': 1, '0000-0003-4734-6538': 1, '0000-0001-6365-1407': 1, '0000-0002-5842-643X': 1, '0000-0002-8363-0109': 1, '0000-0002-7029-1048': 1, '0000-0002-4904-7470': 1, '0000-0002-3254-2598': 1, '0000-0002-5957-2711': 1, '0000-0002-1724-7777': 1, '0000-0001-6939-8430': 1})\n",
      "['0000-0002-7524-9914', '0000-0002-9968-3707', '0000-0001-5554-7714', '0000-0003-0423-2514', '0000-0002-4386-5851', '0000-0002-9679-8357', '0000-0001-5525-0494', '0000-0003-2861-8286', '0000-0002-8984-8600', '0000-0002-7713-1813', '0000-0003-3786-2889', '0000-0002-1673-2164', '0000-0002-6332-5182', '0000-0003-4968-5138', '0000-0001-5790-5116', '0000-0002-7802-8690', '0000-0002-4839-8279', '0000-0002-1112-1209', '0000-0002-5668-7134']\n",
      "Total sample size after apply threshold:  698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(698, 1345)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(698, 1345)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.87      0.92        39\n",
      "          1       1.00      0.61      0.76        18\n",
      "          2       1.00      0.71      0.83        14\n",
      "          3       1.00      0.80      0.89        10\n",
      "          4       0.70      0.70      0.70        10\n",
      "          5       1.00      0.80      0.89        10\n",
      "          6       0.69      0.96      0.80       156\n",
      "          7       0.86      0.90      0.88       158\n",
      "          8       1.00      0.91      0.95        74\n",
      "          9       1.00      0.86      0.93        22\n",
      "         10       0.60      0.30      0.40        10\n",
      "         11       1.00      0.60      0.75        10\n",
      "         12       1.00      0.81      0.90        16\n",
      "         13       1.00      0.80      0.89        30\n",
      "         14       0.82      0.68      0.75        41\n",
      "         15       0.92      0.85      0.88        39\n",
      "         16       1.00      0.93      0.96        14\n",
      "         17       1.00      0.82      0.90        11\n",
      "         18       1.00      0.56      0.72        16\n",
      "\n",
      "avg / total       0.87      0.85      0.85       698\n",
      "\n",
      "[ 34   0   0   0   0   0   1   4   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0  11   0   0   0   0   7   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0  10   0   0   0   1   3   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   8   0   0   2   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   7   0   2   1   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   8   2   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   1   0 149   1   0   0   0   0\n",
      "   0   0   5   0   0   0   0   0   0   0   0   2   0  11 142   0   0   2\n",
      "   0   0   0   1   0   0   0   0   0   0   0   0   0   0   4   2  67   0\n",
      "   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   2   1   0\n",
      "  19   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   3   3\n",
      "   0   0   3   0   0   0   0   1   0   0   0   0   0   0   0   0   0   2\n",
      "   2   0   0   0   6   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   1   1   0   0   0   0  13   0   0   1   0   0   0   0   0   0   0   0\n",
      "   0   4   2   0   0   0   0   0  24   0   0   0   0   0   1   0   0   0\n",
      "   0   0   9   3   0   0   0   0   0   0  28   0   0   0   0   0   0   0\n",
      "   0   0   0   5   1   0   0   0   0   0   0   0  33   0   0   0   0   0\n",
      "   0   0   0   0   1   0   0   0   0   0   0   0   0   0  13   0   0   0\n",
      "   0   0   0   0   0   2   0   0   0   0   0   0   0   0   0   0   9   0\n",
      "   0   0   0   0   0   0   7   0   0   0   0   0   0   0   0   0   0   0\n",
      "   9]\n",
      "svc Accuracy:  0.8495702005730659\n",
      "svc F1:  0.8258253974625088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gao137/intel/intelpython3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/gao137/intel/intelpython3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.85      0.90        39\n",
      "          1       1.00      0.61      0.76        18\n",
      "          2       0.78      0.50      0.61        14\n",
      "          3       1.00      0.70      0.82        10\n",
      "          4       1.00      0.60      0.75        10\n",
      "          5       1.00      0.70      0.82        10\n",
      "          6       0.64      0.95      0.76       156\n",
      "          7       0.80      0.93      0.86       158\n",
      "          8       1.00      0.89      0.94        74\n",
      "          9       1.00      0.86      0.93        22\n",
      "         10       0.00      0.00      0.00        10\n",
      "         11       1.00      0.40      0.57        10\n",
      "         12       1.00      0.56      0.72        16\n",
      "         13       1.00      0.80      0.89        30\n",
      "         14       0.79      0.63      0.70        41\n",
      "         15       0.97      0.87      0.92        39\n",
      "         16       1.00      0.57      0.73        14\n",
      "         17       1.00      0.82      0.90        11\n",
      "         18       1.00      0.06      0.12        16\n",
      "\n",
      "avg / total       0.84      0.81      0.80       698\n",
      "\n",
      "[ 33   0   1   0   0   0   1   4   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0  11   0   0   0   0   7   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   7   0   0   0   2   5   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   7   0   0   1   2   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   6   0   3   1   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   7   2   1   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0 148   1   0   0   0   0\n",
      "   0   0   7   0   0   0   0   0   0   1   0   0   0  10 147   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   4   3  66   0\n",
      "   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   3   0   0\n",
      "  19   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   4   6\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   4\n",
      "   2   0   0   0   4   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   5   2   0   0   0   0   9   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   5   1   0   0   0   0   0  24   0   0   0   0   0   1   0   0   0\n",
      "   0   0  10   4   0   0   0   0   0   0  26   0   0   0   0   0   0   0\n",
      "   0   0   0   4   1   0   0   0   0   0   0   0  34   0   0   0   0   0\n",
      "   0   0   0   0   3   3   0   0   0   0   0   0   0   0   8   0   0   0\n",
      "   0   0   0   0   0   2   0   0   0   0   0   0   0   0   0   0   9   0\n",
      "   0   0   0   0   0   0  15   0   0   0   0   0   0   0   0   0   0   0\n",
      "   1]\n",
      "LR Accuracy:  0.8108882521489972\n",
      "LR F1:  0.7214802932333032\n",
      "For name:  m_guerra\n",
      "total sample size before apply threshold:  18\n",
      "Counter({'0000-0001-6286-4048': 8, '0000-0003-1970-7439': 4, '0000-0002-3655-9004': 3, '0000-0003-3863-8520': 3})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  h_suzuki\n",
      "total sample size before apply threshold:  82\n",
      "Counter({'0000-0003-4682-5086': 39, '0000-0002-8150-140X': 15, '0000-0003-4600-2506': 14, '0000-0002-8555-5448': 9, '0000-0001-5371-6385': 5})\n",
      "['0000-0003-4682-5086', '0000-0003-4600-2506', '0000-0002-8150-140X']\n",
      "Total sample size after apply threshold:  68\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(68, 199)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(68, 199)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.95      0.92        39\n",
      "          1       0.92      0.86      0.89        14\n",
      "          2       0.93      0.87      0.90        15\n",
      "\n",
      "avg / total       0.91      0.91      0.91        68\n",
      "\n",
      "[37  1  1  2 12  0  2  0 13]\n",
      "svc Accuracy:  0.9117647058823529\n",
      "svc F1:  0.9034802043422733\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.97      0.90        39\n",
      "          1       1.00      0.64      0.78        14\n",
      "          2       0.93      0.87      0.90        15\n",
      "\n",
      "avg / total       0.90      0.88      0.88        68\n",
      "\n",
      "[38  0  1  5  9  0  2  0 13]\n",
      "LR Accuracy:  0.8823529411764706\n",
      "LR F1:  0.8613074415173365\n",
      "For name:  m_cohen\n",
      "total sample size before apply threshold:  251\n",
      "Counter({'0000-0003-2038-6070': 103, '0000-0002-1879-3593': 69, '0000-0002-6090-2394': 46, '0000-0001-6731-4053': 13, '0000-0003-3183-2558': 8, '0000-0002-1548-2773': 4, '0000-0001-6362-6148': 4, '0000-0002-5876-6565': 3, '0000-0002-1372-680X': 1})\n",
      "['0000-0001-6731-4053', '0000-0003-2038-6070', '0000-0002-6090-2394', '0000-0002-1879-3593']\n",
      "Total sample size after apply threshold:  231\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(231, 865)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(231, 865)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.23      0.38        13\n",
      "          1       0.99      0.83      0.90       103\n",
      "          2       0.94      0.65      0.77        46\n",
      "          3       0.63      1.00      0.77        69\n",
      "\n",
      "avg / total       0.87      0.81      0.81       231\n",
      "\n",
      "[ 3  0  0 10  0 85  2 16  0  1 30 15  0  0  0 69]\n",
      "svc Accuracy:  0.8095238095238095\n",
      "svc F1:  0.7036628473430149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gao137/intel/intelpython3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/gao137/intel/intelpython3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00        13\n",
      "          1       0.96      0.93      0.95       103\n",
      "          2       0.97      0.63      0.76        46\n",
      "          3       0.68      1.00      0.81        69\n",
      "\n",
      "avg / total       0.82      0.84      0.82       231\n",
      "\n",
      "[ 0  0  0 13  0 96  1  6  0  4 29 13  0  0  0 69]\n",
      "LR Accuracy:  0.8398268398268398\n",
      "LR F1:  0.6301838521252421\n",
      "For name:  m_kobayashi\n",
      "total sample size before apply threshold:  51\n",
      "Counter({'0000-0002-6657-1928': 33, '0000-0003-0219-9108': 9, '0000-0002-6554-8400': 4, '0000-0001-8116-0505': 2, '0000-0002-4001-3581': 2, '0000-0001-6539-7326': 1})\n",
      "['0000-0002-6657-1928']\n",
      "Total sample size after apply threshold:  33\n",
      "For name:  s_wright\n",
      "total sample size before apply threshold:  61\n",
      "Counter({'0000-0001-9973-9697': 44, '0000-0002-8593-6056': 10, '0000-0002-1502-131X': 5, '0000-0003-1034-8054': 2})\n",
      "['0000-0001-9973-9697', '0000-0002-8593-6056']\n",
      "Total sample size after apply threshold:  54\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(54, 208)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(54, 208)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        44\n",
      "          1       1.00      1.00      1.00        10\n",
      "\n",
      "avg / total       1.00      1.00      1.00        54\n",
      "\n",
      "[44  0  0 10]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        44\n",
      "          1       1.00      1.00      1.00        10\n",
      "\n",
      "avg / total       1.00      1.00      1.00        54\n",
      "\n",
      "[44  0  0 10]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  a_mills\n",
      "total sample size before apply threshold:  169\n",
      "Counter({'0000-0001-9863-9950': 115, '0000-0003-4880-7332': 34, '0000-0002-6997-5581': 15, '0000-0002-6893-3857': 3, '0000-0003-4932-8413': 1, '0000-0002-9065-0458': 1})\n",
      "['0000-0001-9863-9950', '0000-0002-6997-5581', '0000-0003-4880-7332']\n",
      "Total sample size after apply threshold:  164\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(164, 555)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(164, 555)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.98      0.91       115\n",
      "          1       1.00      0.40      0.57        15\n",
      "          2       0.88      0.68      0.77        34\n",
      "\n",
      "avg / total       0.88      0.87      0.85       164\n",
      "\n",
      "[113   0   2   8   6   1  11   0  23]\n",
      "svc Accuracy:  0.8658536585365854\n",
      "svc F1:  0.7510249983934195\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      1.00      0.89       115\n",
      "          1       1.00      0.33      0.50        15\n",
      "          2       0.94      0.44      0.60        34\n",
      "\n",
      "avg / total       0.85      0.82      0.80       164\n",
      "\n",
      "[115   0   0   9   5   1  19   0  15]\n",
      "LR Accuracy:  0.823170731707317\n",
      "LR F1:  0.6638242894056847\n",
      "For name:  c_west\n",
      "total sample size before apply threshold:  181\n",
      "Counter({'0000-0002-0839-3449': 155, '0000-0001-7595-6777': 20, '0000-0001-7649-9600': 3, '0000-0002-1149-3723': 2, '0000-0002-3799-4462': 1})\n",
      "['0000-0002-0839-3449', '0000-0001-7595-6777']\n",
      "Total sample size after apply threshold:  175\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(175, 515)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(175, 515)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.99       155\n",
      "          1       1.00      0.80      0.89        20\n",
      "\n",
      "avg / total       0.98      0.98      0.98       175\n",
      "\n",
      "[155   0   4  16]\n",
      "svc Accuracy:  0.9771428571428571\n",
      "svc F1:  0.9380750176928521\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98       155\n",
      "          1       1.00      0.65      0.79        20\n",
      "\n",
      "avg / total       0.96      0.96      0.96       175\n",
      "\n",
      "[155   0   7  13]\n",
      "LR Accuracy:  0.96\n",
      "LR F1:  0.8828983844756716\n",
      "For name:  a_marino\n",
      "total sample size before apply threshold:  15\n",
      "Counter({'0000-0002-1709-538X': 7, '0000-0002-0528-4925': 6, '0000-0003-0308-859X': 1, '0000-0001-8751-8811': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  r_jiang\n",
      "total sample size before apply threshold:  102\n",
      "Counter({'0000-0002-8280-6029': 54, '0000-0002-7533-3753': 28, '0000-0002-3816-4639': 19, '0000-0001-5857-8540': 1})\n",
      "['0000-0002-8280-6029', '0000-0002-3816-4639', '0000-0002-7533-3753']\n",
      "Total sample size after apply threshold:  101\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(101, 239)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(101, 239)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.91      0.88        54\n",
      "          1       0.93      0.68      0.79        19\n",
      "          2       0.80      0.86      0.83        28\n",
      "\n",
      "avg / total       0.86      0.85      0.85       101\n",
      "\n",
      "[49  0  5  5 13  1  3  1 24]\n",
      "svc Accuracy:  0.8514851485148515\n",
      "svc F1:  0.8327826258860741\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.91      0.90        54\n",
      "          1       0.87      0.68      0.76        19\n",
      "          2       0.81      0.89      0.85        28\n",
      "\n",
      "avg / total       0.86      0.86      0.86       101\n",
      "\n",
      "[49  0  5  5 13  1  1  2 25]\n",
      "LR Accuracy:  0.8613861386138614\n",
      "LR F1:  0.8370820260929749\n",
      "For name:  t_becker\n",
      "total sample size before apply threshold:  21\n",
      "Counter({'0000-0002-4117-8249': 12, '0000-0002-5656-4564': 5, '0000-0003-3432-783X': 3, '0000-0002-5193-4044': 1})\n",
      "['0000-0002-4117-8249']\n",
      "Total sample size after apply threshold:  12\n",
      "For name:  s_pedersen\n",
      "total sample size before apply threshold:  322\n",
      "Counter({'0000-0002-7838-8063': 166, '0000-0002-3044-7714': 80, '0000-0002-6500-9263': 40, '0000-0002-4786-6464': 21, '0000-0001-8055-3251': 11, '0000-0002-8566-7693': 1, '0000-0002-4355-1764': 1, '0000-0002-3822-5075': 1, '0000-0001-8017-4227': 1})\n",
      "['0000-0002-7838-8063', '0000-0002-3044-7714', '0000-0002-4786-6464', '0000-0001-8055-3251', '0000-0002-6500-9263']\n",
      "Total sample size after apply threshold:  318\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(318, 547)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(318, 547)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.98      0.99       166\n",
      "          1       0.81      0.99      0.89        80\n",
      "          2       0.93      0.62      0.74        21\n",
      "          3       1.00      0.73      0.84        11\n",
      "          4       1.00      0.93      0.96        40\n",
      "\n",
      "avg / total       0.95      0.94      0.94       318\n",
      "\n",
      "[162   4   0   0   0   0  79   1   0   0   0   8  13   0   0   0   3   0\n",
      "   8   0   0   3   0   0  37]\n",
      "svc Accuracy:  0.940251572327044\n",
      "svc F1:  0.8852923224668835\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.98      0.98       166\n",
      "          1       0.83      0.99      0.90        80\n",
      "          2       0.93      0.67      0.78        21\n",
      "          3       1.00      0.64      0.78        11\n",
      "          4       1.00      0.95      0.97        40\n",
      "\n",
      "avg / total       0.95      0.94      0.94       318\n",
      "\n",
      "[162   4   0   0   0   0  79   1   0   0   0   7  14   0   0   1   3   0\n",
      "   7   0   0   2   0   0  38]\n",
      "LR Accuracy:  0.9433962264150944\n",
      "LR F1:  0.8835148208765229\n",
      "For name:  a_ali\n",
      "total sample size before apply threshold:  62\n",
      "Counter({'0000-0003-0437-8919': 15, '0000-0002-7224-6654': 11, '0000-0001-7913-8544': 9, '0000-0002-4370-007X': 6, '0000-0003-2444-8400': 6, '0000-0003-0552-7322': 6, '0000-0001-5267-2608': 2, '0000-0001-9966-2917': 1, '0000-0001-9673-0080': 1, '0000-0001-6199-0034': 1, '0000-0002-7864-8240': 1, '0000-0003-3030-3371': 1, '0000-0003-4467-5387': 1, '0000-0002-6554-3378': 1})\n",
      "['0000-0003-0437-8919', '0000-0002-7224-6654']\n",
      "Total sample size after apply threshold:  26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(26, 46)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(26, 46)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.93      0.93        15\n",
      "          1       0.91      0.91      0.91        11\n",
      "\n",
      "avg / total       0.92      0.92      0.92        26\n",
      "\n",
      "[14  1  1 10]\n",
      "svc Accuracy:  0.9230769230769231\n",
      "svc F1:  0.9212121212121211\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97        15\n",
      "          1       1.00      0.91      0.95        11\n",
      "\n",
      "avg / total       0.96      0.96      0.96        26\n",
      "\n",
      "[15  0  1 10]\n",
      "LR Accuracy:  0.9615384615384616\n",
      "LR F1:  0.9600614439324117\n",
      "For name:  k_jones\n",
      "total sample size before apply threshold:  607\n",
      "Counter({'0000-0001-7108-9776': 331, '0000-0002-0294-0851': 74, '0000-0001-8923-2999': 55, '0000-0001-8398-2190': 32, '0000-0003-4764-7031': 29, '0000-0002-7380-9797': 18, '0000-0001-9136-0877': 15, '0000-0002-7216-2506': 13, '0000-0003-3815-5713': 9, '0000-0002-8819-8992': 6, '0000-0002-7127-1612': 4, '0000-0002-0242-7097': 4, '0000-0002-6916-8640': 4, '0000-0001-5692-653X': 3, '0000-0002-9982-8742': 3, '0000-0002-0478-8021': 2, '0000-0001-9373-0982': 1, '0000-0001-7335-1379': 1, '0000-0001-6553-8897': 1, '0000-0002-1552-7847': 1, '0000-0001-9115-4192': 1})\n",
      "['0000-0002-7380-9797', '0000-0001-8923-2999', '0000-0002-7216-2506', '0000-0003-4764-7031', '0000-0001-7108-9776', '0000-0001-9136-0877', '0000-0002-0294-0851', '0000-0001-8398-2190']\n",
      "Total sample size after apply threshold:  567\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(567, 928)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(567, 928)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.94      0.97        18\n",
      "          1       1.00      0.78      0.88        55\n",
      "          2       0.83      0.77      0.80        13\n",
      "          3       1.00      0.55      0.71        29\n",
      "          4       0.87      0.99      0.93       331\n",
      "          5       1.00      1.00      1.00        15\n",
      "          6       0.97      0.80      0.87        74\n",
      "          7       1.00      0.75      0.86        32\n",
      "\n",
      "avg / total       0.91      0.90      0.90       567\n",
      "\n",
      "[ 17   0   0   0   1   0   0   0   0  43   0   0  12   0   0   0   0   0\n",
      "  10   0   3   0   0   0   0   0   0  16  12   0   1   0   0   0   1   0\n",
      " 329   0   1   0   0   0   0   0   0  15   0   0   0   0   0   0  15   0\n",
      "  59   0   0   0   1   0   7   0   0  24]\n",
      "svc Accuracy:  0.9047619047619048\n",
      "svc F1:  0.8772585246931324\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.89      0.94        18\n",
      "          1       1.00      0.73      0.84        55\n",
      "          2       1.00      0.38      0.56        13\n",
      "          3       1.00      0.45      0.62        29\n",
      "          4       0.82      1.00      0.90       331\n",
      "          5       1.00      1.00      1.00        15\n",
      "          6       0.98      0.74      0.85        74\n",
      "          7       1.00      0.53      0.69        32\n",
      "\n",
      "avg / total       0.89      0.87      0.86       567\n",
      "\n",
      "[ 16   0   0   0   2   0   0   0   0  40   0   0  15   0   0   0   0   0\n",
      "   5   0   8   0   0   0   0   0   0  13  15   0   1   0   0   0   0   0\n",
      " 331   0   0   0   0   0   0   0   0  15   0   0   0   0   0   0  19   0\n",
      "  55   0   0   0   0   0  15   0   0  17]\n",
      "LR Accuracy:  0.8677248677248677\n",
      "LR F1:  0.7996716034078362\n",
      "For name:  m_becker\n",
      "total sample size before apply threshold:  67\n",
      "Counter({'0000-0001-9890-8815': 32, '0000-0002-9235-0547': 24, '0000-0001-7233-6361': 3, '0000-0003-1187-1699': 3, '0000-0001-6526-1525': 2, '0000-0003-3450-5579': 2, '0000-0002-1751-1056': 1})\n",
      "['0000-0001-9890-8815', '0000-0002-9235-0547']\n",
      "Total sample size after apply threshold:  56\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(56, 240)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(56, 240)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      1.00      0.94        32\n",
      "          1       1.00      0.83      0.91        24\n",
      "\n",
      "avg / total       0.94      0.93      0.93        56\n",
      "\n",
      "[32  0  4 20]\n",
      "svc Accuracy:  0.9285714285714286\n",
      "svc F1:  0.9251336898395721\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97        32\n",
      "          1       1.00      0.92      0.96        24\n",
      "\n",
      "avg / total       0.97      0.96      0.96        56\n",
      "\n",
      "[32  0  2 22]\n",
      "LR Accuracy:  0.9642857142857143\n",
      "LR F1:  0.9631093544137023\n",
      "For name:  c_marshall\n",
      "total sample size before apply threshold:  106\n",
      "Counter({'0000-0003-4186-0368': 32, '0000-0002-7571-5700': 29, '0000-0001-5901-2004': 28, '0000-0002-1285-7648': 6, '0000-0002-8227-2354': 6, '0000-0001-6669-3231': 3, '0000-0002-7397-6472': 1, '0000-0002-0592-7716': 1})\n",
      "['0000-0001-5901-2004', '0000-0003-4186-0368', '0000-0002-7571-5700']\n",
      "Total sample size after apply threshold:  89\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(89, 255)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(89, 255)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.86      0.92        28\n",
      "          1       0.89      1.00      0.94        32\n",
      "          2       1.00      1.00      1.00        29\n",
      "\n",
      "avg / total       0.96      0.96      0.95        89\n",
      "\n",
      "[24  4  0  0 32  0  0  0 29]\n",
      "svc Accuracy:  0.9550561797752809\n",
      "svc F1:  0.9547511312217195\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.82      0.90        28\n",
      "          1       0.86      1.00      0.93        32\n",
      "          2       1.00      1.00      1.00        29\n",
      "\n",
      "avg / total       0.95      0.94      0.94        89\n",
      "\n",
      "[23  5  0  0 32  0  0  0 29]\n",
      "LR Accuracy:  0.9438202247191011\n",
      "LR F1:  0.9431656720659278\n",
      "For name:  s_rafiq\n",
      "total sample size before apply threshold:  33\n",
      "Counter({'0000-0003-4873-4540': 23, '0000-0002-9295-3065': 9, '0000-0003-4821-5783': 1})\n",
      "['0000-0003-4873-4540']\n",
      "Total sample size after apply threshold:  23\n",
      "For name:  h_liang\n",
      "total sample size before apply threshold:  104\n",
      "Counter({'0000-0003-0186-3126': 30, '0000-0001-7633-286X': 27, '0000-0002-3430-9167': 15, '0000-0001-9097-7357': 12, '0000-0001-9496-406X': 10, '0000-0001-5523-6799': 3, '0000-0001-9044-0509': 3, '0000-0002-2950-8559': 2, '0000-0003-1779-9552': 1, '0000-0002-9045-9717': 1})\n",
      "['0000-0002-3430-9167', '0000-0001-7633-286X', '0000-0003-0186-3126', '0000-0001-9496-406X', '0000-0001-9097-7357']\n",
      "Total sample size after apply threshold:  94\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(94, 168)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(94, 168)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.80      0.89        15\n",
      "          1       0.74      0.96      0.84        27\n",
      "          2       0.94      0.97      0.95        30\n",
      "          3       1.00      0.50      0.67        10\n",
      "          4       1.00      0.92      0.96        12\n",
      "\n",
      "avg / total       0.91      0.88      0.88        94\n",
      "\n",
      "[12  3  0  0  0  0 26  1  0  0  0  1 29  0  0  0  5  0  5  0  0  0  1  0\n",
      " 11]\n",
      "svc Accuracy:  0.8829787234042553\n",
      "svc F1:  0.8603213288472986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.73      0.85        15\n",
      "          1       0.71      0.93      0.81        27\n",
      "          2       0.91      0.97      0.94        30\n",
      "          3       1.00      0.50      0.67        10\n",
      "          4       1.00      0.92      0.96        12\n",
      "\n",
      "avg / total       0.89      0.86      0.86        94\n",
      "\n",
      "[11  4  0  0  0  0 25  2  0  0  0  1 29  0  0  0  5  0  5  0  0  0  1  0\n",
      " 11]\n",
      "LR Accuracy:  0.8617021276595744\n",
      "LR F1:  0.8422555471643831\n",
      "For name:  c_davis\n",
      "total sample size before apply threshold:  43\n",
      "Counter({'0000-0002-5045-0507': 34, '0000-0002-3971-3505': 2, '0000-0003-0866-7822': 2, '0000-0002-0024-2742': 2, '0000-0002-3274-5707': 2, '0000-0001-6205-9719': 1})\n",
      "['0000-0002-5045-0507']\n",
      "Total sample size after apply threshold:  34\n",
      "For name:  e_hall\n",
      "total sample size before apply threshold:  115\n",
      "Counter({'0000-0001-5999-5020': 49, '0000-0002-5306-082X': 34, '0000-0002-9477-8619': 24, '0000-0002-9206-4436': 4, '0000-0002-2815-6651': 2, '0000-0003-0244-7458': 2})\n",
      "['0000-0001-5999-5020', '0000-0002-9477-8619', '0000-0002-5306-082X']\n",
      "Total sample size after apply threshold:  107\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(107, 514)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(107, 514)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.96      0.93        49\n",
      "          1       0.90      0.79      0.84        24\n",
      "          2       0.91      0.91      0.91        34\n",
      "\n",
      "avg / total       0.91      0.91      0.91       107\n",
      "\n",
      "[47  1  1  3 19  2  2  1 31]\n",
      "svc Accuracy:  0.9065420560747663\n",
      "svc F1:  0.8956340732112427\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.98      0.94        49\n",
      "          1       0.91      0.83      0.87        24\n",
      "          2       0.97      0.91      0.94        34\n",
      "\n",
      "avg / total       0.93      0.93      0.92       107\n",
      "\n",
      "[48  1  0  3 20  1  2  1 31]\n",
      "LR Accuracy:  0.9252336448598131\n",
      "LR F1:  0.9167118757911598\n",
      "For name:  g_volpe\n",
      "total sample size before apply threshold:  31\n",
      "Counter({'0000-0001-9993-5348': 15, '0000-0001-5057-1846': 14, '0000-0002-3916-5393': 1, '0000-0003-0760-4627': 1})\n",
      "['0000-0001-9993-5348', '0000-0001-5057-1846']\n",
      "Total sample size after apply threshold:  29\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(29, 69)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(29, 69)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.87      0.93        15\n",
      "          1       0.88      1.00      0.93        14\n",
      "\n",
      "avg / total       0.94      0.93      0.93        29\n",
      "\n",
      "[13  2  0 14]\n",
      "svc Accuracy:  0.9310344827586207\n",
      "svc F1:  0.930952380952381\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.93      0.97        15\n",
      "          1       0.93      1.00      0.97        14\n",
      "\n",
      "avg / total       0.97      0.97      0.97        29\n",
      "\n",
      "[14  1  0 14]\n",
      "LR Accuracy:  0.9655172413793104\n",
      "LR F1:  0.9655172413793104\n",
      "For name:  r_lewis\n",
      "total sample size before apply threshold:  427\n",
      "Counter({'0000-0003-3470-923X': 185, '0000-0002-2002-4339': 175, '0000-0003-4044-9104': 41, '0000-0002-4598-7553': 7, '0000-0003-1395-3276': 6, '0000-0003-1859-0021': 4, '0000-0001-9929-2629': 3, '0000-0001-6642-5771': 3, '0000-0002-2680-6235': 1, '0000-0002-6644-6385': 1, '0000-0003-1046-811X': 1})\n",
      "['0000-0002-2002-4339', '0000-0003-3470-923X', '0000-0003-4044-9104']\n",
      "Total sample size after apply threshold:  401\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(401, 900)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(401, 900)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.91      0.95       175\n",
      "          1       0.87      0.99      0.93       185\n",
      "          2       0.97      0.71      0.82        41\n",
      "\n",
      "avg / total       0.94      0.93      0.93       401\n",
      "\n",
      "[159  16   0   0 184   1   0  12  29]\n",
      "svc Accuracy:  0.9276807980049875\n",
      "svc F1:  0.8986497859639574\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.92      0.96       175\n",
      "          1       0.88      1.00      0.94       185\n",
      "          2       1.00      0.73      0.85        41\n",
      "\n",
      "avg / total       0.95      0.94      0.94       401\n",
      "\n",
      "[161  14   0   0 185   0   0  11  30]\n",
      "LR Accuracy:  0.9376558603491272\n",
      "LR F1:  0.9133708722093461\n",
      "For name:  c_rodriguez\n",
      "total sample size before apply threshold:  43\n",
      "Counter({'0000-0001-6697-1692': 23, '0000-0002-4042-4313': 18, '0000-0003-2289-4239': 1, '0000-0003-3927-6883': 1})\n",
      "['0000-0002-4042-4313', '0000-0001-6697-1692']\n",
      "Total sample size after apply threshold:  41\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(41, 93)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(41, 93)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.89      0.94        18\n",
      "          1       0.92      1.00      0.96        23\n",
      "\n",
      "avg / total       0.96      0.95      0.95        41\n",
      "\n",
      "[16  2  0 23]\n",
      "svc Accuracy:  0.9512195121951219\n",
      "svc F1:  0.9497549019607843\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        18\n",
      "          1       1.00      1.00      1.00        23\n",
      "\n",
      "avg / total       1.00      1.00      1.00        41\n",
      "\n",
      "[18  0  0 23]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  p_hall\n",
      "total sample size before apply threshold:  22\n",
      "Counter({'0000-0001-6015-7841': 11, '0000-0001-9218-6233': 9, '0000-0002-4239-4226': 1, '0000-0002-8214-0351': 1})\n",
      "['0000-0001-6015-7841']\n",
      "Total sample size after apply threshold:  11\n",
      "For name:  r_srivastava\n",
      "total sample size before apply threshold:  184\n",
      "Counter({'0000-0002-0065-4069': 144, '0000-0003-3112-4252': 22, '0000-0002-6703-9642': 7, '0000-0001-9328-146X': 6, '0000-0002-0165-1556': 3, '0000-0002-9965-851X': 2})\n",
      "['0000-0002-0065-4069', '0000-0003-3112-4252']\n",
      "Total sample size after apply threshold:  166\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(166, 508)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(166, 508)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.97      0.91       144\n",
      "          1       0.17      0.05      0.07        22\n",
      "\n",
      "avg / total       0.78      0.84      0.80       166\n",
      "\n",
      "[139   5  21   1]\n",
      "svc Accuracy:  0.8433734939759037\n",
      "svc F1:  0.49295112781954886\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      1.00      0.93       144\n",
      "          1       0.00      0.00      0.00        22\n",
      "\n",
      "avg / total       0.75      0.87      0.81       166\n",
      "\n",
      "[144   0  22   0]\n",
      "LR Accuracy:  0.8674698795180723\n",
      "LR F1:  0.46451612903225803\n",
      "For name:  a_macedo\n",
      "total sample size before apply threshold:  29\n",
      "Counter({'0000-0002-2613-4838': 18, '0000-0003-3436-2010': 8, '0000-0002-6854-9855': 2, '0000-0001-6985-4520': 1})\n",
      "['0000-0002-2613-4838']\n",
      "Total sample size after apply threshold:  18\n",
      "For name:  m_schultz\n",
      "total sample size before apply threshold:  40\n",
      "Counter({'0000-0003-3458-1811': 16, '0000-0002-7689-6531': 16, '0000-0003-3455-774X': 4, '0000-0001-7967-5147': 4})\n",
      "['0000-0003-3458-1811', '0000-0002-7689-6531']\n",
      "Total sample size after apply threshold:  32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gao137/intel/intelpython3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/gao137/intel/intelpython3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(32, 161)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(32, 161)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        16\n",
      "          1       1.00      1.00      1.00        16\n",
      "\n",
      "avg / total       1.00      1.00      1.00        32\n",
      "\n",
      "[16  0  0 16]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        16\n",
      "          1       1.00      1.00      1.00        16\n",
      "\n",
      "avg / total       1.00      1.00      1.00        32\n",
      "\n",
      "[16  0  0 16]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  s_jacobs\n",
      "total sample size before apply threshold:  21\n",
      "Counter({'0000-0002-6199-5748': 9, '0000-0002-9959-5627': 8, '0000-0003-4674-4817': 2, '0000-0002-9382-1646': 1, '0000-0002-8103-1700': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  c_hong\n",
      "total sample size before apply threshold:  32\n",
      "Counter({'0000-0002-1058-3073': 23, '0000-0001-7745-9205': 7, '0000-0002-5118-620X': 1, '0000-0002-7397-1671': 1})\n",
      "['0000-0002-1058-3073']\n",
      "Total sample size after apply threshold:  23\n",
      "For name:  r_mohan\n",
      "total sample size before apply threshold:  7\n",
      "Counter({'0000-0001-5335-6631': 4, '0000-0002-1857-4200': 1, '0000-0002-2286-7081': 1, '0000-0002-9943-484X': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  r_hill\n",
      "total sample size before apply threshold:  90\n",
      "Counter({'0000-0003-0394-6048': 16, '0000-0002-2801-0505': 14, '0000-0002-7601-5802': 13, '0000-0002-4623-1563': 12, '0000-0001-6080-8712': 11, '0000-0001-9577-1622': 9, '0000-0002-1923-5673': 7, '0000-0001-7996-7887': 7, '0000-0001-5533-1139': 1})\n",
      "['0000-0002-2801-0505', '0000-0003-0394-6048', '0000-0001-6080-8712', '0000-0002-7601-5802', '0000-0002-4623-1563']\n",
      "Total sample size after apply threshold:  66\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(66, 207)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(66, 207)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.93      0.96        14\n",
      "          1       1.00      0.88      0.93        16\n",
      "          2       1.00      0.91      0.95        11\n",
      "          3       0.65      1.00      0.79        13\n",
      "          4       1.00      0.75      0.86        12\n",
      "\n",
      "avg / total       0.93      0.89      0.90        66\n",
      "\n",
      "[13  0  0  1  0  0 14  0  2  0  0  0 10  1  0  0  0  0 13  0  0  0  0  3\n",
      "  9]\n",
      "svc Accuracy:  0.8939393939393939\n",
      "svc F1:  0.8987397787397787\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.93      0.96        14\n",
      "          1       1.00      0.88      0.93        16\n",
      "          2       0.91      0.91      0.91        11\n",
      "          3       0.68      1.00      0.81        13\n",
      "          4       1.00      0.75      0.86        12\n",
      "\n",
      "avg / total       0.92      0.89      0.90        66\n",
      "\n",
      "[13  0  0  1  0  0 14  1  1  0  0  0 10  1  0  0  0  0 13  0  0  0  0  3\n",
      "  9]\n",
      "LR Accuracy:  0.8939393939393939\n",
      "LR F1:  0.8950060125060124\n",
      "For name:  q_shen\n",
      "total sample size before apply threshold:  57\n",
      "Counter({'0000-0002-1491-5434': 36, '0000-0001-5579-036X': 9, '0000-0003-0968-8051': 7, '0000-0002-4621-4659': 3, '0000-0002-3111-2019': 1, '0000-0001-8767-6852': 1})\n",
      "['0000-0002-1491-5434']\n",
      "Total sample size after apply threshold:  36\n",
      "For name:  l_schmidt\n",
      "total sample size before apply threshold:  13\n",
      "Counter({'0000-0002-3206-6659': 6, '0000-0002-9518-1734': 5, '0000-0001-7565-1455': 1, '0000-0002-3472-4635': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  s_qin\n",
      "total sample size before apply threshold:  42\n",
      "Counter({'0000-0002-3323-8846': 30, '0000-0002-8127-4753': 7, '0000-0001-9437-6292': 4, '0000-0002-3591-4959': 1})\n",
      "['0000-0002-3323-8846']\n",
      "Total sample size after apply threshold:  30\n",
      "For name:  a_fabbri\n",
      "total sample size before apply threshold:  64\n",
      "Counter({'0000-0003-2603-9715': 53, '0000-0003-0097-6348': 5, '0000-0003-2340-9338': 4, '0000-0002-3520-2417': 2})\n",
      "['0000-0003-2603-9715']\n",
      "Total sample size after apply threshold:  53\n",
      "For name:  l_robinson\n",
      "total sample size before apply threshold:  93\n",
      "Counter({'0000-0003-0209-2503': 61, '0000-0002-9016-648X': 13, '0000-0001-6811-0140': 8, '0000-0003-1972-4204': 6, '0000-0001-9287-6082': 3, '0000-0001-9544-5923': 1, '0000-0002-2236-0651': 1})\n",
      "['0000-0002-9016-648X', '0000-0003-0209-2503']\n",
      "Total sample size after apply threshold:  74\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(74, 224)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(74, 224)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.54      0.70        13\n",
      "          1       0.91      1.00      0.95        61\n",
      "\n",
      "avg / total       0.93      0.92      0.91        74\n",
      "\n",
      "[ 7  6  0 61]\n",
      "svc Accuracy:  0.918918918918919\n",
      "svc F1:  0.8265625000000001\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.31      0.47        13\n",
      "          1       0.87      1.00      0.93        61\n",
      "\n",
      "avg / total       0.89      0.88      0.85        74\n",
      "\n",
      "[ 4  9  0 61]\n",
      "LR Accuracy:  0.8783783783783784\n",
      "LR F1:  0.7009429726088909\n",
      "For name:  r_gross\n",
      "total sample size before apply threshold:  71\n",
      "Counter({'0000-0001-5884-3607': 38, '0000-0003-4524-7552': 23, '0000-0003-0311-3003': 10})\n",
      "['0000-0003-4524-7552', '0000-0001-5884-3607', '0000-0003-0311-3003']\n",
      "Total sample size after apply threshold:  71\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(71, 322)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(71, 322)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.91      0.95        23\n",
      "          1       0.95      0.92      0.93        38\n",
      "          2       0.62      0.80      0.70        10\n",
      "\n",
      "avg / total       0.92      0.90      0.91        71\n",
      "\n",
      "[21  0  2  0 35  3  0  2  8]\n",
      "svc Accuracy:  0.9014084507042254\n",
      "svc F1:  0.8611769872639438\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.83      0.90        23\n",
      "          1       0.83      1.00      0.90        38\n",
      "          2       1.00      0.60      0.75        10\n",
      "\n",
      "avg / total       0.91      0.89      0.88        71\n",
      "\n",
      "[19  4  0  0 38  0  0  4  6]\n",
      "LR Accuracy:  0.8873239436619719\n",
      "LR F1:  0.8531746031746031\n",
      "For name:  j_ahn\n",
      "total sample size before apply threshold:  130\n",
      "Counter({'0000-0002-8135-7719': 69, '0000-0002-0177-0192': 26, '0000-0001-9341-009X': 14, '0000-0002-0394-9217': 6, '0000-0001-6928-4038': 4, '0000-0001-5097-2316': 3, '0000-0002-1431-6351': 3, '0000-0002-1050-8575': 1, '0000-0003-1733-1394': 1, '0000-0003-1807-035X': 1, '0000-0003-3625-9906': 1, '0000-0002-4530-0512': 1})\n",
      "['0000-0002-0177-0192', '0000-0001-9341-009X', '0000-0002-8135-7719']\n",
      "Total sample size after apply threshold:  109\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(109, 274)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(109, 274)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.73      0.76        26\n",
      "          1       0.69      0.64      0.67        14\n",
      "          2       0.88      0.91      0.89        69\n",
      "\n",
      "avg / total       0.83      0.83      0.83       109\n",
      "\n",
      "[19  2  5  1  9  4  4  2 63]\n",
      "svc Accuracy:  0.8348623853211009\n",
      "svc F1:  0.7734278959810874\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.69      0.72        26\n",
      "          1       1.00      0.57      0.73        14\n",
      "          2       0.84      0.94      0.89        69\n",
      "\n",
      "avg / total       0.84      0.83      0.83       109\n",
      "\n",
      "[18  0  8  2  8  4  4  0 65]\n",
      "LR Accuracy:  0.8348623853211009\n",
      "LR F1:  0.779227895392279\n",
      "For name:  j_john\n",
      "total sample size before apply threshold:  43\n",
      "Counter({'0000-0003-3654-5099': 16, '0000-0001-8452-0676': 11, '0000-0001-6831-6501': 5, '0000-0002-6411-8927': 4, '0000-0002-6636-3440': 3, '0000-0003-3343-8677': 2, '0000-0003-2696-277X': 1, '0000-0003-2551-2320': 1})\n",
      "['0000-0003-3654-5099', '0000-0001-8452-0676']\n",
      "Total sample size after apply threshold:  27\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(27, 114)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(27, 114)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.88      0.90        16\n",
      "          1       0.83      0.91      0.87        11\n",
      "\n",
      "avg / total       0.89      0.89      0.89        27\n",
      "\n",
      "[14  2  1 10]\n",
      "svc Accuracy:  0.8888888888888888\n",
      "svc F1:  0.8863955119214586\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.88      0.90        16\n",
      "          1       0.83      0.91      0.87        11\n",
      "\n",
      "avg / total       0.89      0.89      0.89        27\n",
      "\n",
      "[14  2  1 10]\n",
      "LR Accuracy:  0.8888888888888888\n",
      "LR F1:  0.8863955119214586\n",
      "For name:  d_lloyd\n",
      "total sample size before apply threshold:  157\n",
      "Counter({'0000-0002-0824-9682': 104, '0000-0003-0658-8995': 50, '0000-0003-3589-7383': 1, '0000-0003-1759-6106': 1, '0000-0003-1497-6808': 1})\n",
      "['0000-0003-0658-8995', '0000-0002-0824-9682']\n",
      "Total sample size after apply threshold:  154\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(154, 262)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(154, 262)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.80      0.89        50\n",
      "          1       0.91      1.00      0.95       104\n",
      "\n",
      "avg / total       0.94      0.94      0.93       154\n",
      "\n",
      "[ 40  10   0 104]\n",
      "svc Accuracy:  0.935064935064935\n",
      "svc F1:  0.9215086646279307\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.80      0.89        50\n",
      "          1       0.91      1.00      0.95       104\n",
      "\n",
      "avg / total       0.94      0.94      0.93       154\n",
      "\n",
      "[ 40  10   0 104]\n",
      "LR Accuracy:  0.935064935064935\n",
      "LR F1:  0.9215086646279307\n",
      "For name:  a_mohammadi\n",
      "total sample size before apply threshold:  8\n",
      "Counter({'0000-0002-8345-8206': 3, '0000-0001-7845-1707': 2, '0000-0001-7491-6423': 1, '0000-0003-4272-2733': 1, '0000-0002-8477-0939': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  d_dean\n",
      "total sample size before apply threshold:  189\n",
      "Counter({'0000-0002-4512-9065': 174, '0000-0002-5688-703X': 10, '0000-0002-8599-773X': 2, '0000-0002-2279-3393': 2, '0000-0003-4793-6511': 1})\n",
      "['0000-0002-5688-703X', '0000-0002-4512-9065']\n",
      "Total sample size after apply threshold:  184\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(184, 290)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(184, 290)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.50      0.67        10\n",
      "          1       0.97      1.00      0.99       174\n",
      "\n",
      "avg / total       0.97      0.97      0.97       184\n",
      "\n",
      "[  5   5   0 174]\n",
      "svc Accuracy:  0.9728260869565217\n",
      "svc F1:  0.826251180358829\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.50      0.67        10\n",
      "          1       0.97      1.00      0.99       174\n",
      "\n",
      "avg / total       0.97      0.97      0.97       184\n",
      "\n",
      "[  5   5   0 174]\n",
      "LR Accuracy:  0.9728260869565217\n",
      "LR F1:  0.826251180358829\n",
      "For name:  s_chang\n",
      "total sample size before apply threshold:  592\n",
      "Counter({'0000-0001-6505-4139': 322, '0000-0002-5620-0867': 61, '0000-0003-3751-1720': 37, '0000-0002-6164-0875': 28, '0000-0002-7624-439X': 22, '0000-0002-2663-5042': 20, '0000-0002-5015-8178': 19, '0000-0003-1523-7986': 15, '0000-0003-4160-7549': 12, '0000-0002-2564-2945': 11, '0000-0003-1488-1649': 11, '0000-0002-0558-0038': 8, '0000-0003-0880-2385': 7, '0000-0002-2163-3910': 6, '0000-0003-1095-4505': 2, '0000-0003-2821-7095': 2, '0000-0003-2929-1510': 2, '0000-0001-9347-3592': 2, '0000-0002-2262-0396': 1, '0000-0001-6364-2404': 1, '0000-0001-7038-6170': 1, '0000-0003-0723-3192': 1, '0000-0002-1267-7591': 1})\n",
      "['0000-0002-5015-8178', '0000-0001-6505-4139', '0000-0002-2564-2945', '0000-0003-1488-1649', '0000-0003-1523-7986', '0000-0003-3751-1720', '0000-0002-6164-0875', '0000-0002-5620-0867', '0000-0002-2663-5042', '0000-0003-4160-7549', '0000-0002-7624-439X']\n",
      "Total sample size after apply threshold:  558\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(558, 456)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(558, 456)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.42      0.55        19\n",
      "          1       0.85      0.97      0.90       322\n",
      "          2       1.00      1.00      1.00        11\n",
      "          3       0.57      0.36      0.44        11\n",
      "          4       0.73      0.53      0.62        15\n",
      "          5       0.72      0.76      0.74        37\n",
      "          6       0.62      0.36      0.45        28\n",
      "          7       0.96      0.90      0.93        61\n",
      "          8       0.93      0.70      0.80        20\n",
      "          9       1.00      0.83      0.91        12\n",
      "         10       0.81      0.59      0.68        22\n",
      "\n",
      "avg / total       0.84      0.85      0.83       558\n",
      "\n",
      "[  8  10   0   0   0   1   0   0   0   0   0   0 311   0   0   1   5   3\n",
      "   1   0   0   1   0   0  11   0   0   0   0   0   0   0   0   1   2   0\n",
      "   4   0   2   2   0   0   0   0   0   6   0   0   8   1   0   0   0   0\n",
      "   0   0   6   0   2   1  28   0   0   0   0   0   0  13   0   1   1   2\n",
      "  10   0   1   0   0   0   4   0   0   0   0   0  55   0   0   2   1   4\n",
      "   0   0   0   0   1   0  14   0   0   0   2   0   0   0   0   0   0   0\n",
      "  10   0   0   8   0   0   0   0   0   1   0   0  13]\n",
      "svc Accuracy:  0.8458781362007168\n",
      "svc F1:  0.7302286682043431\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.37      0.52        19\n",
      "          1       0.81      0.98      0.89       322\n",
      "          2       1.00      1.00      1.00        11\n",
      "          3       1.00      0.18      0.31        11\n",
      "          4       0.56      0.33      0.42        15\n",
      "          5       0.76      0.68      0.71        37\n",
      "          6       0.60      0.21      0.32        28\n",
      "          7       0.94      0.98      0.96        61\n",
      "          8       0.87      0.65      0.74        20\n",
      "          9       1.00      0.83      0.91        12\n",
      "         10       1.00      0.32      0.48        22\n",
      "\n",
      "avg / total       0.83      0.83      0.80       558\n",
      "\n",
      "[  7   9   0   0   0   3   0   0   0   0   0   0 317   0   0   2   1   1\n",
      "   1   0   0   0   0   0  11   0   0   0   0   0   0   0   0   0   5   0\n",
      "   2   0   2   2   0   0   0   0   0   9   0   0   5   1   0   0   0   0\n",
      "   0   0  12   0   0   0  25   0   0   0   0   0   0  19   0   0   1   0\n",
      "   6   0   2   0   0   0   1   0   0   0   0   0  60   0   0   0   1   4\n",
      "   0   0   1   0   1   0  13   0   0   0   2   0   0   0   0   0   0   0\n",
      "  10   0   0  11   0   0   0   1   0   3   0   0   7]\n",
      "LR Accuracy:  0.8297491039426523\n",
      "LR F1:  0.6599419256268922\n",
      "For name:  m_conte\n",
      "total sample size before apply threshold:  118\n",
      "Counter({'0000-0001-9405-7339': 48, '0000-0001-8558-2051': 23, '0000-0002-3622-1476': 21, '0000-0002-1399-0344': 16, '0000-0001-7377-163X': 9, '0000-0002-1770-8561': 1})\n",
      "['0000-0001-9405-7339', '0000-0002-3622-1476', '0000-0002-1399-0344', '0000-0001-8558-2051']\n",
      "Total sample size after apply threshold:  108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(108, 495)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(108, 495)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        48\n",
      "          1       1.00      1.00      1.00        21\n",
      "          2       1.00      1.00      1.00        16\n",
      "          3       1.00      1.00      1.00        23\n",
      "\n",
      "avg / total       1.00      1.00      1.00       108\n",
      "\n",
      "[48  0  0  0  0 21  0  0  0  0 16  0  0  0  0 23]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97        48\n",
      "          1       1.00      1.00      1.00        21\n",
      "          2       1.00      0.94      0.97        16\n",
      "          3       1.00      0.91      0.95        23\n",
      "\n",
      "avg / total       0.97      0.97      0.97       108\n",
      "\n",
      "[48  0  0  0  0 21  0  0  1  0 15  0  2  0  0 21]\n",
      "LR Accuracy:  0.9722222222222222\n",
      "LR F1:  0.9729960899315738\n",
      "For name:  i_wilson\n",
      "total sample size before apply threshold:  220\n",
      "Counter({'0000-0002-0246-738X': 102, '0000-0001-8996-1518': 85, '0000-0001-6893-2873': 27, '0000-0002-9620-7000': 3, '0000-0003-4236-5561': 2, '0000-0001-6670-9328': 1})\n",
      "['0000-0001-8996-1518', '0000-0002-0246-738X', '0000-0001-6893-2873']\n",
      "Total sample size after apply threshold:  214\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(214, 693)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(214, 693)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.91      0.90        85\n",
      "          1       0.83      0.94      0.88       102\n",
      "          2       1.00      0.44      0.62        27\n",
      "\n",
      "avg / total       0.88      0.86      0.85       214\n",
      "\n",
      "[77  8  0  6 96  0  4 11 12]\n",
      "svc Accuracy:  0.8644859813084113\n",
      "svc F1:  0.7985086931073427\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.88      0.94        85\n",
      "          1       0.81      1.00      0.89       102\n",
      "          2       1.00      0.48      0.65        27\n",
      "\n",
      "avg / total       0.91      0.89      0.88       214\n",
      "\n",
      "[ 75  10   0   0 102   0   0  14  13]\n",
      "LR Accuracy:  0.8878504672897196\n",
      "LR F1:  0.8274122807017544\n",
      "For name:  h_yoo\n",
      "total sample size before apply threshold:  22\n",
      "Counter({'0000-0001-6186-3262': 11, '0000-0001-9677-0947': 4, '0000-0001-9819-3135': 3, '0000-0002-8039-9482': 3, '0000-0003-3810-1811': 1})\n",
      "['0000-0001-6186-3262']\n",
      "Total sample size after apply threshold:  11\n",
      "For name:  d_das\n",
      "total sample size before apply threshold:  17\n",
      "Counter({'0000-0002-3833-1169': 8, '0000-0002-7153-4726': 7, '0000-0002-1643-6621': 1, '0000-0002-2548-2734': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  d_carr\n",
      "total sample size before apply threshold:  34\n",
      "Counter({'0000-0002-9476-2166': 26, '0000-0002-8175-5303': 4, '0000-0003-1503-015X': 3, '0000-0003-1435-307X': 1})\n",
      "['0000-0002-9476-2166']\n",
      "Total sample size after apply threshold:  26\n",
      "For name:  s_sahu\n",
      "total sample size before apply threshold:  18\n",
      "Counter({'0000-0002-9529-3939': 7, '0000-0002-7328-6471': 6, '0000-0001-9010-4572': 2, '0000-0003-2133-4694': 1, '0000-0002-4742-9870': 1, '0000-0002-8500-9711': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  m_tsai\n",
      "total sample size before apply threshold:  110\n",
      "Counter({'0000-0002-2962-8913': 48, '0000-0002-2862-7572': 37, '0000-0003-4277-1885': 12, '0000-0002-8597-4132': 5, '0000-0002-6216-8672': 5, '0000-0002-9314-5940': 1, '0000-0001-9556-5642': 1, '0000-0001-8246-7779': 1})\n",
      "['0000-0003-4277-1885', '0000-0002-2962-8913', '0000-0002-2862-7572']\n",
      "Total sample size after apply threshold:  97\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(97, 78)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(97, 78)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.92      0.85        12\n",
      "          1       0.96      0.92      0.94        48\n",
      "          2       0.95      0.95      0.95        37\n",
      "\n",
      "avg / total       0.93      0.93      0.93        97\n",
      "\n",
      "[11  1  0  2 44  2  1  1 35]\n",
      "svc Accuracy:  0.9278350515463918\n",
      "svc F1:  0.9094233349552497\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.92      0.92        12\n",
      "          1       0.98      0.92      0.95        48\n",
      "          2       0.93      1.00      0.96        37\n",
      "\n",
      "avg / total       0.95      0.95      0.95        97\n",
      "\n",
      "[11  1  0  1 44  3  0  0 37]\n",
      "LR Accuracy:  0.9484536082474226\n",
      "LR F1:  0.9413140622818043\n",
      "For name:  m_vitale\n",
      "total sample size before apply threshold:  217\n",
      "Counter({'0000-0002-3261-6868': 98, '0000-0001-5372-7885': 63, '0000-0003-2084-2718': 35, '0000-0002-6740-2472': 12, '0000-0002-3652-7029': 7, '0000-0001-9951-4674': 2})\n",
      "['0000-0002-3261-6868', '0000-0003-2084-2718', '0000-0001-5372-7885', '0000-0002-6740-2472']\n",
      "Total sample size after apply threshold:  208\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(208, 570)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(208, 570)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.99      0.95        98\n",
      "          1       0.97      0.94      0.96        35\n",
      "          2       1.00      0.94      0.97        63\n",
      "          3       1.00      0.67      0.80        12\n",
      "\n",
      "avg / total       0.95      0.95      0.95       208\n",
      "\n",
      "[97  1  0  0  2 33  0  0  4  0 59  0  4  0  0  8]\n",
      "svc Accuracy:  0.9471153846153846\n",
      "svc F1:  0.9175190793247918\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      1.00      0.93        98\n",
      "          1       1.00      0.83      0.91        35\n",
      "          2       1.00      0.94      0.97        63\n",
      "          3       1.00      0.67      0.80        12\n",
      "\n",
      "avg / total       0.94      0.93      0.93       208\n",
      "\n",
      "[98  0  0  0  6 29  0  0  4  0 59  0  4  0  0  8]\n",
      "LR Accuracy:  0.9326923076923077\n",
      "LR F1:  0.9016991120218578\n",
      "For name:  r_castro\n",
      "total sample size before apply threshold:  116\n",
      "Counter({'0000-0002-0959-7363': 43, '0000-0002-7417-0091': 35, '0000-0002-1329-965X': 11, '0000-0002-1263-9034': 6, '0000-0002-4381-3605': 5, '0000-0002-0701-2528': 4, '0000-0002-8054-1469': 3, '0000-0001-6873-9854': 3, '0000-0002-9337-062X': 2, '0000-0002-4698-7993': 2, '0000-0002-3769-7660': 1, '0000-0002-7289-9081': 1})\n",
      "['0000-0002-0959-7363', '0000-0002-1329-965X', '0000-0002-7417-0091']\n",
      "Total sample size after apply threshold:  89\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(89, 200)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(89, 200)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        43\n",
      "          1       1.00      1.00      1.00        11\n",
      "          2       1.00      1.00      1.00        35\n",
      "\n",
      "avg / total       1.00      1.00      1.00        89\n",
      "\n",
      "[43  0  0  0 11  0  0  0 35]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        43\n",
      "          1       1.00      1.00      1.00        11\n",
      "          2       1.00      1.00      1.00        35\n",
      "\n",
      "avg / total       1.00      1.00      1.00        89\n",
      "\n",
      "[43  0  0  0 11  0  0  0 35]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  a_hassan\n",
      "total sample size before apply threshold:  16\n",
      "Counter({'0000-0001-9509-9266': 7, '0000-0002-7719-0805': 4, '0000-0001-9346-3765': 2, '0000-0001-8842-1798': 1, '0000-0002-1853-7987': 1, '0000-0002-5574-8791': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  w_martin\n",
      "total sample size before apply threshold:  259\n",
      "Counter({'0000-0003-1478-6449': 180, '0000-0002-2749-3365': 60, '0000-0002-9947-4374': 18, '0000-0002-8952-3072': 1})\n",
      "['0000-0003-1478-6449', '0000-0002-2749-3365', '0000-0002-9947-4374']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sample size after apply threshold:  258\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(258, 609)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(258, 609)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      1.00      0.92       180\n",
      "          1       0.98      0.77      0.86        60\n",
      "          2       0.00      0.00      0.00        18\n",
      "\n",
      "avg / total       0.83      0.88      0.84       258\n",
      "\n",
      "[180   0   0  13  46   1  17   1   0]\n",
      "svc Accuracy:  0.875968992248062\n",
      "svc F1:  0.5942966690630241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gao137/intel/intelpython3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/gao137/intel/intelpython3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      1.00      0.91       180\n",
      "          1       0.98      0.72      0.83        60\n",
      "          2       0.00      0.00      0.00        18\n",
      "\n",
      "avg / total       0.81      0.86      0.83       258\n",
      "\n",
      "[180   0   0  17  43   0  17   1   0]\n",
      "LR Accuracy:  0.8643410852713178\n",
      "LR F1:  0.5802095535598074\n",
      "For name:  a_krishnan\n",
      "total sample size before apply threshold:  46\n",
      "Counter({'0000-0002-9173-7811': 41, '0000-0002-7489-9229': 3, '0000-0002-7980-4110': 1, '0000-0002-9677-9092': 1})\n",
      "['0000-0002-9173-7811']\n",
      "Total sample size after apply threshold:  41\n",
      "For name:  l_tavares\n",
      "total sample size before apply threshold:  41\n",
      "Counter({'0000-0001-8671-6285': 18, '0000-0001-9487-7978': 7, '0000-0001-8438-7887': 7, '0000-0002-1432-524X': 7, '0000-0003-3190-0194': 2})\n",
      "['0000-0001-8671-6285']\n",
      "Total sample size after apply threshold:  18\n",
      "For name:  t_murakami\n",
      "total sample size before apply threshold:  63\n",
      "Counter({'0000-0002-0314-8807': 59, '0000-0002-2661-2633': 2, '0000-0001-7924-8073': 1, '0000-0002-0754-2879': 1})\n",
      "['0000-0002-0314-8807']\n",
      "Total sample size after apply threshold:  59\n",
      "For name:  x_xiao\n",
      "total sample size before apply threshold:  31\n",
      "Counter({'0000-0002-3987-8668': 11, '0000-0002-9753-6586': 8, '0000-0003-1749-4230': 7, '0000-0002-0240-0038': 5})\n",
      "['0000-0002-3987-8668']\n",
      "Total sample size after apply threshold:  11\n",
      "For name:  j_davies\n",
      "total sample size before apply threshold:  122\n",
      "Counter({'0000-0001-6660-4032': 55, '0000-0001-5888-664X': 14, '0000-0001-7415-6129': 10, '0000-0003-4035-6047': 9, '0000-0002-4108-4357': 8, '0000-0002-1694-5370': 7, '0000-0002-7415-3638': 6, '0000-0002-9482-1066': 4, '0000-0002-9409-8605': 2, '0000-0001-9832-7412': 2, '0000-0003-4664-6862': 2, '0000-0002-8235-5782': 1, '0000-0002-5883-2526': 1, '0000-0002-4986-8594': 1})\n",
      "['0000-0001-6660-4032', '0000-0001-7415-6129', '0000-0001-5888-664X']\n",
      "Total sample size after apply threshold:  79\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(79, 229)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(79, 229)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      1.00      0.90        55\n",
      "          1       1.00      0.50      0.67        10\n",
      "          2       1.00      0.50      0.67        14\n",
      "\n",
      "avg / total       0.88      0.85      0.83        79\n",
      "\n",
      "[55  0  0  5  5  0  7  0  7]\n",
      "svc Accuracy:  0.8481012658227848\n",
      "svc F1:  0.7449908925318761\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      1.00      0.88        55\n",
      "          1       1.00      0.40      0.57        10\n",
      "          2       1.00      0.36      0.53        14\n",
      "\n",
      "avg / total       0.85      0.81      0.78        79\n",
      "\n",
      "[55  0  0  6  4  0  9  0  5]\n",
      "LR Accuracy:  0.810126582278481\n",
      "LR F1:  0.6592481203007519\n",
      "For name:  a_schmidt\n",
      "total sample size before apply threshold:  90\n",
      "Counter({'0000-0002-1090-8165': 51, '0000-0002-3925-9429': 14, '0000-0003-1327-0424': 12, '0000-0002-1185-3012': 9, '0000-0001-8946-1310': 1, '0000-0002-9963-7786': 1, '0000-0002-6448-6367': 1, '0000-0001-6144-9950': 1})\n",
      "['0000-0003-1327-0424', '0000-0002-1090-8165', '0000-0002-3925-9429']\n",
      "Total sample size after apply threshold:  77\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(77, 242)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(77, 242)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        12\n",
      "          1       0.98      1.00      0.99        51\n",
      "          2       1.00      0.93      0.96        14\n",
      "\n",
      "avg / total       0.99      0.99      0.99        77\n",
      "\n",
      "[12  0  0  0 51  0  0  1 13]\n",
      "svc Accuracy:  0.987012987012987\n",
      "svc F1:  0.9844180750329619\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.92      0.96        12\n",
      "          1       0.96      1.00      0.98        51\n",
      "          2       1.00      0.93      0.96        14\n",
      "\n",
      "avg / total       0.98      0.97      0.97        77\n",
      "\n",
      "[11  1  0  0 51  0  0  1 13]\n",
      "LR Accuracy:  0.974025974025974\n",
      "LR F1:  0.9667513109542094\n",
      "For name:  j_nieto\n",
      "total sample size before apply threshold:  56\n",
      "Counter({'0000-0002-0086-252X': 32, '0000-0002-5655-3320': 13, '0000-0001-9075-7100': 4, '0000-0003-2465-3033': 4, '0000-0002-4303-1574': 3})\n",
      "['0000-0002-0086-252X', '0000-0002-5655-3320']\n",
      "Total sample size after apply threshold:  45\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(45, 87)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(45, 87)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        32\n",
      "          1       1.00      1.00      1.00        13\n",
      "\n",
      "avg / total       1.00      1.00      1.00        45\n",
      "\n",
      "[32  0  0 13]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        32\n",
      "          1       1.00      1.00      1.00        13\n",
      "\n",
      "avg / total       1.00      1.00      1.00        45\n",
      "\n",
      "[32  0  0 13]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  s_hasan\n",
      "total sample size before apply threshold:  12\n",
      "Counter({'0000-0002-9089-5367': 4, '0000-0002-0158-703X': 2, '0000-0002-7269-094X': 2, '0000-0001-5589-8741': 1, '0000-0001-7789-2842': 1, '0000-0003-4271-395X': 1, '0000-0001-6832-9150': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  m_teixeira\n",
      "total sample size before apply threshold:  313\n",
      "Counter({'0000-0003-4124-6237': 149, '0000-0002-5676-6174': 51, '0000-0002-4896-5982': 48, '0000-0001-9355-2143': 17, '0000-0002-9466-7951': 17, '0000-0002-6944-3008': 13, '0000-0001-7456-5192': 7, '0000-0002-3338-8588': 4, '0000-0003-3989-9474': 3, '0000-0002-2228-2673': 2, '0000-0003-1205-3233': 2})\n",
      "['0000-0002-4896-5982', '0000-0002-5676-6174', '0000-0001-9355-2143', '0000-0002-6944-3008', '0000-0003-4124-6237', '0000-0002-9466-7951']\n",
      "Total sample size after apply threshold:  295\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(295, 1580)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(295, 1580)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.71      0.83        48\n",
      "          1       0.96      0.98      0.97        51\n",
      "          2       0.87      0.76      0.81        17\n",
      "          3       1.00      0.77      0.87        13\n",
      "          4       0.87      1.00      0.93       149\n",
      "          5       1.00      0.76      0.87        17\n",
      "\n",
      "avg / total       0.92      0.91      0.91       295\n",
      "\n",
      "[ 34   2   1   0  11   0   0  50   0   0   1   0   0   0  13   0   4   0\n",
      "   0   0   1  10   2   0   0   0   0   0 149   0   0   0   0   0   4  13]\n",
      "svc Accuracy:  0.911864406779661\n",
      "svc F1:  0.8800206605247775\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.79      0.88        48\n",
      "          1       0.96      0.96      0.96        51\n",
      "          2       0.92      0.71      0.80        17\n",
      "          3       1.00      0.62      0.76        13\n",
      "          4       0.85      1.00      0.92       149\n",
      "          5       1.00      0.59      0.74        17\n",
      "\n",
      "avg / total       0.91      0.90      0.90       295\n",
      "\n",
      "[ 38   2   0   0   8   0   0  49   0   0   2   0   0   0  12   0   5   0\n",
      "   0   0   1   8   4   0   0   0   0   0 149   0   0   0   0   0   7  10]\n",
      "LR Accuracy:  0.9016949152542373\n",
      "LR F1:  0.8444839721705506\n",
      "For name:  j_koh\n",
      "total sample size before apply threshold:  62\n",
      "Counter({'0000-0002-1814-5051': 40, '0000-0001-6542-0493': 9, '0000-0002-6617-1449': 7, '0000-0002-1293-1932': 5, '0000-0002-3678-4789': 1})\n",
      "['0000-0002-1814-5051']\n",
      "Total sample size after apply threshold:  40\n",
      "For name:  m_amin\n",
      "total sample size before apply threshold:  13\n",
      "Counter({'0000-0002-7822-1124': 4, '0000-0003-0404-2040': 2, '0000-0001-5617-1579': 2, '0000-0002-9701-7102': 2, '0000-0002-3602-5555': 2, '0000-0002-5630-069X': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  h_cho\n",
      "total sample size before apply threshold:  73\n",
      "Counter({'0000-0002-8629-8500': 21, '0000-0003-0861-523X': 17, '0000-0003-0623-5647': 8, '0000-0002-8267-3801': 7, '0000-0002-1737-5701': 7, '0000-0003-1897-1166': 6, '0000-0003-2651-6403': 3, '0000-0002-9799-1538': 2, '0000-0001-7443-167X': 1, '0000-0003-1634-7482': 1})\n",
      "['0000-0002-8629-8500', '0000-0003-0861-523X']\n",
      "Total sample size after apply threshold:  38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(38, 68)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(38, 68)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.95      0.98        21\n",
      "          1       0.94      1.00      0.97        17\n",
      "\n",
      "avg / total       0.98      0.97      0.97        38\n",
      "\n",
      "[20  1  0 17]\n",
      "svc Accuracy:  0.9736842105263158\n",
      "svc F1:  0.9735191637630662\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.95      0.98        21\n",
      "          1       0.94      1.00      0.97        17\n",
      "\n",
      "avg / total       0.98      0.97      0.97        38\n",
      "\n",
      "[20  1  0 17]\n",
      "LR Accuracy:  0.9736842105263158\n",
      "LR F1:  0.9735191637630662\n",
      "For name:  s_lam\n",
      "total sample size before apply threshold:  90\n",
      "Counter({'0000-0003-3294-6637': 69, '0000-0001-7468-1142': 6, '0000-0002-5318-1760': 5, '0000-0002-2982-9192': 3, '0000-0002-1888-1067': 3, '0000-0001-7943-5004': 3, '0000-0002-1471-5176': 1})\n",
      "['0000-0003-3294-6637']\n",
      "Total sample size after apply threshold:  69\n",
      "For name:  t_tran\n",
      "total sample size before apply threshold:  54\n",
      "Counter({'0000-0002-4686-8601': 38, '0000-0002-5132-6495': 8, '0000-0001-6531-8907': 3, '0000-0002-9557-3340': 2, '0000-0002-3355-7951': 1, '0000-0002-0853-2226': 1, '0000-0002-7489-3126': 1})\n",
      "['0000-0002-4686-8601']\n",
      "Total sample size after apply threshold:  38\n",
      "For name:  c_su\n",
      "total sample size before apply threshold:  297\n",
      "Counter({'0000-0003-3604-7858': 140, '0000-0001-8392-7108': 89, '0000-0002-5211-3520': 27, '0000-0001-9295-7587': 12, '0000-0002-9483-4510': 12, '0000-0001-5428-0878': 6, '0000-0003-2504-0466': 5, '0000-0002-7624-1607': 4, '0000-0002-1035-4238': 1, '0000-0003-4580-9607': 1})\n",
      "['0000-0003-3604-7858', '0000-0002-5211-3520', '0000-0001-8392-7108', '0000-0001-9295-7587', '0000-0002-9483-4510']\n",
      "Total sample size after apply threshold:  280\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(280, 422)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(280, 422)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.95      0.92       140\n",
      "          1       0.76      0.70      0.73        27\n",
      "          2       0.91      0.88      0.89        89\n",
      "          3       0.89      0.67      0.76        12\n",
      "          4       0.91      0.83      0.87        12\n",
      "\n",
      "avg / total       0.88      0.89      0.88       280\n",
      "\n",
      "[133   3   3   1   0   3  19   4   0   1   8   3  78   0   0   4   0   0\n",
      "   8   0   1   0   1   0  10]\n",
      "svc Accuracy:  0.8857142857142857\n",
      "svc F1:  0.8348166012814728\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.99      0.93       140\n",
      "          1       0.94      0.59      0.73        27\n",
      "          2       0.93      0.91      0.92        89\n",
      "          3       0.86      0.50      0.63        12\n",
      "          4       0.89      0.67      0.76        12\n",
      "\n",
      "avg / total       0.90      0.89      0.89       280\n",
      "\n",
      "[139   0   1   0   0   6  16   4   0   1   8   0  81   0   0   6   0   0\n",
      "   6   0   1   1   1   1   8]\n",
      "LR Accuracy:  0.8928571428571429\n",
      "LR F1:  0.7935755297334245\n",
      "For name:  s_george\n",
      "total sample size before apply threshold:  78\n",
      "Counter({'0000-0002-8807-0737': 31, '0000-0002-0859-0215': 29, '0000-0002-0444-5870': 11, '0000-0001-6534-3846': 6, '0000-0001-9843-4816': 1})\n",
      "['0000-0002-0859-0215', '0000-0002-8807-0737', '0000-0002-0444-5870']\n",
      "Total sample size after apply threshold:  71\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(71, 164)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(71, 164)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      1.00      0.95        29\n",
      "          1       1.00      0.97      0.98        31\n",
      "          2       1.00      0.82      0.90        11\n",
      "\n",
      "avg / total       0.96      0.96      0.96        71\n",
      "\n",
      "[29  0  0  1 30  0  2  0  9]\n",
      "svc Accuracy:  0.9577464788732394\n",
      "svc F1:  0.9448087431693989\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97        29\n",
      "          1       1.00      1.00      1.00        31\n",
      "          2       1.00      0.82      0.90        11\n",
      "\n",
      "avg / total       0.97      0.97      0.97        71\n",
      "\n",
      "[29  0  0  0 31  0  2  0  9]\n",
      "LR Accuracy:  0.971830985915493\n",
      "LR F1:  0.9555555555555556\n",
      "For name:  j_hong\n",
      "total sample size before apply threshold:  143\n",
      "Counter({'0000-0002-2476-3737': 29, '0000-0002-4592-7083': 26, '0000-0002-2891-5785': 20, '0000-0001-9467-6463': 16, '0000-0001-9912-633X': 12, '0000-0003-2212-2861': 12, '0000-0002-9915-8072': 8, '0000-0003-0617-9307': 6, '0000-0001-7979-5966': 5, '0000-0002-0109-5975': 5, '0000-0001-5172-6889': 4})\n",
      "['0000-0001-9912-633X', '0000-0002-2891-5785', '0000-0003-2212-2861', '0000-0002-2476-3737', '0000-0001-9467-6463', '0000-0002-4592-7083']\n",
      "Total sample size after apply threshold:  115\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(115, 153)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(115, 153)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.92      0.96        12\n",
      "          1       0.82      0.70      0.76        20\n",
      "          2       0.73      0.67      0.70        12\n",
      "          3       0.90      0.97      0.93        29\n",
      "          4       0.88      0.94      0.91        16\n",
      "          5       0.93      1.00      0.96        26\n",
      "\n",
      "avg / total       0.88      0.89      0.88       115\n",
      "\n",
      "[11  0  0  0  1  0  0 14  3  2  1  0  0  2  8  1  0  1  0  1  0 28  0  0\n",
      "  0  0  0  0 15  1  0  0  0  0  0 26]\n",
      "svc Accuracy:  0.8869565217391304\n",
      "svc F1:  0.8690529791979067\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        12\n",
      "          1       0.75      0.75      0.75        20\n",
      "          2       0.78      0.58      0.67        12\n",
      "          3       0.88      0.97      0.92        29\n",
      "          4       1.00      0.94      0.97        16\n",
      "          5       0.96      1.00      0.98        26\n",
      "\n",
      "avg / total       0.89      0.90      0.89       115\n",
      "\n",
      "[12  0  0  0  0  0  0 15  2  3  0  0  0  4  7  1  0  0  0  1  0 28  0  0\n",
      "  0  0  0  0 15  1  0  0  0  0  0 26]\n",
      "LR Accuracy:  0.8956521739130435\n",
      "LR F1:  0.8805955774179135\n",
      "For name:  p_baptista\n",
      "total sample size before apply threshold:  114\n",
      "Counter({'0000-0001-5255-7095': 73, '0000-0001-6331-3731': 30, '0000-0003-1559-9151': 4, '0000-0003-1433-6456': 4, '0000-0001-7651-4700': 3})\n",
      "['0000-0001-5255-7095', '0000-0001-6331-3731']\n",
      "Total sample size after apply threshold:  103\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(103, 190)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(103, 190)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.97        73\n",
      "          1       1.00      0.87      0.93        30\n",
      "\n",
      "avg / total       0.96      0.96      0.96       103\n",
      "\n",
      "[73  0  4 26]\n",
      "svc Accuracy:  0.9611650485436893\n",
      "svc F1:  0.950952380952381\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.97        73\n",
      "          1       1.00      0.87      0.93        30\n",
      "\n",
      "avg / total       0.96      0.96      0.96       103\n",
      "\n",
      "[73  0  4 26]\n",
      "LR Accuracy:  0.9611650485436893\n",
      "LR F1:  0.950952380952381\n",
      "For name:  p_thompson\n",
      "total sample size before apply threshold:  148\n",
      "Counter({'0000-0002-5910-7625': 69, '0000-0002-2268-9748': 48, '0000-0001-6195-3284': 10, '0000-0001-7562-6049': 8, '0000-0002-4688-3414': 6, '0000-0002-6851-8899': 3, '0000-0002-5278-9045': 2, '0000-0002-9161-167X': 1, '0000-0002-3141-3567': 1})\n",
      "['0000-0001-6195-3284', '0000-0002-2268-9748', '0000-0002-5910-7625']\n",
      "Total sample size after apply threshold:  127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(127, 434)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(127, 434)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.60      0.71        10\n",
      "          1       0.85      0.94      0.89        48\n",
      "          2       0.94      0.91      0.93        69\n",
      "\n",
      "avg / total       0.90      0.90      0.90       127\n",
      "\n",
      "[ 6  3  1  0 45  3  1  5 63]\n",
      "svc Accuracy:  0.8976377952755905\n",
      "svc F1:  0.8411473500291207\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.20      0.33        10\n",
      "          1       0.87      0.85      0.86        48\n",
      "          2       0.87      0.99      0.93        69\n",
      "\n",
      "avg / total       0.88      0.87      0.86       127\n",
      "\n",
      "[ 2  5  3  0 41  7  0  1 68]\n",
      "LR Accuracy:  0.8740157480314961\n",
      "LR F1:  0.7072204320324622\n",
      "For name:  a_castro\n",
      "total sample size before apply threshold:  126\n",
      "Counter({'0000-0001-7526-6717': 39, '0000-0002-8311-0840': 17, '0000-0003-0428-9174': 15, '0000-0002-9253-7926': 14, '0000-0001-6964-6879': 13, '0000-0003-4035-3444': 11, '0000-0003-0524-156X': 7, '0000-0003-3052-6225': 4, '0000-0003-0328-1381': 3, '0000-0002-8025-4945': 2, '0000-0003-3327-967X': 1})\n",
      "['0000-0002-8311-0840', '0000-0001-6964-6879', '0000-0002-9253-7926', '0000-0003-4035-3444', '0000-0003-0428-9174', '0000-0001-7526-6717']\n",
      "Total sample size after apply threshold:  109\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(109, 284)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(109, 284)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.76      0.87        17\n",
      "          1       1.00      0.92      0.96        13\n",
      "          2       1.00      0.50      0.67        14\n",
      "          3       1.00      0.64      0.78        11\n",
      "          4       0.47      1.00      0.64        15\n",
      "          5       1.00      0.97      0.99        39\n",
      "\n",
      "avg / total       0.93      0.84      0.85       109\n",
      "\n",
      "[13  0  0  0  4  0  0 12  0  0  1  0  0  0  7  0  7  0  0  0  0  7  4  0\n",
      "  0  0  0  0 15  0  0  0  0  0  1 38]\n",
      "svc Accuracy:  0.8440366972477065\n",
      "svc F1:  0.8160703284107539\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.88      0.94        17\n",
      "          1       1.00      0.92      0.96        13\n",
      "          2       1.00      0.79      0.88        14\n",
      "          3       1.00      0.73      0.84        11\n",
      "          4       0.62      0.87      0.72        15\n",
      "          5       0.93      1.00      0.96        39\n",
      "\n",
      "avg / total       0.92      0.90      0.90       109\n",
      "\n",
      "[15  0  0  0  1  1  0 12  0  0  1  0  0  0 11  0  3  0  0  0  0  8  3  0\n",
      "  0  0  0  0 13  2  0  0  0  0  0 39]\n",
      "LR Accuracy:  0.8990825688073395\n",
      "LR F1:  0.8841317413905133\n",
      "For name:  j_zhang\n",
      "total sample size before apply threshold:  965\n",
      "Counter({'0000-0002-4319-4285': 188, '0000-0002-6601-9180': 58, '0000-0003-2493-5209': 58, '0000-0003-3373-9621': 57, '0000-0002-0889-7057': 40, '0000-0001-8041-1608': 28, '0000-0002-9831-6796': 24, '0000-0002-1905-8750': 24, '0000-0002-9231-0844': 21, '0000-0002-7737-0785': 20, '0000-0002-5822-2226': 20, '0000-0002-5108-2072': 19, '0000-0002-1138-2556': 19, '0000-0002-8798-7316': 18, '0000-0002-2195-2997': 18, '0000-0003-3460-0867': 16, '0000-0003-4649-6526': 16, '0000-0001-5903-6487': 15, '0000-0002-1041-793X': 14, '0000-0001-8683-509X': 14, '0000-0002-7068-5135': 14, '0000-0002-9405-9024': 12, '0000-0001-9803-7140': 11, '0000-0002-8344-5907': 10, '0000-0003-1338-8887': 9, '0000-0003-0391-7298': 9, '0000-0002-1221-3033': 9, '0000-0003-3812-3850': 9, '0000-0002-6457-0235': 8, '0000-0003-1572-8339': 8, '0000-0001-8828-114X': 8, '0000-0001-5289-6062': 8, '0000-0001-8970-4466': 7, '0000-0003-1113-6264': 7, '0000-0003-3526-4586': 7, '0000-0002-1559-1240': 6, '0000-0003-3099-6665': 6, '0000-0002-2540-2749': 6, '0000-0002-0912-1197': 5, '0000-0002-0906-0099': 5, '0000-0003-0589-6267': 5, '0000-0001-9732-798X': 5, '0000-0002-3163-6808': 5, '0000-0002-9976-1605': 5, '0000-0002-4758-0394': 5, '0000-0002-1225-6703': 4, '0000-0001-9697-6689': 4, '0000-0001-7869-8005': 4, '0000-0002-7959-2701': 4, '0000-0003-2725-1259': 4, '0000-0001-7533-998X': 4, '0000-0002-0841-1096': 3, '0000-0002-3267-542X': 3, '0000-0001-9275-5790': 3, '0000-0002-6078-4404': 3, '0000-0002-0437-9834': 3, '0000-0002-6196-8694': 3, '0000-0003-2799-9347': 2, '0000-0002-2282-8146': 2, '0000-0001-9143-2869': 2, '0000-0002-1624-9535': 2, '0000-0002-1161-5460': 2, '0000-0003-3195-9882': 2, '0000-0002-3731-4594': 2, '0000-0001-7238-4021': 2, '0000-0002-7937-1474': 2, '0000-0001-6516-0302': 2, '0000-0002-2261-7605': 2, '0000-0001-8385-2003': 2, '0000-0002-9478-8243': 2, '0000-0001-9777-7956': 2, '0000-0002-5785-2090': 2, '0000-0003-1386-6447': 2, '0000-0002-3063-2039': 2, '0000-0002-7841-3767': 1, '0000-0002-2257-1803': 1, '0000-0002-3412-7769': 1, '0000-0002-6358-0255': 1, '0000-0001-9408-138X': 1, '0000-0002-1994-8374': 1, '0000-0002-3574-8401': 1, '0000-0001-7612-8498': 1, '0000-0002-2114-173X': 1, '0000-0001-7228-6202': 1, '0000-0002-0946-5520': 1, '0000-0002-1397-5224': 1, '0000-0002-2222-3024': 1})\n",
      "['0000-0002-9831-6796', '0000-0002-9405-9024', '0000-0001-5903-6487', '0000-0001-8041-1608', '0000-0002-8798-7316', '0000-0003-3460-0867', '0000-0002-7737-0785', '0000-0002-8344-5907', '0000-0002-9231-0844', '0000-0002-4319-4285', '0000-0002-2195-2997', '0000-0002-1905-8750', '0000-0002-6601-9180', '0000-0003-4649-6526', '0000-0002-5108-2072', '0000-0002-1041-793X', '0000-0002-5822-2226', '0000-0002-1138-2556', '0000-0001-8683-509X', '0000-0002-7068-5135', '0000-0003-3373-9621', '0000-0003-2493-5209', '0000-0002-0889-7057', '0000-0001-9803-7140']\n",
      "Total sample size after apply threshold:  734\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(734, 830)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(734, 830)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.96      0.94        24\n",
      "          1       0.89      0.67      0.76        12\n",
      "          2       0.91      0.67      0.77        15\n",
      "          3       0.89      0.89      0.89        28\n",
      "          4       0.79      0.61      0.69        18\n",
      "          5       0.91      0.62      0.74        16\n",
      "          6       0.69      1.00      0.82        20\n",
      "          7       0.73      0.80      0.76        10\n",
      "          8       0.91      0.95      0.93        21\n",
      "          9       0.78      0.93      0.85       188\n",
      "         10       0.94      0.83      0.88        18\n",
      "         11       0.91      0.42      0.57        24\n",
      "         12       0.96      0.93      0.95        58\n",
      "         13       0.38      0.31      0.34        16\n",
      "         14       0.64      0.84      0.73        19\n",
      "         15       0.61      1.00      0.76        14\n",
      "         16       0.90      0.45      0.60        20\n",
      "         17       0.67      0.63      0.65        19\n",
      "         18       0.92      0.79      0.85        14\n",
      "         19       0.86      0.86      0.86        14\n",
      "         20       0.81      0.81      0.81        57\n",
      "         21       1.00      0.91      0.95        58\n",
      "         22       1.00      0.93      0.96        40\n",
      "         23       1.00      0.64      0.78        11\n",
      "\n",
      "avg / total       0.84      0.83      0.83       734\n",
      "\n",
      "[ 23   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   1   0   0   0   0   8   0   0   0   0   1   0   1   1   0   0\n",
      "   0   1   0   0   0   0   0   0   0   0   0   0   0   0  10   0   0   0\n",
      "   1   1   0   2   0   0   0   1   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0  25   0   0   0   0   0   0   0   1   1   0   0   1   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  11   0   1   0   0   6   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  10\n",
      "   0   0   0   2   0   0   1   2   1   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0  20   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   8   0   0   0   0\n",
      "   0   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0  20   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0\n",
      "   0   0   1   0   2   0   1   0   0 174   0   0   0   0   3   0   0   3\n",
      "   0   0   4   0   0   0   0   0   0   0   0   0   0   0   0   0  15   0\n",
      "   0   0   1   0   0   1   0   0   1   0   0   0   1   0   0   0   0   0\n",
      "   0   1   0   5   1  10   0   1   1   1   1   0   0   0   2   0   0   0\n",
      "   0   0   0   0   0   1   0   1   0   1   0   0  54   0   0   0   0   1\n",
      "   0   0   0   0   0   0   0   1   0   0   0   0   1   0   0   2   0   0\n",
      "   0   5   0   2   0   0   0   2   3   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   1   0   0   0   0  16   1   0   0   1   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  14   0   0\n",
      "   0   0   0   0   0   0   0   0   0   2   0   0   0   0   1   8   0   0\n",
      "   0   0   0   0   9   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   7   0   0   0   0   0   0   0  12   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   2   0   0   0   0   0   1   0   0\n",
      "  11   0   0   0   0   0   1   0   0   0   0   0   0   0   0   1   0   0\n",
      "   0   0   0   0   0   0   0  12   0   0   0   0   0   0   0   1   0   0\n",
      "   2   0   0   6   0   0   0   1   1   0   0   0   0   0  46   0   0   0\n",
      "   0   0   0   0   1   0   1   0   0   1   0   0   0   0   0   2   0   0\n",
      "   0   0   0  53   0   0   0   0   0   0   0   0   0   0   0   1   0   0\n",
      "   0   0   0   1   0   1   0   0   0   0  37   0   0   0   0   0   0   0\n",
      "   1   0   0   2   0   0   0   0   1   0   0   0   0   0   0   0   0   7]\n",
      "svc Accuracy:  0.8310626702997275\n",
      "svc F1:  0.7846247857029027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.96      0.94        24\n",
      "          1       0.89      0.67      0.76        12\n",
      "          2       0.86      0.80      0.83        15\n",
      "          3       0.86      0.89      0.88        28\n",
      "          4       1.00      0.44      0.62        18\n",
      "          5       0.85      0.69      0.76        16\n",
      "          6       0.68      0.95      0.79        20\n",
      "          7       1.00      0.80      0.89        10\n",
      "          8       1.00      0.90      0.95        21\n",
      "          9       0.75      0.98      0.85       188\n",
      "         10       0.93      0.78      0.85        18\n",
      "         11       0.95      0.75      0.84        24\n",
      "         12       0.96      0.95      0.96        58\n",
      "         13       0.70      0.44      0.54        16\n",
      "         14       0.64      0.74      0.68        19\n",
      "         15       0.88      1.00      0.93        14\n",
      "         16       0.78      0.35      0.48        20\n",
      "         17       0.64      0.37      0.47        19\n",
      "         18       1.00      0.57      0.73        14\n",
      "         19       0.92      0.86      0.89        14\n",
      "         20       0.85      0.81      0.83        57\n",
      "         21       0.98      0.95      0.96        58\n",
      "         22       1.00      0.97      0.99        40\n",
      "         23       1.00      0.64      0.78        11\n",
      "\n",
      "avg / total       0.86      0.84      0.84       734\n",
      "\n",
      "[ 23   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   1   0   0   0   0   8   0   0   0   0   0   0   0   2   0   0\n",
      "   0   1   1   0   0   0   0   0   0   0   0   0   0   0  12   0   0   0\n",
      "   1   0   0   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0  25   0   0   1   0   0   0   1   0   1   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   8   0   0   0   0   9   0   0\n",
      "   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0  11\n",
      "   0   0   0   2   0   0   0   1   2   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0  19   0   0   0   0   0   1   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   8   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   2   0   0   0   0   0   0   0   0   0\n",
      "   0   0  19   1   0   0   0   0   1   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0 184   0   0   0   0   0   0   2   0\n",
      "   0   0   1   0   0   0   0   0   0   0   0   0   1   0   0   0  14   0\n",
      "   0   0   1   0   0   1   0   0   1   0   0   0   1   0   0   0   0   0\n",
      "   1   0   0   1   0  18   0   0   1   0   0   2   0   0   0   0   0   0\n",
      "   0   0   0   0   0   1   0   0   0   1   0   0  55   0   0   0   0   1\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   3   0   0\n",
      "   0   7   0   2   0   0   0   1   2   0   0   0   0   0   0   0   0   1\n",
      "   0   0   0   4   0   0   0   0  14   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  14   0   0\n",
      "   0   0   0   0   0   0   0   0   0   2   0   0   1   0   0  10   0   0\n",
      "   0   0   0   0   7   0   0   0   0   0   0   0   0   0   0   1   0   0\n",
      "   0   0   0  11   0   0   0   0   0   0   0   7   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   5   0   0   0   0   0   0   0   0\n",
      "   8   0   1   0   0   0   1   0   0   0   0   0   0   0   0   1   0   0\n",
      "   0   0   0   0   0   0   0  12   0   0   0   0   0   0   2   1   0   0\n",
      "   1   0   0   4   0   1   0   1   1   0   0   0   0   0  46   0   0   0\n",
      "   0   0   0   0   0   0   1   0   0   2   0   0   0   0   0   0   0   0\n",
      "   0   0   0  55   0   0   0   0   0   0   0   0   0   0   0   1   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  39   0   0   1   0   0   0   0\n",
      "   0   0   0   2   0   0   0   0   1   0   0   0   0   0   0   0   0   7]\n",
      "LR Accuracy:  0.8446866485013624\n",
      "LR F1:  0.7992204167749165\n",
      "For name:  j_rodrigues\n",
      "total sample size before apply threshold:  264\n",
      "Counter({'0000-0002-9347-5026': 39, '0000-0002-1418-7860': 34, '0000-0002-3950-528X': 24, '0000-0001-6143-2139': 23, '0000-0001-8657-3800': 20, '0000-0002-5605-656X': 19, '0000-0003-1889-4914': 16, '0000-0001-9796-3193': 13, '0000-0002-9756-1124': 12, '0000-0001-7006-3048': 12, '0000-0001-9187-8094': 9, '0000-0002-4279-6188': 7, '0000-0002-8621-5410': 6, '0000-0003-0424-3248': 5, '0000-0002-3562-6025': 5, '0000-0002-3217-2320': 4, '0000-0003-4552-1953': 3, '0000-0002-3387-2652': 3, '0000-0002-4031-8000': 3, '0000-0002-8315-8553': 2, '0000-0003-2187-9408': 2, '0000-0002-2793-8192': 1, '0000-0002-4790-7959': 1, '0000-0002-6446-6462': 1})\n",
      "['0000-0002-9756-1124', '0000-0001-6143-2139', '0000-0002-5605-656X', '0000-0003-1889-4914', '0000-0001-8657-3800', '0000-0002-3950-528X', '0000-0002-1418-7860', '0000-0002-9347-5026', '0000-0001-7006-3048', '0000-0001-9796-3193']\n",
      "Total sample size after apply threshold:  212\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(212, 530)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(212, 530)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.83      0.91        12\n",
      "          1       1.00      0.91      0.95        23\n",
      "          2       0.89      0.89      0.89        19\n",
      "          3       1.00      1.00      1.00        16\n",
      "          4       1.00      0.60      0.75        20\n",
      "          5       0.95      0.83      0.89        24\n",
      "          6       1.00      0.94      0.97        34\n",
      "          7       0.65      1.00      0.79        39\n",
      "          8       1.00      0.75      0.86        12\n",
      "          9       1.00      0.92      0.96        13\n",
      "\n",
      "avg / total       0.92      0.89      0.89       212\n",
      "\n",
      "[10  0  0  0  0  0  0  2  0  0  0 21  0  0  0  0  0  2  0  0  0  0 17  0\n",
      "  0  0  0  2  0  0  0  0  0 16  0  0  0  0  0  0  0  0  0  0 12  0  0  8\n",
      "  0  0  0  0  2  0  0 20  0  2  0  0  0  0  0  0  0  1 32  1  0  0  0  0\n",
      "  0  0  0  0  0 39  0  0  0  0  0  0  0  0  0  3  9  0  0  0  0  0  0  0\n",
      "  0  1  0 12]\n",
      "svc Accuracy:  0.8867924528301887\n",
      "svc F1:  0.8971980709349132\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.83      0.91        12\n",
      "          1       1.00      0.96      0.98        23\n",
      "          2       0.94      0.89      0.92        19\n",
      "          3       1.00      1.00      1.00        16\n",
      "          4       1.00      0.60      0.75        20\n",
      "          5       0.96      0.92      0.94        24\n",
      "          6       0.97      0.94      0.96        34\n",
      "          7       0.68      1.00      0.81        39\n",
      "          8       1.00      0.75      0.86        12\n",
      "          9       1.00      0.92      0.96        13\n",
      "\n",
      "avg / total       0.93      0.90      0.90       212\n",
      "\n",
      "[10  0  0  0  0  0  0  2  0  0  0 22  0  0  0  0  0  1  0  0  0  0 17  0\n",
      "  0  0  0  2  0  0  0  0  0 16  0  0  0  0  0  0  0  0  0  0 12  0  1  7\n",
      "  0  0  0  0  1  0  0 22  0  1  0  0  0  0  0  0  0  1 32  1  0  0  0  0\n",
      "  0  0  0  0  0 39  0  0  0  0  0  0  0  0  0  3  9  0  0  0  0  0  0  0\n",
      "  0  1  0 12]\n",
      "LR Accuracy:  0.9009433962264151\n",
      "LR F1:  0.9076824556293437\n",
      "For name:  s_watson\n",
      "total sample size before apply threshold:  117\n",
      "Counter({'0000-0001-6699-1765': 45, '0000-0002-2558-3367': 38, '0000-0002-9818-7429': 12, '0000-0002-9643-5580': 8, '0000-0002-9042-2391': 6, '0000-0001-6063-7327': 4, '0000-0002-8112-9687': 4})\n",
      "['0000-0002-2558-3367', '0000-0002-9818-7429', '0000-0001-6699-1765']\n",
      "Total sample size after apply threshold:  95\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(95, 193)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(95, 193)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.87      0.93        38\n",
      "          1       1.00      0.75      0.86        12\n",
      "          2       0.85      1.00      0.92        45\n",
      "\n",
      "avg / total       0.93      0.92      0.92        95\n",
      "\n",
      "[33  0  5  0  9  3  0  0 45]\n",
      "svc Accuracy:  0.9157894736842105\n",
      "svc F1:  0.901695889623455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.89      0.94        38\n",
      "          1       1.00      0.83      0.91        12\n",
      "          2       0.88      1.00      0.94        45\n",
      "\n",
      "avg / total       0.94      0.94      0.94        95\n",
      "\n",
      "[34  0  4  0 10  2  0  0 45]\n",
      "LR Accuracy:  0.9368421052631579\n",
      "LR F1:  0.9303451178451178\n",
      "For name:  c_barros\n",
      "total sample size before apply threshold:  34\n",
      "Counter({'0000-0003-4666-5000': 16, '0000-0003-3244-7467': 13, '0000-0003-2330-398X': 2, '0000-0002-5863-2874': 2, '0000-0003-2236-4553': 1})\n",
      "['0000-0003-3244-7467', '0000-0003-4666-5000']\n",
      "Total sample size after apply threshold:  29\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(29, 189)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(29, 189)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.77      0.87        13\n",
      "          1       0.84      1.00      0.91        16\n",
      "\n",
      "avg / total       0.91      0.90      0.89        29\n",
      "\n",
      "[10  3  0 16]\n",
      "svc Accuracy:  0.896551724137931\n",
      "svc F1:  0.8919254658385094\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.92      0.96        13\n",
      "          1       0.94      1.00      0.97        16\n",
      "\n",
      "avg / total       0.97      0.97      0.97        29\n",
      "\n",
      "[12  1  0 16]\n",
      "LR Accuracy:  0.9655172413793104\n",
      "LR F1:  0.9648484848484848\n",
      "For name:  f_cardoso\n",
      "total sample size before apply threshold:  178\n",
      "Counter({'0000-0002-6692-2249': 139, '0000-0002-0068-9974': 18, '0000-0002-4391-1336': 9, '0000-0002-7042-1287': 7, '0000-0003-2249-9407': 5})\n",
      "['0000-0002-6692-2249', '0000-0002-0068-9974']\n",
      "Total sample size after apply threshold:  157\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(157, 794)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(157, 794)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99       139\n",
      "          1       1.00      0.83      0.91        18\n",
      "\n",
      "avg / total       0.98      0.98      0.98       157\n",
      "\n",
      "[139   0   3  15]\n",
      "svc Accuracy:  0.9808917197452229\n",
      "svc F1:  0.9492073762536396\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.98       139\n",
      "          1       1.00      0.72      0.84        18\n",
      "\n",
      "avg / total       0.97      0.97      0.97       157\n",
      "\n",
      "[139   0   5  13]\n",
      "LR Accuracy:  0.9681528662420382\n",
      "LR F1:  0.9105209164481933\n",
      "For name:  m_pinto\n",
      "total sample size before apply threshold:  201\n",
      "Counter({'0000-0002-4676-1409': 79, '0000-0002-8521-2904': 23, '0000-0002-8122-7084': 15, '0000-0001-9778-2093': 14, '0000-0003-3061-9632': 14, '0000-0003-4684-4797': 14, '0000-0003-3462-7277': 10, '0000-0001-9730-5772': 8, '0000-0002-5928-6483': 7, '0000-0001-9663-8399': 5, '0000-0001-6370-3051': 4, '0000-0001-6835-2561': 3, '0000-0002-6337-3459': 2, '0000-0002-5376-742X': 2, '0000-0002-9890-6657': 1})\n",
      "['0000-0002-8122-7084', '0000-0001-9778-2093', '0000-0002-4676-1409', '0000-0003-3061-9632', '0000-0003-3462-7277', '0000-0003-4684-4797', '0000-0002-8521-2904']\n",
      "Total sample size after apply threshold:  169\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(169, 396)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(169, 396)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.73      0.85        15\n",
      "          1       1.00      1.00      1.00        14\n",
      "          2       0.84      0.99      0.91        79\n",
      "          3       1.00      0.93      0.96        14\n",
      "          4       1.00      0.70      0.82        10\n",
      "          5       0.93      0.93      0.93        14\n",
      "          6       0.94      0.70      0.80        23\n",
      "\n",
      "avg / total       0.91      0.90      0.90       169\n",
      "\n",
      "[11  0  4  0  0  0  0  0 14  0  0  0  0  0  0  0 78  0  0  0  1  0  0  1\n",
      " 13  0  0  0  0  0  3  0  7  0  0  0  0  1  0  0 13  0  0  0  6  0  0  1\n",
      " 16]\n",
      "svc Accuracy:  0.8994082840236687\n",
      "svc F1:  0.8954563419484272\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.80      0.89        15\n",
      "          1       1.00      1.00      1.00        14\n",
      "          2       0.84      0.99      0.91        79\n",
      "          3       1.00      0.93      0.96        14\n",
      "          4       1.00      0.70      0.82        10\n",
      "          5       0.92      0.86      0.89        14\n",
      "          6       0.82      0.61      0.70        23\n",
      "\n",
      "avg / total       0.89      0.89      0.88       169\n",
      "\n",
      "[12  0  3  0  0  0  0  0 14  0  0  0  0  0  0  0 78  0  0  0  1  0  0  1\n",
      " 13  0  0  0  0  0  2  0  7  0  1  0  0  1  0  0 12  1  0  0  8  0  0  1\n",
      " 14]\n",
      "LR Accuracy:  0.8875739644970414\n",
      "LR F1:  0.8816066995273563\n",
      "For name:  j_cuevas\n",
      "total sample size before apply threshold:  78\n",
      "Counter({'0000-0003-2049-3554': 40, '0000-0001-7421-0682': 29, '0000-0001-6327-1404': 6, '0000-0002-6815-3383': 3})\n",
      "['0000-0001-7421-0682', '0000-0003-2049-3554']\n",
      "Total sample size after apply threshold:  69\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(69, 178)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(69, 178)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        29\n",
      "          1       1.00      1.00      1.00        40\n",
      "\n",
      "avg / total       1.00      1.00      1.00        69\n",
      "\n",
      "[29  0  0 40]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        29\n",
      "          1       1.00      1.00      1.00        40\n",
      "\n",
      "avg / total       1.00      1.00      1.00        69\n",
      "\n",
      "[29  0  0 40]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  j_chang\n",
      "total sample size before apply threshold:  360\n",
      "Counter({'0000-0001-5726-9797': 85, '0000-0002-8423-5987': 38, '0000-0002-6596-931X': 33, '0000-0002-0890-9302': 31, '0000-0002-3880-3787': 29, '0000-0002-2717-0101': 23, '0000-0001-5582-0928': 17, '0000-0002-4655-1516': 17, '0000-0002-6477-6938': 15, '0000-0003-3773-182X': 12, '0000-0001-8651-2602': 11, '0000-0001-5039-2186': 9, '0000-0001-7843-2688': 9, '0000-0002-6711-1739': 8, '0000-0002-3974-8089': 5, '0000-0001-7449-4080': 4, '0000-0003-3469-9553': 4, '0000-0001-5241-8175': 3, '0000-0002-3811-1254': 2, '0000-0003-4633-587X': 2, '0000-0003-2613-7585': 1, '0000-0003-0041-4804': 1, '0000-0002-4296-4065': 1})\n",
      "['0000-0001-5726-9797', '0000-0003-3773-182X', '0000-0001-5582-0928', '0000-0002-2717-0101', '0000-0002-6596-931X', '0000-0002-6477-6938', '0000-0002-3880-3787', '0000-0002-0890-9302', '0000-0002-8423-5987', '0000-0002-4655-1516', '0000-0001-8651-2602']\n",
      "Total sample size after apply threshold:  311\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(311, 457)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(311, 457)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.94      0.94        85\n",
      "          1       0.92      1.00      0.96        12\n",
      "          2       0.73      0.65      0.69        17\n",
      "          3       0.70      0.70      0.70        23\n",
      "          4       0.76      0.85      0.80        33\n",
      "          5       0.83      0.67      0.74        15\n",
      "          6       0.76      0.86      0.81        29\n",
      "          7       0.68      0.74      0.71        31\n",
      "          8       0.64      0.66      0.65        38\n",
      "          9       1.00      0.59      0.74        17\n",
      "         10       0.89      0.73      0.80        11\n",
      "\n",
      "avg / total       0.80      0.80      0.80       311\n",
      "\n",
      "[80  0  0  1  0  1  0  0  3  0  0  0 12  0  0  0  0  0  0  0  0  0  0  1\n",
      " 11  2  0  0  2  1  0  0  0  1  0  0 16  2  1  0  0  3  0  0  0  0  1  0\n",
      " 28  0  0  1  3  0  0  2  0  0  0  0 10  0  0  3  0  0  0  0  0  1  0  0\n",
      " 25  3  0  0  0  0  0  2  0  1  0  3 23  1  0  1  3  0  0  2  6  0  1  1\n",
      " 25  0  0  0  0  0  0  0  0  2  5  0 10  0  0  0  1  1  0  0  0  0  1  0\n",
      "  8]\n",
      "svc Accuracy:  0.797427652733119\n",
      "svc F1:  0.7748909763600538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.93      0.92        85\n",
      "          1       0.92      1.00      0.96        12\n",
      "          2       0.92      0.65      0.76        17\n",
      "          3       0.77      0.74      0.76        23\n",
      "          4       0.77      0.82      0.79        33\n",
      "          5       0.79      0.73      0.76        15\n",
      "          6       0.90      0.90      0.90        29\n",
      "          7       0.75      0.77      0.76        31\n",
      "          8       0.67      0.74      0.70        38\n",
      "          9       1.00      0.88      0.94        17\n",
      "         10       0.82      0.82      0.82        11\n",
      "\n",
      "avg / total       0.84      0.83      0.83       311\n",
      "\n",
      "[79  0  0  0  0  1  0  0  5  0  0  0 12  0  0  0  0  0  0  0  0  0  0  1\n",
      " 11  2  0  0  2  1  0  0  0  1  0  0 17  3  1  0  0  1  0  0  2  0  0  0\n",
      " 27  0  0  1  3  0  0  2  0  0  0  0 11  0  0  2  0  0  0  0  0  1  0  0\n",
      " 26  2  0  0  0  0  0  1  0  1  0  1 24  2  0  2  2  0  0  1  4  1  0  2\n",
      " 28  0  0  0  0  0  0  0  0  0  2  0 15  0  0  0  0  1  0  0  0  0  1  0\n",
      "  9]\n",
      "LR Accuracy:  0.8327974276527331\n",
      "LR F1:  0.8240935903942155\n",
      "For name:  a_dias\n",
      "total sample size before apply threshold:  90\n",
      "Counter({'0000-0003-3641-3248': 19, '0000-0001-7048-7991': 14, '0000-0002-2651-6270': 13, '0000-0001-6895-372X': 13, '0000-0002-0865-0257': 11, '0000-0003-3197-9146': 6, '0000-0001-8881-3564': 4, '0000-0002-5111-0774': 3, '0000-0002-6057-9531': 2, '0000-0003-1921-0510': 2, '0000-0003-0060-2872': 1, '0000-0002-6210-8872': 1, '0000-0002-6667-1961': 1})\n",
      "['0000-0002-0865-0257', '0000-0003-3641-3248', '0000-0002-2651-6270', '0000-0001-7048-7991', '0000-0001-6895-372X']\n",
      "Total sample size after apply threshold:  70\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(70, 215)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(70, 215)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.82      0.82        11\n",
      "          1       0.66      1.00      0.79        19\n",
      "          2       1.00      0.69      0.82        13\n",
      "          3       1.00      0.93      0.96        14\n",
      "          4       0.88      0.54      0.67        13\n",
      "\n",
      "avg / total       0.85      0.81      0.81        70\n",
      "\n",
      "[ 9  1  0  0  1  0 19  0  0  0  1  3  9  0  0  0  1  0 13  0  1  5  0  0\n",
      "  7]\n",
      "svc Accuracy:  0.8142857142857143\n",
      "svc F1:  0.8115319865319865\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.91      0.91        11\n",
      "          1       0.72      0.95      0.82        19\n",
      "          2       1.00      0.85      0.92        13\n",
      "          3       0.93      1.00      0.97        14\n",
      "          4       1.00      0.62      0.76        13\n",
      "\n",
      "avg / total       0.90      0.87      0.87        70\n",
      "\n",
      "[10  1  0  0  0  0 18  0  1  0  1  1 11  0  0  0  0  0 14  0  0  5  0  0\n",
      "  8]\n",
      "LR Accuracy:  0.8714285714285714\n",
      "LR F1:  0.8742722794446932\n",
      "For name:  j_choi\n",
      "total sample size before apply threshold:  441\n",
      "Counter({'0000-0002-2775-3315': 98, '0000-0002-8439-6035': 42, '0000-0003-0018-8712': 25, '0000-0003-2379-2226': 23, '0000-0001-9760-9514': 21, '0000-0002-7491-6711': 21, '0000-0003-2206-4593': 20, '0000-0002-4850-8204': 19, '0000-0003-4897-3277': 15, '0000-0003-3257-2508': 15, '0000-0001-5408-9029': 14, '0000-0002-1161-6586': 13, '0000-0002-9663-4790': 13, '0000-0001-7348-9861': 12, '0000-0002-7532-5315': 10, '0000-0002-9210-9681': 10, '0000-0003-4805-7930': 9, '0000-0001-6121-5804': 8, '0000-0002-3864-9521': 7, '0000-0001-9302-7840': 6, '0000-0003-2891-8086': 4, '0000-0003-3179-6892': 4, '0000-0001-7938-8420': 3, '0000-0001-6336-6462': 3, '0000-0003-3284-9407': 3, '0000-0001-5007-7469': 2, '0000-0002-3280-1991': 2, '0000-0002-2894-3364': 2, '0000-0003-3940-8663': 2, '0000-0002-5086-7345': 2, '0000-0001-8047-9821': 2, '0000-0002-6639-8002': 1, '0000-0002-0723-5035': 1, '0000-0002-8328-4082': 1, '0000-0002-4663-3263': 1, '0000-0001-8023-084X': 1, '0000-0002-3863-4442': 1, '0000-0003-2578-1213': 1, '0000-0003-3554-7033': 1, '0000-0003-1060-0096': 1, '0000-0003-2277-1095': 1, '0000-0003-3155-3196': 1})\n",
      "['0000-0003-4897-3277', '0000-0003-0018-8712', '0000-0003-2379-2226', '0000-0002-1161-6586', '0000-0002-8439-6035', '0000-0001-9760-9514', '0000-0002-7532-5315', '0000-0002-7491-6711', '0000-0003-3257-2508', '0000-0002-9663-4790', '0000-0003-2206-4593', '0000-0002-2775-3315', '0000-0001-5408-9029', '0000-0002-4850-8204', '0000-0001-7348-9861', '0000-0002-9210-9681']\n",
      "Total sample size after apply threshold:  371\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(371, 620)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(371, 620)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.33      0.42        15\n",
      "          1       0.39      0.52      0.45        25\n",
      "          2       0.73      0.48      0.58        23\n",
      "          3       0.40      0.31      0.35        13\n",
      "          4       0.97      0.86      0.91        42\n",
      "          5       0.43      0.57      0.49        21\n",
      "          6       0.71      0.50      0.59        10\n",
      "          7       0.40      0.48      0.43        21\n",
      "          8       0.50      0.53      0.52        15\n",
      "          9       0.53      0.62      0.57        13\n",
      "         10       0.82      0.45      0.58        20\n",
      "         11       0.86      0.83      0.84        98\n",
      "         12       0.29      0.14      0.19        14\n",
      "         13       1.00      0.63      0.77        19\n",
      "         14       0.20      0.75      0.31        12\n",
      "         15       0.83      0.50      0.62        10\n",
      "\n",
      "avg / total       0.69      0.62      0.64       371\n",
      "\n",
      "[ 5  0  4  0  0  0  0  0  0  0  0  0  0  0  6  0  0 13  0  2  0  3  0  2\n",
      "  0  0  1  2  1  0  1  0  3  0 11  0  0  0  1  2  0  0  0  0  0  0  6  0\n",
      "  0  2  0  4  0  1  0  3  0  0  0  2  0  0  1  0  0  1  0  0 36  1  0  1\n",
      "  0  1  0  0  0  0  2  0  0  2  0  0  0 12  1  1  1  0  1  3  0  0  0  0\n",
      "  0  0  0  0  0  0  5  0  0  0  0  0  0  0  5  0  0  0  0  1  0  1  0 10\n",
      "  2  1  0  1  3  0  2  0  0  2  0  0  0  1  0  0  8  1  0  1  0  0  1  1\n",
      "  0  1  0  0  0  1  0  1  0  8  0  0  1  0  1  0  0  6  0  0  0  1  0  1\n",
      "  2  0  9  1  0  0  0  0  1  4  0  1  0  4  0  2  0  0  0 81  0  0  5  0\n",
      "  0  2  0  1  1  1  0  1  1  3  0  2  2  0  0  0  0  0  0  0  0  1  0  0\n",
      "  0  0  0  0  0 12  6  0  0  0  0  1  0  0  0  0  0  1  0  1  0  0  9  0\n",
      "  0  0  0  0  0  1  0  1  2  0  0  0  0  0  1  5]\n",
      "svc Accuracy:  0.6199460916442049\n",
      "svc F1:  0.5392430963614747\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.53      0.59        15\n",
      "          1       0.41      0.48      0.44        25\n",
      "          2       0.43      0.65      0.52        23\n",
      "          3       0.45      0.38      0.42        13\n",
      "          4       0.95      0.95      0.95        42\n",
      "          5       0.48      0.52      0.50        21\n",
      "          6       0.88      0.70      0.78        10\n",
      "          7       0.44      0.38      0.41        21\n",
      "          8       0.60      0.40      0.48        15\n",
      "          9       0.50      0.31      0.38        13\n",
      "         10       0.65      0.65      0.65        20\n",
      "         11       0.78      0.92      0.84        98\n",
      "         12       0.43      0.21      0.29        14\n",
      "         13       1.00      0.89      0.94        19\n",
      "         14       0.50      0.25      0.33        12\n",
      "         15       0.78      0.70      0.74        10\n",
      "\n",
      "avg / total       0.67      0.67      0.66       371\n",
      "\n",
      "[ 8  0  5  0  0  2  0  0  0  0  0  0  0  0  0  0  0 12  1  2  0  2  0  0\n",
      "  0  0  2  6  0  0  0  0  3  0 15  0  1  1  1  0  0  0  0  1  0  0  1  0\n",
      "  0  1  0  5  0  0  0  3  0  0  0  4  0  0  0  0  0  0  0  0 40  0  0  1\n",
      "  0  1  0  0  0  0  0  0  0  3  0  0  0 11  0  0  0  0  1  6  0  0  0  0\n",
      "  0  0  2  0  0  0  7  0  0  0  0  0  0  0  1  0  0  6  1  0  0  1  0  8\n",
      "  0  0  1  2  2  0  0  0  0  1  1  0  0  1  0  0  6  1  2  1  0  0  0  2\n",
      "  0  0  0  2  0  1  0  2  0  4  0  2  2  0  0  0  0  3  0  0  0  0  0  0\n",
      "  3  0 13  1  0  0  0  0  0  1  3  1  0  2  0  1  0  0  0 90  0  0  0  0\n",
      "  0  2  0  0  1  1  0  3  0  1  1  2  3  0  0  0  0  0  0  0  0  0  0  0\n",
      "  1  0  0  0  0 17  1  0  1  0  5  1  0  0  0  0  0  1  0  1  0  0  3  0\n",
      "  0  0  2  0  0  1  0  0  0  0  0  0  0  0  0  7]\n",
      "LR Accuracy:  0.6711590296495957\n",
      "LR F1:  0.5789855167789933\n",
      "For name:  m_ahmed\n",
      "total sample size before apply threshold:  27\n",
      "Counter({'0000-0002-4729-9068': 12, '0000-0002-1921-0724': 3, '0000-0002-4863-0402': 3, '0000-0002-4612-1815': 2, '0000-0002-3514-1327': 2, '0000-0002-7745-7522': 1, '0000-0002-9073-4969': 1, '0000-0002-3217-9688': 1, '0000-0002-2237-8456': 1, '0000-0001-7117-1032': 1})\n",
      "['0000-0002-4729-9068']\n",
      "Total sample size after apply threshold:  12\n",
      "For name:  j_jo\n",
      "total sample size before apply threshold:  13\n",
      "Counter({'0000-0001-9721-7641': 7, '0000-0001-8939-1623': 4, '0000-0002-6080-7493': 1, '0000-0002-5366-7605': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  n_dawson\n",
      "total sample size before apply threshold:  31\n",
      "Counter({'0000-0002-6607-9976': 14, '0000-0003-0123-897X': 14, '0000-0001-5389-8692': 2, '0000-0002-2658-8960': 1})\n",
      "['0000-0002-6607-9976', '0000-0003-0123-897X']\n",
      "Total sample size after apply threshold:  28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(28, 91)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(28, 91)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.93      0.96        14\n",
      "          1       0.93      1.00      0.97        14\n",
      "\n",
      "avg / total       0.97      0.96      0.96        28\n",
      "\n",
      "[13  1  0 14]\n",
      "svc Accuracy:  0.9642857142857143\n",
      "svc F1:  0.9642401021711366\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.93      0.96        14\n",
      "          1       0.93      1.00      0.97        14\n",
      "\n",
      "avg / total       0.97      0.96      0.96        28\n",
      "\n",
      "[13  1  0 14]\n",
      "LR Accuracy:  0.9642857142857143\n",
      "LR F1:  0.9642401021711366\n",
      "For name:  j_barbosa\n",
      "total sample size before apply threshold:  35\n",
      "Counter({'0000-0002-1854-1572': 11, '0000-0001-5879-9458': 9, '0000-0002-8664-8152': 9, '0000-0003-4135-2347': 3, '0000-0002-7259-2901': 1, '0000-0002-7828-2912': 1, '0000-0001-7869-5533': 1})\n",
      "['0000-0002-1854-1572']\n",
      "Total sample size after apply threshold:  11\n",
      "For name:  e_o'connor\n",
      "total sample size before apply threshold:  18\n",
      "Counter({'0000-0001-6727-6499': 8, '0000-0002-7810-1915': 5, '0000-0002-6961-6108': 3, '0000-0002-2971-6921': 1, '0000-0002-8228-796X': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  c_zheng\n",
      "total sample size before apply threshold:  46\n",
      "Counter({'0000-0001-5839-1305': 31, '0000-0002-7463-0289': 11, '0000-0002-0657-7914': 3, '0000-0002-6562-870X': 1})\n",
      "['0000-0001-5839-1305', '0000-0002-7463-0289']\n",
      "Total sample size after apply threshold:  42\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(42, 114)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(42, 114)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.97      0.91        31\n",
      "          1       0.86      0.55      0.67        11\n",
      "\n",
      "avg / total       0.86      0.86      0.85        42\n",
      "\n",
      "[30  1  5  6]\n",
      "svc Accuracy:  0.8571428571428571\n",
      "svc F1:  0.7878787878787877\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.97      0.87        31\n",
      "          1       0.75      0.27      0.40        11\n",
      "\n",
      "avg / total       0.78      0.79      0.75        42\n",
      "\n",
      "[30  1  8  3]\n",
      "LR Accuracy:  0.7857142857142857\n",
      "LR F1:  0.6347826086956522\n",
      "For name:  r_hall\n",
      "total sample size before apply threshold:  144\n",
      "Counter({'0000-0002-8318-8728': 92, '0000-0001-5504-6717': 41, '0000-0002-4908-8168': 6, '0000-0002-5460-0090': 3, '0000-0002-7743-630X': 2})\n",
      "['0000-0002-8318-8728', '0000-0001-5504-6717']\n",
      "Total sample size after apply threshold:  133\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(133, 351)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(133, 351)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      1.00      0.95        92\n",
      "          1       1.00      0.78      0.88        41\n",
      "\n",
      "avg / total       0.94      0.93      0.93       133\n",
      "\n",
      "[92  0  9 32]\n",
      "svc Accuracy:  0.9323308270676691\n",
      "svc F1:  0.915040102207396\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      1.00      0.94        92\n",
      "          1       1.00      0.73      0.85        41\n",
      "\n",
      "avg / total       0.93      0.92      0.91       133\n",
      "\n",
      "[92  0 11 30]\n",
      "LR Accuracy:  0.9172932330827067\n",
      "LR F1:  0.8943300830624774\n",
      "For name:  d_hwang\n",
      "total sample size before apply threshold:  52\n",
      "Counter({'0000-0002-2487-2255': 40, '0000-0002-9684-3998': 9, '0000-0001-5275-0354': 2, '0000-0001-6899-1769': 1})\n",
      "['0000-0002-2487-2255']\n",
      "Total sample size after apply threshold:  40\n",
      "For name:  c_shen\n",
      "total sample size before apply threshold:  111\n",
      "Counter({'0000-0002-0747-217X': 56, '0000-0003-2833-2771': 22, '0000-0002-2517-3472': 7, '0000-0001-7392-056X': 6, '0000-0002-5052-7762': 5, '0000-0002-0619-1309': 3, '0000-0001-8635-3429': 3, '0000-0002-3218-0689': 3, '0000-0002-3855-7360': 2, '0000-0003-1645-8211': 1, '0000-0002-0685-1901': 1, '0000-0002-9466-3838': 1, '0000-0002-5093-7657': 1})\n",
      "['0000-0003-2833-2771', '0000-0002-0747-217X']\n",
      "Total sample size after apply threshold:  78\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(78, 203)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(78, 203)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.68      0.81        22\n",
      "          1       0.89      1.00      0.94        56\n",
      "\n",
      "avg / total       0.92      0.91      0.90        78\n",
      "\n",
      "[15  7  0 56]\n",
      "svc Accuracy:  0.9102564102564102\n",
      "svc F1:  0.875993640699523\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.64      0.78        22\n",
      "          1       0.88      1.00      0.93        56\n",
      "\n",
      "avg / total       0.91      0.90      0.89        78\n",
      "\n",
      "[14  8  0 56]\n",
      "LR Accuracy:  0.8974358974358975\n",
      "LR F1:  0.8555555555555556\n",
      "For name:  v_lopes\n",
      "total sample size before apply threshold:  26\n",
      "Counter({'0000-0003-1599-2180': 20, '0000-0003-2278-8559': 3, '0000-0003-2079-4170': 2, '0000-0001-8276-4490': 1})\n",
      "['0000-0003-1599-2180']\n",
      "Total sample size after apply threshold:  20\n",
      "For name:  m_quintana\n",
      "total sample size before apply threshold:  68\n",
      "Counter({'0000-0003-3601-0262': 29, '0000-0002-7036-8658': 17, '0000-0002-3808-8189': 16, '0000-0002-7934-4361': 3, '0000-0001-6190-3324': 2, '0000-0002-2677-6179': 1})\n",
      "['0000-0002-3808-8189', '0000-0002-7036-8658', '0000-0003-3601-0262']\n",
      "Total sample size after apply threshold:  62\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(62, 194)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(62, 194)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        16\n",
      "          1       1.00      1.00      1.00        17\n",
      "          2       1.00      1.00      1.00        29\n",
      "\n",
      "avg / total       1.00      1.00      1.00        62\n",
      "\n",
      "[16  0  0  0 17  0  0  0 29]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        16\n",
      "          1       1.00      1.00      1.00        17\n",
      "          2       1.00      1.00      1.00        29\n",
      "\n",
      "avg / total       1.00      1.00      1.00        62\n",
      "\n",
      "[16  0  0  0 17  0  0  0 29]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  j_nunes\n",
      "total sample size before apply threshold:  62\n",
      "Counter({'0000-0001-6739-0304': 44, '0000-0001-6560-1518': 6, '0000-0003-0109-8268': 3, '0000-0002-0164-249X': 3, '0000-0002-9988-2060': 2, '0000-0002-3741-9513': 2, '0000-0002-9693-2827': 1, '0000-0003-4917-6771': 1})\n",
      "['0000-0001-6739-0304']\n",
      "Total sample size after apply threshold:  44\n",
      "For name:  z_nagy\n",
      "total sample size before apply threshold:  110\n",
      "Counter({'0000-0001-9756-5427': 57, '0000-0002-6700-4829': 50, '0000-0003-4196-2874': 1, '0000-0002-6014-3228': 1, '0000-0002-6493-5601': 1})\n",
      "['0000-0002-6700-4829', '0000-0001-9756-5427']\n",
      "Total sample size after apply threshold:  107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(107, 362)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(107, 362)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.92      0.88        50\n",
      "          1       0.92      0.84      0.88        57\n",
      "\n",
      "avg / total       0.88      0.88      0.88       107\n",
      "\n",
      "[46  4  9 48]\n",
      "svc Accuracy:  0.8785046728971962\n",
      "svc F1:  0.8784622105723023\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.78      0.88        50\n",
      "          1       0.84      1.00      0.91        57\n",
      "\n",
      "avg / total       0.91      0.90      0.90       107\n",
      "\n",
      "[39 11  0 57]\n",
      "LR Accuracy:  0.897196261682243\n",
      "LR F1:  0.8942022471910112\n",
      "For name:  e_brown\n",
      "total sample size before apply threshold:  71\n",
      "Counter({'0000-0002-5995-834X': 28, '0000-0002-8438-6136': 8, '0000-0002-0209-3293': 8, '0000-0003-1411-5792': 7, '0000-0002-2641-1890': 6, '0000-0002-2762-2489': 5, '0000-0003-3806-5339': 4, '0000-0002-1575-2606': 2, '0000-0002-6611-5770': 1, '0000-0002-1398-5721': 1, '0000-0001-7523-0685': 1})\n",
      "['0000-0002-5995-834X']\n",
      "Total sample size after apply threshold:  28\n",
      "For name:  j_nielsen\n",
      "total sample size before apply threshold:  913\n",
      "Counter({'0000-0002-9955-6003': 487, '0000-0001-5568-2916': 105, '0000-0001-9414-1653': 104, '0000-0002-8747-6938': 57, '0000-0002-2831-7718': 39, '0000-0002-2854-8188': 35, '0000-0003-2228-5994': 24, '0000-0002-2058-3579': 23, '0000-0003-1730-3094': 13, '0000-0001-8521-7353': 9, '0000-0002-5211-948X': 8, '0000-0002-8112-8449': 6, '0000-0002-3418-4907': 2, '0000-0002-4760-3875': 1})\n",
      "['0000-0001-9414-1653', '0000-0002-2831-7718', '0000-0003-2228-5994', '0000-0002-9955-6003', '0000-0002-2854-8188', '0000-0002-8747-6938', '0000-0003-1730-3094', '0000-0002-2058-3579', '0000-0001-5568-2916']\n",
      "Total sample size after apply threshold:  887\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(887, 1729)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(887, 1729)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.78      0.82       104\n",
      "          1       1.00      0.51      0.68        39\n",
      "          2       0.88      0.62      0.73        24\n",
      "          3       0.85      0.96      0.90       487\n",
      "          4       0.96      0.63      0.76        35\n",
      "          5       0.80      0.79      0.80        57\n",
      "          6       1.00      0.85      0.92        13\n",
      "          7       0.94      0.74      0.83        23\n",
      "          8       0.93      0.83      0.87       105\n",
      "\n",
      "avg / total       0.87      0.86      0.86       887\n",
      "\n",
      "[ 81   0   1  21   0   0   0   1   0   0  20   0  19   0   0   0   0   0\n",
      "   4   0  15   4   0   1   0   0   0   5   0   1 469   0   7   0   0   5\n",
      "   1   0   0   8  22   2   0   0   2   0   0   0  12   0  45   0   0   0\n",
      "   0   0   0   2   0   0  11   0   0   0   0   0   6   0   0   0  17   0\n",
      "   2   0   0  14   1   1   0   0  87]\n",
      "svc Accuracy:  0.8647125140924464\n",
      "svc F1:  0.8119542297800945\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.77      0.85       104\n",
      "          1       1.00      0.33      0.50        39\n",
      "          2       0.92      0.46      0.61        24\n",
      "          3       0.79      0.99      0.88       487\n",
      "          4       1.00      0.31      0.48        35\n",
      "          5       0.84      0.65      0.73        57\n",
      "          6       1.00      0.46      0.63        13\n",
      "          7       1.00      0.57      0.72        23\n",
      "          8       0.93      0.82      0.87       105\n",
      "\n",
      "avg / total       0.86      0.83      0.82       887\n",
      "\n",
      "[ 80   0   0  24   0   0   0   0   0   0  13   0  26   0   0   0   0   0\n",
      "   2   0  11   8   0   2   0   0   1   2   0   1 481   0   3   0   0   0\n",
      "   1   0   0  17  11   1   0   0   5   0   0   0  20   0  37   0   0   0\n",
      "   0   0   0   6   0   1   6   0   0   0   0   0  10   0   0   0  13   0\n",
      "   0   0   0  19   0   0   0   0  86]\n",
      "LR Accuracy:  0.8320180383314544\n",
      "LR F1:  0.6968491271522824\n",
      "For name:  w_choi\n",
      "total sample size before apply threshold:  118\n",
      "Counter({'0000-0003-1801-9386': 79, '0000-0002-7896-7655': 16, '0000-0002-6623-3806': 7, '0000-0002-4203-0457': 6, '0000-0001-8038-5876': 3, '0000-0002-7183-3400': 3, '0000-0003-4233-0174': 2, '0000-0001-5171-2890': 2})\n",
      "['0000-0003-1801-9386', '0000-0002-7896-7655']\n",
      "Total sample size after apply threshold:  95\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(95, 118)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(95, 118)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.99      0.96        79\n",
      "          1       0.91      0.62      0.74        16\n",
      "\n",
      "avg / total       0.93      0.93      0.92        95\n",
      "\n",
      "[78  1  6 10]\n",
      "svc Accuracy:  0.9263157894736842\n",
      "svc F1:  0.8488979777323336\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.99      0.93        79\n",
      "          1       0.83      0.31      0.45        16\n",
      "\n",
      "avg / total       0.87      0.87      0.85        95\n",
      "\n",
      "[78  1 11  5]\n",
      "LR Accuracy:  0.8736842105263158\n",
      "LR F1:  0.6915584415584416\n",
      "For name:  d_tavares\n",
      "total sample size before apply threshold:  13\n",
      "Counter({'0000-0002-3196-7922': 4, '0000-0002-6811-9572': 3, '0000-0002-6807-8504': 3, '0000-0002-3358-9443': 2, '0000-0003-4646-5914': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  l_alves\n",
      "total sample size before apply threshold:  51\n",
      "Counter({'0000-0001-6245-775X': 14, '0000-0001-5855-2754': 11, '0000-0001-5369-5019': 9, '0000-0002-1972-2658': 5, '0000-0002-7938-9850': 4, '0000-0002-7531-3648': 2, '0000-0002-8944-1851': 2, '0000-0001-8069-6527': 1, '0000-0003-4650-3140': 1, '0000-0002-8400-6148': 1, '0000-0001-6659-6431': 1})\n",
      "['0000-0001-5855-2754', '0000-0001-6245-775X']\n",
      "Total sample size after apply threshold:  25\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(25, 92)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(25, 92)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.91      0.91        11\n",
      "          1       0.93      0.93      0.93        14\n",
      "\n",
      "avg / total       0.92      0.92      0.92        25\n",
      "\n",
      "[10  1  1 13]\n",
      "svc Accuracy:  0.92\n",
      "svc F1:  0.9188311688311688\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.82      0.90        11\n",
      "          1       0.88      1.00      0.93        14\n",
      "\n",
      "avg / total       0.93      0.92      0.92        25\n",
      "\n",
      "[ 9  2  0 14]\n",
      "LR Accuracy:  0.92\n",
      "LR F1:  0.9166666666666667\n",
      "For name:  s_chan\n",
      "total sample size before apply threshold:  176\n",
      "Counter({'0000-0001-8238-798X': 76, '0000-0002-9554-7273': 27, '0000-0002-3270-0525': 19, '0000-0003-0274-7258': 11, '0000-0001-6322-2821': 11, '0000-0002-1568-0489': 11, '0000-0001-8322-7443': 9, '0000-0002-8524-229X': 5, '0000-0002-7707-656X': 3, '0000-0001-5326-2758': 2, '0000-0003-0488-1207': 1, '0000-0002-5193-7560': 1})\n",
      "['0000-0002-3270-0525', '0000-0003-0274-7258', '0000-0002-9554-7273', '0000-0001-8238-798X', '0000-0001-6322-2821', '0000-0002-1568-0489']\n",
      "Total sample size after apply threshold:  155\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(155, 280)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(155, 280)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.89      0.87        19\n",
      "          1       0.92      1.00      0.96        11\n",
      "          2       0.88      0.78      0.82        27\n",
      "          3       0.90      0.93      0.92        76\n",
      "          4       0.80      0.73      0.76        11\n",
      "          5       1.00      0.91      0.95        11\n",
      "\n",
      "avg / total       0.89      0.89      0.89       155\n",
      "\n",
      "[17  0  0  2  0  0  0 11  0  0  0  0  0  0 21  4  2  0  2  1  2 71  0  0\n",
      "  1  0  1  1  8  0  0  0  0  1  0 10]\n",
      "svc Accuracy:  0.8903225806451613\n",
      "svc F1:  0.8803767948722986\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.79      0.81        19\n",
      "          1       1.00      1.00      1.00        11\n",
      "          2       0.95      0.78      0.86        27\n",
      "          3       0.86      0.96      0.91        76\n",
      "          4       0.89      0.73      0.80        11\n",
      "          5       1.00      0.91      0.95        11\n",
      "\n",
      "avg / total       0.89      0.89      0.89       155\n",
      "\n",
      "[15  0  0  4  0  0  0 11  0  0  0  0  0  0 21  5  1  0  2  0  1 73  0  0\n",
      "  1  0  0  2  8  0  0  0  0  1  0 10]\n",
      "LR Accuracy:  0.8903225806451613\n",
      "LR F1:  0.8878611530785445\n",
      "For name:  b_ferreira\n",
      "total sample size before apply threshold:  29\n",
      "Counter({'0000-0002-8565-3101': 10, '0000-0002-6781-2236': 6, '0000-0003-2156-2988': 6, '0000-0002-0221-3160': 3, '0000-0003-1388-5015': 3, '0000-0002-5612-5385': 1})\n",
      "['0000-0002-8565-3101']\n",
      "Total sample size after apply threshold:  10\n",
      "For name:  r_neves\n",
      "total sample size before apply threshold:  25\n",
      "Counter({'0000-0003-4866-5215': 19, '0000-0001-6571-5697': 3, '0000-0003-3819-1714': 2, '0000-0003-2032-9308': 1})\n",
      "['0000-0003-4866-5215']\n",
      "Total sample size after apply threshold:  19\n",
      "For name:  m_cardoso\n",
      "total sample size before apply threshold:  105\n",
      "Counter({'0000-0003-0973-3908': 38, '0000-0002-8137-3700': 24, '0000-0003-2102-1225': 16, '0000-0001-7766-7557': 6, '0000-0001-5124-0432': 5, '0000-0001-8676-1115': 4, '0000-0002-3633-1659': 3, '0000-0002-7578-4052': 2, '0000-0002-9132-9703': 2, '0000-0003-4725-2996': 2, '0000-0003-2447-6882': 1, '0000-0003-0150-7359': 1, '0000-0002-8405-7471': 1})\n",
      "['0000-0002-8137-3700', '0000-0003-2102-1225', '0000-0003-0973-3908']\n",
      "Total sample size after apply threshold:  78\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(78, 271)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(78, 271)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.79      0.88        24\n",
      "          1       0.93      0.81      0.87        16\n",
      "          2       0.82      0.97      0.89        38\n",
      "\n",
      "avg / total       0.90      0.88      0.88        78\n",
      "\n",
      "[19  0  5  0 13  3  0  1 37]\n",
      "svc Accuracy:  0.8846153846153846\n",
      "svc F1:  0.8806512873198219\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.83      0.91        24\n",
      "          1       1.00      0.75      0.86        16\n",
      "          2       0.83      1.00      0.90        38\n",
      "\n",
      "avg / total       0.92      0.90      0.90        78\n",
      "\n",
      "[20  0  4  0 12  4  0  0 38]\n",
      "LR Accuracy:  0.8974358974358975\n",
      "LR F1:  0.8903318903318903\n",
      "For name:  c_shao\n",
      "total sample size before apply threshold:  96\n",
      "Counter({'0000-0003-2618-9342': 61, '0000-0002-6953-2203': 23, '0000-0001-8260-4761': 9, '0000-0002-8691-5177': 3})\n",
      "['0000-0002-6953-2203', '0000-0003-2618-9342']\n",
      "Total sample size after apply threshold:  84\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(84, 145)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(84, 145)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.91      0.89        23\n",
      "          1       0.97      0.95      0.96        61\n",
      "\n",
      "avg / total       0.94      0.94      0.94        84\n",
      "\n",
      "[21  2  3 58]\n",
      "svc Accuracy:  0.9404761904761905\n",
      "svc F1:  0.9261473536135045\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.87      0.89        23\n",
      "          1       0.95      0.97      0.96        61\n",
      "\n",
      "avg / total       0.94      0.94      0.94        84\n",
      "\n",
      "[20  3  2 59]\n",
      "LR Accuracy:  0.9404761904761905\n",
      "LR F1:  0.924119241192412\n",
      "For name:  h_yeo\n",
      "total sample size before apply threshold:  10\n",
      "Counter({'0000-0003-2219-3483': 3, '0000-0003-2629-4353': 2, '0000-0002-2684-0978': 2, '0000-0002-1779-069X': 2, '0000-0002-8403-5790': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  m_goodman\n",
      "total sample size before apply threshold:  100\n",
      "Counter({'0000-0002-5810-1272': 57, '0000-0001-8932-624X': 41, '0000-0003-3880-7822': 1, '0000-0003-1779-4698': 1})\n",
      "['0000-0002-5810-1272', '0000-0001-8932-624X']\n",
      "Total sample size after apply threshold:  98\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(98, 254)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(98, 254)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      1.00      0.92        57\n",
      "          1       1.00      0.76      0.86        41\n",
      "\n",
      "avg / total       0.91      0.90      0.89        98\n",
      "\n",
      "[57  0 10 31]\n",
      "svc Accuracy:  0.8979591836734694\n",
      "svc F1:  0.8902329749103943\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      1.00      0.90        57\n",
      "          1       1.00      0.68      0.81        41\n",
      "\n",
      "avg / total       0.89      0.87      0.86        98\n",
      "\n",
      "[57  0 13 28]\n",
      "LR Accuracy:  0.8673469387755102\n",
      "LR F1:  0.8546159990870705\n",
      "For name:  r_dias\n",
      "total sample size before apply threshold:  26\n",
      "Counter({'0000-0002-9214-2166': 15, '0000-0001-7921-405X': 7, '0000-0002-6804-7409': 3, '0000-0003-1503-998X': 1})\n",
      "['0000-0002-9214-2166']\n",
      "Total sample size after apply threshold:  15\n",
      "For name:  s_sengupta\n",
      "total sample size before apply threshold:  149\n",
      "Counter({'0000-0003-3357-1216': 64, '0000-0002-6365-1770': 31, '0000-0001-7441-5856': 31, '0000-0001-8187-3396': 9, '0000-0002-9665-0088': 7, '0000-0001-7452-979X': 6, '0000-0002-5933-4430': 1})\n",
      "['0000-0002-6365-1770', '0000-0001-7441-5856', '0000-0003-3357-1216']\n",
      "Total sample size after apply threshold:  126\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(126, 327)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(126, 327)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.84      0.90        31\n",
      "          1       0.96      0.81      0.88        31\n",
      "          2       0.88      1.00      0.93        64\n",
      "\n",
      "avg / total       0.92      0.91      0.91       126\n",
      "\n",
      "[26  1  4  1 25  5  0  0 64]\n",
      "svc Accuracy:  0.9126984126984127\n",
      "svc F1:  0.9026837586457125\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.87      0.93        31\n",
      "          1       1.00      0.77      0.87        31\n",
      "          2       0.85      1.00      0.92        64\n",
      "\n",
      "avg / total       0.93      0.91      0.91       126\n",
      "\n",
      "[27  0  4  0 24  7  0  0 64]\n",
      "LR Accuracy:  0.9126984126984127\n",
      "LR F1:  0.908208354946137\n",
      "For name:  y_jung\n",
      "total sample size before apply threshold:  180\n",
      "Counter({'0000-0002-9686-3120': 85, '0000-0003-2098-8143': 39, '0000-0002-2011-1459': 18, '0000-0002-0781-3608': 9, '0000-0003-0357-9508': 9, '0000-0002-9785-0348': 5, '0000-0002-1743-5049': 4, '0000-0003-0169-2865': 4, '0000-0002-8871-1979': 3, '0000-0002-4778-4629': 2, '0000-0001-6615-6401': 1, '0000-0001-7924-6967': 1})\n",
      "['0000-0002-2011-1459', '0000-0002-9686-3120', '0000-0003-2098-8143']\n",
      "Total sample size after apply threshold:  142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(142, 1266)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(142, 1266)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.78      0.85        18\n",
      "          1       0.93      0.92      0.92        85\n",
      "          2       0.79      0.87      0.83        39\n",
      "\n",
      "avg / total       0.89      0.89      0.89       142\n",
      "\n",
      "[14  2  2  0 78  7  1  4 34]\n",
      "svc Accuracy:  0.8873239436619719\n",
      "svc F1:  0.8669433547482327\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.56      0.69        18\n",
      "          1       0.91      0.96      0.94        85\n",
      "          2       0.85      0.90      0.88        39\n",
      "\n",
      "avg / total       0.90      0.89      0.89       142\n",
      "\n",
      "[10  5  3  0 82  3  1  3 35]\n",
      "LR Accuracy:  0.8943661971830986\n",
      "LR F1:  0.8339326765188835\n",
      "For name:  c_franco\n",
      "total sample size before apply threshold:  64\n",
      "Counter({'0000-0003-1958-3851': 28, '0000-0003-2288-1518': 18, '0000-0002-2861-3883': 17, '0000-0003-2729-4064': 1})\n",
      "['0000-0003-2288-1518', '0000-0002-2861-3883', '0000-0003-1958-3851']\n",
      "Total sample size after apply threshold:  63\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(63, 230)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(63, 230)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.94      0.97        18\n",
      "          1       1.00      0.82      0.90        17\n",
      "          2       0.88      1.00      0.93        28\n",
      "\n",
      "avg / total       0.94      0.94      0.94        63\n",
      "\n",
      "[17  0  1  0 14  3  0  0 28]\n",
      "svc Accuracy:  0.9365079365079365\n",
      "svc F1:  0.9359959037378393\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.89      0.94        18\n",
      "          1       1.00      0.94      0.97        17\n",
      "          2       0.90      1.00      0.95        28\n",
      "\n",
      "avg / total       0.96      0.95      0.95        63\n",
      "\n",
      "[16  0  2  0 16  1  0  0 28]\n",
      "LR Accuracy:  0.9523809523809523\n",
      "LR F1:  0.953341994219362\n",
      "For name:  v_wong\n",
      "total sample size before apply threshold:  35\n",
      "Counter({'0000-0001-6751-7942': 14, '0000-0002-2951-8108': 12, '0000-0001-9356-7556': 8, '0000-0003-2844-3789': 1})\n",
      "['0000-0001-6751-7942', '0000-0002-2951-8108']\n",
      "Total sample size after apply threshold:  26\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(26, 76)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(26, 76)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      1.00      0.93        14\n",
      "          1       1.00      0.83      0.91        12\n",
      "\n",
      "avg / total       0.93      0.92      0.92        26\n",
      "\n",
      "[14  0  2 10]\n",
      "svc Accuracy:  0.9230769230769231\n",
      "svc F1:  0.9212121212121211\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      1.00      0.97        14\n",
      "          1       1.00      0.92      0.96        12\n",
      "\n",
      "avg / total       0.96      0.96      0.96        26\n",
      "\n",
      "[14  0  1 11]\n",
      "LR Accuracy:  0.9615384615384616\n",
      "LR F1:  0.9610194902548725\n",
      "For name:  j_feng\n",
      "total sample size before apply threshold:  147\n",
      "Counter({'0000-0003-4762-7532': 102, '0000-0003-1944-718X': 13, '0000-0002-9410-7508': 13, '0000-0002-7141-5823': 12, '0000-0002-5683-849X': 3, '0000-0002-2894-4324': 2, '0000-0002-8662-2198': 1, '0000-0002-6974-2956': 1})\n",
      "['0000-0003-1944-718X', '0000-0003-4762-7532', '0000-0002-7141-5823', '0000-0002-9410-7508']\n",
      "Total sample size after apply threshold:  140\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(140, 175)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(140, 175)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.62      0.76        13\n",
      "          1       0.91      1.00      0.95       102\n",
      "          2       0.78      0.58      0.67        12\n",
      "          3       0.73      0.62      0.67        13\n",
      "\n",
      "avg / total       0.89      0.89      0.88       140\n",
      "\n",
      "[  8   3   0   2   0 102   0   0   0   4   7   1   0   3   2   8]\n",
      "svc Accuracy:  0.8928571428571429\n",
      "svc F1:  0.7621272808188697\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.46      0.63        13\n",
      "          1       0.86      1.00      0.92       102\n",
      "          2       0.75      0.25      0.38        12\n",
      "          3       0.73      0.62      0.67        13\n",
      "\n",
      "avg / total       0.85      0.85      0.83       140\n",
      "\n",
      "[  6   5   0   2   0 102   0   0   0   8   3   1   0   4   1   8]\n",
      "LR Accuracy:  0.85\n",
      "LR F1:  0.6490806342780027\n",
      "For name:  s_murugesan\n",
      "total sample size before apply threshold:  7\n",
      "Counter({'0000-0003-0154-2859': 3, '0000-0001-8386-6536': 2, '0000-0003-3045-3513': 1, '0000-0003-4264-1984': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  j_camacho\n",
      "total sample size before apply threshold:  139\n",
      "Counter({'0000-0002-2507-9814': 81, '0000-0003-3182-5227': 33, '0000-0002-8095-4167': 17, '0000-0001-7528-558X': 8})\n",
      "['0000-0002-8095-4167', '0000-0002-2507-9814', '0000-0003-3182-5227']\n",
      "Total sample size after apply threshold:  131\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(131, 252)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(131, 252)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.88      0.94        17\n",
      "          1       0.94      1.00      0.97        81\n",
      "          2       1.00      0.91      0.95        33\n",
      "\n",
      "avg / total       0.96      0.96      0.96       131\n",
      "\n",
      "[15  2  0  0 81  0  0  3 30]\n",
      "svc Accuracy:  0.9618320610687023\n",
      "svc F1:  0.9533136108734911\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.88      0.94        17\n",
      "          1       0.96      1.00      0.98        81\n",
      "          2       1.00      0.97      0.98        33\n",
      "\n",
      "avg / total       0.98      0.98      0.98       131\n",
      "\n",
      "[15  2  0  0 81  0  0  1 32]\n",
      "LR Accuracy:  0.9770992366412213\n",
      "LR F1:  0.9679778554778555\n",
      "For name:  b_moreno\n",
      "total sample size before apply threshold:  8\n",
      "Counter({'0000-0001-5799-9802': 6, '0000-0002-8881-4329': 1, '0000-0002-1530-4977': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  j_andersen\n",
      "total sample size before apply threshold:  129\n",
      "Counter({'0000-0003-1710-1628': 40, '0000-0001-5613-5236': 19, '0000-0001-6300-9086': 19, '0000-0002-8067-3074': 18, '0000-0003-4528-2120': 15, '0000-0001-8902-8162': 6, '0000-0002-6062-7740': 5, '0000-0003-2444-6210': 4, '0000-0003-1402-8162': 2, '0000-0002-4089-4884': 1})\n",
      "['0000-0003-1710-1628', '0000-0001-5613-5236', '0000-0002-8067-3074', '0000-0003-4528-2120', '0000-0001-6300-9086']\n",
      "Total sample size after apply threshold:  111\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(111, 380)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(111, 380)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.82      0.90        40\n",
      "          1       0.48      0.84      0.62        19\n",
      "          2       1.00      0.83      0.91        18\n",
      "          3       1.00      0.60      0.75        15\n",
      "          4       0.71      0.79      0.75        19\n",
      "\n",
      "avg / total       0.86      0.79      0.81       111\n",
      "\n",
      "[33  4  0  0  3  0 16  0  0  3  0  3 15  0  0  0  6  0  9  0  0  4  0  0\n",
      " 15]\n",
      "svc Accuracy:  0.7927927927927928\n",
      "svc F1:  0.7857170227033241\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.93      0.95        40\n",
      "          1       0.63      1.00      0.78        19\n",
      "          2       1.00      0.89      0.94        18\n",
      "          3       1.00      0.80      0.89        15\n",
      "          4       0.93      0.74      0.82        19\n",
      "\n",
      "avg / total       0.92      0.88      0.89       111\n",
      "\n",
      "[37  2  0  0  1  0 19  0  0  0  1  1 16  0  0  0  3  0 12  0  0  5  0  0\n",
      " 14]\n",
      "LR Accuracy:  0.8828828828828829\n",
      "LR F1:  0.8755645848082823\n",
      "For name:  j_bell\n",
      "total sample size before apply threshold:  34\n",
      "Counter({'0000-0003-1455-3274': 15, '0000-0002-1926-7501': 10, '0000-0001-5480-7975': 5, '0000-0002-0233-9708': 3, '0000-0002-6145-5821': 1})\n",
      "['0000-0002-1926-7501', '0000-0003-1455-3274']\n",
      "Total sample size after apply threshold:  25\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(25, 109)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(25, 109)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.90      0.95        10\n",
      "          1       0.94      1.00      0.97        15\n",
      "\n",
      "avg / total       0.96      0.96      0.96        25\n",
      "\n",
      "[ 9  1  0 15]\n",
      "svc Accuracy:  0.96\n",
      "svc F1:  0.9575551782682513\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.90      0.95        10\n",
      "          1       0.94      1.00      0.97        15\n",
      "\n",
      "avg / total       0.96      0.96      0.96        25\n",
      "\n",
      "[ 9  1  0 15]\n",
      "LR Accuracy:  0.96\n",
      "LR F1:  0.9575551782682513\n",
      "For name:  m_bull\n",
      "total sample size before apply threshold:  5\n",
      "Counter({'0000-0002-4804-9992': 3, '0000-0002-9388-0021': 1, '0000-0002-2324-1195': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  s_gandhi\n",
      "total sample size before apply threshold:  9\n",
      "Counter({'0000-0002-9759-1745': 4, '0000-0002-3650-3780': 2, '0000-0002-6867-1447': 2, '0000-0002-6762-0867': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  c_yang\n",
      "total sample size before apply threshold:  514\n",
      "Counter({'0000-0002-4328-8716': 71, '0000-0002-3476-3802': 53, '0000-0001-6312-3719': 48, '0000-0003-4082-420X': 42, '0000-0002-0352-3144': 41, '0000-0001-9138-3075': 32, '0000-0001-8558-062X': 25, '0000-0003-0475-8399': 18, '0000-0001-5990-1346': 17, '0000-0002-2792-6247': 17, '0000-0002-6238-2871': 17, '0000-0003-0561-2340': 15, '0000-0001-8144-8496': 13, '0000-0001-8109-4974': 13, '0000-0002-9147-3879': 12, '0000-0002-8613-3597': 12, '0000-0002-0521-4230': 11, '0000-0003-4927-4814': 10, '0000-0002-5542-7576': 6, '0000-0003-0760-9209': 5, '0000-0002-0487-0420': 5, '0000-0002-3527-6600': 5, '0000-0003-1163-321X': 3, '0000-0003-3368-3082': 3, '0000-0002-5527-6819': 3, '0000-0002-6815-3316': 3, '0000-0003-3456-0455': 2, '0000-0001-5463-6926': 2, '0000-0001-6067-7505': 2, '0000-0002-9579-4426': 2, '0000-0002-2196-6854': 2, '0000-0002-5682-8531': 1, '0000-0002-1380-9533': 1, '0000-0001-7768-4066': 1, '0000-0001-5615-2693': 1})\n",
      "['0000-0002-3476-3802', '0000-0003-0561-2340', '0000-0001-8144-8496', '0000-0002-0352-3144', '0000-0002-4328-8716', '0000-0001-8109-4974', '0000-0002-9147-3879', '0000-0002-8613-3597', '0000-0003-4082-420X', '0000-0001-5990-1346', '0000-0003-4927-4814', '0000-0002-0521-4230', '0000-0001-9138-3075', '0000-0001-8558-062X', '0000-0001-6312-3719', '0000-0003-0475-8399', '0000-0002-2792-6247', '0000-0002-6238-2871']\n",
      "Total sample size after apply threshold:  467\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(467, 419)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(467, 419)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.77      0.80        53\n",
      "          1       0.80      0.27      0.40        15\n",
      "          2       0.22      0.15      0.18        13\n",
      "          3       0.52      0.39      0.44        41\n",
      "          4       0.77      0.90      0.83        71\n",
      "          5       1.00      0.92      0.96        13\n",
      "          6       0.82      0.75      0.78        12\n",
      "          7       0.75      0.50      0.60        12\n",
      "          8       0.42      0.81      0.55        42\n",
      "          9       0.85      0.65      0.73        17\n",
      "         10       0.38      0.30      0.33        10\n",
      "         11       1.00      0.91      0.95        11\n",
      "         12       0.84      0.84      0.84        32\n",
      "         13       0.86      0.96      0.91        25\n",
      "         14       0.63      0.65      0.64        48\n",
      "         15       0.88      0.83      0.86        18\n",
      "         16       0.78      0.41      0.54        17\n",
      "         17       0.42      0.29      0.34        17\n",
      "\n",
      "avg / total       0.70      0.69      0.68       467\n",
      "\n",
      "[41  0  0  1  2  0  0  1  3  0  1  0  1  0  1  0  0  2  0  4  1  0  0  0\n",
      "  0  0 10  0  0  0  0  0  0  0  0  0  0  0  2  2  3  0  0  0  4  0  0  0\n",
      "  0  0  1  0  1  0  4  0  2 16  2  0  0  0  5  0  1  0  0  0  9  0  0  2\n",
      "  0  0  0  2 64  0  0  0  3  0  1  0  0  0  0  0  0  1  0  0  0  0  0 12\n",
      "  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  9  0  1  0  0  0\n",
      "  0  0  0  2  0  0  1  0  0  0  1  0  0  6  1  0  1  0  1  0  0  0  1  0\n",
      "  0  0  1  3  1  0  0  0 34  0  0  0  1  0  2  0  0  0  0  0  0  0  0  0\n",
      "  0  0  5 11  1  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0  2  0  3  0\n",
      "  0  0  1  0  0  1  0  0  0  0  0  0  0  0  0  0  0 10  0  1  0  0  0  0\n",
      "  2  0  0  0  0  0  0  0  0  1  0  0 27  2  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  1 24  0  0  0  0  0  0  2  5  4  0  0  1  4  0  0  0\n",
      "  0  0 31  0  0  1  0  0  0  1  0  0  2  0  0  0  0  0  0  0  0 15  0  0\n",
      "  0  0  1  0  2  0  0  0  4  1  0  0  1  0  1  0  7  0  1  1  0  1  1  0\n",
      "  0  0  5  0  0  0  0  0  3  0  0  5]\n",
      "svc Accuracy:  0.6873661670235546\n",
      "svc F1:  0.650270693673096\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.81      0.83        53\n",
      "          1       1.00      0.27      0.42        15\n",
      "          2       0.40      0.15      0.22        13\n",
      "          3       0.60      0.44      0.51        41\n",
      "          4       0.76      0.92      0.83        71\n",
      "          5       1.00      1.00      1.00        13\n",
      "          6       1.00      0.83      0.91        12\n",
      "          7       0.75      0.50      0.60        12\n",
      "          8       0.41      0.71      0.52        42\n",
      "          9       0.85      0.65      0.73        17\n",
      "         10       0.33      0.10      0.15        10\n",
      "         11       1.00      0.82      0.90        11\n",
      "         12       0.78      0.91      0.84        32\n",
      "         13       0.96      0.96      0.96        25\n",
      "         14       0.57      0.77      0.65        48\n",
      "         15       0.89      0.94      0.92        18\n",
      "         16       0.62      0.47      0.53        17\n",
      "         17       0.75      0.18      0.29        17\n",
      "\n",
      "avg / total       0.73      0.71      0.69       467\n",
      "\n",
      "[43  0  0  1  0  0  0  0  5  0  0  0  1  0  3  0  0  0  0  4  1  0  0  0\n",
      "  0  0 10  0  0  0  0  0  0  0  0  0  0  0  2  2  3  0  0  0  4  0  0  0\n",
      "  0  0  1  0  1  0  3  0  0 18  2  0  0  1  4  1  1  0  0  0 11  0  0  0\n",
      "  0  0  0  2 65  0  0  0  2  0  0  0  0  0  1  0  1  0  0  0  0  0  0 13\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0  0  0  0\n",
      "  0  0  0  2  0  0  1  0  0  0  2  0  0  6  0  0  0  0  0  0  1  0  2  0\n",
      "  0  0  1  1  1  0  0  0 30  1  0  0  2  0  4  0  1  1  0  0  0  0  0  0\n",
      "  0  0  4 11  1  0  1  0  0  0  0  0  0  0  0  1  4  0  0  0  1  0  1  0\n",
      "  0  0  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  9  1  1  0  0  0  0\n",
      "  0  0  0  0  2  0  0  0  1  0  0  0 29  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  1 24  0  0  0  0  1  0  0  3  4  0  0  0  3  0  0  0\n",
      "  0  0 37  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0 17  0  0\n",
      "  0  0  1  0  1  0  0  0  4  0  0  0  1  0  2  0  8  0  2  0  0  1  1  0\n",
      "  0  1  6  0  0  0  1  0  2  0  0  3]\n",
      "LR Accuracy:  0.7066381156316917\n",
      "LR F1:  0.6569737320719824\n",
      "For name:  s_paul\n",
      "total sample size before apply threshold:  52\n",
      "Counter({'0000-0002-9693-2915': 23, '0000-0001-7560-5512': 8, '0000-0003-1274-6670': 7, '0000-0002-8813-0437': 5, '0000-0003-4104-9209': 4, '0000-0002-7077-8235': 4, '0000-0001-9601-9109': 1})\n",
      "['0000-0002-9693-2915']\n",
      "Total sample size after apply threshold:  23\n",
      "For name:  l_roberts\n",
      "total sample size before apply threshold:  363\n",
      "Counter({'0000-0003-4270-253X': 206, '0000-0001-7885-8574': 120, '0000-0002-1455-5248': 18, '0000-0003-0085-9213': 14, '0000-0003-3892-2900': 3, '0000-0002-0329-8389': 2})\n",
      "['0000-0003-4270-253X', '0000-0003-0085-9213', '0000-0002-1455-5248', '0000-0001-7885-8574']\n",
      "Total sample size after apply threshold:  358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(358, 743)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(358, 743)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      1.00      0.89       206\n",
      "          1       1.00      0.21      0.35        14\n",
      "          2       1.00      0.89      0.94        18\n",
      "          3       1.00      0.68      0.81       120\n",
      "\n",
      "avg / total       0.89      0.86      0.85       358\n",
      "\n",
      "[206   0   0   0  11   3   0   0   2   0  16   0  38   0   0  82]\n",
      "svc Accuracy:  0.8575418994413407\n",
      "svc F1:  0.748961911818167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gao137/intel/intelpython3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/gao137/intel/intelpython3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      1.00      0.90       206\n",
      "          1       0.00      0.00      0.00        14\n",
      "          2       1.00      0.83      0.91        18\n",
      "          3       1.00      0.75      0.86       120\n",
      "\n",
      "avg / total       0.85      0.87      0.85       358\n",
      "\n",
      "[206   0   0   0  14   0   0   0   3   0  15   0  30   0   0  90]\n",
      "LR Accuracy:  0.8687150837988827\n",
      "LR F1:  0.6659593130181365\n",
      "For name:  s_keating\n",
      "total sample size before apply threshold:  54\n",
      "Counter({'0000-0002-8324-3694': 28, '0000-0002-3356-3542': 14, '0000-0001-5357-2721': 10, '0000-0003-3685-2849': 1, '0000-0002-6817-925X': 1})\n",
      "['0000-0002-3356-3542', '0000-0002-8324-3694', '0000-0001-5357-2721']\n",
      "Total sample size after apply threshold:  52\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(52, 381)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(52, 381)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.93      0.90        14\n",
      "          1       0.96      0.96      0.96        28\n",
      "          2       1.00      0.90      0.95        10\n",
      "\n",
      "avg / total       0.94      0.94      0.94        52\n",
      "\n",
      "[13  1  0  1 27  0  1  0  9]\n",
      "svc Accuracy:  0.9423076923076923\n",
      "svc F1:  0.9360686198254257\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.93      0.96        14\n",
      "          1       0.93      1.00      0.97        28\n",
      "          2       1.00      0.90      0.95        10\n",
      "\n",
      "avg / total       0.96      0.96      0.96        52\n",
      "\n",
      "[13  1  0  0 28  0  0  1  9]\n",
      "LR Accuracy:  0.9615384615384616\n",
      "LR F1:  0.9586162084649684\n",
      "For name:  a_bennett\n",
      "total sample size before apply threshold:  56\n",
      "Counter({'0000-0003-3829-0309': 51, '0000-0001-8895-6418': 2, '0000-0001-7448-8182': 1, '0000-0003-4194-9741': 1, '0000-0001-6968-9465': 1})\n",
      "['0000-0003-3829-0309']\n",
      "Total sample size after apply threshold:  51\n",
      "For name:  a_aggarwal\n",
      "total sample size before apply threshold:  22\n",
      "Counter({'0000-0003-3096-3206': 10, '0000-0002-1755-8807': 6, '0000-0002-6696-0296': 5, '0000-0003-0458-5619': 1})\n",
      "['0000-0003-3096-3206']\n",
      "Total sample size after apply threshold:  10\n",
      "For name:  i_moura\n",
      "total sample size before apply threshold:  203\n",
      "Counter({'0000-0003-0971-4977': 149, '0000-0002-2977-0354': 48, '0000-0002-3019-7196': 5, '0000-0001-7859-1881': 1})\n",
      "['0000-0002-2977-0354', '0000-0003-0971-4977']\n",
      "Total sample size after apply threshold:  197\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(197, 548)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(197, 548)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.94      0.97        48\n",
      "          1       0.98      1.00      0.99       149\n",
      "\n",
      "avg / total       0.99      0.98      0.98       197\n",
      "\n",
      "[ 45   3   0 149]\n",
      "svc Accuracy:  0.9847715736040609\n",
      "svc F1:  0.9788875790376166\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.85      0.92        48\n",
      "          1       0.96      1.00      0.98       149\n",
      "\n",
      "avg / total       0.97      0.96      0.96       197\n",
      "\n",
      "[ 41   7   0 149]\n",
      "LR Accuracy:  0.9644670050761421\n",
      "LR F1:  0.9491987474673051\n",
      "For name:  d_teixeira\n",
      "total sample size before apply threshold:  27\n",
      "Counter({'0000-0003-1799-1675': 13, '0000-0002-2162-1450': 12, '0000-0003-2110-4725': 1, '0000-0001-8172-7911': 1})\n",
      "['0000-0003-1799-1675', '0000-0002-2162-1450']\n",
      "Total sample size after apply threshold:  25\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(25, 84)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(25, 84)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        13\n",
      "          1       1.00      1.00      1.00        12\n",
      "\n",
      "avg / total       1.00      1.00      1.00        25\n",
      "\n",
      "[13  0  0 12]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        13\n",
      "          1       1.00      1.00      1.00        12\n",
      "\n",
      "avg / total       1.00      1.00      1.00        25\n",
      "\n",
      "[13  0  0 12]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  c_klein\n",
      "total sample size before apply threshold:  106\n",
      "Counter({'0000-0003-3522-9182': 47, '0000-0002-8230-8038': 21, '0000-0003-2991-1791': 11, '0000-0002-7580-8536': 11, '0000-0001-9736-5994': 9, '0000-0003-1305-0114': 5, '0000-0002-7406-4010': 2})\n",
      "['0000-0003-2991-1791', '0000-0003-3522-9182', '0000-0002-7580-8536', '0000-0002-8230-8038']\n",
      "Total sample size after apply threshold:  90\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(90, 224)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(90, 224)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.27      0.43        11\n",
      "          1       0.77      1.00      0.87        47\n",
      "          2       1.00      0.91      0.95        11\n",
      "          3       1.00      0.76      0.86        21\n",
      "\n",
      "avg / total       0.88      0.84      0.83        90\n",
      "\n",
      "[ 3  8  0  0  0 47  0  0  0  1 10  0  0  5  0 16]\n",
      "svc Accuracy:  0.8444444444444444\n",
      "svc F1:  0.779046904046904\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.09      0.17        11\n",
      "          1       0.72      1.00      0.84        47\n",
      "          2       1.00      0.82      0.90        11\n",
      "          3       1.00      0.71      0.83        21\n",
      "\n",
      "avg / total       0.86      0.80      0.76        90\n",
      "\n",
      "[ 1 10  0  0  0 47  0  0  0  2  9  0  0  6  0 15]\n",
      "LR Accuracy:  0.8\n",
      "LR F1:  0.6848214285714285\n",
      "For name:  m_andersson\n",
      "total sample size before apply threshold:  152\n",
      "Counter({'0000-0001-7582-8791': 40, '0000-0002-7928-8216': 27, '0000-0003-4279-6572': 26, '0000-0002-4921-1461': 15, '0000-0002-3364-6647': 14, '0000-0002-1450-8046': 11, '0000-0002-7267-8377': 9, '0000-0003-3699-138X': 8, '0000-0003-0619-1074': 1, '0000-0001-5057-4908': 1})\n",
      "['0000-0002-1450-8046', '0000-0003-4279-6572', '0000-0001-7582-8791', '0000-0002-3364-6647', '0000-0002-4921-1461', '0000-0002-7928-8216']\n",
      "Total sample size after apply threshold:  133\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(133, 423)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(133, 423)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.55      0.71        11\n",
      "          1       0.95      0.73      0.83        26\n",
      "          2       0.78      1.00      0.88        40\n",
      "          3       1.00      0.79      0.88        14\n",
      "          4       1.00      0.93      0.97        15\n",
      "          5       0.74      0.85      0.79        27\n",
      "\n",
      "avg / total       0.87      0.85      0.85       133\n",
      "\n",
      "[ 6  1  2  0  0  2  0 19  5  0  0  2  0  0 40  0  0  0  0  0  0 11  0  3\n",
      "  0  0  0  0 14  1  0  0  4  0  0 23]\n",
      "svc Accuracy:  0.849624060150376\n",
      "svc F1:  0.8416184797064945\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.64      0.78        11\n",
      "          1       0.90      0.73      0.81        26\n",
      "          2       0.77      1.00      0.87        40\n",
      "          3       1.00      1.00      1.00        14\n",
      "          4       1.00      0.93      0.97        15\n",
      "          5       0.88      0.81      0.85        27\n",
      "\n",
      "avg / total       0.89      0.87      0.87       133\n",
      "\n",
      "[ 7  1  2  0  0  1  0 19  5  0  0  2  0  0 40  0  0  0  0  0  0 14  0  0\n",
      "  0  0  1  0 14  0  0  1  4  0  0 22]\n",
      "LR Accuracy:  0.8721804511278195\n",
      "LR F1:  0.8779207868333518\n",
      "For name:  h_shi\n",
      "total sample size before apply threshold:  21\n",
      "Counter({'0000-0001-8421-0002': 5, '0000-0003-1920-5914': 4, '0000-0002-9523-7742': 4, '0000-0003-0713-4688': 4, '0000-0001-6269-742X': 2, '0000-0003-3831-6898': 1, '0000-0001-6482-8403': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  d_howard\n",
      "total sample size before apply threshold:  79\n",
      "Counter({'0000-0001-9516-9551': 31, '0000-0001-9141-5751': 28, '0000-0002-4907-4292': 19, '0000-0003-3333-9783': 1})\n",
      "['0000-0001-9141-5751', '0000-0002-4907-4292', '0000-0001-9516-9551']\n",
      "Total sample size after apply threshold:  78\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(78, 162)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(78, 162)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.64      0.78        28\n",
      "          1       1.00      0.68      0.81        19\n",
      "          2       0.66      1.00      0.79        31\n",
      "\n",
      "avg / total       0.86      0.79      0.79        78\n",
      "\n",
      "[18  0 10  0 13  6  0  0 31]\n",
      "svc Accuracy:  0.7948717948717948\n",
      "svc F1:  0.7966601635079896\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.68      0.81        28\n",
      "          1       0.92      0.63      0.75        19\n",
      "          2       0.65      0.97      0.78        31\n",
      "\n",
      "avg / total       0.84      0.78      0.78        78\n",
      "\n",
      "[19  0  9  0 12  7  0  1 30]\n",
      "LR Accuracy:  0.782051282051282\n",
      "LR F1:  0.7792438058395504\n",
      "For name:  j_thomsen\n",
      "total sample size before apply threshold:  28\n",
      "Counter({'0000-0002-9336-5695': 18, '0000-0003-2143-8274': 8, '0000-0002-7368-6133': 1, '0000-0002-8275-4847': 1})\n",
      "['0000-0002-9336-5695']\n",
      "Total sample size after apply threshold:  18\n",
      "For name:  v_gupta\n",
      "total sample size before apply threshold:  238\n",
      "Counter({'0000-0002-8850-0485': 63, '0000-0002-6139-1346': 38, '0000-0002-1348-3545': 30, '0000-0002-9190-1757': 26, '0000-0003-4639-3316': 22, '0000-0001-6987-2550': 14, '0000-0002-6157-3705': 14, '0000-0002-1518-6624': 8, '0000-0002-2089-027X': 6, '0000-0003-2809-2966': 5, '0000-0003-1567-1037': 3, '0000-0001-7184-4663': 3, '0000-0003-1565-5918': 3, '0000-0003-2824-3402': 1, '0000-0001-6804-3830': 1, '0000-0001-6955-9134': 1})\n",
      "['0000-0001-6987-2550', '0000-0003-4639-3316', '0000-0002-6157-3705', '0000-0002-8850-0485', '0000-0002-1348-3545', '0000-0002-9190-1757', '0000-0002-6139-1346']\n",
      "Total sample size after apply threshold:  207\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(207, 407)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(207, 407)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.64      0.78        14\n",
      "          1       0.56      0.82      0.67        22\n",
      "          2       0.91      0.71      0.80        14\n",
      "          3       0.79      0.94      0.86        63\n",
      "          4       0.93      0.83      0.88        30\n",
      "          5       1.00      0.69      0.82        26\n",
      "          6       1.00      0.92      0.96        38\n",
      "\n",
      "avg / total       0.87      0.84      0.84       207\n",
      "\n",
      "[ 9  3  0  2  0  0  0  0 18  0  3  1  0  0  0  2 10  2  0  0  0  0  3  0\n",
      " 59  1  0  0  0  2  1  2 25  0  0  0  3  0  5  0 18  0  0  1  0  2  0  0\n",
      " 35]\n",
      "svc Accuracy:  0.8405797101449275\n",
      "svc F1:  0.8226609623305652\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.71      0.83        14\n",
      "          1       0.94      0.68      0.79        22\n",
      "          2       1.00      0.79      0.88        14\n",
      "          3       0.73      0.97      0.84        63\n",
      "          4       0.93      0.93      0.93        30\n",
      "          5       0.86      0.73      0.79        26\n",
      "          6       1.00      0.92      0.96        38\n",
      "\n",
      "avg / total       0.89      0.86      0.86       207\n",
      "\n",
      "[10  0  0  4  0  0  0  0 15  0  4  2  1  0  0  0 11  2  0  1  0  0  1  0\n",
      " 61  0  1  0  0  0  0  2 28  0  0  0  0  0  7  0 19  0  0  0  0  3  0  0\n",
      " 35]\n",
      "LR Accuracy:  0.8647342995169082\n",
      "LR F1:  0.8603325093555808\n",
      "For name:  j_manning\n",
      "total sample size before apply threshold:  16\n",
      "Counter({'0000-0001-7613-4732': 8, '0000-0002-6077-4169': 6, '0000-0002-3572-8005': 1, '0000-0003-2257-6556': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  r_wood\n",
      "total sample size before apply threshold:  97\n",
      "Counter({'0000-0002-9495-6892': 82, '0000-0002-1694-3295': 11, '0000-0002-7906-3324': 2, '0000-0002-3476-395X': 1, '0000-0001-6389-1048': 1})\n",
      "['0000-0002-9495-6892', '0000-0002-1694-3295']\n",
      "Total sample size after apply threshold:  93\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(93, 338)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(93, 338)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98        82\n",
      "          1       1.00      0.73      0.84        11\n",
      "\n",
      "avg / total       0.97      0.97      0.97        93\n",
      "\n",
      "[82  0  3  8]\n",
      "svc Accuracy:  0.967741935483871\n",
      "svc F1:  0.9120705956508037\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      1.00      0.96        82\n",
      "          1       1.00      0.36      0.53        11\n",
      "\n",
      "avg / total       0.93      0.92      0.91        93\n",
      "\n",
      "[82  0  7  4]\n",
      "LR Accuracy:  0.9247311827956989\n",
      "LR F1:  0.7461988304093568\n",
      "For name:  y_ding\n",
      "total sample size before apply threshold:  106\n",
      "Counter({'0000-0003-1352-1000': 21, '0000-0002-6823-4722': 21, '0000-0001-7772-6449': 19, '0000-0002-8845-4618': 15, '0000-0001-7461-0213': 8, '0000-0001-8161-2743': 7, '0000-0003-4761-5486': 4, '0000-0003-0465-7870': 4, '0000-0003-1176-6397': 3, '0000-0001-8312-8672': 2, '0000-0002-9713-5694': 1, '0000-0002-0010-8279': 1})\n",
      "['0000-0003-1352-1000', '0000-0002-6823-4722', '0000-0002-8845-4618', '0000-0001-7772-6449']\n",
      "Total sample size after apply threshold:  76\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(76, 185)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(76, 185)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.95      0.93        21\n",
      "          1       0.95      0.90      0.93        21\n",
      "          2       1.00      1.00      1.00        15\n",
      "          3       0.89      0.89      0.89        19\n",
      "\n",
      "avg / total       0.93      0.93      0.93        76\n",
      "\n",
      "[20  0  0  1  1 19  0  1  0  0 15  0  1  1  0 17]\n",
      "svc Accuracy:  0.9342105263157895\n",
      "svc F1:  0.9379496671343702\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.95      0.95        21\n",
      "          1       0.94      0.81      0.87        21\n",
      "          2       0.94      1.00      0.97        15\n",
      "          3       0.81      0.89      0.85        19\n",
      "\n",
      "avg / total       0.91      0.91      0.91        76\n",
      "\n",
      "[20  0  0  1  1 17  0  3  0  0 15  0  0  1  1 17]\n",
      "LR Accuracy:  0.9078947368421053\n",
      "LR F1:  0.9104794399149237\n",
      "For name:  j_rasmussen\n",
      "total sample size before apply threshold:  33\n",
      "Counter({'0000-0002-3543-690X': 15, '0000-0001-6997-3773': 9, '0000-0003-2898-1771': 6, '0000-0002-8389-6935': 1, '0000-0003-3426-551X': 1, '0000-0003-3257-5653': 1})\n",
      "['0000-0002-3543-690X']\n",
      "Total sample size after apply threshold:  15\n",
      "For name:  n_lee\n",
      "total sample size before apply threshold:  108\n",
      "Counter({'0000-0002-5011-7499': 74, '0000-0002-6663-0713': 20, '0000-0003-2628-6599': 12, '0000-0001-8009-2694': 1, '0000-0002-2756-1102': 1})\n",
      "['0000-0003-2628-6599', '0000-0002-5011-7499', '0000-0002-6663-0713']\n",
      "Total sample size after apply threshold:  106\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(106, 186)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(106, 186)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        12\n",
      "          1       0.99      1.00      0.99        74\n",
      "          2       1.00      0.95      0.97        20\n",
      "\n",
      "avg / total       0.99      0.99      0.99       106\n",
      "\n",
      "[12  0  0  0 74  0  0  1 19]\n",
      "svc Accuracy:  0.9905660377358491\n",
      "svc F1:  0.9892158549876671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.92      0.96        12\n",
      "          1       0.97      1.00      0.99        74\n",
      "          2       1.00      0.95      0.97        20\n",
      "\n",
      "avg / total       0.98      0.98      0.98       106\n",
      "\n",
      "[11  1  0  0 74  0  0  1 19]\n",
      "LR Accuracy:  0.9811320754716981\n",
      "LR F1:  0.9725157933853587\n",
      "For name:  a_oliveira\n",
      "total sample size before apply threshold:  302\n",
      "Counter({'0000-0001-9103-6532': 41, '0000-0003-2186-8100': 24, '0000-0002-6714-5939': 24, '0000-0002-0107-9940': 24, '0000-0001-8012-4203': 20, '0000-0001-8638-5594': 20, '0000-0003-3787-9138': 12, '0000-0003-2790-6294': 12, '0000-0001-6837-5739': 10, '0000-0001-5445-1032': 8, '0000-0001-8753-4950': 8, '0000-0002-7898-5503': 7, '0000-0003-4516-6904': 6, '0000-0003-3162-250X': 6, '0000-0002-0330-3643': 5, '0000-0003-1554-4687': 5, '0000-0003-0402-2971': 5, '0000-0003-4158-6098': 5, '0000-0002-0747-7835': 4, '0000-0002-6477-5345': 4, '0000-0002-0841-4844': 4, '0000-0002-0685-2963': 4, '0000-0001-5526-8109': 4, '0000-0001-5611-6385': 3, '0000-0002-2308-9904': 3, '0000-0001-9287-0959': 3, '0000-0001-6532-1700': 3, '0000-0003-0593-4665': 3, '0000-0002-6859-084X': 3, '0000-0002-2220-5862': 2, '0000-0001-9605-6276': 2, '0000-0001-9955-0915': 2, '0000-0003-1202-7748': 2, '0000-0002-5614-229X': 2, '0000-0003-1214-8240': 2, '0000-0001-8144-4583': 1, '0000-0002-7284-9359': 1, '0000-0001-5098-3939': 1, '0000-0001-6422-9486': 1, '0000-0002-3070-1604': 1, '0000-0001-7690-7037': 1, '0000-0002-8453-1719': 1, '0000-0002-2977-6000': 1, '0000-0002-7537-0984': 1, '0000-0003-2763-9501': 1})\n",
      "['0000-0003-3787-9138', '0000-0001-8012-4203', '0000-0001-8638-5594', '0000-0003-2186-8100', '0000-0003-2790-6294', '0000-0001-6837-5739', '0000-0002-6714-5939', '0000-0002-0107-9940', '0000-0001-9103-6532']\n",
      "Total sample size after apply threshold:  187\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(187, 409)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(187, 409)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.83      0.91        12\n",
      "          1       1.00      1.00      1.00        20\n",
      "          2       0.64      0.70      0.67        20\n",
      "          3       0.92      0.96      0.94        24\n",
      "          4       1.00      0.83      0.91        12\n",
      "          5       1.00      0.80      0.89        10\n",
      "          6       0.92      0.96      0.94        24\n",
      "          7       1.00      0.88      0.93        24\n",
      "          8       0.76      0.85      0.80        41\n",
      "\n",
      "avg / total       0.89      0.88      0.88       187\n",
      "\n",
      "[10  0  1  0  0  0  0  0  1  0 20  0  0  0  0  0  0  0  0  0 14  2  0  0\n",
      "  0  0  4  0  0  0 23  0  0  0  0  1  0  0  0  0 10  0  0  0  2  0  0  1\n",
      "  0  0  8  1  0  0  0  0  0  0  0  0 23  0  1  0  0  0  0  0  0  1 21  2\n",
      "  0  0  6  0  0  0  0  0 35]\n",
      "svc Accuracy:  0.8770053475935828\n",
      "svc F1:  0.8876910476253661\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.92      0.96        12\n",
      "          1       1.00      1.00      1.00        20\n",
      "          2       0.88      0.70      0.78        20\n",
      "          3       0.96      1.00      0.98        24\n",
      "          4       1.00      1.00      1.00        12\n",
      "          5       1.00      0.90      0.95        10\n",
      "          6       0.92      1.00      0.96        24\n",
      "          7       0.95      0.88      0.91        24\n",
      "          8       0.85      0.95      0.90        41\n",
      "\n",
      "avg / total       0.93      0.93      0.93       187\n",
      "\n",
      "[11  0  0  0  0  0  0  0  1  0 20  0  0  0  0  0  0  0  0  0 14  1  0  0\n",
      "  0  1  4  0  0  0 24  0  0  0  0  0  0  0  0  0 12  0  0  0  0  0  0  0\n",
      "  0  0  9  1  0  0  0  0  0  0  0  0 24  0  0  0  0  0  0  0  0  1 21  2\n",
      "  0  0  2  0  0  0  0  0 39]\n",
      "LR Accuracy:  0.93048128342246\n",
      "LR F1:  0.9367616641215931\n",
      "For name:  h_yin\n",
      "total sample size before apply threshold:  130\n",
      "Counter({'0000-0002-9762-4818': 69, '0000-0001-7693-377X': 32, '0000-0002-0720-5311': 13, '0000-0002-0810-1696': 5, '0000-0001-6553-0887': 5, '0000-0003-1765-496X': 4, '0000-0002-1175-4516': 1, '0000-0002-0682-6781': 1})\n",
      "['0000-0002-9762-4818', '0000-0001-7693-377X', '0000-0002-0720-5311']\n",
      "Total sample size after apply threshold:  114\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(114, 301)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(114, 301)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.93      0.86        69\n",
      "          1       0.88      0.72      0.79        32\n",
      "          2       0.78      0.54      0.64        13\n",
      "\n",
      "avg / total       0.83      0.82      0.82       114\n",
      "\n",
      "[64  3  2  9 23  0  6  0  7]\n",
      "svc Accuracy:  0.8245614035087719\n",
      "svc F1:  0.7647773165014544\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.96      0.86        69\n",
      "          1       0.95      0.66      0.78        32\n",
      "          2       0.75      0.46      0.57        13\n",
      "\n",
      "avg / total       0.83      0.82      0.81       114\n",
      "\n",
      "[66  1  2 11 21  0  7  0  6]\n",
      "LR Accuracy:  0.8157894736842105\n",
      "LR F1:  0.737317149081855\n",
      "For name:  k_brown\n",
      "total sample size before apply threshold:  231\n",
      "Counter({'0000-0003-2434-0037': 89, '0000-0002-0729-4959': 61, '0000-0003-3382-5546': 33, '0000-0002-6803-5336': 12, '0000-0003-2472-5754': 9, '0000-0001-7716-1425': 7, '0000-0001-9428-9420': 6, '0000-0002-1047-4328': 3, '0000-0001-6836-1572': 3, '0000-0001-8350-5888': 2, '0000-0001-7766-6810': 1, '0000-0002-0201-0558': 1, '0000-0002-2358-8578': 1, '0000-0002-9093-8742': 1, '0000-0001-5348-7893': 1, '0000-0001-5748-5123': 1})\n",
      "['0000-0003-2434-0037', '0000-0003-3382-5546', '0000-0002-0729-4959', '0000-0002-6803-5336']\n",
      "Total sample size after apply threshold:  195\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(195, 534)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(195, 534)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.94      0.91        89\n",
      "          1       1.00      0.82      0.90        33\n",
      "          2       0.86      0.89      0.87        61\n",
      "          3       1.00      0.83      0.91        12\n",
      "\n",
      "avg / total       0.90      0.90      0.90       195\n",
      "\n",
      "[84  0  5  0  4 27  2  0  7  0 54  0  0  0  2 10]\n",
      "svc Accuracy:  0.8974358974358975\n",
      "svc F1:  0.8982755323218157\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.92      0.93        89\n",
      "          1       1.00      0.82      0.90        33\n",
      "          2       0.82      0.95      0.88        61\n",
      "          3       1.00      0.83      0.91        12\n",
      "\n",
      "avg / total       0.92      0.91      0.91       195\n",
      "\n",
      "[82  0  7  0  2 27  4  0  3  0 58  0  0  0  2 10]\n",
      "LR Accuracy:  0.9076923076923077\n",
      "LR F1:  0.9049242424242424\n",
      "For name:  s_hong\n",
      "total sample size before apply threshold:  383\n",
      "Counter({'0000-0002-8344-6774': 102, '0000-0002-8888-6007': 84, '0000-0002-0300-1944': 83, '0000-0002-6305-8731': 27, '0000-0001-7291-1020': 19, '0000-0002-0324-2414': 15, '0000-0003-3031-2753': 12, '0000-0003-2401-6368': 12, '0000-0002-2667-1983': 10, '0000-0003-4926-1044': 3, '0000-0002-0020-6215': 3, '0000-0002-8473-919X': 2, '0000-0002-4800-636X': 2, '0000-0001-8722-3124': 1, '0000-0002-6905-7932': 1, '0000-0002-3755-3683': 1, '0000-0002-2498-7546': 1, '0000-0002-9470-5700': 1, '0000-0003-1119-4456': 1, '0000-0001-5049-8810': 1, '0000-0003-0721-4012': 1, '0000-0003-4989-292X': 1})\n",
      "['0000-0002-8888-6007', '0000-0003-3031-2753', '0000-0002-2667-1983', '0000-0002-8344-6774', '0000-0001-7291-1020', '0000-0002-0324-2414', '0000-0003-2401-6368', '0000-0002-6305-8731', '0000-0002-0300-1944']\n",
      "Total sample size after apply threshold:  364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(364, 476)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(364, 476)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.73      0.78        84\n",
      "          1       0.64      0.58      0.61        12\n",
      "          2       0.67      0.20      0.31        10\n",
      "          3       0.91      0.91      0.91       102\n",
      "          4       0.65      0.58      0.61        19\n",
      "          5       0.45      0.60      0.51        15\n",
      "          6       1.00      0.83      0.91        12\n",
      "          7       0.92      0.85      0.88        27\n",
      "          8       0.68      0.86      0.76        83\n",
      "\n",
      "avg / total       0.80      0.79      0.79       364\n",
      "\n",
      "[61  1  0  1  2  2  0  0 17  2  7  0  1  0  0  0  1  1  2  2  2  0  1  0\n",
      "  0  0  3  0  0  0 93  0  3  0  0  6  1  0  0  3 11  1  0  0  3  3  1  0\n",
      "  1  0  9  0  0  1  0  0  0  0  0  2 10  0  0  0  0  0  0  1  1  0 23  2\n",
      "  3  0  1  3  2  2  0  1 71]\n",
      "svc Accuracy:  0.7884615384615384\n",
      "svc F1:  0.6987405950747809\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.86      0.87        84\n",
      "          1       0.67      0.67      0.67        12\n",
      "          2       0.50      0.20      0.29        10\n",
      "          3       0.89      0.96      0.92       102\n",
      "          4       0.75      0.47      0.58        19\n",
      "          5       0.60      0.20      0.30        15\n",
      "          6       1.00      0.83      0.91        12\n",
      "          7       0.96      0.85      0.90        27\n",
      "          8       0.71      0.90      0.79        83\n",
      "\n",
      "avg / total       0.82      0.82      0.81       364\n",
      "\n",
      "[72  1  1  1  1  0  0  0  8  3  8  0  1  0  0  0  0  0  3  2  2  0  0  0\n",
      "  0  0  3  0  0  0 98  0  0  0  0  4  0  0  0  4  9  0  0  0  6  2  1  0\n",
      "  3  0  3  0  0  6  0  0  0  0  0  2 10  0  0  0  0  0  0  0  0  0 23  4\n",
      "  1  0  1  3  2  0  0  1 75]\n",
      "LR Accuracy:  0.8241758241758241\n",
      "LR F1:  0.6927760194823075\n",
      "For name:  l_zhou\n",
      "total sample size before apply threshold:  49\n",
      "Counter({'0000-0001-5973-7873': 16, '0000-0003-2800-5981': 10, '0000-0001-9014-6350': 8, '0000-0001-8900-2835': 6, '0000-0002-0393-4787': 4, '0000-0001-9032-0910': 3, '0000-0002-0133-3048': 1, '0000-0001-8554-0900': 1})\n",
      "['0000-0001-5973-7873', '0000-0003-2800-5981']\n",
      "Total sample size after apply threshold:  26\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(26, 58)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(26, 58)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        16\n",
      "          1       1.00      1.00      1.00        10\n",
      "\n",
      "avg / total       1.00      1.00      1.00        26\n",
      "\n",
      "[16  0  0 10]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        16\n",
      "          1       1.00      1.00      1.00        10\n",
      "\n",
      "avg / total       1.00      1.00      1.00        26\n",
      "\n",
      "[16  0  0 10]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  h_jiang\n",
      "total sample size before apply threshold:  135\n",
      "Counter({'0000-0002-2975-7977': 52, '0000-0002-5778-4008': 16, '0000-0002-1947-4420': 15, '0000-0002-4388-6548': 13, '0000-0003-0561-5058': 10, '0000-0002-1156-9046': 8, '0000-0001-9892-4292': 4, '0000-0002-4577-2886': 4, '0000-0003-3187-2023': 3, '0000-0002-5840-007X': 3, '0000-0003-4173-8565': 3, '0000-0002-7827-0719': 1, '0000-0002-0962-902X': 1, '0000-0003-0951-0624': 1, '0000-0001-6460-408X': 1})\n",
      "['0000-0002-4388-6548', '0000-0002-1947-4420', '0000-0003-0561-5058', '0000-0002-5778-4008', '0000-0002-2975-7977']\n",
      "Total sample size after apply threshold:  106\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(106, 191)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(106, 191)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.92      0.77        13\n",
      "          1       0.62      0.53      0.57        15\n",
      "          2       0.50      0.60      0.55        10\n",
      "          3       1.00      0.69      0.81        16\n",
      "          4       0.83      0.83      0.83        52\n",
      "\n",
      "avg / total       0.77      0.75      0.76       106\n",
      "\n",
      "[12  0  0  0  1  2  8  1  0  4  0  0  6  0  4  1  1  3 11  0  3  4  2  0\n",
      " 43]\n",
      "svc Accuracy:  0.7547169811320755\n",
      "svc F1:  0.7065629114016211\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.92      0.89        13\n",
      "          1       0.88      0.47      0.61        15\n",
      "          2       0.67      0.60      0.63        10\n",
      "          3       1.00      0.81      0.90        16\n",
      "          4       0.79      0.94      0.86        52\n",
      "\n",
      "avg / total       0.83      0.82      0.81       106\n",
      "\n",
      "[12  0  0  0  1  0  7  1  0  7  0  0  6  0  4  0  0  2 13  1  2  1  0  0\n",
      " 49]\n",
      "LR Accuracy:  0.8207547169811321\n",
      "LR F1:  0.7770728670752343\n",
      "For name:  a_lewis\n",
      "total sample size before apply threshold:  98\n",
      "Counter({'0000-0002-4075-3651': 41, '0000-0002-2519-7976': 37, '0000-0002-7986-0956': 8, '0000-0002-0756-7320': 6, '0000-0002-4195-1035': 4, '0000-0001-5373-7231': 1, '0000-0003-4737-2525': 1})\n",
      "['0000-0002-2519-7976', '0000-0002-4075-3651']\n",
      "Total sample size after apply threshold:  78\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(78, 207)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(78, 207)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      1.00      0.94        37\n",
      "          1       1.00      0.88      0.94        41\n",
      "\n",
      "avg / total       0.94      0.94      0.94        78\n",
      "\n",
      "[37  0  5 36]\n",
      "svc Accuracy:  0.9358974358974359\n",
      "svc F1:  0.9358868979122144\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      1.00      0.94        37\n",
      "          1       1.00      0.88      0.94        41\n",
      "\n",
      "avg / total       0.94      0.94      0.94        78\n",
      "\n",
      "[37  0  5 36]\n",
      "LR Accuracy:  0.9358974358974359\n",
      "LR F1:  0.9358868979122144\n",
      "For name:  c_meyer\n",
      "total sample size before apply threshold:  136\n",
      "Counter({'0000-0001-7599-3973': 34, '0000-0002-9877-1393': 29, '0000-0003-1334-2512': 27, '0000-0002-7214-9598': 18, '0000-0002-2268-3055': 14, '0000-0003-0851-2767': 6, '0000-0001-9958-8913': 5, '0000-0002-3166-3101': 3})\n",
      "['0000-0002-9877-1393', '0000-0001-7599-3973', '0000-0003-1334-2512', '0000-0002-7214-9598', '0000-0002-2268-3055']\n",
      "Total sample size after apply threshold:  122\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(122, 806)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(122, 806)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      1.00      0.94        29\n",
      "          1       1.00      0.97      0.99        34\n",
      "          2       0.96      0.85      0.90        27\n",
      "          3       1.00      1.00      1.00        18\n",
      "          4       1.00      1.00      1.00        14\n",
      "\n",
      "avg / total       0.96      0.96      0.96       122\n",
      "\n",
      "[29  0  0  0  0  0 33  1  0  0  4  0 23  0  0  0  0  0 18  0  0  0  0  0\n",
      " 14]\n",
      "svc Accuracy:  0.9590163934426229\n",
      "svc F1:  0.9645038564294278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.98        29\n",
      "          1       1.00      0.97      0.99        34\n",
      "          2       0.96      0.93      0.94        27\n",
      "          3       0.95      1.00      0.97        18\n",
      "          4       1.00      1.00      1.00        14\n",
      "\n",
      "avg / total       0.98      0.98      0.98       122\n",
      "\n",
      "[29  0  0  0  0  0 33  1  0  0  1  0 25  1  0  0  0  0 18  0  0  0  0  0\n",
      " 14]\n",
      "LR Accuracy:  0.9754098360655737\n",
      "LR F1:  0.9768989347422732\n",
      "For name:  a_islam\n",
      "total sample size before apply threshold:  18\n",
      "Counter({'0000-0003-3375-9817': 5, '0000-0001-9060-7970': 4, '0000-0002-2139-7508': 3, '0000-0003-1561-0680': 2, '0000-0002-7274-0855': 1, '0000-0002-9902-0639': 1, '0000-0001-8270-5968': 1, '0000-0001-9608-0823': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  k_fujita\n",
      "total sample size before apply threshold:  42\n",
      "Counter({'0000-0002-3821-8393': 16, '0000-0002-6902-9085': 10, '0000-0002-2518-2125': 6, '0000-0002-1477-5187': 5, '0000-0001-7556-4714': 3, '0000-0002-1900-5325': 1, '0000-0002-1744-3583': 1})\n",
      "['0000-0002-6902-9085', '0000-0002-3821-8393']\n",
      "Total sample size after apply threshold:  26\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(26, 52)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(26, 52)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        10\n",
      "          1       1.00      1.00      1.00        16\n",
      "\n",
      "avg / total       1.00      1.00      1.00        26\n",
      "\n",
      "[10  0  0 16]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        10\n",
      "          1       1.00      1.00      1.00        16\n",
      "\n",
      "avg / total       1.00      1.00      1.00        26\n",
      "\n",
      "[10  0  0 16]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  a_khan\n",
      "total sample size before apply threshold:  226\n",
      "Counter({'0000-0001-8640-8530': 59, '0000-0002-2571-3600': 24, '0000-0001-5129-756X': 19, '0000-0002-0760-8647': 14, '0000-0003-3490-799X': 13, '0000-0003-3655-2854': 12, '0000-0003-0254-3546': 11, '0000-0002-0751-0930': 9, '0000-0001-5955-3783': 9, '0000-0002-9325-6640': 8, '0000-0002-8748-1841': 7, '0000-0002-8748-4065': 5, '0000-0003-4057-8053': 5, '0000-0002-3806-5956': 5, '0000-0002-5796-6573': 4, '0000-0002-6655-129X': 3, '0000-0001-6079-0567': 3, '0000-0002-0007-2536': 3, '0000-0001-9293-3999': 2, '0000-0002-3746-5034': 2, '0000-0002-2048-225X': 2, '0000-0001-9338-9323': 2, '0000-0002-7524-6270': 1, '0000-0003-3340-3036': 1, '0000-0003-1562-2577': 1, '0000-0001-7763-1490': 1, '0000-0002-0338-8325': 1})\n",
      "['0000-0003-3490-799X', '0000-0001-5129-756X', '0000-0002-0760-8647', '0000-0003-3655-2854', '0000-0003-0254-3546', '0000-0001-8640-8530', '0000-0002-2571-3600']\n",
      "Total sample size after apply threshold:  152\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(152, 365)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(152, 365)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.77      0.80        13\n",
      "          1       0.95      0.95      0.95        19\n",
      "          2       1.00      0.93      0.96        14\n",
      "          3       1.00      0.83      0.91        12\n",
      "          4       1.00      0.64      0.78        11\n",
      "          5       0.83      0.98      0.90        59\n",
      "          6       0.90      0.79      0.84        24\n",
      "\n",
      "avg / total       0.90      0.89      0.89       152\n",
      "\n",
      "[10  0  0  0  0  1  2  0 18  0  0  0  1  0  0  0 13  0  0  1  0  0  0  0\n",
      " 10  0  2  0  0  0  0  0  7  4  0  0  1  0  0  0 58  0  2  0  0  0  0  3\n",
      " 19]\n",
      "svc Accuracy:  0.8881578947368421\n",
      "svc F1:  0.8772670459328965\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.85      0.85        13\n",
      "          1       1.00      1.00      1.00        19\n",
      "          2       1.00      0.93      0.96        14\n",
      "          3       1.00      0.83      0.91        12\n",
      "          4       1.00      0.64      0.78        11\n",
      "          5       0.84      1.00      0.91        59\n",
      "          6       0.95      0.79      0.86        24\n",
      "\n",
      "avg / total       0.92      0.91      0.91       152\n",
      "\n",
      "[11  0  0  0  0  1  1  0 19  0  0  0  0  0  0  0 13  0  0  1  0  0  0  0\n",
      " 10  0  2  0  0  0  0  0  7  4  0  0  0  0  0  0 59  0  2  0  0  0  0  3\n",
      " 19]\n",
      "LR Accuracy:  0.9078947368421053\n",
      "LR F1:  0.8963357916846288\n",
      "For name:  a_kim\n",
      "total sample size before apply threshold:  25\n",
      "Counter({'0000-0003-4074-0516': 8, '0000-0001-8263-6528': 6, '0000-0003-2861-8366': 2, '0000-0003-4101-6642': 2, '0000-0003-1861-3801': 2, '0000-0002-8733-6046': 2, '0000-0002-8390-0041': 1, '0000-0001-8484-5892': 1, '0000-0003-1539-1246': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  m_martinez\n",
      "total sample size before apply threshold:  69\n",
      "Counter({'0000-0002-6681-4950': 29, '0000-0003-2180-4537': 28, '0000-0002-6289-4586': 3, '0000-0003-0260-2366': 3, '0000-0003-2166-1097': 2, '0000-0002-4386-3290': 1, '0000-0001-9645-5058': 1, '0000-0003-0741-2940': 1, '0000-0003-2137-8048': 1})\n",
      "['0000-0002-6681-4950', '0000-0003-2180-4537']\n",
      "Total sample size after apply threshold:  57\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(57, 433)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(57, 433)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97        29\n",
      "          1       1.00      0.93      0.96        28\n",
      "\n",
      "avg / total       0.97      0.96      0.96        57\n",
      "\n",
      "[29  0  2 26]\n",
      "svc Accuracy:  0.9649122807017544\n",
      "svc F1:  0.9648148148148148\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      1.00      0.95        29\n",
      "          1       1.00      0.89      0.94        28\n",
      "\n",
      "avg / total       0.95      0.95      0.95        57\n",
      "\n",
      "[29  0  3 25]\n",
      "LR Accuracy:  0.9473684210526315\n",
      "LR F1:  0.9471079492731209\n",
      "For name:  m_aslam\n",
      "total sample size before apply threshold:  55\n",
      "Counter({'0000-0003-1361-5357': 29, '0000-0002-8529-4217': 17, '0000-0001-8812-6887': 4, '0000-0001-9418-3714': 4, '0000-0003-2498-3526': 1})\n",
      "['0000-0003-1361-5357', '0000-0002-8529-4217']\n",
      "Total sample size after apply threshold:  46\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(46, 147)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(46, 147)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.97      0.95        29\n",
      "          1       0.94      0.88      0.91        17\n",
      "\n",
      "avg / total       0.93      0.93      0.93        46\n",
      "\n",
      "[28  1  2 15]\n",
      "svc Accuracy:  0.9347826086956522\n",
      "svc F1:  0.9291217257318953\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97        29\n",
      "          1       1.00      0.88      0.94        17\n",
      "\n",
      "avg / total       0.96      0.96      0.96        46\n",
      "\n",
      "[29  0  2 15]\n",
      "LR Accuracy:  0.9565217391304348\n",
      "LR F1:  0.9520833333333333\n",
      "For name:  j_wolf\n",
      "total sample size before apply threshold:  65\n",
      "Counter({'0000-0003-3112-6602': 56, '0000-0003-4129-8221': 3, '0000-0002-7825-3118': 3, '0000-0002-1437-982X': 2, '0000-0002-7458-2002': 1})\n",
      "['0000-0003-3112-6602']\n",
      "Total sample size after apply threshold:  56\n",
      "For name:  s_agrawal\n",
      "total sample size before apply threshold:  30\n",
      "Counter({'0000-0002-2806-1943': 20, '0000-0001-6295-6954': 6, '0000-0003-3214-786X': 2, '0000-0002-5524-6206': 2})\n",
      "['0000-0002-2806-1943']\n",
      "Total sample size after apply threshold:  20\n",
      "For name:  a_othman\n",
      "total sample size before apply threshold:  40\n",
      "Counter({'0000-0002-0092-5594': 22, '0000-0002-3827-8695': 13, '0000-0002-2437-8564': 3, '0000-0002-3708-985X': 1, '0000-0002-3982-3157': 1})\n",
      "['0000-0002-3827-8695', '0000-0002-0092-5594']\n",
      "Total sample size after apply threshold:  35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(35, 160)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(35, 160)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.92      0.96        13\n",
      "          1       0.96      1.00      0.98        22\n",
      "\n",
      "avg / total       0.97      0.97      0.97        35\n",
      "\n",
      "[12  1  0 22]\n",
      "svc Accuracy:  0.9714285714285714\n",
      "svc F1:  0.9688888888888889\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.92      0.96        13\n",
      "          1       0.96      1.00      0.98        22\n",
      "\n",
      "avg / total       0.97      0.97      0.97        35\n",
      "\n",
      "[12  1  0 22]\n",
      "LR Accuracy:  0.9714285714285714\n",
      "LR F1:  0.9688888888888889\n",
      "For name:  k_evans\n",
      "total sample size before apply threshold:  29\n",
      "Counter({'0000-0002-6856-8423': 16, '0000-0002-9819-1049': 9, '0000-0001-6981-7703': 3, '0000-0003-2850-7674': 1})\n",
      "['0000-0002-6856-8423']\n",
      "Total sample size after apply threshold:  16\n",
      "For name:  k_yoo\n",
      "total sample size before apply threshold:  10\n",
      "Counter({'0000-0002-5213-4575': 7, '0000-0001-7952-7902': 1, '0000-0002-6186-7535': 1, '0000-0002-5539-345X': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  d_turner\n",
      "total sample size before apply threshold:  71\n",
      "Counter({'0000-0002-3754-6459': 27, '0000-0003-1603-7994': 26, '0000-0002-3447-7662': 5, '0000-0002-0249-4513': 4, '0000-0002-8891-9155': 3, '0000-0002-7369-8791': 3, '0000-0002-2891-2664': 2, '0000-0001-6802-1703': 1})\n",
      "['0000-0002-3754-6459', '0000-0003-1603-7994']\n",
      "Total sample size after apply threshold:  53\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(53, 108)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(53, 108)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      1.00      0.95        27\n",
      "          1       1.00      0.88      0.94        26\n",
      "\n",
      "avg / total       0.95      0.94      0.94        53\n",
      "\n",
      "[27  0  3 23]\n",
      "svc Accuracy:  0.9433962264150944\n",
      "svc F1:  0.9430719656283566\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      1.00      0.96        27\n",
      "          1       1.00      0.92      0.96        26\n",
      "\n",
      "avg / total       0.96      0.96      0.96        53\n",
      "\n",
      "[27  0  2 24]\n",
      "LR Accuracy:  0.9622641509433962\n",
      "LR F1:  0.9621428571428572\n",
      "For name:  j_king\n",
      "total sample size before apply threshold:  75\n",
      "Counter({'0000-0003-0596-4506': 21, '0000-0002-8174-9173': 20, '0000-0003-4530-9987': 20, '0000-0002-6048-8277': 7, '0000-0003-2171-8321': 5, '0000-0003-4947-0241': 1, '0000-0003-0494-153X': 1})\n",
      "['0000-0002-8174-9173', '0000-0003-4530-9987', '0000-0003-0596-4506']\n",
      "Total sample size after apply threshold:  61\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(61, 2671)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(61, 2671)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.95      0.97        20\n",
      "          1       0.76      0.80      0.78        20\n",
      "          2       0.81      0.81      0.81        21\n",
      "\n",
      "avg / total       0.86      0.85      0.85        61\n",
      "\n",
      "[19  1  0  0 16  4  0  4 17]\n",
      "svc Accuracy:  0.8524590163934426\n",
      "svc F1:  0.8547901962536107\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.95      0.97        20\n",
      "          1       0.81      0.85      0.83        20\n",
      "          2       0.86      0.86      0.86        21\n",
      "\n",
      "avg / total       0.89      0.89      0.89        61\n",
      "\n",
      "[19  1  0  0 17  3  0  3 18]\n",
      "LR Accuracy:  0.8852459016393442\n",
      "LR F1:  0.8869233747282528\n",
      "For name:  b_shen\n",
      "total sample size before apply threshold:  36\n",
      "Counter({'0000-0003-2899-1531': 29, '0000-0002-5237-6144': 4, '0000-0003-3287-9438': 2, '0000-0001-9687-9010': 1})\n",
      "['0000-0003-2899-1531']\n",
      "Total sample size after apply threshold:  29\n",
      "For name:  s_mishra\n",
      "total sample size before apply threshold:  116\n",
      "Counter({'0000-0001-8492-5470': 29, '0000-0003-4091-3018': 24, '0000-0001-6080-4148': 16, '0000-0002-0403-6575': 16, '0000-0003-3511-8319': 8, '0000-0002-3080-9754': 5, '0000-0003-3899-0495': 5, '0000-0001-8151-2988': 3, '0000-0002-1016-0206': 3, '0000-0003-2049-3618': 2, '0000-0003-1003-9884': 2, '0000-0001-6634-1877': 1, '0000-0003-2846-4221': 1, '0000-0002-5202-2645': 1})\n",
      "['0000-0001-8492-5470', '0000-0001-6080-4148', '0000-0002-0403-6575', '0000-0003-4091-3018']\n",
      "Total sample size after apply threshold:  85\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(85, 240)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(85, 240)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.90      0.87        29\n",
      "          1       0.80      0.75      0.77        16\n",
      "          2       0.94      1.00      0.97        16\n",
      "          3       1.00      0.92      0.96        24\n",
      "\n",
      "avg / total       0.90      0.89      0.89        85\n",
      "\n",
      "[26  3  0  0  3 12  1  0  0  0 16  0  2  0  0 22]\n",
      "svc Accuracy:  0.8941176470588236\n",
      "svc F1:  0.8917697309702919\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.90      0.87        29\n",
      "          1       0.81      0.81      0.81        16\n",
      "          2       1.00      1.00      1.00        16\n",
      "          3       1.00      0.92      0.96        24\n",
      "\n",
      "avg / total       0.91      0.91      0.91        85\n",
      "\n",
      "[26  3  0  0  3 13  0  0  0  0 16  0  2  0  0 22]\n",
      "LR Accuracy:  0.9058823529411765\n",
      "LR F1:  0.9089221014492754\n",
      "For name:  c_o'connor\n",
      "total sample size before apply threshold:  10\n",
      "Counter({'0000-0001-8134-075X': 4, '0000-0002-3541-708X': 2, '0000-0002-7638-9804': 2, '0000-0002-8359-7759': 1, '0000-0002-1670-3937': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  e_svensson\n",
      "total sample size before apply threshold:  87\n",
      "Counter({'0000-0001-9006-016X': 56, '0000-0001-6706-6336': 23, '0000-0002-4349-849X': 7, '0000-0002-5565-3266': 1})\n",
      "['0000-0001-6706-6336', '0000-0001-9006-016X']\n",
      "Total sample size after apply threshold:  79\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(79, 183)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(79, 183)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.74      0.85        23\n",
      "          1       0.90      1.00      0.95        56\n",
      "\n",
      "avg / total       0.93      0.92      0.92        79\n",
      "\n",
      "[17  6  0 56]\n",
      "svc Accuracy:  0.9240506329113924\n",
      "svc F1:  0.8995762711864406\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.61      0.76        23\n",
      "          1       0.86      1.00      0.93        56\n",
      "\n",
      "avg / total       0.90      0.89      0.88        79\n",
      "\n",
      "[14  9  0 56]\n",
      "LR Accuracy:  0.8860759493670886\n",
      "LR F1:  0.8411882957337503\n",
      "For name:  o_ahmed\n",
      "total sample size before apply threshold:  48\n",
      "Counter({'0000-0001-8129-0289': 23, '0000-0002-3204-381X': 18, '0000-0002-2854-2552': 3, '0000-0002-1439-0076': 3, '0000-0002-6519-6564': 1})\n",
      "['0000-0002-3204-381X', '0000-0001-8129-0289']\n",
      "Total sample size after apply threshold:  41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(41, 83)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(41, 83)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.78      0.88        18\n",
      "          1       0.85      1.00      0.92        23\n",
      "\n",
      "avg / total       0.92      0.90      0.90        41\n",
      "\n",
      "[14  4  0 23]\n",
      "svc Accuracy:  0.9024390243902439\n",
      "svc F1:  0.8975000000000001\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.78      0.88        18\n",
      "          1       0.85      1.00      0.92        23\n",
      "\n",
      "avg / total       0.92      0.90      0.90        41\n",
      "\n",
      "[14  4  0 23]\n",
      "LR Accuracy:  0.9024390243902439\n",
      "LR F1:  0.8975000000000001\n",
      "For name:  t_shimada\n",
      "total sample size before apply threshold:  144\n",
      "Counter({'0000-0002-5791-0000': 111, '0000-0002-8361-0730': 26, '0000-0002-1685-6781': 4, '0000-0001-6647-5541': 3})\n",
      "['0000-0002-5791-0000', '0000-0002-8361-0730']\n",
      "Total sample size after apply threshold:  137\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(137, 311)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(137, 311)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99       111\n",
      "          1       1.00      0.92      0.96        26\n",
      "\n",
      "avg / total       0.99      0.99      0.99       137\n",
      "\n",
      "[111   0   2  24]\n",
      "svc Accuracy:  0.9854014598540146\n",
      "svc F1:  0.9755357142857144\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99       111\n",
      "          1       1.00      0.92      0.96        26\n",
      "\n",
      "avg / total       0.99      0.99      0.99       137\n",
      "\n",
      "[111   0   2  24]\n",
      "LR Accuracy:  0.9854014598540146\n",
      "LR F1:  0.9755357142857144\n",
      "For name:  a_watts\n",
      "total sample size before apply threshold:  25\n",
      "Counter({'0000-0003-4299-2717': 14, '0000-0002-8385-1091': 9, '0000-0003-0623-4601': 1, '0000-0003-3480-582X': 1})\n",
      "['0000-0003-4299-2717']\n",
      "Total sample size after apply threshold:  14\n",
      "For name:  b_oliveira\n",
      "total sample size before apply threshold:  60\n",
      "Counter({'0000-0002-7710-4284': 22, '0000-0002-7687-4746': 17, '0000-0002-6767-6596': 13, '0000-0001-7712-0025': 6, '0000-0002-4817-6385': 2})\n",
      "['0000-0002-6767-6596', '0000-0002-7710-4284', '0000-0002-7687-4746']\n",
      "Total sample size after apply threshold:  52\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(52, 112)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(52, 112)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.92      0.92        13\n",
      "          1       0.88      0.95      0.91        22\n",
      "          2       1.00      0.88      0.94        17\n",
      "\n",
      "avg / total       0.93      0.92      0.92        52\n",
      "\n",
      "[12  1  0  1 21  0  0  2 15]\n",
      "svc Accuracy:  0.9230769230769231\n",
      "svc F1:  0.9245401337792641\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.85      0.88        13\n",
      "          1       0.84      0.95      0.89        22\n",
      "          2       1.00      0.88      0.94        17\n",
      "\n",
      "avg / total       0.91      0.90      0.90        52\n",
      "\n",
      "[11  2  0  1 21  0  0  2 15]\n",
      "LR Accuracy:  0.9038461538461539\n",
      "LR F1:  0.9037056737588652\n",
      "For name:  t_ito\n",
      "total sample size before apply threshold:  69\n",
      "Counter({'0000-0001-7443-3157': 34, '0000-0003-4516-9283': 17, '0000-0003-0686-8129': 5, '0000-0002-4237-3564': 4, '0000-0003-1971-4313': 3, '0000-0001-9873-099X': 3, '0000-0003-1279-228X': 1, '0000-0001-6015-9302': 1, '0000-0002-9274-7050': 1})\n",
      "['0000-0003-4516-9283', '0000-0001-7443-3157']\n",
      "Total sample size after apply threshold:  51\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(51, 104)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(51, 104)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.76      0.87        17\n",
      "          1       0.89      1.00      0.94        34\n",
      "\n",
      "avg / total       0.93      0.92      0.92        51\n",
      "\n",
      "[13  4  0 34]\n",
      "svc Accuracy:  0.9215686274509803\n",
      "svc F1:  0.9055555555555554\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.76      0.87        17\n",
      "          1       0.89      1.00      0.94        34\n",
      "\n",
      "avg / total       0.93      0.92      0.92        51\n",
      "\n",
      "[13  4  0 34]\n",
      "LR Accuracy:  0.9215686274509803\n",
      "LR F1:  0.9055555555555554\n",
      "For name:  t_jackson\n",
      "total sample size before apply threshold:  47\n",
      "Counter({'0000-0001-6351-2773': 23, '0000-0001-6749-9959': 9, '0000-0003-1669-6666': 6, '0000-0003-3214-3973': 3, '0000-0001-8404-4251': 2, '0000-0002-0248-2627': 2, '0000-0002-5489-6020': 1, '0000-0003-2387-6411': 1})\n",
      "['0000-0001-6351-2773']\n",
      "Total sample size after apply threshold:  23\n",
      "For name:  m_romero\n",
      "total sample size before apply threshold:  29\n",
      "Counter({'0000-0003-0578-1099': 21, '0000-0003-4582-7397': 4, '0000-0003-1563-8149': 2, '0000-0001-6682-7025': 2})\n",
      "['0000-0003-0578-1099']\n",
      "Total sample size after apply threshold:  21\n",
      "For name:  j_west\n",
      "total sample size before apply threshold:  198\n",
      "Counter({'0000-0002-1135-9356': 125, '0000-0002-6004-0202': 41, '0000-0002-7252-8651': 13, '0000-0002-5211-2405': 7, '0000-0001-8369-0075': 4, '0000-0002-4118-0322': 4, '0000-0003-0021-9638': 2, '0000-0001-7340-2885': 1, '0000-0002-6268-9750': 1})\n",
      "['0000-0002-7252-8651', '0000-0002-1135-9356', '0000-0002-6004-0202']\n",
      "Total sample size after apply threshold:  179\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(179, 382)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(179, 382)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.54      0.70        13\n",
      "          1       0.91      0.99      0.95       125\n",
      "          2       0.97      0.83      0.89        41\n",
      "\n",
      "avg / total       0.93      0.92      0.92       179\n",
      "\n",
      "[  7   6   0   0 124   1   0   7  34]\n",
      "svc Accuracy:  0.9217877094972067\n",
      "svc F1:  0.8471005758671488\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.62      0.76        13\n",
      "          1       0.91      1.00      0.95       125\n",
      "          2       1.00      0.83      0.91        41\n",
      "\n",
      "avg / total       0.94      0.93      0.93       179\n",
      "\n",
      "[  8   5   0   0 125   0   0   7  34]\n",
      "LR Accuracy:  0.9329608938547486\n",
      "LR F1:  0.8742566339512905\n",
      "For name:  c_guo\n",
      "total sample size before apply threshold:  6\n",
      "Counter({'0000-0001-9253-3469': 2, '0000-0002-0432-8121': 2, '0000-0002-4000-8141': 1, '0000-0003-2182-3287': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  m_hansen\n",
      "total sample size before apply threshold:  252\n",
      "Counter({'0000-0001-5372-4828': 55, '0000-0002-8087-8731': 40, '0000-0002-4663-8742': 29, '0000-0001-7114-8051': 27, '0000-0003-3333-2856': 24, '0000-0002-8619-1519': 17, '0000-0002-2607-461X': 16, '0000-0002-5695-6728': 11, '0000-0002-1582-7866': 6, '0000-0003-1684-8578': 6, '0000-0002-1940-0616': 5, '0000-0003-3083-4850': 4, '0000-0001-7879-2106': 4, '0000-0002-3621-1809': 4, '0000-0001-9681-2393': 2, '0000-0002-7589-0074': 1, '0000-0001-9611-8131': 1})\n",
      "['0000-0001-5372-4828', '0000-0002-8619-1519', '0000-0002-5695-6728', '0000-0003-3333-2856', '0000-0002-4663-8742', '0000-0001-7114-8051', '0000-0002-2607-461X', '0000-0002-8087-8731']\n",
      "Total sample size after apply threshold:  219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(219, 631)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(219, 631)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      1.00      0.79        55\n",
      "          1       1.00      0.76      0.87        17\n",
      "          2       1.00      0.64      0.78        11\n",
      "          3       1.00      0.75      0.86        24\n",
      "          4       1.00      0.86      0.93        29\n",
      "          5       0.95      0.74      0.83        27\n",
      "          6       1.00      0.94      0.97        16\n",
      "          7       0.91      0.80      0.85        40\n",
      "\n",
      "avg / total       0.89      0.84      0.85       219\n",
      "\n",
      "[55  0  0  0  0  0  0  0  3 13  0  0  0  0  0  1  4  0  7  0  0  0  0  0\n",
      "  5  0  0 18  0  1  0  0  2  0  0  0 25  0  0  2  7  0  0  0  0 20  0  0\n",
      "  1  0  0  0  0  0 15  0  8  0  0  0  0  0  0 32]\n",
      "svc Accuracy:  0.8447488584474886\n",
      "svc F1:  0.8584545144222564\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.98      0.85        55\n",
      "          1       1.00      0.88      0.94        17\n",
      "          2       1.00      0.82      0.90        11\n",
      "          3       1.00      0.75      0.86        24\n",
      "          4       0.93      0.90      0.91        29\n",
      "          5       0.96      0.85      0.90        27\n",
      "          6       1.00      1.00      1.00        16\n",
      "          7       0.95      0.88      0.91        40\n",
      "\n",
      "avg / total       0.91      0.89      0.90       219\n",
      "\n",
      "[54  0  0  0  0  0  0  1  1 15  0  0  1  0  0  0  2  0  9  0  0  0  0  0\n",
      "  5  0  0 18  0  1  0  0  2  0  0  0 26  0  0  1  4  0  0  0  0 23  0  0\n",
      "  0  0  0  0  0  0 16  0  4  0  0  0  1  0  0 35]\n",
      "LR Accuracy:  0.8949771689497716\n",
      "LR F1:  0.9085461191361599\n",
      "For name:  x_qian\n",
      "total sample size before apply threshold:  20\n",
      "Counter({'0000-0003-1627-288X': 16, '0000-0003-2487-8785': 2, '0000-0002-4119-2913': 1, '0000-0002-8199-6502': 1})\n",
      "['0000-0003-1627-288X']\n",
      "Total sample size after apply threshold:  16\n",
      "For name:  m_wagner\n",
      "total sample size before apply threshold:  314\n",
      "Counter({'0000-0002-9778-7684': 141, '0000-0003-2589-6440': 98, '0000-0003-3421-4763': 16, '0000-0002-9831-9110': 15, '0000-0002-4402-3234': 15, '0000-0003-3967-9527': 10, '0000-0002-7367-5629': 8, '0000-0001-9742-0471': 5, '0000-0001-7609-9172': 3, '0000-0001-6501-839X': 2, '0000-0002-6924-7226': 1})\n",
      "['0000-0003-3967-9527', '0000-0002-9831-9110', '0000-0002-9778-7684', '0000-0003-3421-4763', '0000-0003-2589-6440', '0000-0002-4402-3234']\n",
      "Total sample size after apply threshold:  295\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(295, 1456)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(295, 1456)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.20      0.33        10\n",
      "          1       1.00      0.87      0.93        15\n",
      "          2       0.82      1.00      0.90       141\n",
      "          3       1.00      0.50      0.67        16\n",
      "          4       1.00      0.90      0.95        98\n",
      "          5       1.00      0.87      0.93        15\n",
      "\n",
      "avg / total       0.92      0.90      0.89       295\n",
      "\n",
      "[  2   0   8   0   0   0   0  13   2   0   0   0   0   0 141   0   0   0\n",
      "   0   0   8   8   0   0   0   0  10   0  88   0   0   0   2   0   0  13]\n",
      "svc Accuracy:  0.8983050847457628\n",
      "svc F1:  0.784537595021466\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.20      0.33        10\n",
      "          1       1.00      0.93      0.97        15\n",
      "          2       0.82      1.00      0.90       141\n",
      "          3       1.00      0.38      0.55        16\n",
      "          4       1.00      0.93      0.96        98\n",
      "          5       1.00      0.67      0.80        15\n",
      "\n",
      "avg / total       0.91      0.89      0.88       295\n",
      "\n",
      "[  2   0   8   0   0   0   0  14   1   0   0   0   0   0 141   0   0   0\n",
      "   0   0  10   6   0   0   0   0   7   0  91   0   0   0   5   0   0  10]\n",
      "LR Accuracy:  0.8949152542372881\n",
      "LR F1:  0.7513710915973043\n",
      "For name:  d_campos\n",
      "total sample size before apply threshold:  49\n",
      "Counter({'0000-0003-1982-3288': 36, '0000-0002-3448-2111': 8, '0000-0003-0174-6640': 4, '0000-0003-0762-7124': 1})\n",
      "['0000-0003-1982-3288']\n",
      "Total sample size after apply threshold:  36\n",
      "For name:  r_clark\n",
      "total sample size before apply threshold:  152\n",
      "Counter({'0000-0002-7291-8553': 77, '0000-0002-6807-5426': 60, '0000-0002-1194-5048': 9, '0000-0002-3534-698X': 3, '0000-0003-4368-5145': 2, '0000-0001-6779-3736': 1})\n",
      "['0000-0002-7291-8553', '0000-0002-6807-5426']\n",
      "Total sample size after apply threshold:  137\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(137, 288)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(137, 288)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      1.00      0.96        77\n",
      "          1       1.00      0.90      0.95        60\n",
      "\n",
      "avg / total       0.96      0.96      0.96       137\n",
      "\n",
      "[77  0  6 54]\n",
      "svc Accuracy:  0.9562043795620438\n",
      "svc F1:  0.9549342105263159\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      1.00      0.94        77\n",
      "          1       1.00      0.85      0.92        60\n",
      "\n",
      "avg / total       0.94      0.93      0.93       137\n",
      "\n",
      "[77  0  9 51]\n",
      "LR Accuracy:  0.9343065693430657\n",
      "LR F1:  0.9318520974962692\n",
      "For name:  b_zhou\n",
      "total sample size before apply threshold:  20\n",
      "Counter({'0000-0002-1535-6283': 13, '0000-0003-2846-1813': 2, '0000-0003-2634-1527': 1, '0000-0001-9774-2737': 1, '0000-0003-1560-4950': 1, '0000-0003-0638-2428': 1, '0000-0003-4421-9787': 1})\n",
      "['0000-0002-1535-6283']\n",
      "Total sample size after apply threshold:  13\n",
      "For name:  x_yan\n",
      "total sample size before apply threshold:  111\n",
      "Counter({'0000-0002-6114-5743': 44, '0000-0001-8547-4210': 18, '0000-0003-3973-3669': 14, '0000-0002-7528-5771': 12, '0000-0001-9327-5756': 7, '0000-0003-2091-6967': 6, '0000-0001-8221-9345': 3, '0000-0001-5026-0239': 3, '0000-0001-5606-0158': 2, '0000-0002-8292-130X': 1, '0000-0002-1300-5498': 1})\n",
      "['0000-0001-8547-4210', '0000-0003-3973-3669', '0000-0002-7528-5771', '0000-0002-6114-5743']\n",
      "Total sample size after apply threshold:  88\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(88, 128)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(88, 128)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.89      0.94        18\n",
      "          1       1.00      0.93      0.96        14\n",
      "          2       1.00      0.67      0.80        12\n",
      "          3       0.86      1.00      0.93        44\n",
      "\n",
      "avg / total       0.93      0.92      0.92        88\n",
      "\n",
      "[16  0  0  2  0 13  0  1  0  0  8  4  0  0  0 44]\n",
      "svc Accuracy:  0.9204545454545454\n",
      "svc F1:  0.9076138057562206\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.83      0.91        18\n",
      "          1       1.00      0.93      0.96        14\n",
      "          2       1.00      0.67      0.80        12\n",
      "          3       0.85      1.00      0.92        44\n",
      "\n",
      "avg / total       0.92      0.91      0.91        88\n",
      "\n",
      "[15  0  0  3  0 13  0  1  0  0  8  4  0  0  0 44]\n",
      "LR Accuracy:  0.9090909090909091\n",
      "LR F1:  0.8971801346801346\n",
      "For name:  x_li\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total sample size before apply threshold:  867\n",
      "Counter({'0000-0002-5555-9034': 244, '0000-0002-5981-2762': 71, '0000-0002-4044-2888': 67, '0000-0001-6508-8355': 51, '0000-0001-8791-7505': 30, '0000-0002-4115-3287': 30, '0000-0002-2497-020X': 26, '0000-0002-6200-1178': 26, '0000-0002-7844-8417': 22, '0000-0003-1359-5130': 21, '0000-0002-4793-0550': 20, '0000-0001-8718-2780': 18, '0000-0002-4446-2480': 16, '0000-0003-1568-1999': 14, '0000-0001-9814-0383': 13, '0000-0002-7646-1132': 13, '0000-0003-0724-0982': 12, '0000-0002-2510-2236': 11, '0000-0002-3828-0971': 10, '0000-0002-4675-5367': 10, '0000-0002-6646-0929': 8, '0000-0002-4350-3375': 7, '0000-0002-7939-5150': 7, '0000-0001-7038-5119': 6, '0000-0002-9121-7883': 5, '0000-0002-4828-4183': 5, '0000-0001-8184-3197': 5, '0000-0001-6449-1505': 5, '0000-0003-0606-434X': 5, '0000-0003-0220-9003': 5, '0000-0002-0046-2016': 5, '0000-0001-7111-8485': 4, '0000-0002-4230-5676': 4, '0000-0002-6007-5149': 4, '0000-0003-4514-0149': 4, '0000-0003-1117-9619': 4, '0000-0003-2999-9818': 3, '0000-0001-7073-7532': 3, '0000-0002-0986-6682': 3, '0000-0003-1382-3295': 3, '0000-0003-3882-7073': 3, '0000-0002-4842-5054': 3, '0000-0003-1990-0159': 3, '0000-0002-7270-3376': 3, '0000-0003-0493-417X': 2, '0000-0001-8662-0865': 2, '0000-0003-3050-8529': 2, '0000-0003-3272-5180': 2, '0000-0002-3428-3569': 2, '0000-0002-5263-7017': 2, '0000-0001-9790-5663': 2, '0000-0003-2747-644X': 2, '0000-0002-1959-2160': 2, '0000-0002-0377-2925': 2, '0000-0003-3951-9646': 2, '0000-0002-2397-6079': 1, '0000-0001-7458-8263': 1, '0000-0002-7671-6326': 1, '0000-0003-0191-9484': 1, '0000-0002-0220-8310': 1, '0000-0002-6433-2085': 1, '0000-0002-8669-6314': 1, '0000-0001-5580-5605': 1, '0000-0001-8088-2338': 1, '0000-0002-6093-1099': 1, '0000-0002-1708-4314': 1, '0000-0002-3919-2658': 1, '0000-0001-7420-6253': 1})\n",
      "['0000-0002-4446-2480', '0000-0001-9814-0383', '0000-0002-7646-1132', '0000-0003-1568-1999', '0000-0002-5555-9034', '0000-0001-8791-7505', '0000-0002-3828-0971', '0000-0003-0724-0982', '0000-0002-4044-2888', '0000-0002-7844-8417', '0000-0002-4793-0550', '0000-0001-8718-2780', '0000-0002-4675-5367', '0000-0002-2497-020X', '0000-0002-4115-3287', '0000-0002-6200-1178', '0000-0003-1359-5130', '0000-0002-5981-2762', '0000-0002-2510-2236', '0000-0001-6508-8355']\n",
      "Total sample size after apply threshold:  725\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(725, 614)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(725, 614)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.88      0.88        16\n",
      "          1       1.00      0.62      0.76        13\n",
      "          2       1.00      1.00      1.00        13\n",
      "          3       0.69      0.64      0.67        14\n",
      "          4       0.91      0.92      0.92       244\n",
      "          5       0.87      0.67      0.75        30\n",
      "          6       0.41      0.70      0.52        10\n",
      "          7       1.00      0.50      0.67        12\n",
      "          8       0.63      0.72      0.67        67\n",
      "          9       0.77      0.77      0.77        22\n",
      "         10       0.95      0.90      0.92        20\n",
      "         11       0.68      0.72      0.70        18\n",
      "         12       0.89      0.80      0.84        10\n",
      "         13       0.96      0.92      0.94        26\n",
      "         14       0.81      0.83      0.82        30\n",
      "         15       0.95      0.81      0.88        26\n",
      "         16       0.88      0.71      0.79        21\n",
      "         17       0.68      0.80      0.74        71\n",
      "         18       1.00      1.00      1.00        11\n",
      "         19       0.89      0.82      0.86        51\n",
      "\n",
      "avg / total       0.84      0.83      0.83       725\n",
      "\n",
      "[ 14   0   0   0   0   0   1   0   1   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   8   0   0   2   0   1   0   0   0   0   0   0   0   0   0\n",
      "   0   2   0   0   0   0  13   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   9   0   0   0   0   2   0   0   0\n",
      "   1   0   0   0   0   2   0   0   0   0   0   2 225   3   1   0   2   1\n",
      "   0   0   0   0   1   0   1   8   0   0   0   0   0   0   1  20   1   0\n",
      "   4   0   1   1   0   0   2   0   0   0   0   0   0   0   0   0   2   0\n",
      "   7   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   1   0   0   6   1   1   0   0   0   0   0   0   0   3   0   0   1   0\n",
      "   0   1   4   0   1   0  48   1   0   2   0   0   1   0   0   7   0   1\n",
      "   0   0   0   0   1   0   3   0   0  17   0   0   0   0   0   0   0   0\n",
      "   0   1   0   0   0   0   2   0   0   0   0   0  18   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   2   0   0   0   3   0   0  13   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   2   0   0   0   0   0   0   0\n",
      "   8   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0\n",
      "   0   0   0  24   0   1   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   2   0   0   1   0   1  25   0   1   0   0   0   0   0   0   0   0   0\n",
      "   1   0   2   0   0   0   0   0   0  21   0   2   0   0   1   0   0   0\n",
      "   1   0   0   0   2   0   0   0   0   0   0   0  15   2   0   0   0   0\n",
      "   0   1   1   0   0   0   5   1   0   2   0   0   1   0   0  57   0   3\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  11   0   0   0   0   0   3   0   1   0   2   1   0   0   0   0   1   0\n",
      "   0   1   0  42]\n",
      "svc Accuracy:  0.8289655172413793\n",
      "svc F1:  0.804493019347462\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.94      0.91        16\n",
      "          1       1.00      0.46      0.63        13\n",
      "          2       1.00      1.00      1.00        13\n",
      "          3       0.83      0.36      0.50        14\n",
      "          4       0.85      0.96      0.90       244\n",
      "          5       0.95      0.70      0.81        30\n",
      "          6       0.80      0.40      0.53        10\n",
      "          7       1.00      0.42      0.59        12\n",
      "          8       0.60      0.78      0.68        67\n",
      "          9       0.89      0.77      0.83        22\n",
      "         10       1.00      0.85      0.92        20\n",
      "         11       0.79      0.83      0.81        18\n",
      "         12       1.00      0.80      0.89        10\n",
      "         13       0.93      0.96      0.94        26\n",
      "         14       0.81      0.73      0.77        30\n",
      "         15       0.92      0.85      0.88        26\n",
      "         16       1.00      0.67      0.80        21\n",
      "         17       0.73      0.73      0.73        71\n",
      "         18       1.00      1.00      1.00        11\n",
      "         19       0.85      0.86      0.85        51\n",
      "\n",
      "avg / total       0.84      0.83      0.83       725\n",
      "\n",
      "[ 15   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   6   0   0   3   0   0   0   3   0   0   0   0   0   0   0\n",
      "   0   1   0   0   0   0  13   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   5   3   0   0   0   5   0   0   0\n",
      "   0   0   0   0   0   1   0   0   0   0   0   0 235   1   0   0   3   0\n",
      "   0   0   0   0   1   0   0   4   0   0   0   0   0   0   1  21   1   0\n",
      "   4   0   0   0   0   0   1   0   0   1   0   1   0   0   0   0   2   0\n",
      "   4   0   2   0   0   0   0   0   0   1   0   0   0   1   0   0   0   0\n",
      "   3   0   0   5   2   0   0   0   0   0   0   0   0   1   0   1   1   0\n",
      "   0   1   5   0   0   0  52   1   0   1   0   0   1   0   0   5   0   0\n",
      "   0   0   0   0   1   0   0   0   1  17   0   0   0   0   0   0   0   0\n",
      "   0   3   0   0   0   0   2   0   0   0   1   0  17   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   2   0   0  15   0   0\n",
      "   0   0   0   1   0   0   0   0   0   0   1   0   0   0   0   0   0   0\n",
      "   8   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0  25   0   1   0   0   0   0   0   0   0   0   3   0   0   0\n",
      "   4   0   0   0   0   1  22   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   1   1   0   1   0   0   0  22   0   1   0   0   1   0   0   0\n",
      "   4   0   0   0   1   0   0   0   0   0   0   0  14   1   0   0   0   0\n",
      "   0   0  10   0   0   0   3   0   0   2   0   1   1   0   0  52   0   2\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  11   0   0   0   0   0   3   0   0   0   1   0   0   0   0   0   1   0\n",
      "   0   2   0  44]\n",
      "LR Accuracy:  0.8317241379310345\n",
      "LR F1:  0.7991745884027168\n",
      "For name:  j_burton\n",
      "total sample size before apply threshold:  46\n",
      "Counter({'0000-0003-1176-7592': 34, '0000-0003-2817-7353': 6, '0000-0001-5267-1277': 4, '0000-0002-3205-8819': 2})\n",
      "['0000-0003-1176-7592']\n",
      "Total sample size after apply threshold:  34\n",
      "For name:  x_feng\n",
      "total sample size before apply threshold:  102\n",
      "Counter({'0000-0001-6894-7979': 37, '0000-0002-3212-3051': 25, '0000-0002-9057-1549': 17, '0000-0002-6920-1519': 9, '0000-0002-9523-6096': 8, '0000-0002-0443-0628': 2, '0000-0002-9473-2848': 2, '0000-0003-1945-1605': 1, '0000-0001-8226-3389': 1})\n",
      "['0000-0002-3212-3051', '0000-0001-6894-7979', '0000-0002-9057-1549']\n",
      "Total sample size after apply threshold:  79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(79, 100)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(79, 100)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.76      0.83        25\n",
      "          1       0.82      0.86      0.84        37\n",
      "          2       0.79      0.88      0.83        17\n",
      "\n",
      "avg / total       0.84      0.84      0.84        79\n",
      "\n",
      "[19  5  1  2 32  3  0  2 15]\n",
      "svc Accuracy:  0.8354430379746836\n",
      "svc F1:  0.8338418510043223\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.80      0.85        25\n",
      "          1       0.84      0.86      0.85        37\n",
      "          2       0.79      0.88      0.83        17\n",
      "\n",
      "avg / total       0.85      0.85      0.85        79\n",
      "\n",
      "[20  4  1  2 32  3  0  2 15]\n",
      "LR Accuracy:  0.8481012658227848\n",
      "LR F1:  0.8459101654846336\n",
      "For name:  w_hussein\n",
      "total sample size before apply threshold:  33\n",
      "Counter({'0000-0001-5392-1880': 18, '0000-0002-7416-4521': 13, '0000-0001-5928-6240': 1, '0000-0002-7589-7479': 1})\n",
      "['0000-0001-5392-1880', '0000-0002-7416-4521']\n",
      "Total sample size after apply threshold:  31\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(31, 77)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(31, 77)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.94      0.97        18\n",
      "          1       0.93      1.00      0.96        13\n",
      "\n",
      "avg / total       0.97      0.97      0.97        31\n",
      "\n",
      "[17  1  0 13]\n",
      "svc Accuracy:  0.967741935483871\n",
      "svc F1:  0.9671957671957672\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        18\n",
      "          1       1.00      1.00      1.00        13\n",
      "\n",
      "avg / total       1.00      1.00      1.00        31\n",
      "\n",
      "[18  0  0 13]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  c_santos\n",
      "total sample size before apply threshold:  293\n",
      "Counter({'0000-0002-0405-3500': 68, '0000-0003-4129-6381': 41, '0000-0001-6074-7825': 38, '0000-0002-7014-8014': 37, '0000-0002-7109-1101': 25, '0000-0002-4575-1807': 22, '0000-0003-4681-0941': 17, '0000-0002-6725-8925': 10, '0000-0003-0023-7203': 9, '0000-0003-4380-7990': 8, '0000-0002-8567-0032': 4, '0000-0001-6315-7433': 3, '0000-0002-4751-2180': 3, '0000-0001-5693-9795': 2, '0000-0001-9198-2668': 2, '0000-0002-0938-523X': 1, '0000-0001-7927-9718': 1, '0000-0001-5181-2461': 1, '0000-0001-5577-0799': 1})\n",
      "['0000-0003-4129-6381', '0000-0002-6725-8925', '0000-0002-7109-1101', '0000-0002-7014-8014', '0000-0001-6074-7825', '0000-0002-0405-3500', '0000-0003-4681-0941', '0000-0002-4575-1807']\n",
      "Total sample size after apply threshold:  258\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(258, 589)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(258, 589)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.88      0.88        41\n",
      "          1       1.00      0.90      0.95        10\n",
      "          2       0.73      0.76      0.75        25\n",
      "          3       0.97      0.81      0.88        37\n",
      "          4       0.97      0.97      0.97        38\n",
      "          5       0.89      0.94      0.91        68\n",
      "          6       0.75      0.88      0.81        17\n",
      "          7       1.00      0.95      0.98        22\n",
      "\n",
      "avg / total       0.90      0.90      0.90       258\n",
      "\n",
      "[36  0  2  0  1  2  0  0  0  9  0  0  0  1  0  0  3  0 19  0  0  3  0  0\n",
      "  1  0  2 30  0  1  3  0  0  0  1  0 37  0  0  0  1  0  1  0  0 64  2  0\n",
      "  0  0  1  1  0  0 15  0  0  0  0  0  0  1  0 21]\n",
      "svc Accuracy:  0.8953488372093024\n",
      "svc F1:  0.8910491379502432\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.88      0.86        41\n",
      "          1       1.00      0.80      0.89        10\n",
      "          2       0.90      0.72      0.80        25\n",
      "          3       1.00      0.89      0.94        37\n",
      "          4       0.95      0.97      0.96        38\n",
      "          5       0.87      0.97      0.92        68\n",
      "          6       0.84      0.94      0.89        17\n",
      "          7       1.00      0.91      0.95        22\n",
      "\n",
      "avg / total       0.91      0.91      0.91       258\n",
      "\n",
      "[36  0  2  0  1  2  0  0  1  8  0  0  0  1  0  0  4  0 18  0  1  2  0  0\n",
      "  1  0  0 33  0  2  1  0  0  0  0  0 37  1  0  0  0  0  0  0  0 66  2  0\n",
      "  0  0  0  0  0  1 16  0  1  0  0  0  0  1  0 20]\n",
      "LR Accuracy:  0.9069767441860465\n",
      "LR F1:  0.9009830447330447\n",
      "For name:  j_figueroa\n",
      "total sample size before apply threshold:  59\n",
      "Counter({'0000-0003-3036-6604': 51, '0000-0003-4769-8736': 3, '0000-0002-7457-0650': 3, '0000-0002-3403-1484': 2})\n",
      "['0000-0003-3036-6604']\n",
      "Total sample size after apply threshold:  51\n",
      "For name:  w_cui\n",
      "total sample size before apply threshold:  14\n",
      "Counter({'0000-0003-2245-6052': 7, '0000-0002-6324-5772': 4, '0000-0001-7281-3471': 1, '0000-0002-6938-9582': 1, '0000-0003-4875-4285': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  d_moreira\n",
      "total sample size before apply threshold:  26\n",
      "Counter({'0000-0003-4517-244X': 18, '0000-0003-1961-7281': 4, '0000-0003-0605-8003': 2, '0000-0002-8257-5785': 1, '0000-0002-4801-2225': 1})\n",
      "['0000-0003-4517-244X']\n",
      "Total sample size after apply threshold:  18\n",
      "For name:  m_graham\n",
      "total sample size before apply threshold:  64\n",
      "Counter({'0000-0002-7290-1217': 34, '0000-0003-4983-4949': 27, '0000-0002-4170-1095': 2, '0000-0003-0708-7957': 1})\n",
      "['0000-0002-7290-1217', '0000-0003-4983-4949']\n",
      "Total sample size after apply threshold:  61\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(61, 161)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(61, 161)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.94      0.97        34\n",
      "          1       0.93      1.00      0.96        27\n",
      "\n",
      "avg / total       0.97      0.97      0.97        61\n",
      "\n",
      "[32  2  0 27]\n",
      "svc Accuracy:  0.9672131147540983\n",
      "svc F1:  0.9669913419913421\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.99        34\n",
      "          1       1.00      0.96      0.98        27\n",
      "\n",
      "avg / total       0.98      0.98      0.98        61\n",
      "\n",
      "[34  0  1 26]\n",
      "LR Accuracy:  0.9836065573770492\n",
      "LR F1:  0.9833196609242548\n",
      "For name:  g_dias\n",
      "total sample size before apply threshold:  9\n",
      "Counter({'0000-0002-3774-6661': 5, '0000-0001-8548-1146': 2, '0000-0001-7291-6569': 1, '0000-0002-0524-1239': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  h_yoshida\n",
      "total sample size before apply threshold:  72\n",
      "Counter({'0000-0001-6890-4397': 38, '0000-0002-2540-0225': 19, '0000-0001-6360-5988': 13, '0000-0002-7283-8617': 2})\n",
      "['0000-0002-2540-0225', '0000-0001-6360-5988', '0000-0001-6890-4397']\n",
      "Total sample size after apply threshold:  70\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(70, 129)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(70, 129)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      1.00      0.93        19\n",
      "          1       1.00      0.85      0.92        13\n",
      "          2       1.00      0.97      0.99        38\n",
      "\n",
      "avg / total       0.96      0.96      0.96        70\n",
      "\n",
      "[19  0  0  2 11  0  1  0 37]\n",
      "svc Accuracy:  0.9571428571428572\n",
      "svc F1:  0.9433875338753387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.95      0.95        19\n",
      "          1       1.00      0.85      0.92        13\n",
      "          2       0.95      1.00      0.97        38\n",
      "\n",
      "avg / total       0.96      0.96      0.96        70\n",
      "\n",
      "[18  0  1  1 11  1  0  0 38]\n",
      "LR Accuracy:  0.9571428571428572\n",
      "LR F1:  0.9461313540260908\n",
      "For name:  m_branco\n",
      "total sample size before apply threshold:  56\n",
      "Counter({'0000-0001-9447-1548': 21, '0000-0002-8140-1257': 18, '0000-0003-4439-1923': 12, '0000-0001-5238-1069': 5})\n",
      "['0000-0002-8140-1257', '0000-0001-9447-1548', '0000-0003-4439-1923']\n",
      "Total sample size after apply threshold:  51\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(51, 147)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(51, 147)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.89      0.94        18\n",
      "          1       0.91      1.00      0.95        21\n",
      "          2       0.92      0.92      0.92        12\n",
      "\n",
      "avg / total       0.94      0.94      0.94        51\n",
      "\n",
      "[16  1  1  0 21  0  0  1 11]\n",
      "svc Accuracy:  0.9411764705882353\n",
      "svc F1:  0.937462863933452\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.97        18\n",
      "          1       1.00      1.00      1.00        21\n",
      "          2       1.00      0.92      0.96        12\n",
      "\n",
      "avg / total       0.98      0.98      0.98        51\n",
      "\n",
      "[18  0  0  0 21  0  1  0 11]\n",
      "LR Accuracy:  0.9803921568627451\n",
      "LR F1:  0.9764982373678026\n",
      "For name:  k_chong\n",
      "total sample size before apply threshold:  39\n",
      "Counter({'0000-0003-2587-1323': 28, '0000-0002-7350-597X': 4, '0000-0003-4754-8957': 4, '0000-0003-0786-842X': 3})\n",
      "['0000-0003-2587-1323']\n",
      "Total sample size after apply threshold:  28\n",
      "For name:  j_kumar\n",
      "total sample size before apply threshold:  16\n",
      "Counter({'0000-0002-9754-3305': 9, '0000-0002-4153-1495': 3, '0000-0002-0159-0546': 2, '0000-0001-9666-8280': 2})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  a_shenoy\n",
      "total sample size before apply threshold:  33\n",
      "Counter({'0000-0001-6228-9303': 24, '0000-0003-4306-7582': 3, '0000-0001-8639-2751': 3, '0000-0003-4200-8599': 2, '0000-0003-4611-9333': 1})\n",
      "['0000-0001-6228-9303']\n",
      "Total sample size after apply threshold:  24\n",
      "For name:  h_yang\n",
      "total sample size before apply threshold:  417\n",
      "Counter({'0000-0002-8482-6031': 65, '0000-0003-3459-4516': 45, '0000-0003-3864-9895': 44, '0000-0002-7056-3648': 40, '0000-0003-3602-9772': 34, '0000-0001-6483-2373': 31, '0000-0002-6707-8481': 18, '0000-0002-0470-676X': 15, '0000-0002-1711-363X': 15, '0000-0002-2965-4353': 13, '0000-0001-5298-2462': 11, '0000-0002-4287-0026': 11, '0000-0002-0762-7194': 10, '0000-0003-4647-1388': 6, '0000-0002-0435-6763': 5, '0000-0003-3325-1378': 5, '0000-0001-5779-1833': 5, '0000-0002-9187-1367': 5, '0000-0001-8061-6179': 4, '0000-0003-1445-3468': 3, '0000-0002-9596-4907': 3, '0000-0003-4265-7492': 3, '0000-0001-7255-9653': 3, '0000-0003-0445-2824': 3, '0000-0001-9644-6207': 2, '0000-0003-1139-2751': 2, '0000-0002-2060-8991': 2, '0000-0002-4690-8503': 2, '0000-0002-3471-8235': 2, '0000-0003-0727-0874': 1, '0000-0001-5117-5394': 1, '0000-0001-6436-3036': 1, '0000-0003-1943-8857': 1, '0000-0002-2628-4676': 1, '0000-0002-4732-1990': 1, '0000-0001-5140-9664': 1, '0000-0002-2252-2606': 1, '0000-0003-4825-2867': 1, '0000-0003-2326-7317': 1})\n",
      "['0000-0002-2965-4353', '0000-0001-5298-2462', '0000-0002-7056-3648', '0000-0003-3459-4516', '0000-0002-0762-7194', '0000-0003-3602-9772', '0000-0002-6707-8481', '0000-0003-3864-9895', '0000-0002-8482-6031', '0000-0001-6483-2373', '0000-0002-0470-676X', '0000-0002-4287-0026', '0000-0002-1711-363X']\n",
      "Total sample size after apply threshold:  352\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(352, 565)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(352, 565)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.46      0.52        13\n",
      "          1       0.57      0.73      0.64        11\n",
      "          2       0.89      0.82      0.86        40\n",
      "          3       0.57      0.87      0.68        45\n",
      "          4       0.62      1.00      0.77        10\n",
      "          5       1.00      0.91      0.95        34\n",
      "          6       1.00      0.67      0.80        18\n",
      "          7       0.95      0.84      0.89        44\n",
      "          8       0.86      0.78      0.82        65\n",
      "          9       1.00      1.00      1.00        31\n",
      "         10       0.65      0.73      0.69        15\n",
      "         11       1.00      0.91      0.95        11\n",
      "         12       1.00      0.47      0.64        15\n",
      "\n",
      "avg / total       0.85      0.81      0.82       352\n",
      "\n",
      "[ 6  0  0  4  1  0  0  0  2  0  0  0  0  0  8  0  1  0  0  0  2  0  0  0\n",
      "  0  0  0  0 33  4  1  0  0  0  0  0  2  0  0  0  2  0 39  2  0  0  0  2\n",
      "  0  0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0  0  0  1  0  0 31  0\n",
      "  0  0  0  2  0  0  0  2  0  1  0  0 12  0  2  0  1  0  0  1  2  0  2  1\n",
      "  0  0 37  1  0  0  0  0  2  0  2  8  1  0  0  0 51  0  1  0  0  0  0  0\n",
      "  0  0  0  0  0  0 31  0  0  0  0  0  1  2  0  0  0  0  1  0 11  0  0  1\n",
      "  0  0  0  0  0  0  0  0  0  0 10  0  0  0  0  8  0  0  0  0  0  0  0  0\n",
      "  7]\n",
      "svc Accuracy:  0.8125\n",
      "svc F1:  0.7858893027643441\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.31      0.44        13\n",
      "          1       0.64      0.64      0.64        11\n",
      "          2       0.86      0.95      0.90        40\n",
      "          3       0.63      0.84      0.72        45\n",
      "          4       0.71      1.00      0.83        10\n",
      "          5       1.00      1.00      1.00        34\n",
      "          6       1.00      0.78      0.88        18\n",
      "          7       0.91      0.89      0.90        44\n",
      "          8       0.77      0.86      0.81        65\n",
      "          9       1.00      1.00      1.00        31\n",
      "         10       1.00      0.47      0.64        15\n",
      "         11       1.00      0.82      0.90        11\n",
      "         12       1.00      0.47      0.64        15\n",
      "\n",
      "avg / total       0.86      0.84      0.83       352\n",
      "\n",
      "[ 4  1  0  2  1  0  0  1  4  0  0  0  0  0  7  0  0  0  0  0  3  1  0  0\n",
      "  0  0  0  0 38  2  0  0  0  0  0  0  0  0  0  0  1  0 38  1  0  0  0  5\n",
      "  0  0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0 34  0\n",
      "  0  0  0  0  0  0  0  1  1  0  0  0 14  0  2  0  0  0  0  0  1  0  1  1\n",
      "  0  0 39  2  0  0  0  0  0  0  1  7  1  0  0  0 56  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0 31  0  0  0  0  0  4  2  0  0  0  0  2  0  7  0  0  1\n",
      "  0  0  0  0  0  0  0  1  0  0  9  0  0  0  0  8  0  0  0  0  0  0  0  0\n",
      "  7]\n",
      "LR Accuracy:  0.8352272727272727\n",
      "LR F1:  0.7921989263443537\n",
      "For name:  m_magnusson\n",
      "total sample size before apply threshold:  67\n",
      "Counter({'0000-0003-1710-5936': 24, '0000-0002-6565-4027': 22, '0000-0002-3141-8544': 10, '0000-0002-7574-1095': 7, '0000-0002-8049-2142': 3, '0000-0001-5388-6608': 1})\n",
      "['0000-0002-3141-8544', '0000-0002-6565-4027', '0000-0003-1710-5936']\n",
      "Total sample size after apply threshold:  56\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(56, 180)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(56, 180)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.90      0.95        10\n",
      "          1       0.88      1.00      0.94        22\n",
      "          2       1.00      0.92      0.96        24\n",
      "\n",
      "avg / total       0.95      0.95      0.95        56\n",
      "\n",
      "[ 9  1  0  0 22  0  0  2 22]\n",
      "svc Accuracy:  0.9464285714285714\n",
      "svc F1:  0.946686790983008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.90      0.95        10\n",
      "          1       0.92      1.00      0.96        22\n",
      "          2       1.00      0.96      0.98        24\n",
      "\n",
      "avg / total       0.97      0.96      0.96        56\n",
      "\n",
      "[ 9  1  0  0 22  0  0  1 23]\n",
      "LR Accuracy:  0.9642857142857143\n",
      "LR F1:  0.9608711881461285\n",
      "For name:  m_foster\n",
      "total sample size before apply threshold:  103\n",
      "Counter({'0000-0001-9645-7491': 43, '0000-0002-4524-141X': 43, '0000-0001-6392-7418': 6, '0000-0002-4453-7788': 5, '0000-0002-3100-0885': 3, '0000-0003-2257-4825': 3})\n",
      "['0000-0001-9645-7491', '0000-0002-4524-141X']\n",
      "Total sample size after apply threshold:  86\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(86, 161)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(86, 161)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      1.00      0.96        43\n",
      "          1       1.00      0.91      0.95        43\n",
      "\n",
      "avg / total       0.96      0.95      0.95        86\n",
      "\n",
      "[43  0  4 39]\n",
      "svc Accuracy:  0.9534883720930233\n",
      "svc F1:  0.9533875338753388\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      1.00      0.97        43\n",
      "          1       1.00      0.93      0.96        43\n",
      "\n",
      "avg / total       0.97      0.97      0.97        86\n",
      "\n",
      "[43  0  3 40]\n",
      "LR Accuracy:  0.9651162790697675\n",
      "LR F1:  0.9650737782591038\n",
      "For name:  j_lynch\n",
      "total sample size before apply threshold:  20\n",
      "Counter({'0000-0002-1227-2252': 7, '0000-0003-0889-2616': 6, '0000-0003-3624-2741': 4, '0000-0003-0108-2127': 2, '0000-0002-4094-3738': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  j_boyle\n",
      "total sample size before apply threshold:  14\n",
      "Counter({'0000-0002-9404-6901': 10, '0000-0002-5687-9857': 1, '0000-0002-3616-1637': 1, '0000-0001-6173-6765': 1, '0000-0002-6330-6870': 1})\n",
      "['0000-0002-9404-6901']\n",
      "Total sample size after apply threshold:  10\n",
      "For name:  r_turner\n",
      "total sample size before apply threshold:  147\n",
      "Counter({'0000-0001-5055-9644': 58, '0000-0001-7534-2935': 44, '0000-0003-4278-8302': 19, '0000-0001-5523-0645': 15, '0000-0002-0393-8593': 7, '0000-0002-9263-0776': 2, '0000-0002-3938-5513': 1, '0000-0003-0853-5360': 1})\n",
      "['0000-0001-7534-2935', '0000-0001-5055-9644', '0000-0001-5523-0645', '0000-0003-4278-8302']\n",
      "Total sample size after apply threshold:  136\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(136, 446)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(136, 446)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.64      0.78        44\n",
      "          1       0.75      1.00      0.86        58\n",
      "          2       1.00      0.87      0.93        15\n",
      "          3       1.00      0.95      0.97        19\n",
      "\n",
      "avg / total       0.89      0.86      0.86       136\n",
      "\n",
      "[28 16  0  0  0 58  0  0  0  2 13  0  0  1  0 18]\n",
      "svc Accuracy:  0.8602941176470589\n",
      "svc F1:  0.8846453596453596\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.66      0.78        44\n",
      "          1       0.77      1.00      0.87        58\n",
      "          2       1.00      0.87      0.93        15\n",
      "          3       1.00      0.95      0.97        19\n",
      "\n",
      "avg / total       0.89      0.87      0.86       136\n",
      "\n",
      "[29 15  0  0  0 58  0  0  1  1 13  0  0  1  0 18]\n",
      "LR Accuracy:  0.8676470588235294\n",
      "LR F1:  0.8893771591140012\n",
      "For name:  s_brooks\n",
      "total sample size before apply threshold:  58\n",
      "Counter({'0000-0002-8437-9788': 32, '0000-0002-4592-4974': 16, '0000-0002-5701-0125': 7, '0000-0001-6377-1644': 3})\n",
      "['0000-0002-8437-9788', '0000-0002-4592-4974']\n",
      "Total sample size after apply threshold:  48\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(48, 179)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(48, 179)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      1.00      0.93        32\n",
      "          1       1.00      0.69      0.81        16\n",
      "\n",
      "avg / total       0.91      0.90      0.89        48\n",
      "\n",
      "[32  0  5 11]\n",
      "svc Accuracy:  0.8958333333333334\n",
      "svc F1:  0.8711755233494364\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      1.00      0.93        32\n",
      "          1       1.00      0.69      0.81        16\n",
      "\n",
      "avg / total       0.91      0.90      0.89        48\n",
      "\n",
      "[32  0  5 11]\n",
      "LR Accuracy:  0.8958333333333334\n",
      "LR F1:  0.8711755233494364\n",
      "For name:  p_moreira\n",
      "total sample size before apply threshold:  217\n",
      "Counter({'0000-0001-5177-6747': 133, '0000-0002-7035-7799': 68, '0000-0002-2800-3903': 6, '0000-0002-5454-7971': 4, '0000-0003-0452-6790': 2, '0000-0002-0004-851X': 2, '0000-0001-7247-6815': 1, '0000-0001-6919-0904': 1})\n",
      "['0000-0001-5177-6747', '0000-0002-7035-7799']\n",
      "Total sample size after apply threshold:  201\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(201, 1355)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(201, 1355)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99       133\n",
      "          1       0.99      0.97      0.98        68\n",
      "\n",
      "avg / total       0.99      0.99      0.99       201\n",
      "\n",
      "[132   1   2  66]\n",
      "svc Accuracy:  0.9850746268656716\n",
      "svc F1:  0.9832709113607989\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98       133\n",
      "          1       0.97      0.97      0.97        68\n",
      "\n",
      "avg / total       0.98      0.98      0.98       201\n",
      "\n",
      "[131   2   2  66]\n",
      "LR Accuracy:  0.9800995024875622\n",
      "LR F1:  0.9777753206545776\n",
      "For name:  s_mukhopadhyay\n",
      "total sample size before apply threshold:  119\n",
      "Counter({'0000-0001-8033-5748': 49, '0000-0001-9660-2599': 37, '0000-0003-1242-9958': 18, '0000-0003-4790-3090': 8, '0000-0002-1838-2815': 5, '0000-0002-4056-2185': 1, '0000-0002-6290-6380': 1})\n",
      "['0000-0003-1242-9958', '0000-0001-8033-5748', '0000-0001-9660-2599']\n",
      "Total sample size after apply threshold:  104\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(104, 189)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(104, 189)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.78      0.82        18\n",
      "          1       1.00      0.88      0.93        49\n",
      "          2       0.80      0.97      0.88        37\n",
      "\n",
      "avg / total       0.91      0.89      0.90       104\n",
      "\n",
      "[14  0  4  1 43  5  1  0 36]\n",
      "svc Accuracy:  0.8942307692307693\n",
      "svc F1:  0.8787869336493878\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.56      0.69        18\n",
      "          1       1.00      0.92      0.96        49\n",
      "          2       0.75      0.97      0.85        37\n",
      "\n",
      "avg / total       0.90      0.88      0.87       104\n",
      "\n",
      "[10  0  8  0 45  4  1  0 36]\n",
      "LR Accuracy:  0.875\n",
      "LR F1:  0.8313869348179478\n",
      "For name:  a_hudson\n",
      "total sample size before apply threshold:  129\n",
      "Counter({'0000-0003-1105-7646': 86, '0000-0002-0192-776X': 15, '0000-0003-1849-9666': 13, '0000-0001-7292-5406': 6, '0000-0001-6436-2025': 5, '0000-0001-9016-6917': 4})\n",
      "['0000-0002-0192-776X', '0000-0003-1849-9666', '0000-0003-1105-7646']\n",
      "Total sample size after apply threshold:  114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(114, 243)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(114, 243)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.87      0.93        15\n",
      "          1       1.00      0.62      0.76        13\n",
      "          2       0.92      1.00      0.96        86\n",
      "\n",
      "avg / total       0.94      0.94      0.93       114\n",
      "\n",
      "[13  0  2  0  8  5  0  0 86]\n",
      "svc Accuracy:  0.9385964912280702\n",
      "svc F1:  0.8837900150749313\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.80      0.89        15\n",
      "          1       1.00      0.31      0.47        13\n",
      "          2       0.88      1.00      0.93        86\n",
      "\n",
      "avg / total       0.91      0.89      0.88       114\n",
      "\n",
      "[12  0  3  0  4  9  0  0 86]\n",
      "LR Accuracy:  0.8947368421052632\n",
      "LR F1:  0.7647532442928863\n",
      "For name:  d_thomas\n",
      "total sample size before apply threshold:  62\n",
      "Counter({'0000-0001-8832-5907': 17, '0000-0002-8141-3362': 11, '0000-0002-8278-5934': 10, '0000-0002-1307-6042': 6, '0000-0002-7976-4956': 6, '0000-0002-1053-129X': 5, '0000-0001-9415-5991': 4, '0000-0001-6867-5504': 2, '0000-0003-4295-9765': 1})\n",
      "['0000-0002-8278-5934', '0000-0001-8832-5907', '0000-0002-8141-3362']\n",
      "Total sample size after apply threshold:  38\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(38, 75)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(38, 75)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.70      0.82        10\n",
      "          1       0.85      1.00      0.92        17\n",
      "          2       1.00      1.00      1.00        11\n",
      "\n",
      "avg / total       0.93      0.92      0.92        38\n",
      "\n",
      "[ 7  3  0  0 17  0  0  0 11]\n",
      "svc Accuracy:  0.9210526315789473\n",
      "svc F1:  0.9141494435612083\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.80      0.89        10\n",
      "          1       0.89      1.00      0.94        17\n",
      "          2       1.00      1.00      1.00        11\n",
      "\n",
      "avg / total       0.95      0.95      0.95        38\n",
      "\n",
      "[ 8  2  0  0 17  0  0  0 11]\n",
      "LR Accuracy:  0.9473684210526315\n",
      "LR F1:  0.9444444444444445\n",
      "For name:  w_smith\n",
      "total sample size before apply threshold:  61\n",
      "Counter({'0000-0002-4610-998X': 37, '0000-0003-2108-3899': 11, '0000-0002-5785-6489': 6, '0000-0001-9640-1172': 4, '0000-0003-1749-023X': 1, '0000-0001-6611-0817': 1, '0000-0002-8814-015X': 1})\n",
      "['0000-0002-4610-998X', '0000-0003-2108-3899']\n",
      "Total sample size after apply threshold:  48\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(48, 169)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(48, 169)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.99        37\n",
      "          1       1.00      0.91      0.95        11\n",
      "\n",
      "avg / total       0.98      0.98      0.98        48\n",
      "\n",
      "[37  0  1 10]\n",
      "svc Accuracy:  0.9791666666666666\n",
      "svc F1:  0.9695238095238095\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.99        37\n",
      "          1       1.00      0.91      0.95        11\n",
      "\n",
      "avg / total       0.98      0.98      0.98        48\n",
      "\n",
      "[37  0  1 10]\n",
      "LR Accuracy:  0.9791666666666666\n",
      "LR F1:  0.9695238095238095\n",
      "For name:  l_martin\n",
      "total sample size before apply threshold:  253\n",
      "Counter({'0000-0001-8702-9946': 105, '0000-0003-4352-0914': 55, '0000-0002-5887-4937': 36, '0000-0003-1919-8646': 14, '0000-0001-7241-7110': 12, '0000-0002-1231-7932': 11, '0000-0002-0594-4516': 7, '0000-0002-5330-5700': 6, '0000-0003-1889-2513': 4, '0000-0002-4770-2849': 1, '0000-0001-9731-8071': 1, '0000-0001-9371-3402': 1})\n",
      "['0000-0001-7241-7110', '0000-0002-1231-7932', '0000-0002-5887-4937', '0000-0003-1919-8646', '0000-0001-8702-9946', '0000-0003-4352-0914']\n",
      "Total sample size after apply threshold:  233\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(233, 1160)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(233, 1160)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.58      0.74        12\n",
      "          1       1.00      0.73      0.84        11\n",
      "          2       1.00      0.89      0.94        36\n",
      "          3       0.92      0.79      0.85        14\n",
      "          4       0.81      0.99      0.89       105\n",
      "          5       1.00      0.84      0.91        55\n",
      "\n",
      "avg / total       0.91      0.89      0.89       233\n",
      "\n",
      "[  7   0   0   0   5   0   0   8   0   0   3   0   0   0  32   0   4   0\n",
      "   0   0   0  11   3   0   0   0   0   1 104   0   0   0   0   0   9  46]\n",
      "svc Accuracy:  0.8927038626609443\n",
      "svc F1:  0.8616454394888314\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.42      0.59        12\n",
      "          1       1.00      0.73      0.84        11\n",
      "          2       1.00      0.86      0.93        36\n",
      "          3       0.92      0.79      0.85        14\n",
      "          4       0.81      0.99      0.89       105\n",
      "          5       1.00      0.89      0.94        55\n",
      "\n",
      "avg / total       0.91      0.89      0.89       233\n",
      "\n",
      "[  5   0   0   0   7   0   0   8   0   0   3   0   0   0  31   0   5   0\n",
      "   0   0   0  11   3   0   0   0   0   1 104   0   0   0   0   0   6  49]\n",
      "LR Accuracy:  0.8927038626609443\n",
      "LR F1:  0.8394798487877305\n",
      "For name:  c_garcia\n",
      "total sample size before apply threshold:  106\n",
      "Counter({'0000-0003-2825-1701': 46, '0000-0002-7583-5585': 37, '0000-0002-4400-9141': 13, '0000-0001-5260-5093': 8, '0000-0001-5138-6191': 1, '0000-0001-7997-9837': 1})\n",
      "['0000-0002-7583-5585', '0000-0003-2825-1701', '0000-0002-4400-9141']\n",
      "Total sample size after apply threshold:  96\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(96, 250)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(96, 250)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.86      0.93        37\n",
      "          1       0.85      1.00      0.92        46\n",
      "          2       1.00      0.77      0.87        13\n",
      "\n",
      "avg / total       0.93      0.92      0.92        96\n",
      "\n",
      "[32  5  0  0 46  0  0  3 10]\n",
      "svc Accuracy:  0.9166666666666666\n",
      "svc F1:  0.9057004830917874\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.89      0.90        37\n",
      "          1       0.92      0.96      0.94        46\n",
      "          2       1.00      0.92      0.96        13\n",
      "\n",
      "avg / total       0.93      0.93      0.93        96\n",
      "\n",
      "[33  4  0  2 44  0  1  0 12]\n",
      "LR Accuracy:  0.9270833333333334\n",
      "LR F1:  0.9334266006023512\n",
      "For name:  g_huang\n",
      "total sample size before apply threshold:  160\n",
      "Counter({'0000-0001-7004-826X': 52, '0000-0003-2965-0341': 31, '0000-0002-0001-888X': 22, '0000-0002-8391-4013': 17, '0000-0003-2170-0084': 16, '0000-0002-2249-1248': 9, '0000-0003-2518-8145': 6, '0000-0003-1695-1153': 2, '0000-0003-1073-4967': 2, '0000-0002-7597-9386': 2, '0000-0001-7780-7409': 1})\n",
      "['0000-0003-2965-0341', '0000-0003-2170-0084', '0000-0001-7004-826X', '0000-0002-0001-888X', '0000-0002-8391-4013']\n",
      "Total sample size after apply threshold:  138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(138, 299)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(138, 299)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      1.00      0.91        31\n",
      "          1       0.92      0.69      0.79        16\n",
      "          2       0.93      0.98      0.95        52\n",
      "          3       1.00      0.82      0.90        22\n",
      "          4       1.00      0.94      0.97        17\n",
      "\n",
      "avg / total       0.93      0.92      0.92       138\n",
      "\n",
      "[31  0  0  0  0  3 11  2  0  0  1  0 51  0  0  1  1  2 18  0  1  0  0  0\n",
      " 16]\n",
      "svc Accuracy:  0.9202898550724637\n",
      "svc F1:  0.9040893978661984\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      1.00      0.94        31\n",
      "          1       0.92      0.69      0.79        16\n",
      "          2       0.89      0.98      0.94        52\n",
      "          3       1.00      0.82      0.90        22\n",
      "          4       1.00      0.94      0.97        17\n",
      "\n",
      "avg / total       0.93      0.92      0.92       138\n",
      "\n",
      "[31  0  0  0  0  2 11  3  0  0  0  1 51  0  0  1  0  3 18  0  1  0  0  0\n",
      " 16]\n",
      "LR Accuracy:  0.9202898550724637\n",
      "LR F1:  0.9061170022637913\n",
      "For name:  j_huber\n",
      "total sample size before apply threshold:  96\n",
      "Counter({'0000-0001-7243-8958': 59, '0000-0002-4790-7633': 21, '0000-0003-1046-2754': 14, '0000-0003-0073-0321': 2})\n",
      "['0000-0001-7243-8958', '0000-0003-1046-2754', '0000-0002-4790-7633']\n",
      "Total sample size after apply threshold:  94\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(94, 232)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(94, 232)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.92      0.92        59\n",
      "          1       0.58      0.79      0.67        14\n",
      "          2       0.94      0.71      0.81        21\n",
      "\n",
      "avg / total       0.87      0.85      0.85        94\n",
      "\n",
      "[54  4  1  3 11  0  2  4 15]\n",
      "svc Accuracy:  0.851063829787234\n",
      "svc F1:  0.7975772382552044\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      1.00      0.89        59\n",
      "          1       1.00      0.50      0.67        14\n",
      "          2       1.00      0.67      0.80        21\n",
      "\n",
      "avg / total       0.88      0.85      0.84        94\n",
      "\n",
      "[59  0  0  7  7  0  7  0 14]\n",
      "LR Accuracy:  0.851063829787234\n",
      "LR F1:  0.7868686868686868\n",
      "For name:  j_qin\n",
      "total sample size before apply threshold:  96\n",
      "Counter({'0000-0002-8559-616X': 48, '0000-0003-2448-8058': 38, '0000-0002-8186-5705': 4, '0000-0001-6271-068X': 3, '0000-0002-9166-3533': 3})\n",
      "['0000-0003-2448-8058', '0000-0002-8559-616X']\n",
      "Total sample size after apply threshold:  86\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(86, 87)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(86, 87)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.95      0.97        38\n",
      "          1       0.96      1.00      0.98        48\n",
      "\n",
      "avg / total       0.98      0.98      0.98        86\n",
      "\n",
      "[36  2  0 48]\n",
      "svc Accuracy:  0.9767441860465116\n",
      "svc F1:  0.9762824048538334\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.95      0.97        38\n",
      "          1       0.96      1.00      0.98        48\n",
      "\n",
      "avg / total       0.98      0.98      0.98        86\n",
      "\n",
      "[36  2  0 48]\n",
      "LR Accuracy:  0.9767441860465116\n",
      "LR F1:  0.9762824048538334\n",
      "For name:  t_ho\n",
      "total sample size before apply threshold:  83\n",
      "Counter({'0000-0002-3489-3594': 68, '0000-0003-0914-306X': 13, '0000-0003-1412-711X': 2})\n",
      "['0000-0002-3489-3594', '0000-0003-0914-306X']\n",
      "Total sample size after apply threshold:  81\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(81, 294)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(81, 294)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.99        68\n",
      "          1       1.00      0.85      0.92        13\n",
      "\n",
      "avg / total       0.98      0.98      0.97        81\n",
      "\n",
      "[68  0  2 11]\n",
      "svc Accuracy:  0.9753086419753086\n",
      "svc F1:  0.951086956521739\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97        68\n",
      "          1       1.00      0.69      0.82        13\n",
      "\n",
      "avg / total       0.95      0.95      0.95        81\n",
      "\n",
      "[68  0  4  9]\n",
      "LR Accuracy:  0.9506172839506173\n",
      "LR F1:  0.8948051948051947\n",
      "For name:  c_keller\n",
      "total sample size before apply threshold:  15\n",
      "Counter({'0000-0003-0529-3490': 10, '0000-0001-7400-9428': 2, '0000-0001-7915-7622': 2, '0000-0003-2505-7487': 1})\n",
      "['0000-0003-0529-3490']\n",
      "Total sample size after apply threshold:  10\n",
      "For name:  m_maia\n",
      "total sample size before apply threshold:  99\n",
      "Counter({'0000-0002-7034-8091': 88, '0000-0002-1716-6205': 6, '0000-0003-4383-3822': 3, '0000-0001-6395-1469': 1, '0000-0001-6688-2745': 1})\n",
      "['0000-0002-7034-8091']\n",
      "Total sample size after apply threshold:  88\n",
      "For name:  p_bates\n",
      "total sample size before apply threshold:  154\n",
      "Counter({'0000-0001-6861-5421': 87, '0000-0002-3918-5976': 49, '0000-0002-1291-3363': 17, '0000-0001-9192-9963': 1})\n",
      "['0000-0002-1291-3363', '0000-0002-3918-5976', '0000-0001-6861-5421']\n",
      "Total sample size after apply threshold:  153\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(153, 444)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(153, 444)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.82      0.90        17\n",
      "          1       1.00      0.80      0.89        49\n",
      "          2       0.87      1.00      0.93        87\n",
      "\n",
      "avg / total       0.93      0.92      0.91       153\n",
      "\n",
      "[14  0  3  0 39 10  0  0 87]\n",
      "svc Accuracy:  0.9150326797385621\n",
      "svc F1:  0.9066902420792363\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.76      0.87        17\n",
      "          1       1.00      0.76      0.86        49\n",
      "          2       0.84      1.00      0.92        87\n",
      "\n",
      "avg / total       0.91      0.90      0.89       153\n",
      "\n",
      "[13  0  4  0 37 12  0  0 87]\n",
      "LR Accuracy:  0.8954248366013072\n",
      "LR F1:  0.8809737522099823\n",
      "For name:  s_chow\n",
      "total sample size before apply threshold:  29\n",
      "Counter({'0000-0001-9471-4722': 21, '0000-0002-3600-0497': 6, '0000-0003-0544-6928': 1, '0000-0002-4392-3863': 1})\n",
      "['0000-0001-9471-4722']\n",
      "Total sample size after apply threshold:  21\n",
      "For name:  m_simon\n",
      "total sample size before apply threshold:  66\n",
      "Counter({'0000-0003-3655-6329': 44, '0000-0003-2349-7219': 12, '0000-0003-0611-495X': 5, '0000-0002-1509-2847': 3, '0000-0003-3080-3675': 1, '0000-0002-0065-6486': 1})\n",
      "['0000-0003-3655-6329', '0000-0003-2349-7219']\n",
      "Total sample size after apply threshold:  56\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(56, 195)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(56, 195)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.91      0.95        44\n",
      "          1       0.75      1.00      0.86        12\n",
      "\n",
      "avg / total       0.95      0.93      0.93        56\n",
      "\n",
      "[40  4  0 12]\n",
      "svc Accuracy:  0.9285714285714286\n",
      "svc F1:  0.9047619047619047\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.95      0.95        44\n",
      "          1       0.83      0.83      0.83        12\n",
      "\n",
      "avg / total       0.93      0.93      0.93        56\n",
      "\n",
      "[42  2  2 10]\n",
      "LR Accuracy:  0.9285714285714286\n",
      "LR F1:  0.893939393939394\n",
      "For name:  s_kar\n",
      "total sample size before apply threshold:  36\n",
      "Counter({'0000-0002-9411-2091': 20, '0000-0002-3788-372X': 7, '0000-0002-5032-4770': 4, '0000-0003-3702-6207': 3, '0000-0002-0498-812X': 2})\n",
      "['0000-0002-9411-2091']\n",
      "Total sample size after apply threshold:  20\n",
      "For name:  d_vlachos\n",
      "total sample size before apply threshold:  101\n",
      "Counter({'0000-0002-6795-8403': 80, '0000-0003-3740-2575': 19, '0000-0002-0430-2386': 1, '0000-0001-7225-2862': 1})\n",
      "['0000-0002-6795-8403', '0000-0003-3740-2575']\n",
      "Total sample size after apply threshold:  99\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(99, 133)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(99, 133)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99        80\n",
      "          1       1.00      0.95      0.97        19\n",
      "\n",
      "avg / total       0.99      0.99      0.99        99\n",
      "\n",
      "[80  0  1 18]\n",
      "svc Accuracy:  0.98989898989899\n",
      "svc F1:  0.9833808964243747\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99        80\n",
      "          1       1.00      0.89      0.94        19\n",
      "\n",
      "avg / total       0.98      0.98      0.98        99\n",
      "\n",
      "[80  0  2 17]\n",
      "LR Accuracy:  0.9797979797979798\n",
      "LR F1:  0.9660493827160493\n",
      "For name:  e_law\n",
      "total sample size before apply threshold:  12\n",
      "Counter({'0000-0002-4021-2150': 5, '0000-0001-5089-6341': 3, '0000-0003-4456-1259': 3, '0000-0001-5591-7316': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  m_ribeiro\n",
      "total sample size before apply threshold:  134\n",
      "Counter({'0000-0001-8906-0189': 25, '0000-0002-5964-5001': 17, '0000-0001-5693-7861': 16, '0000-0001-6422-3279': 13, '0000-0001-9365-6057': 12, '0000-0001-6357-8115': 10, '0000-0002-7434-4813': 7, '0000-0002-9350-6419': 7, '0000-0003-4529-7832': 5, '0000-0003-1167-5559': 5, '0000-0001-9538-821X': 5, '0000-0001-7350-8751': 4, '0000-0003-3373-3246': 3, '0000-0003-4684-1262': 2, '0000-0001-9575-008X': 1, '0000-0003-1704-1362': 1, '0000-0002-6071-6479': 1})\n",
      "['0000-0001-6422-3279', '0000-0001-6357-8115', '0000-0001-8906-0189', '0000-0002-5964-5001', '0000-0001-5693-7861', '0000-0001-9365-6057']\n",
      "Total sample size after apply threshold:  93\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(93, 383)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(93, 383)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        13\n",
      "          1       1.00      0.60      0.75        10\n",
      "          2       0.90      0.72      0.80        25\n",
      "          3       1.00      0.65      0.79        17\n",
      "          4       0.44      0.94      0.60        16\n",
      "          5       1.00      0.75      0.86        12\n",
      "\n",
      "avg / total       0.88      0.77      0.79        93\n",
      "\n",
      "[13  0  0  0  0  0  0  6  1  0  3  0  0  0 18  0  7  0  0  0  0 11  6  0\n",
      "  0  0  1  0 15  0  0  0  0  0  3  9]\n",
      "svc Accuracy:  0.7741935483870968\n",
      "svc F1:  0.7988095238095237\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        13\n",
      "          1       1.00      0.80      0.89        10\n",
      "          2       0.78      0.84      0.81        25\n",
      "          3       1.00      0.82      0.90        17\n",
      "          4       0.59      0.81      0.68        16\n",
      "          5       1.00      0.75      0.86        12\n",
      "\n",
      "avg / total       0.87      0.84      0.85        93\n",
      "\n",
      "[13  0  0  0  0  0  0  8  2  0  0  0  0  0 21  0  4  0  0  0  1 14  2  0\n",
      "  0  0  3  0 13  0  0  0  0  0  3  9]\n",
      "LR Accuracy:  0.8387096774193549\n",
      "LR F1:  0.8568600644152427\n",
      "For name:  r_king\n",
      "total sample size before apply threshold:  69\n",
      "Counter({'0000-0001-7208-4387': 39, '0000-0002-3550-4255': 19, '0000-0003-1312-5593': 8, '0000-0001-7495-6599': 2, '0000-0002-7926-3764': 1})\n",
      "['0000-0001-7208-4387', '0000-0002-3550-4255']\n",
      "Total sample size after apply threshold:  58\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(58, 151)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(58, 151)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      1.00      0.89        39\n",
      "          1       1.00      0.47      0.64        19\n",
      "\n",
      "avg / total       0.86      0.83      0.81        58\n",
      "\n",
      "[39  0 10  9]\n",
      "svc Accuracy:  0.8275862068965517\n",
      "svc F1:  0.7646103896103896\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      1.00      0.88        39\n",
      "          1       1.00      0.42      0.59        19\n",
      "\n",
      "avg / total       0.85      0.81      0.78        58\n",
      "\n",
      "[39  0 11  8]\n",
      "LR Accuracy:  0.8103448275862069\n",
      "LR F1:  0.7344985434873075\n",
      "For name:  o_nielsen\n",
      "total sample size before apply threshold:  212\n",
      "Counter({'0000-0003-4612-8635': 142, '0000-0003-3535-7862': 54, '0000-0002-0088-3937': 15, '0000-0001-8839-8671': 1})\n",
      "['0000-0003-4612-8635', '0000-0002-0088-3937', '0000-0003-3535-7862']\n",
      "Total sample size after apply threshold:  211\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(211, 308)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(211, 308)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.99      0.97       142\n",
      "          1       0.87      0.87      0.87        15\n",
      "          2       1.00      0.93      0.96        54\n",
      "\n",
      "avg / total       0.96      0.96      0.96       211\n",
      "\n",
      "[140   2   0   2  13   0   4   0  50]\n",
      "svc Accuracy:  0.9620853080568721\n",
      "svc F1:  0.9334757834757834\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97       142\n",
      "          1       1.00      0.73      0.85        15\n",
      "          2       1.00      0.91      0.95        54\n",
      "\n",
      "avg / total       0.96      0.96      0.96       211\n",
      "\n",
      "[142   0   0   4  11   0   5   0  49]\n",
      "LR Accuracy:  0.957345971563981\n",
      "LR F1:  0.9222978110946566\n",
      "For name:  j_moreno\n",
      "total sample size before apply threshold:  138\n",
      "Counter({'0000-0003-0087-4659': 44, '0000-0002-8887-6087': 30, '0000-0002-7646-9345': 22, '0000-0001-9561-7764': 21, '0000-0002-3729-9523': 9, '0000-0002-0555-9888': 5, '0000-0002-3684-3726': 4, '0000-0001-6499-1120': 3})\n",
      "['0000-0002-8887-6087', '0000-0001-9561-7764', '0000-0002-7646-9345', '0000-0003-0087-4659']\n",
      "Total sample size after apply threshold:  117\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(117, 398)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(117, 398)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.87      0.84        30\n",
      "          1       1.00      1.00      1.00        21\n",
      "          2       1.00      1.00      1.00        22\n",
      "          3       0.90      0.86      0.88        44\n",
      "\n",
      "avg / total       0.92      0.91      0.91       117\n",
      "\n",
      "[26  0  0  4  0 21  0  0  0  0 22  0  6  0  0 38]\n",
      "svc Accuracy:  0.9145299145299145\n",
      "svc F1:  0.9306076519129782\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.90      0.92        30\n",
      "          1       1.00      1.00      1.00        21\n",
      "          2       1.00      1.00      1.00        22\n",
      "          3       0.93      0.95      0.94        44\n",
      "\n",
      "avg / total       0.96      0.96      0.96       117\n",
      "\n",
      "[27  0  0  3  0 21  0  0  0  0 22  0  2  0  0 42]\n",
      "LR Accuracy:  0.9572649572649573\n",
      "LR F1:  0.9647686155018093\n",
      "For name:  f_yu\n",
      "total sample size before apply threshold:  78\n",
      "Counter({'0000-0001-9306-1731': 30, '0000-0003-0268-199X': 23, '0000-0002-5221-281X': 7, '0000-0001-5808-9376': 6, '0000-0002-8140-8344': 5, '0000-0003-3859-4839': 5, '0000-0002-0358-2793': 2})\n",
      "['0000-0001-9306-1731', '0000-0003-0268-199X']\n",
      "Total sample size after apply threshold:  53\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(53, 156)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(53, 156)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.93      0.92        30\n",
      "          1       0.91      0.87      0.89        23\n",
      "\n",
      "avg / total       0.91      0.91      0.91        53\n",
      "\n",
      "[28  2  3 20]\n",
      "svc Accuracy:  0.9056603773584906\n",
      "svc F1:  0.9034608378870674\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.97      0.95        30\n",
      "          1       0.95      0.91      0.93        23\n",
      "\n",
      "avg / total       0.94      0.94      0.94        53\n",
      "\n",
      "[29  1  2 21]\n",
      "LR Accuracy:  0.9433962264150944\n",
      "LR F1:  0.9420765027322404\n",
      "For name:  f_esposito\n",
      "total sample size before apply threshold:  342\n",
      "Counter({'0000-0002-5099-9786': 92, '0000-0003-1051-5924': 91, '0000-0001-9340-6875': 53, '0000-0002-4420-2611': 44, '0000-0003-2550-0805': 26, '0000-0001-9725-7977': 25, '0000-0001-7781-2558': 7, '0000-0003-0586-5866': 2, '0000-0001-9962-1648': 1, '0000-0002-1075-3239': 1})\n",
      "['0000-0003-2550-0805', '0000-0001-9340-6875', '0000-0002-4420-2611', '0000-0002-5099-9786', '0000-0001-9725-7977', '0000-0003-1051-5924']\n",
      "Total sample size after apply threshold:  331\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(331, 1340)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(331, 1340)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.92      0.94        26\n",
      "          1       1.00      0.96      0.98        53\n",
      "          2       0.91      0.98      0.95        44\n",
      "          3       0.99      0.98      0.98        92\n",
      "          4       1.00      0.96      0.98        25\n",
      "          5       0.97      0.99      0.98        91\n",
      "\n",
      "avg / total       0.97      0.97      0.97       331\n",
      "\n",
      "[24  0  1  1  0  0  1 51  1  0  0  0  0  0 43  0  0  1  0  0  1 90  0  1\n",
      "  0  0  0  0 24  1  0  0  1  0  0 90]\n",
      "svc Accuracy:  0.972809667673716\n",
      "svc F1:  0.968076651681562\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.92      0.96        26\n",
      "          1       1.00      0.96      0.98        53\n",
      "          2       0.93      0.98      0.96        44\n",
      "          3       0.98      0.98      0.98        92\n",
      "          4       1.00      0.96      0.98        25\n",
      "          5       0.96      0.99      0.97        91\n",
      "\n",
      "avg / total       0.97      0.97      0.97       331\n",
      "\n",
      "[24  0  1  0  0  1  0 51  1  0  0  1  0  0 43  1  0  0  0  0  1 90  0  1\n",
      "  0  0  0  0 24  1  0  0  0  1  0 90]\n",
      "LR Accuracy:  0.972809667673716\n",
      "LR F1:  0.9711917442662784\n",
      "For name:  p_miranda\n",
      "total sample size before apply threshold:  69\n",
      "Counter({'0000-0002-6793-8111': 37, '0000-0002-2890-0268': 14, '0000-0002-6418-3614': 7, '0000-0003-4348-110X': 6, '0000-0001-6496-697X': 3, '0000-0002-4288-9456': 1, '0000-0002-3249-0193': 1})\n",
      "['0000-0002-6793-8111', '0000-0002-2890-0268']\n",
      "Total sample size after apply threshold:  51\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(51, 129)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(51, 129)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      1.00      0.95        37\n",
      "          1       1.00      0.71      0.83        14\n",
      "\n",
      "avg / total       0.93      0.92      0.92        51\n",
      "\n",
      "[37  0  4 10]\n",
      "svc Accuracy:  0.9215686274509803\n",
      "svc F1:  0.891025641025641\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      1.00      0.91        37\n",
      "          1       1.00      0.50      0.67        14\n",
      "\n",
      "avg / total       0.88      0.86      0.85        51\n",
      "\n",
      "[37  0  7  7]\n",
      "LR Accuracy:  0.8627450980392157\n",
      "LR F1:  0.7901234567901234\n",
      "For name:  s_yang\n",
      "total sample size before apply threshold:  611\n",
      "Counter({'0000-0002-6469-8415': 108, '0000-0003-1301-3030': 94, '0000-0002-8835-5302': 43, '0000-0001-6795-8879': 36, '0000-0003-1751-4975': 33, '0000-0002-8572-4977': 31, '0000-0002-9394-9148': 26, '0000-0002-9879-0164': 25, '0000-0001-7892-7648': 21, '0000-0002-1726-0576': 20, '0000-0001-5684-6388': 19, '0000-0002-6888-7993': 17, '0000-0001-9170-2566': 17, '0000-0003-1809-2938': 14, '0000-0001-9282-2041': 14, '0000-0003-3408-2019': 12, '0000-0002-8244-3002': 12, '0000-0001-9947-2822': 10, '0000-0002-4409-3160': 10, '0000-0002-0281-5858': 9, '0000-0002-2068-7618': 5, '0000-0001-6129-627X': 5, '0000-0002-8200-9898': 5, '0000-0001-8727-7528': 5, '0000-0001-7727-9669': 4, '0000-0002-8002-5800': 4, '0000-0001-7222-4917': 3, '0000-0002-5990-8529': 2, '0000-0003-0338-4268': 2, '0000-0002-6880-8861': 1, '0000-0001-7207-4082': 1, '0000-0001-7522-1463': 1, '0000-0003-3742-9989': 1, '0000-0002-3888-3211': 1})\n",
      "['0000-0002-6888-7993', '0000-0001-9947-2822', '0000-0003-1751-4975', '0000-0002-6469-8415', '0000-0003-3408-2019', '0000-0003-1809-2938', '0000-0003-1301-3030', '0000-0001-9282-2041', '0000-0002-8572-4977', '0000-0002-9394-9148', '0000-0002-4409-3160', '0000-0001-5684-6388', '0000-0001-7892-7648', '0000-0001-9170-2566', '0000-0002-8244-3002', '0000-0001-6795-8879', '0000-0002-9879-0164', '0000-0002-8835-5302', '0000-0002-1726-0576']\n",
      "Total sample size after apply threshold:  562\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(562, 632)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(562, 632)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.30      0.47      0.36        17\n",
      "          1       1.00      0.50      0.67        10\n",
      "          2       0.73      0.91      0.81        33\n",
      "          3       0.63      0.90      0.74       108\n",
      "          4       1.00      0.58      0.74        12\n",
      "          5       1.00      0.86      0.92        14\n",
      "          6       0.96      0.90      0.93        94\n",
      "          7       1.00      0.79      0.88        14\n",
      "          8       0.76      0.61      0.68        31\n",
      "          9       1.00      0.73      0.84        26\n",
      "         10       0.33      0.30      0.32        10\n",
      "         11       1.00      1.00      1.00        19\n",
      "         12       0.73      0.52      0.61        21\n",
      "         13       0.85      0.65      0.73        17\n",
      "         14       0.60      0.50      0.55        12\n",
      "         15       0.83      0.69      0.76        36\n",
      "         16       0.60      0.60      0.60        25\n",
      "         17       1.00      0.81      0.90        43\n",
      "         18       0.94      0.80      0.86        20\n",
      "\n",
      "avg / total       0.81      0.77      0.78       562\n",
      "\n",
      "[ 8  0  0  7  0  0  1  0  0  0  0  0  0  1  0  0  0  0  0  0  5  0  4  0\n",
      "  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0 30  1  0  0  0  0  1  0\n",
      "  0  0  1  0  0  0  0  0  0  1  0  0 97  0  0  2  0  1  0  0  0  0  0  2\n",
      "  3  1  0  1  3  0  1  1  7  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1\n",
      "  0  0  1  0 12  0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  5  0  0\n",
      " 85  0  0  0  0  0  0  0  0  1  0  0  0  2  0  0  1  0  0  0 11  0  0  0\n",
      "  0  0  0  0  0  0  0  0  1  0  1  5  0  0  0  0 19  0  0  0  0  0  0  1\n",
      "  4  0  0  1  0  2  4  0  0  0  0  0 19  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  4  0  0  0  0  0  0  3  0  3  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0 19  0  0  0  0  0  0  0  0  0  5  1  0  0  0  0  0  0  4  0\n",
      " 11  0  0  0  0  0  0  2  0  0  1  0  0  0  0  0  0  0  0  0 11  0  0  3\n",
      "  0  0  0  0  0  5  0  0  0  0  0  0  0  0  0  0  6  0  1  0  0  2  0  0\n",
      "  6  0  0  1  0  1  0  0  0  0  0  1 25  0  0  0  1  0  2  3  0  0  0  0\n",
      "  3  0  0  0  0  1  0  0 15  0  0  1  0  0  5  0  0  0  0  0  0  0  0  0\n",
      "  0  1  0  1 35  0  1  0  0  2  0  0  0  0  0  0  1  0  0  0  0  0  0  0\n",
      " 16]\n",
      "svc Accuracy:  0.7722419928825622\n",
      "svc F1:  0.7316773944063413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.24      0.38        17\n",
      "          1       1.00      0.30      0.46        10\n",
      "          2       0.76      0.88      0.82        33\n",
      "          3       0.59      0.94      0.73       108\n",
      "          4       1.00      0.50      0.67        12\n",
      "          5       1.00      0.86      0.92        14\n",
      "          6       0.92      0.94      0.93        94\n",
      "          7       1.00      0.79      0.88        14\n",
      "          8       0.81      0.71      0.76        31\n",
      "          9       1.00      0.81      0.89        26\n",
      "         10       0.67      0.20      0.31        10\n",
      "         11       1.00      1.00      1.00        19\n",
      "         12       0.62      0.62      0.62        21\n",
      "         13       0.86      0.71      0.77        17\n",
      "         14       0.70      0.58      0.64        12\n",
      "         15       0.87      0.75      0.81        36\n",
      "         16       0.80      0.64      0.71        25\n",
      "         17       0.94      0.79      0.86        43\n",
      "         18       1.00      0.85      0.92        20\n",
      "\n",
      "avg / total       0.83      0.79      0.79       562\n",
      "\n",
      "[  4   0   0  10   0   0   1   0   0   0   0   0   0   1   0   1   0   0\n",
      "   0   0   3   0   7   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0  29   2   0   0   0   0   1   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   0 102   0   0   3   0   0   0   0   0   0   0   2\n",
      "   1   0   0   0   0   0   1   4   6   0   0   0   0   0   0   0   0   0\n",
      "   0   0   1   0   0   0   0   0   2   0  12   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   5   0   0  88   0   0   0   0   0\n",
      "   0   0   0   1   0   0   0   0   0   0   1   0   0   0  11   0   0   0\n",
      "   0   0   0   0   0   0   2   0   0   0   1   6   0   0   0   0  22   0\n",
      "   0   0   0   0   0   1   1   0   0   0   0   0   5   0   0   0   0   0\n",
      "  21   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0\n",
      "   0   0   2   0   7   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0  19   0   0   0   0   0   0   0   0   0   5   2   0   0\n",
      "   0   0   0   0   1   0  13   0   0   0   0   0   0   0   0   0   3   0\n",
      "   0   0   0   0   0   0   0   0  12   0   0   2   0   0   0   0   0   5\n",
      "   0   0   0   0   0   0   0   0   0   0   7   0   0   0   0   0   0   0\n",
      "   7   0   0   1   0   1   0   0   0   0   0   0  27   0   0   0   0   0\n",
      "   2   3   0   0   0   0   3   0   0   0   0   1   0   0  16   0   0   0\n",
      "   0   0   5   0   0   3   0   0   0   0   0   0   0   1   0   0  34   0\n",
      "   0   0   0   3   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  17]\n",
      "LR Accuracy:  0.791814946619217\n",
      "LR F1:  0.7404065670117226\n",
      "For name:  d_huang\n",
      "total sample size before apply threshold:  38\n",
      "Counter({'0000-0002-6192-259X': 20, '0000-0003-2048-4500': 15, '0000-0002-1497-1284': 2, '0000-0002-0658-8752': 1})\n",
      "['0000-0002-6192-259X', '0000-0003-2048-4500']\n",
      "Total sample size after apply threshold:  35\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(35, 94)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(35, 94)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        20\n",
      "          1       1.00      1.00      1.00        15\n",
      "\n",
      "avg / total       1.00      1.00      1.00        35\n",
      "\n",
      "[20  0  0 15]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        20\n",
      "          1       1.00      1.00      1.00        15\n",
      "\n",
      "avg / total       1.00      1.00      1.00        35\n",
      "\n",
      "[20  0  0 15]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  h_kuo\n",
      "total sample size before apply threshold:  144\n",
      "Counter({'0000-0002-3295-2984': 98, '0000-0003-1336-1203': 26, '0000-0001-6752-2231': 16, '0000-0002-0349-6983': 2, '0000-0001-9102-5104': 1, '0000-0002-0573-2636': 1})\n",
      "['0000-0003-1336-1203', '0000-0001-6752-2231', '0000-0002-3295-2984']\n",
      "Total sample size after apply threshold:  140\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(140, 154)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(140, 154)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.88      0.92        26\n",
      "          1       0.88      0.88      0.88        16\n",
      "          2       0.95      0.97      0.96        98\n",
      "\n",
      "avg / total       0.94      0.94      0.94       140\n",
      "\n",
      "[23  0  3  0 14  2  1  2 95]\n",
      "svc Accuracy:  0.9428571428571428\n",
      "svc F1:  0.9181986531986531\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.85      0.88        26\n",
      "          1       1.00      0.81      0.90        16\n",
      "          2       0.93      0.98      0.96        98\n",
      "\n",
      "avg / total       0.94      0.94      0.93       140\n",
      "\n",
      "[22  0  4  0 13  3  2  0 96]\n",
      "LR Accuracy:  0.9357142857142857\n",
      "LR F1:  0.910591868244982\n",
      "For name:  a_santoro\n",
      "total sample size before apply threshold:  189\n",
      "Counter({'0000-0002-0798-6816': 83, '0000-0003-1709-9492': 58, '0000-0002-5086-1453': 21, '0000-0003-2503-8219': 10, '0000-0002-1014-197X': 9, '0000-0002-6193-2050': 8})\n",
      "['0000-0003-1709-9492', '0000-0002-5086-1453', '0000-0003-2503-8219', '0000-0002-0798-6816']\n",
      "Total sample size after apply threshold:  172\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(172, 717)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(172, 717)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.84      0.92        58\n",
      "          1       1.00      1.00      1.00        21\n",
      "          2       1.00      0.70      0.82        10\n",
      "          3       0.87      1.00      0.93        83\n",
      "\n",
      "avg / total       0.94      0.93      0.93       172\n",
      "\n",
      "[49  0  0  9  0 21  0  0  0  0  7  3  0  0  0 83]\n",
      "svc Accuracy:  0.9302325581395349\n",
      "svc F1:  0.9180003829737293\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.91      0.95        58\n",
      "          1       1.00      1.00      1.00        21\n",
      "          2       1.00      0.40      0.57        10\n",
      "          3       0.88      1.00      0.94        83\n",
      "\n",
      "avg / total       0.94      0.94      0.93       172\n",
      "\n",
      "[53  0  0  5  0 21  0  0  0  0  4  6  0  0  0 83]\n",
      "LR Accuracy:  0.936046511627907\n",
      "LR F1:  0.8660591584320398\n",
      "For name:  q_lu\n",
      "total sample size before apply threshold:  35\n",
      "Counter({'0000-0002-2804-0827': 22, '0000-0002-4261-5121': 5, '0000-0002-4514-0969': 4, '0000-0002-7952-2332': 3, '0000-0001-6234-4384': 1})\n",
      "['0000-0002-2804-0827']\n",
      "Total sample size after apply threshold:  22\n",
      "For name:  s_kumar\n",
      "total sample size before apply threshold:  419\n",
      "Counter({'0000-0003-4326-5941': 130, '0000-0002-4003-4411': 42, '0000-0003-2405-3791': 25, '0000-0003-0658-8709': 21, '0000-0001-8373-105X': 19, '0000-0001-5902-6641': 18, '0000-0001-5940-9490': 14, '0000-0003-0562-2645': 13, '0000-0003-2130-7493': 13, '0000-0003-0423-2880': 9, '0000-0001-9905-4831': 9, '0000-0002-5082-8602': 8, '0000-0002-1457-5804': 8, '0000-0001-9261-2263': 8, '0000-0002-0605-6908': 7, '0000-0001-5396-5509': 7, '0000-0002-6701-6889': 6, '0000-0002-5474-8095': 4, '0000-0002-0384-1580': 4, '0000-0003-2920-8924': 4, '0000-0003-3514-8999': 4, '0000-0001-9673-5842': 4, '0000-0001-6511-5309': 4, '0000-0002-2358-2344': 4, '0000-0002-4405-4444': 3, '0000-0003-2685-9940': 3, '0000-0002-4044-7005': 3, '0000-0001-6594-9266': 3, '0000-0001-5223-1466': 2, '0000-0001-8407-3562': 2, '0000-0001-9132-1202': 2, '0000-0002-9054-2123': 2, '0000-0002-6772-7250': 2, '0000-0001-6680-718X': 1, '0000-0001-9525-4836': 1, '0000-0001-7070-057X': 1, '0000-0002-6425-278X': 1, '0000-0003-0832-4811': 1, '0000-0002-5838-3994': 1, '0000-0002-2284-4576': 1, '0000-0001-6221-4512': 1, '0000-0001-9411-3863': 1, '0000-0002-0951-4214': 1, '0000-0003-3389-5368': 1, '0000-0002-4190-620X': 1})\n",
      "['0000-0001-8373-105X', '0000-0001-5940-9490', '0000-0003-4326-5941', '0000-0003-2405-3791', '0000-0002-4003-4411', '0000-0003-0658-8709', '0000-0003-0562-2645', '0000-0001-5902-6641', '0000-0003-2130-7493']\n",
      "Total sample size after apply threshold:  295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(295, 666)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(295, 666)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.79      0.86        19\n",
      "          1       0.92      0.79      0.85        14\n",
      "          2       0.95      0.93      0.94       130\n",
      "          3       0.42      0.84      0.56        25\n",
      "          4       1.00      0.79      0.88        42\n",
      "          5       1.00      0.81      0.89        21\n",
      "          6       1.00      1.00      1.00        13\n",
      "          7       1.00      0.83      0.91        18\n",
      "          8       1.00      0.85      0.92        13\n",
      "\n",
      "avg / total       0.92      0.87      0.89       295\n",
      "\n",
      "[ 15   0   0   4   0   0   0   0   0   0  11   0   3   0   0   0   0   0\n",
      "   1   1 121   7   0   0   0   0   0   0   0   4  21   0   0   0   0   0\n",
      "   0   0   1   8  33   0   0   0   0   0   0   1   3   0  17   0   0   0\n",
      "   0   0   0   0   0   0  13   0   0   0   0   1   2   0   0   0  15   0\n",
      "   0   0   0   2   0   0   0   0  11]\n",
      "svc Accuracy:  0.8711864406779661\n",
      "svc F1:  0.8668639574759527\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.84      0.91        19\n",
      "          1       1.00      0.64      0.78        14\n",
      "          2       0.81      1.00      0.89       130\n",
      "          3       0.88      0.60      0.71        25\n",
      "          4       1.00      0.81      0.89        42\n",
      "          5       1.00      0.90      0.95        21\n",
      "          6       1.00      1.00      1.00        13\n",
      "          7       1.00      0.83      0.91        18\n",
      "          8       1.00      0.85      0.92        13\n",
      "\n",
      "avg / total       0.91      0.89      0.89       295\n",
      "\n",
      "[ 16   0   3   0   0   0   0   0   0   0   9   4   1   0   0   0   0   0\n",
      "   0   0 130   0   0   0   0   0   0   0   0  10  15   0   0   0   0   0\n",
      "   0   0   8   0  34   0   0   0   0   0   0   1   1   0  19   0   0   0\n",
      "   0   0   0   0   0   0  13   0   0   0   0   3   0   0   0   0  15   0\n",
      "   0   0   2   0   0   0   0   0  11]\n",
      "LR Accuracy:  0.888135593220339\n",
      "LR F1:  0.8861272591627165\n",
      "For name:  s_rocha\n",
      "total sample size before apply threshold:  139\n",
      "Counter({'0000-0002-2413-4981': 47, '0000-0003-4196-2217': 41, '0000-0002-0396-3019': 36, '0000-0002-9705-1511': 8, '0000-0002-4686-2410': 5, '0000-0002-1324-5707': 2})\n",
      "['0000-0002-2413-4981', '0000-0003-4196-2217', '0000-0002-0396-3019']\n",
      "Total sample size after apply threshold:  124\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(124, 236)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(124, 236)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      1.00      0.95        47\n",
      "          1       1.00      1.00      1.00        41\n",
      "          2       1.00      0.86      0.93        36\n",
      "\n",
      "avg / total       0.96      0.96      0.96       124\n",
      "\n",
      "[47  0  0  0 41  0  5  0 31]\n",
      "svc Accuracy:  0.9596774193548387\n",
      "svc F1:  0.9582893612744359\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      1.00      0.96        47\n",
      "          1       1.00      1.00      1.00        41\n",
      "          2       1.00      0.89      0.94        36\n",
      "\n",
      "avg / total       0.97      0.97      0.97       124\n",
      "\n",
      "[47  0  0  0 41  0  4  0 32]\n",
      "LR Accuracy:  0.967741935483871\n",
      "LR F1:  0.9667867146858743\n",
      "For name:  t_han\n",
      "total sample size before apply threshold:  53\n",
      "Counter({'0000-0002-9063-4052': 42, '0000-0002-3095-7714': 8, '0000-0003-3535-8582': 2, '0000-0003-1404-1578': 1})\n",
      "['0000-0002-9063-4052']\n",
      "Total sample size after apply threshold:  42\n",
      "For name:  m_sandberg\n",
      "total sample size before apply threshold:  59\n",
      "Counter({'0000-0002-4812-0103': 39, '0000-0001-5486-7556': 9, '0000-0002-8915-8730': 7, '0000-0003-0809-0656': 2, '0000-0001-9495-3571': 2})\n",
      "['0000-0002-4812-0103']\n",
      "Total sample size after apply threshold:  39\n",
      "For name:  j_marshall\n",
      "total sample size before apply threshold:  80\n",
      "Counter({'0000-0003-1361-2869': 28, '0000-0002-8344-2589': 16, '0000-0002-0494-2295': 11, '0000-0001-5491-2919': 10, '0000-0002-2784-1817': 8, '0000-0002-5829-9688': 5, '0000-0001-7617-4101': 1, '0000-0002-7864-803X': 1})\n",
      "['0000-0001-5491-2919', '0000-0002-0494-2295', '0000-0002-8344-2589', '0000-0003-1361-2869']\n",
      "Total sample size after apply threshold:  65\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(65, 249)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(65, 249)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.80      0.89        10\n",
      "          1       1.00      0.64      0.78        11\n",
      "          2       0.70      1.00      0.82        16\n",
      "          3       1.00      0.96      0.98        28\n",
      "\n",
      "avg / total       0.93      0.89      0.89        65\n",
      "\n",
      "[ 8  0  2  0  0  7  4  0  0  0 16  0  0  0  1 27]\n",
      "svc Accuracy:  0.8923076923076924\n",
      "svc F1:  0.8672494172494173\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.80      0.89        10\n",
      "          1       1.00      0.64      0.78        11\n",
      "          2       0.73      1.00      0.84        16\n",
      "          3       1.00      1.00      1.00        28\n",
      "\n",
      "avg / total       0.93      0.91      0.91        65\n",
      "\n",
      "[ 8  0  2  0  0  7  4  0  0  0 16  0  0  0  0 28]\n",
      "LR Accuracy:  0.9076923076923077\n",
      "LR F1:  0.8771929824561404\n",
      "For name:  f_bianchi\n",
      "total sample size before apply threshold:  131\n",
      "Counter({'0000-0002-3459-9301': 54, '0000-0001-7880-5624': 37, '0000-0002-2863-1598': 16, '0000-0003-2996-3604': 12, '0000-0001-5197-5279': 11, '0000-0002-7145-3846': 1})\n",
      "['0000-0001-7880-5624', '0000-0002-2863-1598', '0000-0003-2996-3604', '0000-0002-3459-9301', '0000-0001-5197-5279']\n",
      "Total sample size after apply threshold:  130\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(130, 669)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(130, 669)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.97      0.99        37\n",
      "          1       1.00      1.00      1.00        16\n",
      "          2       1.00      0.83      0.91        12\n",
      "          3       0.95      1.00      0.97        54\n",
      "          4       1.00      1.00      1.00        11\n",
      "\n",
      "avg / total       0.98      0.98      0.98       130\n",
      "\n",
      "[36  0  0  1  0  0 16  0  0  0  0  0 10  2  0  0  0  0 54  0  0  0  0  0\n",
      " 11]\n",
      "svc Accuracy:  0.9769230769230769\n",
      "svc F1:  0.9736730503853792\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        37\n",
      "          1       1.00      1.00      1.00        16\n",
      "          2       1.00      0.92      0.96        12\n",
      "          3       0.98      1.00      0.99        54\n",
      "          4       1.00      1.00      1.00        11\n",
      "\n",
      "avg / total       0.99      0.99      0.99       130\n",
      "\n",
      "[37  0  0  0  0  0 16  0  0  0  0  0 11  1  0  0  0  0 54  0  0  0  0  0\n",
      " 11]\n",
      "LR Accuracy:  0.9923076923076923\n",
      "LR F1:  0.9894694854407657\n",
      "For name:  c_liu\n",
      "total sample size before apply threshold:  681\n",
      "Counter({'0000-0003-3622-9707': 63, '0000-0001-7016-8990': 58, '0000-0003-2336-4731': 56, '0000-0001-8816-4832': 51, '0000-0002-0703-0742': 46, '0000-0002-6109-5707': 29, '0000-0002-2521-924X': 29, '0000-0001-7433-2081': 27, '0000-0002-5723-177X': 25, '0000-0001-8063-7906': 23, '0000-0001-5546-3852': 19, '0000-0001-9918-1638': 18, '0000-0001-7888-9725': 17, '0000-0002-5323-6733': 16, '0000-0003-3410-445X': 16, '0000-0003-1882-3892': 16, '0000-0002-3151-7956': 16, '0000-0001-7343-3884': 14, '0000-0002-6202-0993': 13, '0000-0001-7364-8412': 12, '0000-0002-8582-912X': 11, '0000-0002-4693-5667': 11, '0000-0001-8918-5627': 10, '0000-0002-6439-8754': 10, '0000-0002-6650-6245': 7, '0000-0003-1028-2454': 7, '0000-0001-7954-0736': 5, '0000-0002-9780-9062': 5, '0000-0002-6257-0389': 5, '0000-0002-2106-202X': 4, '0000-0002-2145-5034': 4, '0000-0002-5586-4776': 3, '0000-0002-9210-2754': 3, '0000-0003-2544-7215': 3, '0000-0001-6049-2615': 3, '0000-0003-1975-3988': 2, '0000-0003-2352-5734': 2, '0000-0003-3283-028X': 2, '0000-0002-4230-9159': 2, '0000-0002-8997-5941': 2, '0000-0001-5352-1326': 2, '0000-0001-6526-5024': 2, '0000-0002-1797-3920': 1, '0000-0001-5886-5785': 1, '0000-0001-9657-7717': 1, '0000-0001-7023-6578': 1, '0000-0001-5095-8039': 1, '0000-0002-4035-8686': 1, '0000-0002-7263-5289': 1, '0000-0003-1196-7447': 1, '0000-0002-2834-9461': 1, '0000-0002-8345-8847': 1, '0000-0002-9954-3755': 1, '0000-0002-5717-3370': 1})\n",
      "['0000-0001-7343-3884', '0000-0002-5723-177X', '0000-0001-7016-8990', '0000-0002-8582-912X', '0000-0002-5323-6733', '0000-0001-5546-3852', '0000-0002-6109-5707', '0000-0001-7888-9725', '0000-0002-0703-0742', '0000-0003-3622-9707', '0000-0002-6202-0993', '0000-0002-4693-5667', '0000-0001-7364-8412', '0000-0001-9918-1638', '0000-0001-8816-4832', '0000-0001-7433-2081', '0000-0001-8918-5627', '0000-0002-2521-924X', '0000-0001-8063-7906', '0000-0002-6439-8754', '0000-0003-2336-4731', '0000-0003-3410-445X', '0000-0003-1882-3892', '0000-0002-3151-7956']\n",
      "Total sample size after apply threshold:  606\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(606, 1534)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(606, 1534)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.93      0.93        14\n",
      "          1       0.73      0.64      0.68        25\n",
      "          2       0.77      0.81      0.79        58\n",
      "          3       0.25      0.27      0.26        11\n",
      "          4       0.34      0.69      0.46        16\n",
      "          5       0.60      0.63      0.62        19\n",
      "          6       0.71      0.69      0.70        29\n",
      "          7       0.70      0.41      0.52        17\n",
      "          8       0.68      0.57      0.62        46\n",
      "          9       0.75      0.87      0.81        63\n",
      "         10       0.17      0.08      0.11        13\n",
      "         11       1.00      0.73      0.84        11\n",
      "         12       1.00      0.75      0.86        12\n",
      "         13       0.39      0.50      0.44        18\n",
      "         14       0.63      0.84      0.72        51\n",
      "         15       0.63      0.63      0.63        27\n",
      "         16       0.47      0.70      0.56        10\n",
      "         17       1.00      0.93      0.96        29\n",
      "         18       0.88      0.61      0.72        23\n",
      "         19       0.25      0.10      0.14        10\n",
      "         20       0.81      0.77      0.79        56\n",
      "         21       0.77      0.62      0.69        16\n",
      "         22       0.55      0.38      0.44        16\n",
      "         23       0.88      0.88      0.88        16\n",
      "\n",
      "avg / total       0.70      0.69      0.69       606\n",
      "\n",
      "[13  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0\n",
      "  0 16  1  0  2  0  0  0  0  0  0  0  0  1  3  0  1  0  0  0  1  0  0  0\n",
      "  0  1 47  0  1  0  0  0  0  0  0  0  0  0  1  0  2  0  0  1  3  0  2  0\n",
      "  0  0  0  3  1  0  0  0  4  0  0  0  0  1  1  0  0  0  1  0  0  0  0  0\n",
      "  0  1  0  0 11  0  0  0  2  0  0  0  0  0  1  0  0  0  0  0  0  1  0  0\n",
      "  0  0  0  1  2 12  0  0  0  1  0  0  0  1  0  1  0  0  0  0  1  0  0  0\n",
      "  0  1  1  0  0  0 20  0  1  1  1  0  0  1  0  0  1  0  0  1  0  0  1  0\n",
      "  0  1  1  0  3  0  1  7  0  0  0  0  0  0  2  2  0  0  0  0  0  0  0  0\n",
      "  0  0  1  1  3  0  1  0 26  3  0  0  0  3  4  0  1  0  0  0  2  0  0  1\n",
      "  0  0  0  0  1  1  0  0  0 55  4  0  0  0  0  2  0  0  0  0  0  0  0  0\n",
      "  0  0  1  1  0  0  1  0  0  9  1  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  1  0  0  0  1  0  0  8  0  0  0  0  1  0  0  0  0  0  0  0\n",
      "  0  0  0  0  1  0  0  0  0  0  0  0  9  1  1  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  2  1  1  0  0  0  0  0  0  0  9  4  0  0  0  0  0  0  0  0  1\n",
      "  0  1  1  0  1  0  0  0  2  0  0  0  0  3 43  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  1  3  0  1  0  0  0  0  0  0  4 17  0  0  0  0  0  1  0  0\n",
      "  0  0  1  0  1  0  0  0  0  0  0  0  0  0  0  0  7  0  0  0  1  0  0  0\n",
      "  0  0  0  1  0  1  0  0  0  0  0  0  0  0  0  0  0 27  0  0  0  0  0  0\n",
      "  0  0  0  1  0  1  0  1  1  0  0  0  0  0  2  2  0  0 14  0  1  0  0  0\n",
      "  1  0  1  0  0  0  1  0  1  2  0  0  0  1  0  1  0  0  0  1  1  0  0  0\n",
      "  0  0  2  0  2  1  1  0  0  2  0  0  0  0  0  0  1  0  0  1 43  1  2  0\n",
      "  0  1  0  2  0  0  0  0  0  0  0  0  0  1  0  1  0  0  1  0  0 10  0  0\n",
      "  0  0  4  0  0  0  3  1  0  0  0  0  0  0  1  0  1  0  0  0  0  0  6  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  1  0  0  0  0  0  0  0 14]\n",
      "svc Accuracy:  0.6914191419141914\n",
      "svc F1:  0.6317127673751285\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.93      0.90        14\n",
      "          1       0.84      0.84      0.84        25\n",
      "          2       0.93      0.88      0.90        58\n",
      "          3       0.45      0.45      0.45        11\n",
      "          4       0.77      0.62      0.69        16\n",
      "          5       0.67      0.74      0.70        19\n",
      "          6       0.60      0.90      0.72        29\n",
      "          7       0.58      0.65      0.61        17\n",
      "          8       1.00      0.76      0.86        46\n",
      "          9       0.77      0.87      0.82        63\n",
      "         10       0.00      0.00      0.00        13\n",
      "         11       1.00      1.00      1.00        11\n",
      "         12       1.00      0.83      0.91        12\n",
      "         13       0.37      0.39      0.38        18\n",
      "         14       0.63      0.86      0.73        51\n",
      "         15       0.80      0.74      0.77        27\n",
      "         16       1.00      0.40      0.57        10\n",
      "         17       1.00      0.97      0.98        29\n",
      "         18       0.79      0.65      0.71        23\n",
      "         19       1.00      0.20      0.33        10\n",
      "         20       0.82      0.89      0.85        56\n",
      "         21       0.53      0.56      0.55        16\n",
      "         22       0.78      0.44      0.56        16\n",
      "         23       0.79      0.94      0.86        16\n",
      "\n",
      "avg / total       0.77      0.76      0.76       606\n",
      "\n",
      "[13  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0 21  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0  0  0  1  0  0  0\n",
      "  0  1 51  0  0  0  1  0  0  0  0  0  0  0  1  0  0  0  0  0  1  1  2  0\n",
      "  0  1  0  5  0  0  1  0  0  0  0  0  0  1  1  0  0  0  2  0  0  0  0  0\n",
      "  0  1  0  0 10  0  0  0  0  0  0  0  0  1  2  0  0  0  0  0  2  0  0  0\n",
      "  0  0  0  0  2 14  0  0  0  0  0  0  0  0  1  1  0  0  0  0  0  1  0  0\n",
      "  0  0  1  0  0  0 26  1  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  1  0  0 11  0  0  0  0  0  0  2  2  0  0  0  0  0  1  0  0\n",
      "  0  0  0  0  0  0  1  0 35  1  0  0  0  2  4  0  0  0  0  0  0  1  0  2\n",
      "  0  0  0  0  0  2  3  0  0 55  3  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  1  0  0 10  0  0  0  0  0  0  0  0  1  0  1  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0 11  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  1  0  0  0  0  0  0  0  0 10  0  0  0  0  0  1  0  0  0  0  0\n",
      "  0  0  0  2  0  1  0  1  0  0  0  0  0  7  5  0  0  0  0  0  0  1  0  1\n",
      "  0  0  1  2  0  0  0  0  0  0  0  0  0  3 44  0  0  0  0  0  1  0  0  0\n",
      "  1  0  0  0  0  1  0  1  0  0  0  0  0  1  2 20  0  0  0  0  1  0  0  0\n",
      "  0  0  0  0  0  0  2  0  0  0  0  0  0  0  1  0  4  0  0  0  2  1  0  0\n",
      "  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0 28  0  0  0  0  0  0\n",
      "  0  0  0  0  0  1  0  1  0  0  0  0  0  0  2  1  0  0 15  0  1  1  0  1\n",
      "  1  0  0  0  0  0  2  1  0  2  0  0  0  1  0  0  0  0  0  2  1  0  0  0\n",
      "  0  0  0  0  0  1  2  0  0  1  1  0  0  0  0  0  0  0  0  0 50  1  0  0\n",
      "  0  1  0  1  0  0  1  1  0  0  0  0  0  2  0  1  0  0  0  0  0  9  0  0\n",
      "  0  0  2  0  0  0  3  1  0  1  0  0  0  0  2  0  0  0  0  0  0  0  7  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0 15]\n",
      "LR Accuracy:  0.764026402640264\n",
      "LR F1:  0.696025350233706\n",
      "For name:  d_sanders\n",
      "total sample size before apply threshold:  16\n",
      "Counter({'0000-0001-5593-1564': 8, '0000-0003-2383-8693': 6, '0000-0002-6523-783X': 1, '0000-0001-6265-6249': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  r_brito\n",
      "total sample size before apply threshold:  51\n",
      "Counter({'0000-0001-9128-2557': 28, '0000-0001-5214-9681': 11, '0000-0002-8488-6472': 8, '0000-0002-7807-3053': 2, '0000-0003-3633-6422': 2})\n",
      "['0000-0001-5214-9681', '0000-0001-9128-2557']\n",
      "Total sample size after apply threshold:  39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(39, 163)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(39, 163)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.73      0.84        11\n",
      "          1       0.90      1.00      0.95        28\n",
      "\n",
      "avg / total       0.93      0.92      0.92        39\n",
      "\n",
      "[ 8  3  0 28]\n",
      "svc Accuracy:  0.9230769230769231\n",
      "svc F1:  0.8956289027653881\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.82      0.90        11\n",
      "          1       0.93      1.00      0.97        28\n",
      "\n",
      "avg / total       0.95      0.95      0.95        39\n",
      "\n",
      "[ 9  2  0 28]\n",
      "LR Accuracy:  0.9487179487179487\n",
      "LR F1:  0.9327586206896552\n",
      "For name:  w_chang\n",
      "total sample size before apply threshold:  91\n",
      "Counter({'0000-0003-0116-1386': 50, '0000-0003-2283-1377': 23, '0000-0003-4880-6006': 14, '0000-0002-6155-8644': 2, '0000-0002-5268-290X': 1, '0000-0002-7437-4211': 1})\n",
      "['0000-0003-2283-1377', '0000-0003-4880-6006', '0000-0003-0116-1386']\n",
      "Total sample size after apply threshold:  87\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(87, 148)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(87, 148)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.87      0.80        23\n",
      "          1       0.88      0.50      0.64        14\n",
      "          2       0.94      0.98      0.96        50\n",
      "\n",
      "avg / total       0.88      0.87      0.87        87\n",
      "\n",
      "[20  1  2  6  7  1  1  0 49]\n",
      "svc Accuracy:  0.8735632183908046\n",
      "svc F1:  0.7990493166963755\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.78      0.82        23\n",
      "          1       0.82      0.64      0.72        14\n",
      "          2       0.89      0.98      0.93        50\n",
      "\n",
      "avg / total       0.87      0.87      0.87        87\n",
      "\n",
      "[18  2  3  2  9  3  1  0 49]\n",
      "LR Accuracy:  0.8735632183908046\n",
      "LR F1:  0.8238383838383839\n",
      "For name:  a_murray\n",
      "total sample size before apply threshold:  76\n",
      "Counter({'0000-0002-4094-962X': 32, '0000-0002-0929-9315': 14, '0000-0001-7143-287X': 9, '0000-0001-6762-588X': 7, '0000-0001-5014-1096': 7, '0000-0001-7047-8139': 4, '0000-0001-9648-2902': 3})\n",
      "['0000-0002-4094-962X', '0000-0002-0929-9315']\n",
      "Total sample size after apply threshold:  46\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(46, 146)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(46, 146)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      1.00      0.86        32\n",
      "          1       1.00      0.29      0.44        14\n",
      "\n",
      "avg / total       0.83      0.78      0.74        46\n",
      "\n",
      "[32  0 10  4]\n",
      "svc Accuracy:  0.782608695652174\n",
      "svc F1:  0.6546546546546547\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      1.00      0.88        32\n",
      "          1       1.00      0.36      0.53        14\n",
      "\n",
      "avg / total       0.85      0.80      0.77        46\n",
      "\n",
      "[32  0  9  5]\n",
      "LR Accuracy:  0.8043478260869565\n",
      "LR F1:  0.7015140591204038\n",
      "For name:  b_cao\n",
      "total sample size before apply threshold:  58\n",
      "Counter({'0000-0002-9462-496X': 39, '0000-0003-3588-972X': 14, '0000-0003-3401-6900': 4, '0000-0003-4443-2326': 1})\n",
      "['0000-0002-9462-496X', '0000-0003-3588-972X']\n",
      "Total sample size after apply threshold:  53\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(53, 100)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(53, 100)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.92      0.94        39\n",
      "          1       0.80      0.86      0.83        14\n",
      "\n",
      "avg / total       0.91      0.91      0.91        53\n",
      "\n",
      "[36  3  2 12]\n",
      "svc Accuracy:  0.9056603773584906\n",
      "svc F1:  0.8813255709807435\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.97      0.93        39\n",
      "          1       0.90      0.64      0.75        14\n",
      "\n",
      "avg / total       0.89      0.89      0.88        53\n",
      "\n",
      "[38  1  5  9]\n",
      "LR Accuracy:  0.8867924528301887\n",
      "LR F1:  0.8384146341463415\n",
      "For name:  k_sohn\n",
      "total sample size before apply threshold:  31\n",
      "Counter({'0000-0002-3237-044X': 17, '0000-0001-8941-1188': 12, '0000-0001-9791-2126': 1, '0000-0002-7270-7094': 1})\n",
      "['0000-0002-3237-044X', '0000-0001-8941-1188']\n",
      "Total sample size after apply threshold:  29\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(29, 130)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(29, 130)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.94      0.94        17\n",
      "          1       0.92      0.92      0.92        12\n",
      "\n",
      "avg / total       0.93      0.93      0.93        29\n",
      "\n",
      "[16  1  1 11]\n",
      "svc Accuracy:  0.9310344827586207\n",
      "svc F1:  0.928921568627451\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.94      0.94        17\n",
      "          1       0.92      0.92      0.92        12\n",
      "\n",
      "avg / total       0.93      0.93      0.93        29\n",
      "\n",
      "[16  1  1 11]\n",
      "LR Accuracy:  0.9310344827586207\n",
      "LR F1:  0.928921568627451\n",
      "For name:  m_bennett\n",
      "total sample size before apply threshold:  208\n",
      "Counter({'0000-0002-2565-1825': 110, '0000-0002-8369-8349': 90, '0000-0002-3063-8844': 7, '0000-0001-9914-3850': 1})\n",
      "['0000-0002-2565-1825', '0000-0002-8369-8349']\n",
      "Total sample size after apply threshold:  200\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(200, 476)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(200, 476)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.97      0.88       110\n",
      "          1       0.96      0.71      0.82        90\n",
      "\n",
      "avg / total       0.87      0.85      0.85       200\n",
      "\n",
      "[107   3  26  64]\n",
      "svc Accuracy:  0.855\n",
      "svc F1:  0.8479725302089067\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.91      0.90       110\n",
      "          1       0.89      0.88      0.88        90\n",
      "\n",
      "avg / total       0.89      0.90      0.89       200\n",
      "\n",
      "[100  10  11  79]\n",
      "LR Accuracy:  0.895\n",
      "LR F1:  0.8938294699057104\n",
      "For name:  a_sharma\n",
      "total sample size before apply threshold:  223\n",
      "Counter({'0000-0002-2653-0806': 85, '0000-0003-3349-4417': 23, '0000-0002-7668-3501': 14, '0000-0002-0172-5033': 12, '0000-0003-2264-2007': 10, '0000-0003-0553-4039': 9, '0000-0001-6906-190X': 9, '0000-0002-7029-9867': 8, '0000-0002-7442-8494': 8, '0000-0003-3281-2081': 6, '0000-0001-6539-9970': 6, '0000-0001-5061-9731': 5, '0000-0002-6201-7639': 5, '0000-0002-4117-8775': 4, '0000-0002-8458-9216': 3, '0000-0002-5251-9045': 3, '0000-0001-7570-852X': 2, '0000-0002-1655-5997': 2, '0000-0002-4374-4259': 2, '0000-0003-4841-0108': 2, '0000-0002-6862-136X': 2, '0000-0002-4342-6656': 1, '0000-0003-4433-4355': 1, '0000-0002-7170-1627': 1})\n",
      "['0000-0003-3349-4417', '0000-0002-0172-5033', '0000-0002-7668-3501', '0000-0002-2653-0806', '0000-0003-2264-2007']\n",
      "Total sample size after apply threshold:  144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(144, 223)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(144, 223)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.91      0.95        23\n",
      "          1       1.00      0.58      0.74        12\n",
      "          2       1.00      0.93      0.96        14\n",
      "          3       0.90      1.00      0.95        85\n",
      "          4       1.00      0.90      0.95        10\n",
      "\n",
      "avg / total       0.94      0.94      0.93       144\n",
      "\n",
      "[21  0  0  2  0  0  7  0  5  0  0  0 13  1  0  0  0  0 85  0  0  0  0  1\n",
      "  9]\n",
      "svc Accuracy:  0.9375\n",
      "svc F1:  0.9102879228430536\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.91      0.95        23\n",
      "          1       1.00      0.25      0.40        12\n",
      "          2       1.00      0.93      0.96        14\n",
      "          3       0.85      1.00      0.92        85\n",
      "          4       1.00      0.70      0.82        10\n",
      "\n",
      "avg / total       0.91      0.90      0.88       144\n",
      "\n",
      "[21  0  0  2  0  0  3  0  9  0  0  0 13  1  0  0  0  0 85  0  0  0  0  3\n",
      "  7]\n",
      "LR Accuracy:  0.8958333333333334\n",
      "LR F1:  0.8119913496384085\n",
      "For name:  z_wei\n",
      "total sample size before apply threshold:  54\n",
      "Counter({'0000-0001-8747-2251': 11, '0000-0002-4446-6502': 11, '0000-0003-2687-0293': 8, '0000-0001-6948-7572': 7, '0000-0001-8558-4639': 5, '0000-0002-2311-9800': 4, '0000-0001-7436-6409': 4, '0000-0002-8694-1260': 2, '0000-0002-9670-4752': 2})\n",
      "['0000-0001-8747-2251', '0000-0002-4446-6502']\n",
      "Total sample size after apply threshold:  22\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(22, 79)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(22, 79)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      1.00      0.96        11\n",
      "          1       1.00      0.91      0.95        11\n",
      "\n",
      "avg / total       0.96      0.95      0.95        22\n",
      "\n",
      "[11  0  1 10]\n",
      "svc Accuracy:  0.9545454545454546\n",
      "svc F1:  0.9544513457556936\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        11\n",
      "          1       1.00      1.00      1.00        11\n",
      "\n",
      "avg / total       1.00      1.00      1.00        22\n",
      "\n",
      "[11  0  0 11]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  x_gu\n",
      "total sample size before apply threshold:  61\n",
      "Counter({'0000-0002-9373-987X': 23, '0000-0002-8521-3667': 13, '0000-0003-2266-5516': 7, '0000-0002-0437-5606': 5, '0000-0003-2641-1740': 5, '0000-0001-8299-6451': 4, '0000-0003-3803-3951': 4})\n",
      "['0000-0002-8521-3667', '0000-0002-9373-987X']\n",
      "Total sample size after apply threshold:  36\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(36, 83)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(36, 83)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.77      0.87        13\n",
      "          1       0.88      1.00      0.94        23\n",
      "\n",
      "avg / total       0.93      0.92      0.91        36\n",
      "\n",
      "[10  3  0 23]\n",
      "svc Accuracy:  0.9166666666666666\n",
      "svc F1:  0.904170363797693\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.85      0.92        13\n",
      "          1       0.92      1.00      0.96        23\n",
      "\n",
      "avg / total       0.95      0.94      0.94        36\n",
      "\n",
      "[11  2  0 23]\n",
      "LR Accuracy:  0.9444444444444444\n",
      "LR F1:  0.9375\n",
      "For name:  l_yang\n",
      "total sample size before apply threshold:  193\n",
      "Counter({'0000-0002-3756-8789': 48, '0000-0002-4351-2503': 30, '0000-0002-8654-4927': 23, '0000-0002-5964-3233': 23, '0000-0002-1698-6666': 11, '0000-0002-3681-2874': 9, '0000-0002-0192-4323': 8, '0000-0002-5392-558X': 8, '0000-0002-3294-0879': 6, '0000-0002-5421-9249': 5, '0000-0003-3894-873X': 5, '0000-0003-1057-9194': 5, '0000-0001-6573-6359': 2, '0000-0002-9937-1383': 2, '0000-0001-7965-2674': 2, '0000-0001-6497-1680': 2, '0000-0003-4294-9233': 1, '0000-0001-5709-6566': 1, '0000-0002-0639-0973': 1, '0000-0001-5396-7280': 1})\n",
      "['0000-0002-8654-4927', '0000-0002-4351-2503', '0000-0002-5964-3233', '0000-0002-1698-6666', '0000-0002-3756-8789']\n",
      "Total sample size after apply threshold:  135\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(135, 263)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(135, 263)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.74      0.85        23\n",
      "          1       1.00      0.87      0.93        30\n",
      "          2       1.00      0.96      0.98        23\n",
      "          3       1.00      1.00      1.00        11\n",
      "          4       0.81      1.00      0.90        48\n",
      "\n",
      "avg / total       0.93      0.92      0.92       135\n",
      "\n",
      "[17  0  0  0  6  0 26  0  0  4  0  0 22  0  1  0  0  0 11  0  0  0  0  0\n",
      " 48]\n",
      "svc Accuracy:  0.9185185185185185\n",
      "svc F1:  0.9307090936062898\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.70      0.82        23\n",
      "          1       1.00      0.87      0.93        30\n",
      "          2       1.00      0.96      0.98        23\n",
      "          3       1.00      0.64      0.78        11\n",
      "          4       0.75      1.00      0.86        48\n",
      "\n",
      "avg / total       0.91      0.88      0.88       135\n",
      "\n",
      "[16  0  0  0  7  0 26  0  0  4  0  0 22  0  1  0  0  0  7  4  0  0  0  0\n",
      " 48]\n",
      "LR Accuracy:  0.8814814814814815\n",
      "LR F1:  0.8723565323565323\n",
      "For name:  h_hassan\n",
      "total sample size before apply threshold:  22\n",
      "Counter({'0000-0002-6035-0040': 9, '0000-0002-2815-7996': 5, '0000-0002-9567-0896': 3, '0000-0001-5167-8063': 2, '0000-0002-6166-1342': 1, '0000-0001-7274-9414': 1, '0000-0003-0359-1208': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  f_chen\n",
      "total sample size before apply threshold:  40\n",
      "Counter({'0000-0002-2191-0930': 8, '0000-0002-9646-3338': 8, '0000-0002-8004-1720': 6, '0000-0002-7251-1956': 6, '0000-0003-0013-479X': 5, '0000-0002-6366-5064': 2, '0000-0002-9193-5412': 1, '0000-0001-9942-8872': 1, '0000-0002-4346-6906': 1, '0000-0002-1963-6784': 1, '0000-0003-2077-3812': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  g_rossi\n",
      "total sample size before apply threshold:  245\n",
      "Counter({'0000-0001-8377-2898': 86, '0000-0002-5588-1306': 85, '0000-0001-6916-2049': 24, '0000-0003-3519-2420': 19, '0000-0001-9688-9454': 14, '0000-0002-5102-5019': 10, '0000-0003-0984-3197': 3, '0000-0002-9926-115X': 2, '0000-0002-9761-6754': 1, '0000-0002-5572-1759': 1})\n",
      "['0000-0003-3519-2420', '0000-0002-5102-5019', '0000-0002-5588-1306', '0000-0001-9688-9454', '0000-0001-8377-2898', '0000-0001-6916-2049']\n",
      "Total sample size after apply threshold:  238\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(238, 1083)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(238, 1083)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.79      0.88        19\n",
      "          1       1.00      0.70      0.82        10\n",
      "          2       1.00      0.89      0.94        85\n",
      "          3       1.00      1.00      1.00        14\n",
      "          4       0.84      1.00      0.91        86\n",
      "          5       1.00      1.00      1.00        24\n",
      "\n",
      "avg / total       0.94      0.93      0.93       238\n",
      "\n",
      "[15  0  0  0  4  0  0  7  0  0  3  0  0  0 76  0  9  0  0  0  0 14  0  0\n",
      "  0  0  0  0 86  0  0  0  0  0  0 24]\n",
      "svc Accuracy:  0.9327731092436975\n",
      "svc F1:  0.9274792248074069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.79      0.88        19\n",
      "          1       1.00      0.70      0.82        10\n",
      "          2       1.00      0.94      0.97        85\n",
      "          3       1.00      0.86      0.92        14\n",
      "          4       0.86      1.00      0.92        86\n",
      "          5       1.00      1.00      1.00        24\n",
      "\n",
      "avg / total       0.95      0.94      0.94       238\n",
      "\n",
      "[15  0  0  0  4  0  0  7  0  0  3  0  0  0 80  0  5  0  0  0  0 12  2  0\n",
      "  0  0  0  0 86  0  0  0  0  0  0 24]\n",
      "LR Accuracy:  0.9411764705882353\n",
      "LR F1:  0.9205645714184613\n",
      "For name:  s_patil\n",
      "total sample size before apply threshold:  65\n",
      "Counter({'0000-0001-8454-589X': 17, '0000-0001-6167-2511': 16, '0000-0001-8562-8670': 7, '0000-0003-4952-3120': 5, '0000-0002-0950-549X': 5, '0000-0003-3898-3594': 5, '0000-0002-9278-5786': 4, '0000-0002-1597-855X': 3, '0000-0002-9812-1972': 2, '0000-0001-8502-0884': 1})\n",
      "['0000-0001-8454-589X', '0000-0001-6167-2511']\n",
      "Total sample size after apply threshold:  33\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(33, 45)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(33, 45)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        17\n",
      "          1       1.00      1.00      1.00        16\n",
      "\n",
      "avg / total       1.00      1.00      1.00        33\n",
      "\n",
      "[17  0  0 16]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97        17\n",
      "          1       1.00      0.94      0.97        16\n",
      "\n",
      "avg / total       0.97      0.97      0.97        33\n",
      "\n",
      "[17  0  1 15]\n",
      "LR Accuracy:  0.9696969696969697\n",
      "LR F1:  0.9695852534562213\n",
      "For name:  m_kelly\n",
      "total sample size before apply threshold:  97\n",
      "Counter({'0000-0002-6380-1150': 19, '0000-0002-1735-3342': 17, '0000-0001-7963-2139': 16, '0000-0001-6221-7406': 12, '0000-0003-3114-8780': 11, '0000-0003-1799-055X': 10, '0000-0003-3210-0295': 4, '0000-0002-6541-2992': 3, '0000-0002-2029-5841': 2, '0000-0003-2882-4450': 1, '0000-0003-0900-0691': 1, '0000-0002-0995-2425': 1})\n",
      "['0000-0001-7963-2139', '0000-0003-3114-8780', '0000-0003-1799-055X', '0000-0001-6221-7406', '0000-0002-1735-3342', '0000-0002-6380-1150']\n",
      "Total sample size after apply threshold:  85\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(85, 320)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(85, 320)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        16\n",
      "          1       1.00      1.00      1.00        11\n",
      "          2       1.00      0.70      0.82        10\n",
      "          3       1.00      0.50      0.67        12\n",
      "          4       1.00      0.65      0.79        17\n",
      "          5       0.56      1.00      0.72        19\n",
      "\n",
      "avg / total       0.90      0.82      0.83        85\n",
      "\n",
      "[16  0  0  0  0  0  0 11  0  0  0  0  0  0  7  0  0  3  0  0  0  6  0  6\n",
      "  0  0  0  0 11  6  0  0  0  0  0 19]\n",
      "svc Accuracy:  0.8235294117647058\n",
      "svc F1:  0.8321485827035217\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        16\n",
      "          1       1.00      1.00      1.00        11\n",
      "          2       1.00      0.70      0.82        10\n",
      "          3       1.00      0.50      0.67        12\n",
      "          4       1.00      0.76      0.87        17\n",
      "          5       0.59      1.00      0.75        19\n",
      "\n",
      "avg / total       0.91      0.85      0.85        85\n",
      "\n",
      "[16  0  0  0  0  0  0 11  0  0  0  0  0  0  7  0  0  3  0  0  0  6  0  6\n",
      "  0  0  0  0 13  4  0  0  0  0  0 19]\n",
      "LR Accuracy:  0.8470588235294118\n",
      "LR F1:  0.8503267973856209\n",
      "For name:  m_cheung\n",
      "total sample size before apply threshold:  13\n",
      "Counter({'0000-0001-7144-618X': 8, '0000-0002-2764-2113': 2, '0000-0002-8076-0725': 2, '0000-0002-0021-3802': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  j_weaver\n",
      "total sample size before apply threshold:  7\n",
      "Counter({'0000-0001-7394-2443': 4, '0000-0001-6733-7554': 1, '0000-0003-3361-2946': 1, '0000-0002-5556-4757': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  c_chien\n",
      "total sample size before apply threshold:  157\n",
      "Counter({'0000-0002-6690-6038': 71, '0000-0001-9806-486X': 60, '0000-0001-8704-0336': 25, '0000-0002-5532-1018': 1})\n",
      "['0000-0001-9806-486X', '0000-0001-8704-0336', '0000-0002-6690-6038']\n",
      "Total sample size after apply threshold:  156\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(156, 164)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(156, 164)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.88      0.86        60\n",
      "          1       1.00      0.88      0.94        25\n",
      "          2       0.90      0.90      0.90        71\n",
      "\n",
      "avg / total       0.89      0.89      0.89       156\n",
      "\n",
      "[53  0  7  3 22  0  7  0 64]\n",
      "svc Accuracy:  0.8910256410256411\n",
      "svc F1:  0.8997890937854539\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.87      0.87        60\n",
      "          1       1.00      0.92      0.96        25\n",
      "          2       0.89      0.92      0.90        71\n",
      "\n",
      "avg / total       0.90      0.90      0.90       156\n",
      "\n",
      "[52  0  8  2 23  0  6  0 65]\n",
      "LR Accuracy:  0.8974358974358975\n",
      "LR F1:  0.9092592592592593\n",
      "For name:  s_yun\n",
      "total sample size before apply threshold:  102\n",
      "Counter({'0000-0001-7737-4746': 76, '0000-0002-1498-3701': 24, '0000-0002-3774-0622': 1, '0000-0002-9510-5133': 1})\n",
      "['0000-0001-7737-4746', '0000-0002-1498-3701']\n",
      "Total sample size after apply threshold:  100\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(100, 71)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(100, 71)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.93      0.93        76\n",
      "          1       0.78      0.75      0.77        24\n",
      "\n",
      "avg / total       0.89      0.89      0.89       100\n",
      "\n",
      "[71  5  6 18]\n",
      "svc Accuracy:  0.89\n",
      "svc F1:  0.8470310109859547\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.95      0.94        76\n",
      "          1       0.82      0.75      0.78        24\n",
      "\n",
      "avg / total       0.90      0.90      0.90       100\n",
      "\n",
      "[72  4  6 18]\n",
      "LR Accuracy:  0.9\n",
      "LR F1:  0.8588368153585544\n",
      "For name:  s_jung\n",
      "total sample size before apply threshold:  76\n",
      "Counter({'0000-0002-3174-8965': 57, '0000-0002-3566-5649': 9, '0000-0003-3670-1952': 4, '0000-0003-4864-8175': 1, '0000-0002-8196-1748': 1, '0000-0001-7266-1084': 1, '0000-0002-3884-4335': 1, '0000-0002-5194-7339': 1, '0000-0001-6389-2315': 1})\n",
      "['0000-0002-3174-8965']\n",
      "Total sample size after apply threshold:  57\n",
      "For name:  e_gomes\n",
      "total sample size before apply threshold:  40\n",
      "Counter({'0000-0002-6941-4872': 20, '0000-0001-6378-6942': 8, '0000-0002-4238-3738': 8, '0000-0001-8528-8741': 2, '0000-0002-0636-6041': 2})\n",
      "['0000-0002-6941-4872']\n",
      "Total sample size after apply threshold:  20\n",
      "For name:  t_yamaguchi\n",
      "total sample size before apply threshold:  62\n",
      "Counter({'0000-0003-4590-8592': 30, '0000-0001-9043-4408': 15, '0000-0003-0214-4983': 7, '0000-0002-7533-430X': 6, '0000-0002-5063-9924': 2, '0000-0001-8454-1995': 1, '0000-0001-5341-4184': 1})\n",
      "['0000-0001-9043-4408', '0000-0003-4590-8592']\n",
      "Total sample size after apply threshold:  45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(45, 51)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(45, 51)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97        15\n",
      "          1       1.00      0.97      0.98        30\n",
      "\n",
      "avg / total       0.98      0.98      0.98        45\n",
      "\n",
      "[15  0  1 29]\n",
      "svc Accuracy:  0.9777777777777777\n",
      "svc F1:  0.9753963914707491\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        15\n",
      "          1       1.00      1.00      1.00        30\n",
      "\n",
      "avg / total       1.00      1.00      1.00        45\n",
      "\n",
      "[15  0  0 30]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  p_oliveira\n",
      "total sample size before apply threshold:  358\n",
      "Counter({'0000-0002-4989-5699': 71, '0000-0002-5201-9948': 71, '0000-0003-0307-354X': 55, '0000-0001-9519-4044': 50, '0000-0002-2470-0795': 40, '0000-0002-0938-152X': 20, '0000-0003-3161-8367': 16, '0000-0001-8478-7135': 8, '0000-0002-4989-2113': 7, '0000-0002-3078-2950': 7, '0000-0002-1850-6670': 5, '0000-0001-7217-5705': 2, '0000-0002-4460-9489': 2, '0000-0002-3898-2623': 1, '0000-0002-5799-390X': 1, '0000-0003-3123-4259': 1, '0000-0001-9843-7558': 1})\n",
      "['0000-0003-3161-8367', '0000-0001-9519-4044', '0000-0002-4989-5699', '0000-0002-0938-152X', '0000-0002-5201-9948', '0000-0003-0307-354X', '0000-0002-2470-0795']\n",
      "Total sample size after apply threshold:  323\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(323, 693)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(323, 693)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.81      0.90        16\n",
      "          1       0.98      0.88      0.93        50\n",
      "          2       0.88      0.94      0.91        71\n",
      "          3       1.00      0.90      0.95        20\n",
      "          4       0.73      0.93      0.81        71\n",
      "          5       0.96      0.78      0.86        55\n",
      "          6       0.83      0.72      0.77        40\n",
      "\n",
      "avg / total       0.88      0.87      0.87       323\n",
      "\n",
      "[13  0  0  0  2  0  1  0 44  0  0  3  0  3  0  0 67  0  4  0  0  0  0  0\n",
      " 18  2  0  0  0  1  2  0 66  0  2  0  0  4  0  8 43  0  0  0  3  0  6  2\n",
      " 29]\n",
      "svc Accuracy:  0.8668730650154799\n",
      "svc F1:  0.8757069583803908\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.81      0.90        16\n",
      "          1       0.91      0.98      0.94        50\n",
      "          2       0.92      0.94      0.93        71\n",
      "          3       1.00      0.80      0.89        20\n",
      "          4       0.80      0.99      0.88        71\n",
      "          5       0.96      0.82      0.88        55\n",
      "          6       0.94      0.75      0.83        40\n",
      "\n",
      "avg / total       0.91      0.90      0.90       323\n",
      "\n",
      "[13  0  1  0  2  0  0  0 49  0  0  0  0  1  0  2 67  0  2  0  0  0  0  0\n",
      " 16  3  1  0  0  0  1  0 70  0  0  0  0  3  0  6 45  1  0  3  1  0  5  1\n",
      " 30]\n",
      "LR Accuracy:  0.8978328173374613\n",
      "LR F1:  0.8934990400077085\n",
      "For name:  r_torres\n",
      "total sample size before apply threshold:  40\n",
      "Counter({'0000-0002-9109-4035': 17, '0000-0001-8030-5522': 7, '0000-0002-4041-270X': 7, '0000-0002-3777-8671': 4, '0000-0002-8205-8518': 2, '0000-0001-7090-7925': 2, '0000-0001-9606-0398': 1})\n",
      "['0000-0002-9109-4035']\n",
      "Total sample size after apply threshold:  17\n",
      "For name:  a_esteves\n",
      "total sample size before apply threshold:  63\n",
      "Counter({'0000-0002-7983-5742': 22, '0000-0001-8403-2015': 19, '0000-0003-2239-2976': 11, '0000-0002-7837-5983': 4, '0000-0002-9614-0635': 4, '0000-0001-6769-1844': 3})\n",
      "['0000-0003-2239-2976', '0000-0001-8403-2015', '0000-0002-7983-5742']\n",
      "Total sample size after apply threshold:  52\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(52, 105)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(52, 105)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.91      0.95        11\n",
      "          1       0.95      1.00      0.97        19\n",
      "          2       1.00      1.00      1.00        22\n",
      "\n",
      "avg / total       0.98      0.98      0.98        52\n",
      "\n",
      "[10  1  0  0 19  0  0  0 22]\n",
      "svc Accuracy:  0.9807692307692307\n",
      "svc F1:  0.9755799755799756\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.91      0.95        11\n",
      "          1       0.95      1.00      0.97        19\n",
      "          2       1.00      1.00      1.00        22\n",
      "\n",
      "avg / total       0.98      0.98      0.98        52\n",
      "\n",
      "[10  1  0  0 19  0  0  0 22]\n",
      "LR Accuracy:  0.9807692307692307\n",
      "LR F1:  0.9755799755799756\n",
      "For name:  l_stevens\n",
      "total sample size before apply threshold:  77\n",
      "Counter({'0000-0003-3372-3419': 49, '0000-0003-3847-5979': 26, '0000-0002-6075-8273': 1, '0000-0002-1345-6520': 1})\n",
      "['0000-0003-3372-3419', '0000-0003-3847-5979']\n",
      "Total sample size after apply threshold:  75\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(75, 168)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(75, 168)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.94      0.97        49\n",
      "          1       0.90      1.00      0.95        26\n",
      "\n",
      "avg / total       0.96      0.96      0.96        75\n",
      "\n",
      "[46  3  0 26]\n",
      "svc Accuracy:  0.96\n",
      "svc F1:  0.9569377990430622\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.96      0.98        49\n",
      "          1       0.93      1.00      0.96        26\n",
      "\n",
      "avg / total       0.98      0.97      0.97        75\n",
      "\n",
      "[47  2  0 26]\n",
      "LR Accuracy:  0.9733333333333334\n",
      "LR F1:  0.9710648148148149\n",
      "For name:  a_chang\n",
      "total sample size before apply threshold:  178\n",
      "Counter({'0000-0001-9506-0425': 74, '0000-0002-6877-5510': 72, '0000-0001-7309-9687': 19, '0000-0002-8416-359X': 5, '0000-0001-7096-7151': 4, '0000-0002-5694-0136': 4})\n",
      "['0000-0001-7309-9687', '0000-0001-9506-0425', '0000-0002-6877-5510']\n",
      "Total sample size after apply threshold:  165\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(165, 662)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(165, 662)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.79      0.83        19\n",
      "          1       0.80      0.95      0.86        74\n",
      "          2       0.97      0.81      0.88        72\n",
      "\n",
      "avg / total       0.88      0.87      0.87       165\n",
      "\n",
      "[15  4  0  2 70  2  0 14 58]\n",
      "svc Accuracy:  0.8666666666666667\n",
      "svc F1:  0.8587729143284699\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.79      0.88        19\n",
      "          1       0.89      0.86      0.88        74\n",
      "          2       0.86      0.93      0.89        72\n",
      "\n",
      "avg / total       0.89      0.88      0.88       165\n",
      "\n",
      "[15  3  1  0 64 10  0  5 67]\n",
      "LR Accuracy:  0.8848484848484849\n",
      "LR F1:  0.8841328677589758\n",
      "For name:  l_song\n",
      "total sample size before apply threshold:  58\n",
      "Counter({'0000-0003-0585-8519': 38, '0000-0003-1691-9583': 15, '0000-0002-0400-8283': 3, '0000-0003-2454-1576': 1, '0000-0002-7299-5719': 1})\n",
      "['0000-0003-1691-9583', '0000-0003-0585-8519']\n",
      "Total sample size after apply threshold:  53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(53, 134)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(53, 134)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.93      0.97        15\n",
      "          1       0.97      1.00      0.99        38\n",
      "\n",
      "avg / total       0.98      0.98      0.98        53\n",
      "\n",
      "[14  1  0 38]\n",
      "svc Accuracy:  0.9811320754716981\n",
      "svc F1:  0.9762651141961487\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.87      0.93        15\n",
      "          1       0.95      1.00      0.97        38\n",
      "\n",
      "avg / total       0.96      0.96      0.96        53\n",
      "\n",
      "[13  2  0 38]\n",
      "LR Accuracy:  0.9622641509433962\n",
      "LR F1:  0.9514652014652014\n",
      "For name:  j_delgado\n",
      "total sample size before apply threshold:  123\n",
      "Counter({'0000-0002-5157-4376': 85, '0000-0002-6948-8062': 26, '0000-0003-4074-981X': 6, '0000-0002-8075-4704': 3, '0000-0002-0166-5464': 2, '0000-0002-1026-4523': 1})\n",
      "['0000-0002-6948-8062', '0000-0002-5157-4376']\n",
      "Total sample size after apply threshold:  111\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(111, 583)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(111, 583)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.85      0.88        26\n",
      "          1       0.95      0.98      0.97        85\n",
      "\n",
      "avg / total       0.95      0.95      0.95       111\n",
      "\n",
      "[22  4  2 83]\n",
      "svc Accuracy:  0.9459459459459459\n",
      "svc F1:  0.9225581395348836\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.81      0.88        26\n",
      "          1       0.94      0.99      0.97        85\n",
      "\n",
      "avg / total       0.95      0.95      0.94       111\n",
      "\n",
      "[21  5  1 84]\n",
      "LR Accuracy:  0.9459459459459459\n",
      "LR F1:  0.9202586206896551\n",
      "For name:  p_jensen\n",
      "total sample size before apply threshold:  319\n",
      "Counter({'0000-0003-2387-0650': 98, '0000-0003-1648-7186': 65, '0000-0001-6524-7723': 55, '0000-0003-4359-848X': 28, '0000-0001-7058-6930': 17, '0000-0003-4718-1630': 16, '0000-0002-9819-1516': 14, '0000-0002-8856-2395': 12, '0000-0001-8792-7711': 11, '0000-0002-5248-0523': 1, '0000-0002-9588-3189': 1, '0000-0002-9297-2098': 1})\n",
      "['0000-0003-4359-848X', '0000-0002-9819-1516', '0000-0003-2387-0650', '0000-0001-8792-7711', '0000-0003-4718-1630', '0000-0001-6524-7723', '0000-0002-8856-2395', '0000-0003-1648-7186', '0000-0001-7058-6930']\n",
      "Total sample size after apply threshold:  316\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(316, 944)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(316, 944)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.93      0.96        28\n",
      "          1       1.00      0.93      0.96        14\n",
      "          2       0.75      1.00      0.86        98\n",
      "          3       1.00      0.36      0.53        11\n",
      "          4       1.00      0.88      0.93        16\n",
      "          5       0.98      0.84      0.90        55\n",
      "          6       1.00      0.67      0.80        12\n",
      "          7       1.00      0.89      0.94        65\n",
      "          8       1.00      0.94      0.97        17\n",
      "\n",
      "avg / total       0.92      0.90      0.89       316\n",
      "\n",
      "[26  0  2  0  0  0  0  0  0  0 13  1  0  0  0  0  0  0  0  0 98  0  0  0\n",
      "  0  0  0  0  0  6  4  0  1  0  0  0  0  0  2  0 14  0  0  0  0  0  0  9\n",
      "  0  0 46  0  0  0  0  0  4  0  0  0  8  0  0  0  0  7  0  0  0  0 58  0\n",
      "  0  0  1  0  0  0  0  0 16]\n",
      "svc Accuracy:  0.8955696202531646\n",
      "svc F1:  0.8741098778116239\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.89      0.94        28\n",
      "          1       1.00      0.93      0.96        14\n",
      "          2       0.78      1.00      0.88        98\n",
      "          3       1.00      0.36      0.53        11\n",
      "          4       1.00      0.88      0.93        16\n",
      "          5       0.98      0.85      0.91        55\n",
      "          6       1.00      0.67      0.80        12\n",
      "          7       1.00      0.95      0.98        65\n",
      "          8       1.00      0.94      0.97        17\n",
      "\n",
      "avg / total       0.93      0.91      0.91       316\n",
      "\n",
      "[25  0  3  0  0  0  0  0  0  0 13  1  0  0  0  0  0  0  0  0 98  0  0  0\n",
      "  0  0  0  0  0  6  4  0  1  0  0  0  0  0  2  0 14  0  0  0  0  0  0  8\n",
      "  0  0 47  0  0  0  0  0  4  0  0  0  8  0  0  0  0  3  0  0  0  0 62  0\n",
      "  0  0  1  0  0  0  0  0 16]\n",
      "LR Accuracy:  0.9082278481012658\n",
      "LR F1:  0.8785246819689889\n",
      "For name:  t_allen\n",
      "total sample size before apply threshold:  48\n",
      "Counter({'0000-0002-3911-7914': 17, '0000-0002-3512-9475': 14, '0000-0002-0667-6138': 10, '0000-0002-2372-7259': 3, '0000-0002-1107-1416': 2, '0000-0002-2972-7911': 1, '0000-0002-1252-6056': 1})\n",
      "['0000-0002-3512-9475', '0000-0002-0667-6138', '0000-0002-3911-7914']\n",
      "Total sample size after apply threshold:  41\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(41, 78)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(41, 78)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.93      0.96        14\n",
      "          1       0.71      1.00      0.83        10\n",
      "          2       1.00      0.82      0.90        17\n",
      "\n",
      "avg / total       0.93      0.90      0.91        41\n",
      "\n",
      "[13  1  0  0 10  0  0  3 14]\n",
      "svc Accuracy:  0.9024390243902439\n",
      "svc F1:  0.8998407009159698\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      1.00      0.82        14\n",
      "          1       0.83      0.50      0.62        10\n",
      "          2       1.00      0.88      0.94        17\n",
      "\n",
      "avg / total       0.86      0.83      0.82        41\n",
      "\n",
      "[14  0  0  5  5  0  1  1 15]\n",
      "LR Accuracy:  0.8292682926829268\n",
      "LR F1:  0.7953431372549019\n",
      "For name:  j_sullivan\n",
      "total sample size before apply threshold:  79\n",
      "Counter({'0000-0003-1457-2950': 26, '0000-0001-5445-708X': 17, '0000-0003-4489-4926': 14, '0000-0003-3209-0218': 9, '0000-0001-6732-0699': 7, '0000-0002-5952-3805': 2, '0000-0003-2906-2232': 1, '0000-0002-7279-4319': 1, '0000-0002-3746-3047': 1, '0000-0002-5441-4343': 1})\n",
      "['0000-0003-4489-4926', '0000-0001-5445-708X', '0000-0003-1457-2950']\n",
      "Total sample size after apply threshold:  57\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(57, 175)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(57, 175)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.79      0.88        14\n",
      "          1       0.81      0.76      0.79        17\n",
      "          2       0.80      0.92      0.86        26\n",
      "\n",
      "avg / total       0.85      0.84      0.84        57\n",
      "\n",
      "[11  1  2  0 13  4  0  2 24]\n",
      "svc Accuracy:  0.8421052631578947\n",
      "svc F1:  0.8416738816738817\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.86      0.92        14\n",
      "          1       0.93      0.76      0.84        17\n",
      "          2       0.81      0.96      0.88        26\n",
      "\n",
      "avg / total       0.89      0.88      0.88        57\n",
      "\n",
      "[12  0  2  0 13  4  0  1 25]\n",
      "LR Accuracy:  0.8771929824561403\n",
      "LR F1:  0.8796598609841394\n",
      "For name:  s_rogers\n",
      "total sample size before apply threshold:  224\n",
      "Counter({'0000-0002-5989-6142': 200, '0000-0003-3578-4477': 9, '0000-0003-1139-4730': 8, '0000-0002-3432-5044': 3, '0000-0003-0516-7929': 2, '0000-0001-9974-7152': 1, '0000-0002-0809-2726': 1})\n",
      "['0000-0002-5989-6142']\n",
      "Total sample size after apply threshold:  200\n",
      "For name:  h_yoon\n",
      "total sample size before apply threshold:  72\n",
      "Counter({'0000-0002-5403-1617': 34, '0000-0003-2180-4940': 10, '0000-0002-7139-0419': 7, '0000-0002-7552-0479': 6, '0000-0003-3087-8853': 5, '0000-0002-9597-6342': 4, '0000-0002-6211-7680': 3, '0000-0002-8553-0152': 1, '0000-0001-7547-0320': 1, '0000-0003-3524-8762': 1})\n",
      "['0000-0002-5403-1617', '0000-0003-2180-4940']\n",
      "Total sample size after apply threshold:  44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(44, 55)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(44, 55)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97        34\n",
      "          1       1.00      0.80      0.89        10\n",
      "\n",
      "avg / total       0.96      0.95      0.95        44\n",
      "\n",
      "[34  0  2  8]\n",
      "svc Accuracy:  0.9545454545454546\n",
      "svc F1:  0.9301587301587302\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      1.00      0.96        34\n",
      "          1       1.00      0.70      0.82        10\n",
      "\n",
      "avg / total       0.94      0.93      0.93        44\n",
      "\n",
      "[34  0  3  7]\n",
      "LR Accuracy:  0.9318181818181818\n",
      "LR F1:  0.8906379453189727\n",
      "For name:  a_young\n",
      "total sample size before apply threshold:  442\n",
      "Counter({'0000-0002-1202-6297': 138, '0000-0001-5702-4220': 78, '0000-0002-4163-6772': 54, '0000-0002-9367-9213': 38, '0000-0002-7288-3469': 31, '0000-0001-8551-5078': 25, '0000-0003-3969-3249': 22, '0000-0002-0077-137X': 22, '0000-0001-6251-0944': 20, '0000-0002-8486-0643': 8, '0000-0002-8127-7380': 2, '0000-0002-1994-9211': 1, '0000-0002-1486-5561': 1, '0000-0001-6800-1454': 1, '0000-0003-4822-6335': 1})\n",
      "['0000-0001-6251-0944', '0000-0001-8551-5078', '0000-0002-4163-6772', '0000-0002-7288-3469', '0000-0002-1202-6297', '0000-0001-5702-4220', '0000-0003-3969-3249', '0000-0002-0077-137X', '0000-0002-9367-9213']\n",
      "Total sample size after apply threshold:  428\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(428, 915)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(428, 915)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.70      0.78        20\n",
      "          1       1.00      0.72      0.84        25\n",
      "          2       0.87      0.63      0.73        54\n",
      "          3       1.00      0.65      0.78        31\n",
      "          4       0.67      0.98      0.79       138\n",
      "          5       1.00      0.85      0.92        78\n",
      "          6       1.00      0.86      0.93        22\n",
      "          7       0.67      0.45      0.54        22\n",
      "          8       0.97      0.84      0.90        38\n",
      "\n",
      "avg / total       0.85      0.81      0.81       428\n",
      "\n",
      "[ 14   0   0   0   5   0   0   1   0   0  18   0   0   7   0   0   0   0\n",
      "   0   0  34   0  16   0   0   4   0   0   0   0  20  11   0   0   0   0\n",
      "   0   0   2   0 135   0   0   0   1   1   0   0   0  11  66   0   0   0\n",
      "   0   0   0   0   3   0  19   0   0   0   0   3   0   9   0   0  10   0\n",
      "   1   0   0   0   5   0   0   0  32]\n",
      "svc Accuracy:  0.8130841121495327\n",
      "svc F1:  0.8011162416172687\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.55      0.71        20\n",
      "          1       1.00      0.72      0.84        25\n",
      "          2       0.84      0.67      0.74        54\n",
      "          3       1.00      0.65      0.78        31\n",
      "          4       0.64      0.99      0.77       138\n",
      "          5       1.00      0.87      0.93        78\n",
      "          6       1.00      0.82      0.90        22\n",
      "          7       1.00      0.14      0.24        22\n",
      "          8       1.00      0.87      0.93        38\n",
      "\n",
      "avg / total       0.86      0.80      0.79       428\n",
      "\n",
      "[ 11   0   1   0   8   0   0   0   0   0  18   0   0   7   0   0   0   0\n",
      "   0   0  36   0  18   0   0   0   0   0   0   0  20  11   0   0   0   0\n",
      "   0   0   2   0 136   0   0   0   0   0   0   0   0  10  68   0   0   0\n",
      "   0   0   0   0   4   0  18   0   0   0   0   4   0  15   0   0   3   0\n",
      "   0   0   0   0   5   0   0   0  33]\n",
      "LR Accuracy:  0.8014018691588785\n",
      "LR F1:  0.7608088972487559\n",
      "For name:  m_richardson\n",
      "total sample size before apply threshold:  175\n",
      "Counter({'0000-0001-5672-9552': 166, '0000-0002-1650-0064': 5, '0000-0002-7390-9480': 3, '0000-0003-2694-5486': 1})\n",
      "['0000-0001-5672-9552']\n",
      "Total sample size after apply threshold:  166\n",
      "For name:  c_ryan\n",
      "total sample size before apply threshold:  159\n",
      "Counter({'0000-0003-2158-9427': 71, '0000-0003-1915-546X': 29, '0000-0003-2750-9854': 17, '0000-0001-5864-4325': 15, '0000-0003-2891-3912': 14, '0000-0002-1802-0128': 9, '0000-0003-0986-6110': 2, '0000-0002-9674-3946': 1, '0000-0002-6455-936X': 1})\n",
      "['0000-0001-5864-4325', '0000-0003-1915-546X', '0000-0003-2750-9854', '0000-0003-2158-9427', '0000-0003-2891-3912']\n",
      "Total sample size after apply threshold:  146\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(146, 358)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(146, 358)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.40      0.57        15\n",
      "          1       1.00      0.76      0.86        29\n",
      "          2       1.00      0.88      0.94        17\n",
      "          3       0.78      1.00      0.88        71\n",
      "          4       1.00      0.86      0.92        14\n",
      "\n",
      "avg / total       0.89      0.86      0.85       146\n",
      "\n",
      "[ 6  0  0  9  0  0 22  0  7  0  0  0 15  2  0  0  0  0 71  0  0  0  0  2\n",
      " 12]\n",
      "svc Accuracy:  0.863013698630137\n",
      "svc F1:  0.8342587604842506\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.33      0.50        15\n",
      "          1       1.00      0.76      0.86        29\n",
      "          2       1.00      0.76      0.87        17\n",
      "          3       0.76      1.00      0.86        71\n",
      "          4       1.00      0.86      0.92        14\n",
      "\n",
      "avg / total       0.88      0.84      0.83       146\n",
      "\n",
      "[ 5  0  0 10  0  0 22  0  7  0  0  0 13  4  0  0  0  0 71  0  0  0  0  2\n",
      " 12]\n",
      "LR Accuracy:  0.8424657534246576\n",
      "LR F1:  0.8026189496777733\n",
      "For name:  l_jensen\n",
      "total sample size before apply threshold:  275\n",
      "Counter({'0000-0001-7885-715X': 140, '0000-0002-0267-8312': 37, '0000-0003-3199-1743': 37, '0000-0002-0020-1537': 28, '0000-0002-1446-2084': 22, '0000-0003-2338-357X': 8, '0000-0001-9059-5869': 1, '0000-0003-2786-5920': 1, '0000-0002-1648-3970': 1})\n",
      "['0000-0002-0020-1537', '0000-0002-1446-2084', '0000-0001-7885-715X', '0000-0002-0267-8312', '0000-0003-3199-1743']\n",
      "Total sample size after apply threshold:  264\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(264, 879)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(264, 879)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.68      0.72        28\n",
      "          1       1.00      0.77      0.87        22\n",
      "          2       0.85      0.97      0.91       140\n",
      "          3       1.00      0.84      0.91        37\n",
      "          4       1.00      0.84      0.91        37\n",
      "\n",
      "avg / total       0.90      0.89      0.89       264\n",
      "\n",
      "[ 19   0   9   0   0   2  17   3   0   0   4   0 136   0   0   0   0   6\n",
      "  31   0   0   0   6   0  31]\n",
      "svc Accuracy:  0.8863636363636364\n",
      "svc F1:  0.8637944164603434\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.75      0.84        28\n",
      "          1       1.00      0.73      0.84        22\n",
      "          2       0.85      0.99      0.91       140\n",
      "          3       1.00      0.84      0.91        37\n",
      "          4       1.00      0.84      0.91        37\n",
      "\n",
      "avg / total       0.91      0.90      0.90       264\n",
      "\n",
      "[ 21   0   7   0   0   0  16   6   0   0   1   0 139   0   0   0   0   6\n",
      "  31   0   0   0   6   0  31]\n",
      "LR Accuracy:  0.9015151515151515\n",
      "LR F1:  0.8840216718266255\n",
      "For name:  h_ferreira\n",
      "total sample size before apply threshold:  135\n",
      "Counter({'0000-0003-3611-6040': 53, '0000-0002-3261-3712': 22, '0000-0002-3770-9393': 16, '0000-0001-9143-504X': 13, '0000-0002-4323-3942': 12, '0000-0003-0834-2956': 12, '0000-0002-8895-2422': 7})\n",
      "['0000-0003-3611-6040', '0000-0001-9143-504X', '0000-0002-4323-3942', '0000-0002-3770-9393', '0000-0002-3261-3712', '0000-0003-0834-2956']\n",
      "Total sample size after apply threshold:  128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(128, 462)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(128, 462)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.96      0.91        53\n",
      "          1       1.00      0.92      0.96        13\n",
      "          2       1.00      0.67      0.80        12\n",
      "          3       1.00      1.00      1.00        16\n",
      "          4       0.91      0.95      0.93        22\n",
      "          5       1.00      0.83      0.91        12\n",
      "\n",
      "avg / total       0.93      0.92      0.92       128\n",
      "\n",
      "[51  0  0  0  2  0  1 12  0  0  0  0  4  0  8  0  0  0  0  0  0 16  0  0\n",
      "  1  0  0  0 21  0  2  0  0  0  0 10]\n",
      "svc Accuracy:  0.921875\n",
      "svc F1:  0.9188564213564213\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.98      0.92        53\n",
      "          1       1.00      0.92      0.96        13\n",
      "          2       1.00      0.67      0.80        12\n",
      "          3       1.00      1.00      1.00        16\n",
      "          4       0.95      0.95      0.95        22\n",
      "          5       1.00      0.83      0.91        12\n",
      "\n",
      "avg / total       0.94      0.93      0.93       128\n",
      "\n",
      "[52  0  0  0  1  0  1 12  0  0  0  0  4  0  8  0  0  0  0  0  0 16  0  0\n",
      "  1  0  0  0 21  0  2  0  0  0  0 10]\n",
      "LR Accuracy:  0.9296875\n",
      "LR F1:  0.9239983909895414\n",
      "For name:  a_mahmoud\n",
      "total sample size before apply threshold:  48\n",
      "Counter({'0000-0003-2459-5195': 19, '0000-0002-4716-7524': 15, '0000-0001-8905-9196': 7, '0000-0003-2959-0692': 4, '0000-0002-8703-841X': 3})\n",
      "['0000-0002-4716-7524', '0000-0003-2459-5195']\n",
      "Total sample size after apply threshold:  34\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(34, 51)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(34, 51)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97        15\n",
      "          1       1.00      0.95      0.97        19\n",
      "\n",
      "avg / total       0.97      0.97      0.97        34\n",
      "\n",
      "[15  0  1 18]\n",
      "svc Accuracy:  0.9705882352941176\n",
      "svc F1:  0.970357454228422\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97        15\n",
      "          1       1.00      0.95      0.97        19\n",
      "\n",
      "avg / total       0.97      0.97      0.97        34\n",
      "\n",
      "[15  0  1 18]\n",
      "LR Accuracy:  0.9705882352941176\n",
      "LR F1:  0.970357454228422\n",
      "For name:  y_liao\n",
      "total sample size before apply threshold:  115\n",
      "Counter({'0000-0002-4434-3780': 42, '0000-0001-9496-4190': 21, '0000-0002-9384-336X': 15, '0000-0002-8217-8202': 13, '0000-0002-0399-0201': 13, '0000-0002-4360-7932': 7, '0000-0001-5658-8948': 2, '0000-0002-4401-8275': 1, '0000-0002-2107-918X': 1})\n",
      "['0000-0002-9384-336X', '0000-0001-9496-4190', '0000-0002-8217-8202', '0000-0002-0399-0201', '0000-0002-4434-3780']\n",
      "Total sample size after apply threshold:  104\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(104, 184)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(104, 184)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.93      0.97        15\n",
      "          1       0.74      0.81      0.77        21\n",
      "          2       1.00      0.85      0.92        13\n",
      "          3       0.92      0.85      0.88        13\n",
      "          4       0.82      0.86      0.84        42\n",
      "\n",
      "avg / total       0.86      0.86      0.86       104\n",
      "\n",
      "[14  0  0  0  1  0 17  0  0  4  0  1 11  0  1  0  0  0 11  2  0  5  0  1\n",
      " 36]\n",
      "svc Accuracy:  0.8557692307692307\n",
      "svc F1:  0.8744240966197662\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.93      0.97        15\n",
      "          1       0.79      0.71      0.75        21\n",
      "          2       0.92      0.85      0.88        13\n",
      "          3       1.00      0.85      0.92        13\n",
      "          4       0.79      0.90      0.84        42\n",
      "\n",
      "avg / total       0.86      0.86      0.86       104\n",
      "\n",
      "[14  0  0  0  1  0 15  0  0  6  0  1 11  0  1  0  0  0 11  2  0  3  1  0\n",
      " 38]\n",
      "LR Accuracy:  0.8557692307692307\n",
      "LR F1:  0.8713256704980843\n",
      "For name:  m_svensson\n",
      "total sample size before apply threshold:  142\n",
      "Counter({'0000-0003-1179-7003': 40, '0000-0003-1695-7934': 35, '0000-0002-8304-1398': 20, '0000-0003-1113-7478': 18, '0000-0003-4972-4416': 17, '0000-0001-8077-9824': 12})\n",
      "['0000-0001-8077-9824', '0000-0002-8304-1398', '0000-0003-4972-4416', '0000-0003-1695-7934', '0000-0003-1113-7478', '0000-0003-1179-7003']\n",
      "Total sample size after apply threshold:  142\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(142, 429)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(142, 429)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.75      0.86        12\n",
      "          1       0.93      0.70      0.80        20\n",
      "          2       1.00      0.71      0.83        17\n",
      "          3       1.00      0.74      0.85        35\n",
      "          4       0.38      0.94      0.54        18\n",
      "          5       0.97      0.85      0.91        40\n",
      "\n",
      "avg / total       0.90      0.79      0.82       142\n",
      "\n",
      "[ 9  0  0  0  2  1  0 14  0  0  6  0  0  0 12  0  5  0  0  0  0 26  9  0\n",
      "  0  1  0  0 17  0  0  0  0  0  6 34]\n",
      "svc Accuracy:  0.7887323943661971\n",
      "svc F1:  0.7972562144636762\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.75      0.86        12\n",
      "          1       0.95      0.95      0.95        20\n",
      "          2       1.00      0.82      0.90        17\n",
      "          3       0.86      0.86      0.86        35\n",
      "          4       0.50      0.56      0.53        18\n",
      "          5       0.86      0.95      0.90        40\n",
      "\n",
      "avg / total       0.86      0.85      0.85       142\n",
      "\n",
      "[ 9  0  0  1  0  2  0 19  0  0  1  0  0  0 14  1  2  0  0  0  0 30  5  0\n",
      "  0  1  0  3 10  4  0  0  0  0  2 38]\n",
      "LR Accuracy:  0.8450704225352113\n",
      "LR F1:  0.8330982024954862\n",
      "For name:  p_tsai\n",
      "total sample size before apply threshold:  73\n",
      "Counter({'0000-0002-8650-647X': 31, '0000-0002-3095-3991': 14, '0000-0003-4681-0685': 12, '0000-0001-8217-6285': 9, '0000-0003-1274-574X': 4, '0000-0003-2809-0733': 2, '0000-0003-0005-1476': 1})\n",
      "['0000-0002-3095-3991', '0000-0002-8650-647X', '0000-0003-4681-0685']\n",
      "Total sample size after apply threshold:  57\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(57, 102)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(57, 102)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.79      0.88        14\n",
      "          1       0.89      1.00      0.94        31\n",
      "          2       1.00      0.92      0.96        12\n",
      "\n",
      "avg / total       0.94      0.93      0.93        57\n",
      "\n",
      "[11  3  0  0 31  0  0  1 11]\n",
      "svc Accuracy:  0.9298245614035088\n",
      "svc F1:  0.9253052261747913\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.64      0.78        14\n",
      "          1       0.84      1.00      0.91        31\n",
      "          2       1.00      0.92      0.96        12\n",
      "\n",
      "avg / total       0.91      0.89      0.89        57\n",
      "\n",
      "[ 9  5  0  0 31  0  0  1 11]\n",
      "LR Accuracy:  0.8947368421052632\n",
      "LR F1:  0.8836317135549873\n",
      "For name:  r_berry\n",
      "total sample size before apply threshold:  85\n",
      "Counter({'0000-0002-7162-5046': 49, '0000-0002-6140-1583': 28, '0000-0002-1861-6722': 4, '0000-0002-4272-9858': 4})\n",
      "['0000-0002-6140-1583', '0000-0002-7162-5046']\n",
      "Total sample size after apply threshold:  77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(77, 191)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(77, 191)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.96      0.98        28\n",
      "          1       0.98      1.00      0.99        49\n",
      "\n",
      "avg / total       0.99      0.99      0.99        77\n",
      "\n",
      "[27  1  0 49]\n",
      "svc Accuracy:  0.987012987012987\n",
      "svc F1:  0.9858585858585859\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.96      0.98        28\n",
      "          1       0.98      1.00      0.99        49\n",
      "\n",
      "avg / total       0.99      0.99      0.99        77\n",
      "\n",
      "[27  1  0 49]\n",
      "LR Accuracy:  0.987012987012987\n",
      "LR F1:  0.9858585858585859\n",
      "For name:  j_kwok\n",
      "total sample size before apply threshold:  100\n",
      "Counter({'0000-0001-9574-6195': 76, '0000-0002-9798-9083': 23, '0000-0001-7444-6935': 1})\n",
      "['0000-0001-9574-6195', '0000-0002-9798-9083']\n",
      "Total sample size after apply threshold:  99\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(99, 883)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(99, 883)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        76\n",
      "          1       1.00      1.00      1.00        23\n",
      "\n",
      "avg / total       1.00      1.00      1.00        99\n",
      "\n",
      "[76  0  0 23]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        76\n",
      "          1       1.00      1.00      1.00        23\n",
      "\n",
      "avg / total       1.00      1.00      1.00        99\n",
      "\n",
      "[76  0  0 23]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  m_schneider\n",
      "total sample size before apply threshold:  367\n",
      "Counter({'0000-0001-9645-1938': 110, '0000-0002-9570-3491': 91, '0000-0002-9260-7357': 56, '0000-0001-7190-3379': 34, '0000-0002-7114-2060': 29, '0000-0002-1223-1266': 14, '0000-0001-7147-8915': 10, '0000-0002-3842-2618': 10, '0000-0003-1488-4743': 8, '0000-0001-7534-5431': 2, '0000-0001-9846-7132': 2, '0000-0002-4918-1389': 1})\n",
      "['0000-0002-9570-3491', '0000-0002-9260-7357', '0000-0001-9645-1938', '0000-0001-7190-3379', '0000-0001-7147-8915', '0000-0002-3842-2618', '0000-0002-7114-2060', '0000-0002-1223-1266']\n",
      "Total sample size after apply threshold:  354\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(354, 1357)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(354, 1357)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.78      0.84        91\n",
      "          1       0.95      0.73      0.83        56\n",
      "          2       0.59      0.97      0.73       110\n",
      "          3       1.00      0.50      0.67        34\n",
      "          4       1.00      1.00      1.00        10\n",
      "          5       1.00      0.40      0.57        10\n",
      "          6       0.90      0.31      0.46        29\n",
      "          7       1.00      0.71      0.83        14\n",
      "\n",
      "avg / total       0.83      0.76      0.75       354\n",
      "\n",
      "[ 71   2  18   0   0   0   0   0   2  41  13   0   0   0   0   0   2   0\n",
      " 107   0   0   0   1   0   0   0  17  17   0   0   0   0   0   0   0   0\n",
      "  10   0   0   0   0   0   6   0   0   4   0   0   3   0  17   0   0   0\n",
      "   9   0   0   0   4   0   0   0   0  10]\n",
      "svc Accuracy:  0.7598870056497176\n",
      "svc F1:  0.741795407496145\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.81      0.89        91\n",
      "          1       0.98      0.77      0.86        56\n",
      "          2       0.58      0.98      0.73       110\n",
      "          3       0.95      0.56      0.70        34\n",
      "          4       1.00      0.80      0.89        10\n",
      "          5       1.00      0.10      0.18        10\n",
      "          6       0.91      0.34      0.50        29\n",
      "          7       1.00      0.57      0.73        14\n",
      "\n",
      "avg / total       0.85      0.77      0.76       354\n",
      "\n",
      "[ 74   1  16   0   0   0   0   0   0  43  13   0   0   0   0   0   0   0\n",
      " 108   1   0   0   1   0   0   0  15  19   0   0   0   0   0   0   2   0\n",
      "   8   0   0   0   0   0   9   0   0   1   0   0   1   0  18   0   0   0\n",
      "  10   0   0   0   6   0   0   0   0   8]\n",
      "LR Accuracy:  0.7655367231638418\n",
      "LR F1:  0.6850653117520589\n",
      "For name:  k_wood\n",
      "total sample size before apply threshold:  24\n",
      "Counter({'0000-0002-8774-8112': 17, '0000-0001-9170-6129': 4, '0000-0002-1020-7860': 3})\n",
      "['0000-0002-8774-8112']\n",
      "Total sample size after apply threshold:  17\n",
      "For name:  c_viegas\n",
      "total sample size before apply threshold:  67\n",
      "Counter({'0000-0002-1545-6479': 26, '0000-0002-8796-5037': 21, '0000-0003-1394-0731': 19, '0000-0002-5765-3665': 1})\n",
      "['0000-0002-1545-6479', '0000-0002-8796-5037', '0000-0003-1394-0731']\n",
      "Total sample size after apply threshold:  66\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(66, 133)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(66, 133)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.96      0.98        26\n",
      "          1       1.00      1.00      1.00        21\n",
      "          2       0.95      1.00      0.97        19\n",
      "\n",
      "avg / total       0.99      0.98      0.98        66\n",
      "\n",
      "[25  0  1  0 21  0  0  0 19]\n",
      "svc Accuracy:  0.9848484848484849\n",
      "svc F1:  0.9849170437405732\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        26\n",
      "          1       1.00      1.00      1.00        21\n",
      "          2       1.00      1.00      1.00        19\n",
      "\n",
      "avg / total       1.00      1.00      1.00        66\n",
      "\n",
      "[26  0  0  0 21  0  0  0 19]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  r_d'souza\n",
      "total sample size before apply threshold:  83\n",
      "Counter({'0000-0002-1505-5173': 67, '0000-0002-9724-4540': 9, '0000-0001-9028-1990': 6, '0000-0001-7887-5016': 1})\n",
      "['0000-0002-1505-5173']\n",
      "Total sample size after apply threshold:  67\n",
      "For name:  s_shim\n",
      "total sample size before apply threshold:  41\n",
      "Counter({'0000-0001-8043-2257': 14, '0000-0003-4143-7383': 10, '0000-0001-5203-6038': 10, '0000-0002-5188-688X': 7})\n",
      "['0000-0001-8043-2257', '0000-0003-4143-7383', '0000-0001-5203-6038']\n",
      "Total sample size after apply threshold:  34\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(34, 63)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(34, 63)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      1.00      0.93        14\n",
      "          1       1.00      0.80      0.89        10\n",
      "          2       1.00      1.00      1.00        10\n",
      "\n",
      "avg / total       0.95      0.94      0.94        34\n",
      "\n",
      "[14  0  0  2  8  0  0  0 10]\n",
      "svc Accuracy:  0.9411764705882353\n",
      "svc F1:  0.9407407407407408\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      1.00      0.90        14\n",
      "          1       1.00      0.70      0.82        10\n",
      "          2       1.00      1.00      1.00        10\n",
      "\n",
      "avg / total       0.93      0.91      0.91        34\n",
      "\n",
      "[14  0  0  3  7  0  0  0 10]\n",
      "LR Accuracy:  0.9117647058823529\n",
      "LR F1:  0.9089184060721062\n",
      "For name:  j_herrero\n",
      "total sample size before apply threshold:  105\n",
      "Counter({'0000-0003-1986-3482': 51, '0000-0001-7313-717X': 46, '0000-0002-0146-2464': 6, '0000-0001-8501-1187': 2})\n",
      "['0000-0003-1986-3482', '0000-0001-7313-717X']\n",
      "Total sample size after apply threshold:  97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(97, 1023)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(97, 1023)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      1.00      0.94        51\n",
      "          1       1.00      0.87      0.93        46\n",
      "\n",
      "avg / total       0.94      0.94      0.94        97\n",
      "\n",
      "[51  0  6 40]\n",
      "svc Accuracy:  0.9381443298969072\n",
      "svc F1:  0.9373385012919897\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      1.00      0.94        51\n",
      "          1       1.00      0.87      0.93        46\n",
      "\n",
      "avg / total       0.94      0.94      0.94        97\n",
      "\n",
      "[51  0  6 40]\n",
      "LR Accuracy:  0.9381443298969072\n",
      "LR F1:  0.9373385012919897\n",
      "For name:  m_acosta\n",
      "total sample size before apply threshold:  47\n",
      "Counter({'0000-0002-5018-339X': 24, '0000-0003-4827-7271': 17, '0000-0003-0611-6672': 4, '0000-0001-9504-883X': 2})\n",
      "['0000-0003-4827-7271', '0000-0002-5018-339X']\n",
      "Total sample size after apply threshold:  41\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(41, 141)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(41, 141)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      1.00      0.94        17\n",
      "          1       1.00      0.92      0.96        24\n",
      "\n",
      "avg / total       0.96      0.95      0.95        41\n",
      "\n",
      "[17  0  2 22]\n",
      "svc Accuracy:  0.9512195121951219\n",
      "svc F1:  0.9504830917874396\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.94      0.94        17\n",
      "          1       0.96      0.96      0.96        24\n",
      "\n",
      "avg / total       0.95      0.95      0.95        41\n",
      "\n",
      "[16  1  1 23]\n",
      "LR Accuracy:  0.9512195121951219\n",
      "LR F1:  0.9497549019607843\n",
      "For name:  a_chan\n",
      "total sample size before apply threshold:  249\n",
      "Counter({'0000-0003-1551-3995': 218, '0000-0002-2886-2513': 10, '0000-0002-1771-163X': 7, '0000-0003-3553-7249': 5, '0000-0003-2267-4949': 4, '0000-0001-6953-0120': 3, '0000-0001-7216-191X': 1, '0000-0002-5788-333X': 1})\n",
      "['0000-0003-1551-3995', '0000-0002-2886-2513']\n",
      "Total sample size after apply threshold:  228\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(228, 512)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(228, 512)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99       218\n",
      "          1       1.00      0.60      0.75        10\n",
      "\n",
      "avg / total       0.98      0.98      0.98       228\n",
      "\n",
      "[218   0   4   6]\n",
      "svc Accuracy:  0.9824561403508771\n",
      "svc F1:  0.8704545454545454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gao137/intel/intelpython3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/gao137/intel/intelpython3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98       218\n",
      "          1       0.00      0.00      0.00        10\n",
      "\n",
      "avg / total       0.91      0.96      0.93       228\n",
      "\n",
      "[218   0  10   0]\n",
      "LR Accuracy:  0.956140350877193\n",
      "LR F1:  0.4887892376681614\n",
      "For name:  p_kelly\n",
      "total sample size before apply threshold:  55\n",
      "Counter({'0000-0003-0500-1865': 27, '0000-0001-9040-1868': 11, '0000-0001-8933-2367': 6, '0000-0003-4338-6225': 5, '0000-0002-7490-5772': 5, '0000-0002-8813-8877': 1})\n",
      "['0000-0001-9040-1868', '0000-0003-0500-1865']\n",
      "Total sample size after apply threshold:  38\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(38, 100)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(38, 100)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.82      0.86        11\n",
      "          1       0.93      0.96      0.95        27\n",
      "\n",
      "avg / total       0.92      0.92      0.92        38\n",
      "\n",
      "[ 9  2  1 26]\n",
      "svc Accuracy:  0.9210526315789473\n",
      "svc F1:  0.9012987012987013\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.64      0.78        11\n",
      "          1       0.87      1.00      0.93        27\n",
      "\n",
      "avg / total       0.91      0.89      0.89        38\n",
      "\n",
      "[ 7  4  0 27]\n",
      "LR Accuracy:  0.8947368421052632\n",
      "LR F1:  0.8544061302681992\n",
      "For name:  j_weiner\n",
      "total sample size before apply threshold:  61\n",
      "Counter({'0000-0002-3352-2847': 35, '0000-0002-0736-7943': 15, '0000-0001-5810-5807': 10, '0000-0002-8099-760X': 1})\n",
      "['0000-0002-3352-2847', '0000-0002-0736-7943', '0000-0001-5810-5807']\n",
      "Total sample size after apply threshold:  60\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(60, 187)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(60, 187)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      0.94      0.81        35\n",
      "          1       0.70      0.47      0.56        15\n",
      "          2       1.00      0.40      0.57        10\n",
      "\n",
      "avg / total       0.76      0.73      0.71        60\n",
      "\n",
      "[33  2  0  8  7  0  5  1  4]\n",
      "svc Accuracy:  0.7333333333333333\n",
      "svc F1:  0.6487477954144621\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      1.00      0.79        35\n",
      "          1       1.00      0.40      0.57        15\n",
      "          2       0.00      0.00      0.00        10\n",
      "\n",
      "avg / total       0.63      0.68      0.60        60\n",
      "\n",
      "[35  0  0  9  6  0 10  0  0]\n",
      "LR Accuracy:  0.6833333333333333\n",
      "LR F1:  0.45264847512038525\n",
      "For name:  b_yu\n",
      "total sample size before apply threshold:  24\n",
      "Counter({'0000-0002-1258-4168': 9, '0000-0002-0531-2236': 8, '0000-0002-7259-8190': 5, '0000-0001-6013-5220': 1, '0000-0001-7266-4197': 1})\n",
      "[]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gao137/intel/intelpython3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/gao137/intel/intelpython3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  s_lucas\n",
      "total sample size before apply threshold:  96\n",
      "Counter({'0000-0002-8713-2457': 64, '0000-0003-1287-7996': 19, '0000-0002-5555-5594': 10, '0000-0002-3409-2033': 2, '0000-0003-3059-7453': 1})\n",
      "['0000-0002-8713-2457', '0000-0002-5555-5594', '0000-0003-1287-7996']\n",
      "Total sample size after apply threshold:  93\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(93, 249)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(93, 249)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98        64\n",
      "          1       1.00      0.80      0.89        10\n",
      "          2       1.00      0.95      0.97        19\n",
      "\n",
      "avg / total       0.97      0.97      0.97        93\n",
      "\n",
      "[64  0  0  2  8  0  1  0 18]\n",
      "svc Accuracy:  0.967741935483871\n",
      "svc F1:  0.9463203661676944\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97        64\n",
      "          1       1.00      0.80      0.89        10\n",
      "          2       1.00      0.89      0.94        19\n",
      "\n",
      "avg / total       0.96      0.96      0.96        93\n",
      "\n",
      "[64  0  0  2  8  0  2  0 17]\n",
      "LR Accuracy:  0.956989247311828\n",
      "LR F1:  0.9343434343434343\n",
      "For name:  e_davis\n",
      "total sample size before apply threshold:  21\n",
      "Counter({'0000-0002-3529-098X': 17, '0000-0001-5413-5398': 2, '0000-0002-4731-1602': 1, '0000-0002-4925-1447': 1})\n",
      "['0000-0002-3529-098X']\n",
      "Total sample size after apply threshold:  17\n",
      "For name:  z_yu\n",
      "total sample size before apply threshold:  135\n",
      "Counter({'0000-0002-2979-9608': 62, '0000-0002-6165-8522': 24, '0000-0003-4420-5836': 12, '0000-0002-9152-6491': 11, '0000-0002-1401-2294': 9, '0000-0002-5797-5373': 7, '0000-0001-5828-2635': 4, '0000-0002-8762-999X': 2, '0000-0001-5913-9646': 2, '0000-0002-2311-4030': 1, '0000-0001-6348-5293': 1})\n",
      "['0000-0002-6165-8522', '0000-0002-2979-9608', '0000-0002-9152-6491', '0000-0003-4420-5836']\n",
      "Total sample size after apply threshold:  109\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(109, 159)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(109, 159)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.92      0.83        24\n",
      "          1       0.93      0.89      0.91        62\n",
      "          2       1.00      0.64      0.78        11\n",
      "          3       0.79      0.92      0.85        12\n",
      "\n",
      "avg / total       0.88      0.87      0.87       109\n",
      "\n",
      "[22  2  0  0  5 55  0  2  2  1  7  1  0  1  0 11]\n",
      "svc Accuracy:  0.8715596330275229\n",
      "svc F1:  0.840802803066954\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.88      0.89        24\n",
      "          1       0.90      0.97      0.93        62\n",
      "          2       1.00      0.64      0.78        11\n",
      "          3       0.83      0.83      0.83        12\n",
      "\n",
      "avg / total       0.90      0.90      0.90       109\n",
      "\n",
      "[21  3  0  0  1 60  0  1  1  2  7  1  0  2  0 10]\n",
      "LR Accuracy:  0.8990825688073395\n",
      "LR F1:  0.8587401726318105\n",
      "For name:  c_pan\n",
      "total sample size before apply threshold:  161\n",
      "Counter({'0000-0002-2652-5134': 66, '0000-0002-6654-9309': 33, '0000-0001-8700-583X': 23, '0000-0001-6327-9692': 15, '0000-0003-0108-3138': 11, '0000-0002-1031-7488': 7, '0000-0002-0089-7482': 6})\n",
      "['0000-0002-6654-9309', '0000-0001-8700-583X', '0000-0003-0108-3138', '0000-0002-2652-5134', '0000-0001-6327-9692']\n",
      "Total sample size after apply threshold:  148\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(148, 144)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(148, 144)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.61      0.61        33\n",
      "          1       0.87      0.87      0.87        23\n",
      "          2       0.57      0.36      0.44        11\n",
      "          3       0.75      0.83      0.79        66\n",
      "          4       1.00      0.80      0.89        15\n",
      "\n",
      "avg / total       0.75      0.75      0.75       148\n",
      "\n",
      "[20  2  1 10  0  0 20  0  3  0  5  0  4  2  0  8  1  2 55  0  0  0  0  3\n",
      " 12]\n",
      "svc Accuracy:  0.75\n",
      "svc F1:  0.7200652126520127\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.61      0.63        33\n",
      "          1       0.83      0.65      0.73        23\n",
      "          2       0.67      0.18      0.29        11\n",
      "          3       0.72      0.89      0.80        66\n",
      "          4       0.93      0.93      0.93        15\n",
      "\n",
      "avg / total       0.74      0.74      0.73       148\n",
      "\n",
      "[20  1  1 11  0  0 15  0  8  0  6  0  2  3  0  4  2  0 59  1  0  0  0  1\n",
      " 14]\n",
      "LR Accuracy:  0.7432432432432432\n",
      "LR F1:  0.6765945736677444\n",
      "For name:  x_cao\n",
      "total sample size before apply threshold:  74\n",
      "Counter({'0000-0002-3004-7518': 25, '0000-0001-7222-5450': 14, '0000-0002-3476-9833': 12, '0000-0002-4782-853X': 11, '0000-0001-7571-6482': 10, '0000-0002-6771-0571': 1, '0000-0001-8124-7491': 1})\n",
      "['0000-0001-7571-6482', '0000-0002-3004-7518', '0000-0001-7222-5450', '0000-0002-4782-853X', '0000-0002-3476-9833']\n",
      "Total sample size after apply threshold:  72\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(72, 269)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(72, 269)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        10\n",
      "          1       0.93      1.00      0.96        25\n",
      "          2       1.00      0.93      0.96        14\n",
      "          3       0.82      0.82      0.82        11\n",
      "          4       0.82      0.75      0.78        12\n",
      "\n",
      "avg / total       0.92      0.92      0.92        72\n",
      "\n",
      "[10  0  0  0  0  0 25  0  0  0  0  1 13  0  0  0  0  0  9  2  0  1  0  2\n",
      "  9]\n",
      "svc Accuracy:  0.9166666666666666\n",
      "svc F1:  0.9050583876670834\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        10\n",
      "          1       0.93      1.00      0.96        25\n",
      "          2       1.00      0.93      0.96        14\n",
      "          3       0.91      0.91      0.91        11\n",
      "          4       0.91      0.83      0.87        12\n",
      "\n",
      "avg / total       0.95      0.94      0.94        72\n",
      "\n",
      "[10  0  0  0  0  0 25  0  0  0  0  1 13  0  0  0  0  0 10  1  0  1  0  1\n",
      " 10]\n",
      "LR Accuracy:  0.9444444444444444\n",
      "LR F1:  0.9406315101967276\n",
      "For name:  j_yoo\n",
      "total sample size before apply threshold:  112\n",
      "Counter({'0000-0001-8378-1583': 41, '0000-0001-7120-8464': 19, '0000-0002-3924-6919': 15, '0000-0002-3150-1727': 9, '0000-0002-5488-7925': 7, '0000-0001-7119-5421': 6, '0000-0003-3881-1995': 5, '0000-0003-2611-3399': 5, '0000-0002-0259-6237': 2, '0000-0003-0639-3944': 1, '0000-0002-2330-4053': 1, '0000-0002-9508-0757': 1})\n",
      "['0000-0001-7120-8464', '0000-0002-3924-6919', '0000-0001-8378-1583']\n",
      "Total sample size after apply threshold:  75\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(75, 77)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(75, 77)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        19\n",
      "          1       1.00      0.93      0.97        15\n",
      "          2       0.98      1.00      0.99        41\n",
      "\n",
      "avg / total       0.99      0.99      0.99        75\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19  0  0  0 14  1  0  0 41]\n",
      "svc Accuracy:  0.9866666666666667\n",
      "svc F1:  0.9844896828694086\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        19\n",
      "          1       1.00      0.93      0.97        15\n",
      "          2       0.98      1.00      0.99        41\n",
      "\n",
      "avg / total       0.99      0.99      0.99        75\n",
      "\n",
      "[19  0  0  0 14  1  0  0 41]\n",
      "LR Accuracy:  0.9866666666666667\n",
      "LR F1:  0.9844896828694086\n",
      "For name:  l_wong\n",
      "total sample size before apply threshold:  131\n",
      "Counter({'0000-0003-1241-5441': 66, '0000-0002-4005-7330': 38, '0000-0002-7437-123X': 16, '0000-0003-0569-3398': 4, '0000-0001-8449-4973': 4, '0000-0001-8653-2734': 3})\n",
      "['0000-0003-1241-5441', '0000-0002-7437-123X', '0000-0002-4005-7330']\n",
      "Total sample size after apply threshold:  120\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(120, 329)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(120, 329)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      1.00      0.91        66\n",
      "          1       1.00      0.75      0.86        16\n",
      "          2       1.00      0.76      0.87        38\n",
      "\n",
      "avg / total       0.91      0.89      0.89       120\n",
      "\n",
      "[66  0  0  4 12  0  9  0 29]\n",
      "svc Accuracy:  0.8916666666666667\n",
      "svc F1:  0.8777197755067029\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      1.00      0.90        66\n",
      "          1       1.00      0.75      0.86        16\n",
      "          2       1.00      0.74      0.85        38\n",
      "\n",
      "avg / total       0.90      0.88      0.88       120\n",
      "\n",
      "[66  0  0  4 12  0 10  0 28]\n",
      "LR Accuracy:  0.8833333333333333\n",
      "LR F1:  0.8699124315562671\n",
      "For name:  h_chen\n",
      "total sample size before apply threshold:  986\n",
      "Counter({'0000-0001-5108-8338': 147, '0000-0002-5799-6705': 93, '0000-0003-0708-6073': 73, '0000-0001-6758-1995': 49, '0000-0001-5051-9896': 40, '0000-0003-0676-4610': 40, '0000-0002-7748-4440': 39, '0000-0001-6883-3752': 36, '0000-0003-3191-0093': 28, '0000-0002-0753-6161': 25, '0000-0002-5958-9361': 24, '0000-0001-9751-5729': 22, '0000-0002-3319-5743': 21, '0000-0001-5972-2778': 20, '0000-0001-9477-5541': 20, '0000-0002-7823-3272': 19, '0000-0003-1158-5510': 19, '0000-0003-4876-634X': 18, '0000-0002-7480-9940': 16, '0000-0003-2014-7571': 16, '0000-0002-9510-4923': 15, '0000-0002-4074-5838': 15, '0000-0003-4053-7147': 14, '0000-0002-9835-5138': 12, '0000-0001-6208-1481': 12, '0000-0003-1663-1598': 10, '0000-0001-7897-9851': 9, '0000-0003-0676-3079': 9, '0000-0002-4292-0441': 9, '0000-0001-6315-9850': 8, '0000-0002-3800-5013': 8, '0000-0002-3722-5399': 8, '0000-0002-5037-4559': 7, '0000-0002-0595-3420': 7, '0000-0002-8795-1911': 6, '0000-0003-4060-9489': 6, '0000-0001-7900-3391': 6, '0000-0002-2883-4627': 5, '0000-0002-0310-0088': 5, '0000-0002-7516-9092': 4, '0000-0003-3111-9824': 4, '0000-0003-1997-5476': 3, '0000-0002-5498-5621': 3, '0000-0002-9330-5473': 3, '0000-0002-5905-8050': 2, '0000-0002-1724-8649': 2, '0000-0003-1211-3259': 2, '0000-0001-9998-1205': 2, '0000-0001-9644-8202': 2, '0000-0002-5662-5945': 2, '0000-0001-7775-8571': 1, '0000-0001-6260-1117': 1, '0000-0002-5266-9975': 1, '0000-0002-6181-5292': 1, '0000-0003-2117-7385': 1, '0000-0002-1124-0312': 1, '0000-0002-4796-8452': 1, '0000-0001-8234-1535': 1, '0000-0001-6171-1162': 1, '0000-0002-2456-5271': 1, '0000-0002-4242-216X': 1, '0000-0002-8400-3780': 1, '0000-0002-0978-1806': 1, '0000-0002-9840-4876': 1, '0000-0001-6524-1292': 1, '0000-0002-5378-2697': 1, '0000-0003-1613-468X': 1, '0000-0002-0457-9895': 1, '0000-0002-2121-1822': 1, '0000-0002-4425-3128': 1, '0000-0002-7431-6046': 1})\n",
      "['0000-0002-7480-9940', '0000-0001-5972-2778', '0000-0003-4876-634X', '0000-0002-9835-5138', '0000-0002-7748-4440', '0000-0001-6883-3752', '0000-0001-9477-5541', '0000-0002-0753-6161', '0000-0003-1663-1598', '0000-0002-7823-3272', '0000-0003-3191-0093', '0000-0001-5051-9896', '0000-0003-0708-6073', '0000-0003-0676-4610', '0000-0002-9510-4923', '0000-0002-5799-6705', '0000-0003-4053-7147', '0000-0001-6208-1481', '0000-0001-5108-8338', '0000-0003-2014-7571', '0000-0002-3319-5743', '0000-0002-4074-5838', '0000-0002-5958-9361', '0000-0001-6758-1995', '0000-0001-9751-5729', '0000-0003-1158-5510']\n",
      "Total sample size after apply threshold:  843\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(843, 1655)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(843, 1655)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.30      0.38      0.33        16\n",
      "          1       0.63      0.85      0.72        20\n",
      "          2       0.52      0.83      0.64        18\n",
      "          3       0.75      0.75      0.75        12\n",
      "          4       0.74      0.79      0.77        39\n",
      "          5       1.00      0.92      0.96        36\n",
      "          6       0.35      0.45      0.39        20\n",
      "          7       0.50      0.52      0.51        25\n",
      "          8       1.00      0.20      0.33        10\n",
      "          9       0.81      0.89      0.85        19\n",
      "         10       0.67      0.64      0.65        28\n",
      "         11       0.60      0.60      0.60        40\n",
      "         12       0.88      0.90      0.89        73\n",
      "         13       0.72      0.65      0.68        40\n",
      "         14       1.00      0.67      0.80        15\n",
      "         15       0.61      0.82      0.70        93\n",
      "         16       0.92      0.79      0.85        14\n",
      "         17       0.85      0.92      0.88        12\n",
      "         18       0.99      0.86      0.92       147\n",
      "         19       1.00      0.81      0.90        16\n",
      "         20       0.86      0.90      0.88        21\n",
      "         21       0.80      0.53      0.64        15\n",
      "         22       0.88      0.96      0.92        24\n",
      "         23       0.98      0.92      0.95        49\n",
      "         24       1.00      0.41      0.58        22\n",
      "         25       0.93      0.68      0.79        19\n",
      "\n",
      "avg / total       0.80      0.77      0.78       843\n",
      "\n",
      "[  6   0   0   0   1   0   1   2   0   0   0   1   0   1   0   3   0   0\n",
      "   0   0   1   0   0   0   0   0   0  17   0   0   0   0   1   0   0   0\n",
      "   0   1   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  15   0   0   0   0   0   0   0   0   1   0   0   0   2   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   1   9   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   2   0   0   0   0   0   0   0\n",
      "  31   0   0   2   0   0   2   1   0   0   0   2   0   0   0   0   0   1\n",
      "   0   0   0   0   0   0   0   0   0  33   0   0   0   0   0   0   0   0\n",
      "   0   3   0   0   0   0   0   0   0   0   0   0   2   0   2   0   0   0\n",
      "   9   0   0   0   0   0   2   1   0   4   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   2   0   0  13   0   1   1   2   0   0   0   6\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   2   0\n",
      "   2   0   1   0   0   0   0   5   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   1   0  17   0   0   0   0   0   1   0   0\n",
      "   0   0   0   0   0   0   0   0   1   2   1   0   1   0   0   0   0   0\n",
      "  18   1   0   1   0   3   0   0   0   0   0   0   0   0   0   0   3   1\n",
      "   0   1   3   0   0   1   0   1   0  24   0   0   0   6   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   1   0   0   0   2   0   0   0   0   0\n",
      "  66   2   0   0   0   1   0   0   0   0   0   0   0   1   1   1   0   1\n",
      "   0   0   2   0   0   0   0   3   4  26   0   0   0   0   1   0   1   0\n",
      "   0   0   0   0   1   1   0   1   0   0   0   0   0   1   0   0   0   0\n",
      "  10   0   0   0   0   0   0   1   0   0   0   0   0   3   4   0   1   0\n",
      "   1   1   0   0   3   4   0   0   0  76   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   2   1   0   0   0   0   0   0   0   0\n",
      "  11   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   1   0   0   0  11   0   0   0   0   0   0   0   0\n",
      "   2   1   1   0   1   0   1   1   0   0   0   0   1   4   0   4   1   1\n",
      " 127   0   0   0   1   1   0   0   1   0   0   0   0   0   1   0   0   0\n",
      "   0   0   0   0   0   1   0   0   0  13   0   0   0   0   0   0   2   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  19   0   0   0   0   0   1   1   1   0   1   0   0   1   0   0   1   0\n",
      "   0   0   0   0   0   0   0   0   1   8   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0\n",
      "  23   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   1   0\n",
      "   0   2   0   0   0   0   0   0   0  45   0   0   0   0   1   0   1   0\n",
      "   1   3   0   1   1   2   0   0   0   3   0   0   0   0   0   0   0   0\n",
      "   9   0   0   0   1   0   0   0   3   0   0   0   0   0   1   0   0   1\n",
      "   0   0   0   0   0   0   0   0   0  13]\n",
      "svc Accuracy:  0.7722419928825622\n",
      "svc F1:  0.7264805707017495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.38      0.46        16\n",
      "          1       0.58      0.70      0.64        20\n",
      "          2       0.57      0.72      0.63        18\n",
      "          3       1.00      0.67      0.80        12\n",
      "          4       0.81      0.87      0.84        39\n",
      "          5       1.00      0.97      0.99        36\n",
      "          6       0.83      0.50      0.62        20\n",
      "          7       0.58      0.60      0.59        25\n",
      "          8       1.00      0.30      0.46        10\n",
      "          9       0.74      0.89      0.81        19\n",
      "         10       0.73      0.68      0.70        28\n",
      "         11       0.66      0.68      0.67        40\n",
      "         12       0.85      0.93      0.89        73\n",
      "         13       0.77      0.75      0.76        40\n",
      "         14       1.00      0.67      0.80        15\n",
      "         15       0.67      0.84      0.74        93\n",
      "         16       0.92      0.86      0.89        14\n",
      "         17       0.91      0.83      0.87        12\n",
      "         18       0.90      0.97      0.93       147\n",
      "         19       1.00      0.88      0.93        16\n",
      "         20       0.91      0.95      0.93        21\n",
      "         21       0.67      0.53      0.59        15\n",
      "         22       1.00      0.96      0.98        24\n",
      "         23       1.00      0.96      0.98        49\n",
      "         24       0.80      0.36      0.50        22\n",
      "         25       1.00      0.79      0.88        19\n",
      "\n",
      "avg / total       0.82      0.81      0.81       843\n",
      "\n",
      "[  6   0   0   0   0   0   0   2   0   1   0   1   0   1   0   1   0   0\n",
      "   2   0   2   0   0   0   0   0   0  14   1   0   0   0   0   0   0   0\n",
      "   0   1   0   0   0   3   0   0   0   0   0   0   0   0   1   0   0   0\n",
      "  13   0   0   0   0   0   0   0   0   1   2   0   0   2   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   8   1   0   1   0   0   0   0   1\n",
      "   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0\n",
      "  34   0   0   1   0   0   0   0   1   0   0   2   0   0   0   0   0   1\n",
      "   0   0   0   0   0   0   0   0   0  35   0   0   0   0   0   0   0   0\n",
      "   0   1   0   0   0   0   0   0   0   0   0   0   1   0   2   0   0   0\n",
      "  10   0   0   0   0   0   1   1   0   3   1   0   1   0   0   0   0   0\n",
      "   0   0   0   0   0   0   2   0   0  15   0   0   2   2   0   0   0   3\n",
      "   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   3   0   0   1   0   0   0   6   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   1   0  17   0   0   0   0   0   1   0   0\n",
      "   0   0   0   0   0   0   0   0   0   2   1   0   2   0   0   0   0   0\n",
      "  19   0   0   1   0   3   0   0   0   0   0   0   0   0   0   0   0   1\n",
      "   0   0   2   0   0   2   0   2   1  27   0   0   0   4   0   0   0   0\n",
      "   0   0   0   0   1   0   0   0   1   0   0   0   0   0   0   0   0   0\n",
      "  68   2   0   0   0   1   1   0   0   0   0   0   0   0   0   1   0   0\n",
      "   0   0   0   0   0   1   0   1   3  30   0   1   0   0   3   0   0   0\n",
      "   0   0   0   0   1   1   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  10   0   0   0   3   0   0   0   0   0   0   0   0   2   2   0   0   0\n",
      "   0   2   0   0   2   3   1   0   0  78   0   0   1   0   0   2   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   1   0   0   1   0   0   0\n",
      "  12   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   1   0   0   0  10   1   0   0   0   0   0   0   0\n",
      "   0   1   1   0   0   0   0   0   0   0   0   0   0   3   0   0   0   0\n",
      " 142   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   1   0   0   1   0   0   0  14   0   0   0   0   0   0   1   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  20   0   0   0   0   0   1   1   1   0   0   0   0   1   0   0   1   0\n",
      "   0   0   0   2   0   0   0   0   0   8   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0\n",
      "  23   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0\n",
      "   0   1   0   0   0   0   0   0   0  47   0   0   0   1   1   0   1   0\n",
      "   0   2   0   1   1   3   0   0   0   4   0   0   0   0   0   0   0   0\n",
      "   8   0   0   0   0   0   0   0   1   0   0   0   0   0   1   0   0   0\n",
      "   0   0   2   0   0   0   0   0   0  15]\n",
      "LR Accuracy:  0.8137603795966786\n",
      "LR F1:  0.7650939950809639\n",
      "For name:  c_huang\n",
      "total sample size before apply threshold:  425\n",
      "Counter({'0000-0002-6557-211X': 117, '0000-0002-5234-4986': 48, '0000-0001-5357-0451': 36, '0000-0001-9521-5650': 33, '0000-0002-5824-3318': 17, '0000-0002-1486-741X': 15, '0000-0002-1956-6229': 14, '0000-0003-1019-784X': 14, '0000-0001-9993-8004': 13, '0000-0001-8736-9545': 10, '0000-0002-0266-3233': 9, '0000-0002-4390-6502': 9, '0000-0002-1941-1200': 8, '0000-0001-6052-0663': 8, '0000-0002-4704-548X': 8, '0000-0002-8062-8708': 8, '0000-0001-6946-2105': 7, '0000-0002-9174-7542': 7, '0000-0002-9769-8075': 5, '0000-0003-4187-2967': 4, '0000-0002-9020-305X': 4, '0000-0003-2981-4537': 4, '0000-0003-3905-8915': 3, '0000-0003-3331-181X': 2, '0000-0001-5958-5849': 2, '0000-0003-3445-4353': 2, '0000-0001-7266-3610': 2, '0000-0003-4378-4776': 2, '0000-0002-2921-4294': 2, '0000-0001-7865-1020': 1, '0000-0001-9824-5716': 1, '0000-0001-7392-6363': 1, '0000-0002-1366-5170': 1, '0000-0002-0425-8311': 1, '0000-0002-4803-1477': 1, '0000-0002-0972-6444': 1, '0000-0001-5179-0872': 1, '0000-0003-1446-6787': 1, '0000-0003-3527-2789': 1, '0000-0002-6987-0536': 1, '0000-0003-0055-9798': 1})\n",
      "['0000-0001-5357-0451', '0000-0001-9993-8004', '0000-0001-8736-9545', '0000-0002-1486-741X', '0000-0002-6557-211X', '0000-0002-5234-4986', '0000-0002-5824-3318', '0000-0002-1956-6229', '0000-0003-1019-784X', '0000-0001-9521-5650']\n",
      "Total sample size after apply threshold:  317\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(317, 636)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(317, 636)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.69      0.69      0.69        36\n",
      "          1       1.00      0.85      0.92        13\n",
      "          2       1.00      0.80      0.89        10\n",
      "          3       0.92      0.73      0.81        15\n",
      "          4       0.88      0.85      0.87       117\n",
      "          5       0.67      0.85      0.75        48\n",
      "          6       0.44      0.47      0.46        17\n",
      "          7       0.70      1.00      0.82        14\n",
      "          8       0.45      0.36      0.40        14\n",
      "          9       1.00      0.79      0.88        33\n",
      "\n",
      "avg / total       0.80      0.79      0.79       317\n",
      "\n",
      "[ 25   0   0   0   4   3   3   0   1   0   0  11   0   0   0   2   0   0\n",
      "   0   0   1   0   8   0   0   0   1   0   0   0   1   0   0  11   0   2\n",
      "   0   1   0   0   4   0   0   0 100   6   3   2   2   0   2   0   0   0\n",
      "   3  41   1   0   1   0   1   0   0   0   4   2   8   0   2   0   0   0\n",
      "   0   0   0   0   0  14   0   0   2   0   0   0   2   3   2   0   5   0\n",
      "   0   0   0   1   1   2   0   3   0  26]\n",
      "svc Accuracy:  0.7854889589905363\n",
      "svc F1:  0.7494937459708285\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.72      0.69        36\n",
      "          1       1.00      1.00      1.00        13\n",
      "          2       1.00      0.80      0.89        10\n",
      "          3       0.92      0.73      0.81        15\n",
      "          4       0.85      0.90      0.87       117\n",
      "          5       0.69      0.83      0.75        48\n",
      "          6       0.50      0.35      0.41        17\n",
      "          7       0.78      1.00      0.88        14\n",
      "          8       1.00      0.36      0.53        14\n",
      "          9       0.96      0.82      0.89        33\n",
      "\n",
      "avg / total       0.81      0.80      0.80       317\n",
      "\n",
      "[ 26   0   0   0   5   4   1   0   0   0   0  13   0   0   0   0   0   0\n",
      "   0   0   1   0   8   0   0   0   1   0   0   0   0   0   0  11   0   2\n",
      "   0   1   0   1   5   0   0   0 105   5   2   0   0   0   2   0   0   0\n",
      "   5  40   1   0   0   0   2   0   0   0   5   4   6   0   0   0   0   0\n",
      "   0   0   0   0   0  14   0   0   3   0   0   0   3   2   1   0   5   0\n",
      "   0   0   0   1   1   1   0   3   0  27]\n",
      "LR Accuracy:  0.804416403785489\n",
      "LR F1:  0.7723478107336226\n",
      "For name:  s_chong\n",
      "total sample size before apply threshold:  40\n",
      "Counter({'0000-0001-8062-7281': 24, '0000-0002-3095-875X': 12, '0000-0002-8854-9529': 2, '0000-0003-3054-9275': 1, '0000-0002-3627-4025': 1})\n",
      "['0000-0001-8062-7281', '0000-0002-3095-875X']\n",
      "Total sample size after apply threshold:  36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(36, 157)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(36, 157)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98        24\n",
      "          1       1.00      0.92      0.96        12\n",
      "\n",
      "avg / total       0.97      0.97      0.97        36\n",
      "\n",
      "[24  0  1 11]\n",
      "svc Accuracy:  0.9722222222222222\n",
      "svc F1:  0.9680567879325643\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        24\n",
      "          1       1.00      1.00      1.00        12\n",
      "\n",
      "avg / total       1.00      1.00      1.00        36\n",
      "\n",
      "[24  0  0 12]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  z_wu\n",
      "total sample size before apply threshold:  221\n",
      "Counter({'0000-0003-0807-7195': 52, '0000-0002-9596-9134': 35, '0000-0002-2982-2177': 31, '0000-0002-4468-3240': 25, '0000-0002-0708-6770': 14, '0000-0002-4004-9728': 11, '0000-0002-3719-406X': 9, '0000-0002-6424-6777': 8, '0000-0003-1660-0724': 6, '0000-0002-1824-9563': 5, '0000-0002-2463-242X': 5, '0000-0003-2009-991X': 5, '0000-0002-9383-1270': 4, '0000-0002-4739-1139': 4, '0000-0003-4460-9785': 3, '0000-0002-9774-7770': 2, '0000-0002-7776-563X': 1, '0000-0002-8687-0466': 1})\n",
      "['0000-0002-4004-9728', '0000-0002-0708-6770', '0000-0002-9596-9134', '0000-0003-0807-7195', '0000-0002-4468-3240', '0000-0002-2982-2177']\n",
      "Total sample size after apply threshold:  168\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(168, 318)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(168, 318)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.73      0.76        11\n",
      "          1       1.00      1.00      1.00        14\n",
      "          2       0.72      0.83      0.77        35\n",
      "          3       1.00      0.90      0.95        52\n",
      "          4       0.83      0.60      0.70        25\n",
      "          5       0.77      0.97      0.86        31\n",
      "\n",
      "avg / total       0.86      0.85      0.85       168\n",
      "\n",
      "[ 8  0  0  0  3  0  0 14  0  0  0  0  0  0 29  0  0  6  0  0  5 47  0  0\n",
      "  2  0  5  0 15  3  0  0  1  0  0 30]\n",
      "svc Accuracy:  0.8511904761904762\n",
      "svc F1:  0.8399250534134254\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.73      0.80        11\n",
      "          1       1.00      1.00      1.00        14\n",
      "          2       0.79      0.94      0.86        35\n",
      "          3       1.00      0.94      0.97        52\n",
      "          4       0.85      0.68      0.76        25\n",
      "          5       0.82      0.90      0.86        31\n",
      "\n",
      "avg / total       0.89      0.89      0.89       168\n",
      "\n",
      "[ 8  0  0  0  2  1  0 14  0  0  0  0  0  0 33  0  0  2  0  0  3 49  0  0\n",
      "  1  0  4  0 17  3  0  0  2  0  1 28]\n",
      "LR Accuracy:  0.8869047619047619\n",
      "LR F1:  0.874088983989974\n",
      "For name:  m_swamy\n",
      "total sample size before apply threshold:  134\n",
      "Counter({'0000-0002-3108-3633': 96, '0000-0003-3977-3425': 25, '0000-0002-8084-5534': 9, '0000-0002-7834-7480': 4})\n",
      "['0000-0002-3108-3633', '0000-0003-3977-3425']\n",
      "Total sample size after apply threshold:  121\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(121, 215)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(121, 215)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99        96\n",
      "          1       1.00      0.92      0.96        25\n",
      "\n",
      "avg / total       0.98      0.98      0.98       121\n",
      "\n",
      "[96  0  2 23]\n",
      "svc Accuracy:  0.9834710743801653\n",
      "svc F1:  0.974012027491409\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.98        96\n",
      "          1       1.00      0.88      0.94        25\n",
      "\n",
      "avg / total       0.98      0.98      0.97       121\n",
      "\n",
      "[96  0  3 22]\n",
      "LR Accuracy:  0.9752066115702479\n",
      "LR F1:  0.9603927986906711\n",
      "For name:  k_nomura\n",
      "total sample size before apply threshold:  38\n",
      "Counter({'0000-0003-3661-6328': 32, '0000-0002-6425-4574': 3, '0000-0003-0625-1778': 1, '0000-0002-5912-074X': 1, '0000-0001-7891-9795': 1})\n",
      "['0000-0003-3661-6328']\n",
      "Total sample size after apply threshold:  32\n",
      "For name:  m_wu\n",
      "total sample size before apply threshold:  658\n",
      "Counter({'0000-0002-1940-6428': 219, '0000-0002-7074-8087': 194, '0000-0002-1674-443X': 56, '0000-0003-3327-828X': 42, '0000-0001-6587-7055': 33, '0000-0002-8811-9203': 29, '0000-0002-7509-1643': 22, '0000-0003-2045-9372': 13, '0000-0002-9161-7940': 11, '0000-0003-3712-1554': 10, '0000-0001-7672-9357': 6, '0000-0003-2113-0245': 5, '0000-0001-6847-7065': 5, '0000-0003-0977-3600': 4, '0000-0003-1372-4764': 2, '0000-0003-1734-7994': 2, '0000-0002-3269-1681': 2, '0000-0002-0183-0490': 1, '0000-0001-6646-050X': 1, '0000-0002-6646-951X': 1})\n",
      "['0000-0002-8811-9203', '0000-0001-6587-7055', '0000-0002-9161-7940', '0000-0003-3712-1554', '0000-0003-3327-828X', '0000-0003-2045-9372', '0000-0002-7509-1643', '0000-0002-1674-443X', '0000-0002-1940-6428', '0000-0002-7074-8087']\n",
      "Total sample size after apply threshold:  629\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(629, 320)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(629, 320)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.48      0.34      0.40        29\n",
      "          1       0.83      0.88      0.85        33\n",
      "          2       0.88      0.64      0.74        11\n",
      "          3       1.00      0.90      0.95        10\n",
      "          4       0.72      0.62      0.67        42\n",
      "          5       1.00      0.92      0.96        13\n",
      "          6       0.40      0.27      0.32        22\n",
      "          7       0.67      0.66      0.67        56\n",
      "          8       0.80      0.86      0.83       219\n",
      "          9       0.80      0.83      0.81       194\n",
      "\n",
      "avg / total       0.76      0.77      0.77       629\n",
      "\n",
      "[ 10   0   0   0   0   0   0   2  12   5   0  29   0   0   1   0   0   1\n",
      "   1   1   0   0   7   0   0   0   0   0   0   4   0   0   0   9   0   0\n",
      "   0   0   0   1   0   1   0   0  26   0   0   2   4   9   0   0   0   0\n",
      "   0  12   0   0   1   0   0   1   0   0   1   0   6   3   7   4   6   1\n",
      "   0   0   2   0   2  37   4   4   3   3   0   0   3   0   2   6 189  13\n",
      "   2   0   1   0   3   0   5   4  18 161]\n",
      "svc Accuracy:  0.7726550079491256\n",
      "svc F1:  0.7198709904344579\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.03      0.07        29\n",
      "          1       0.97      0.88      0.92        33\n",
      "          2       1.00      0.73      0.84        11\n",
      "          3       1.00      0.90      0.95        10\n",
      "          4       0.79      0.64      0.71        42\n",
      "          5       1.00      0.85      0.92        13\n",
      "          6       0.62      0.23      0.33        22\n",
      "          7       0.75      0.73      0.74        56\n",
      "          8       0.76      0.89      0.82       219\n",
      "          9       0.78      0.87      0.82       194\n",
      "\n",
      "avg / total       0.80      0.79      0.76       629\n",
      "\n",
      "[  1   0   0   0   0   0   0   2  22   4   0  29   0   0   1   0   0   1\n",
      "   1   1   0   0   8   0   0   0   0   0   0   3   0   0   0   9   0   0\n",
      "   0   0   0   1   0   0   0   0  27   0   0   1   4  10   0   0   0   0\n",
      "   0  11   0   0   1   1   0   1   0   0   2   0   5   2   8   4   0   0\n",
      "   0   0   1   0   2  41   6   6   0   0   0   0   0   0   0   5 196  18\n",
      "   0   0   0   0   3   0   1   3  19 168]\n",
      "LR Accuracy:  0.7869634340222575\n",
      "LR F1:  0.7119081932926983\n",
      "For name:  e_lee\n",
      "total sample size before apply threshold:  300\n",
      "Counter({'0000-0003-0232-7704': 81, '0000-0003-0418-1454': 48, '0000-0001-7494-1776': 48, '0000-0003-1255-9808': 40, '0000-0001-7188-3857': 29, '0000-0002-6369-7429': 16, '0000-0001-9670-3242': 10, '0000-0001-8131-6872': 8, '0000-0001-5144-2552': 3, '0000-0003-4725-4959': 3, '0000-0001-6506-4150': 3, '0000-0003-2848-7298': 2, '0000-0001-9580-8974': 2, '0000-0002-3156-2036': 1, '0000-0002-4156-9637': 1, '0000-0002-4513-9888': 1, '0000-0001-5816-2449': 1, '0000-0002-0934-3187': 1, '0000-0002-2674-912X': 1, '0000-0001-7976-5724': 1})\n",
      "['0000-0001-9670-3242', '0000-0003-1255-9808', '0000-0003-0418-1454', '0000-0002-6369-7429', '0000-0001-7494-1776', '0000-0001-7188-3857', '0000-0003-0232-7704']\n",
      "Total sample size after apply threshold:  272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(272, 452)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(272, 452)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.70      0.82        10\n",
      "          1       1.00      0.90      0.95        40\n",
      "          2       0.92      0.69      0.79        48\n",
      "          3       0.62      0.50      0.55        16\n",
      "          4       0.68      0.79      0.73        48\n",
      "          5       0.43      0.31      0.36        29\n",
      "          6       0.66      0.84      0.74        81\n",
      "\n",
      "avg / total       0.74      0.73      0.73       272\n",
      "\n",
      "[ 7  0  0  0  0  0  3  0 36  0  0  0  0  4  0  0 33  1  1  3 10  0  0  1\n",
      "  8  1  3  3  0  0  0  0 38  5  5  0  0  0  2  8  9 10  0  0  2  2  8  1\n",
      " 68]\n",
      "svc Accuracy:  0.7316176470588235\n",
      "svc F1:  0.7054622745734996\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.70      0.82        10\n",
      "          1       1.00      0.95      0.97        40\n",
      "          2       0.86      0.79      0.83        48\n",
      "          3       0.80      0.50      0.62        16\n",
      "          4       0.65      0.81      0.72        48\n",
      "          5       0.35      0.21      0.26        29\n",
      "          6       0.73      0.86      0.79        81\n",
      "\n",
      "avg / total       0.75      0.76      0.75       272\n",
      "\n",
      "[ 7  0  2  0  0  0  1  0 38  0  0  0  0  2  0  0 38  0  0  2  8  0  0  0\n",
      "  8  3  3  2  0  0  0  0 39  6  3  0  0  2  1 10  6 10  0  0  2  1  8  0\n",
      " 70]\n",
      "LR Accuracy:  0.7573529411764706\n",
      "LR F1:  0.7162017424924356\n",
      "For name:  j_weber\n",
      "total sample size before apply threshold:  146\n",
      "Counter({'0000-0003-2218-2952': 37, '0000-0002-5493-5886': 34, '0000-0002-2799-7352': 29, '0000-0001-9062-3591': 17, '0000-0002-6835-5065': 16, '0000-0001-5817-0975': 10, '0000-0003-3758-1569': 3})\n",
      "['0000-0002-5493-5886', '0000-0001-9062-3591', '0000-0002-2799-7352', '0000-0001-5817-0975', '0000-0003-2218-2952', '0000-0002-6835-5065']\n",
      "Total sample size after apply threshold:  143\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(143, 434)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(143, 434)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.94      0.97        34\n",
      "          1       1.00      0.35      0.52        17\n",
      "          2       1.00      0.79      0.88        29\n",
      "          3       1.00      0.50      0.67        10\n",
      "          4       0.56      1.00      0.72        37\n",
      "          5       0.73      0.50      0.59        16\n",
      "\n",
      "avg / total       0.86      0.78      0.77       143\n",
      "\n",
      "[32  0  0  0  2  0  0  6  0  0  8  3  0  0 23  0  6  0  0  0  0  5  5  0\n",
      "  0  0  0  0 37  0  0  0  0  0  8  8]\n",
      "svc Accuracy:  0.7762237762237763\n",
      "svc F1:  0.7256262243246906\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.97      0.99        34\n",
      "          1       1.00      0.53      0.69        17\n",
      "          2       1.00      0.83      0.91        29\n",
      "          3       1.00      0.80      0.89        10\n",
      "          4       0.62      1.00      0.76        37\n",
      "          5       0.78      0.44      0.56        16\n",
      "\n",
      "avg / total       0.88      0.83      0.82       143\n",
      "\n",
      "[33  0  0  0  1  0  0  9  0  0  6  2  0  0 24  0  5  0  0  0  0  8  2  0\n",
      "  0  0  0  0 37  0  0  0  0  0  9  7]\n",
      "LR Accuracy:  0.8251748251748252\n",
      "LR F1:  0.799136363893148\n",
      "For name:  c_fox\n",
      "total sample size before apply threshold:  102\n",
      "Counter({'0000-0001-9480-5704': 54, '0000-0002-4644-2619': 35, '0000-0001-6934-3624': 11, '0000-0002-9278-1777': 2})\n",
      "['0000-0001-9480-5704', '0000-0002-4644-2619', '0000-0001-6934-3624']\n",
      "Total sample size after apply threshold:  100\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(100, 292)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(100, 292)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.98      0.91        54\n",
      "          1       0.94      0.83      0.88        35\n",
      "          2       1.00      0.55      0.71        11\n",
      "\n",
      "avg / total       0.89      0.88      0.87       100\n",
      "\n",
      "[53  1  0  6 29  0  4  1  6]\n",
      "svc Accuracy:  0.88\n",
      "svc F1:  0.8302177125706537\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      1.00      0.90        54\n",
      "          1       1.00      0.86      0.92        35\n",
      "          2       1.00      0.36      0.53        11\n",
      "\n",
      "avg / total       0.90      0.88      0.87       100\n",
      "\n",
      "[54  0  0  5 30  0  7  0  4]\n",
      "LR Accuracy:  0.88\n",
      "LR F1:  0.7854700854700855\n",
      "For name:  s_thompson\n",
      "total sample size before apply threshold:  45\n",
      "Counter({'0000-0003-0327-7155': 36, '0000-0003-4784-8386': 3, '0000-0001-9689-1490': 2, '0000-0001-9637-2041': 2, '0000-0002-6847-0397': 1, '0000-0002-0457-6926': 1})\n",
      "['0000-0003-0327-7155']\n",
      "Total sample size after apply threshold:  36\n",
      "For name:  b_choi\n",
      "total sample size before apply threshold:  24\n",
      "Counter({'0000-0002-6090-869X': 12, '0000-0002-8657-7497': 4, '0000-0002-1461-9985': 3, '0000-0002-8381-336X': 1, '0000-0002-1412-9951': 1, '0000-0002-4984-5958': 1, '0000-0002-2950-2069': 1, '0000-0002-6561-8851': 1})\n",
      "['0000-0002-6090-869X']\n",
      "Total sample size after apply threshold:  12\n",
      "For name:  j_schwartz\n",
      "total sample size before apply threshold:  51\n",
      "Counter({'0000-0002-6472-0184': 34, '0000-0001-8239-8857': 10, '0000-0003-2057-1831': 5, '0000-0001-5487-7260': 1, '0000-0001-9636-8181': 1})\n",
      "['0000-0002-6472-0184', '0000-0001-8239-8857']\n",
      "Total sample size after apply threshold:  44\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(44, 144)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(44, 144)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      1.00      0.93        34\n",
      "          1       1.00      0.50      0.67        10\n",
      "\n",
      "avg / total       0.90      0.89      0.87        44\n",
      "\n",
      "[34  0  5  5]\n",
      "svc Accuracy:  0.8863636363636364\n",
      "svc F1:  0.7990867579908676\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      1.00      0.92        34\n",
      "          1       1.00      0.40      0.57        10\n",
      "\n",
      "avg / total       0.88      0.86      0.84        44\n",
      "\n",
      "[34  0  6  4]\n",
      "LR Accuracy:  0.8636363636363636\n",
      "LR F1:  0.7451737451737452\n",
      "For name:  a_brooks\n",
      "total sample size before apply threshold:  185\n",
      "Counter({'0000-0002-4085-9683': 134, '0000-0002-2691-1668': 19, '0000-0003-3551-6982': 14, '0000-0002-8486-4441': 9, '0000-0002-5309-7307': 6, '0000-0003-1334-6230': 3})\n",
      "['0000-0003-3551-6982', '0000-0002-4085-9683', '0000-0002-2691-1668']\n",
      "Total sample size after apply threshold:  167\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(167, 539)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(167, 539)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.71      0.83        14\n",
      "          1       0.95      1.00      0.97       134\n",
      "          2       1.00      0.84      0.91        19\n",
      "\n",
      "avg / total       0.96      0.96      0.96       167\n",
      "\n",
      "[ 10   4   0   0 134   0   0   3  16]\n",
      "svc Accuracy:  0.9580838323353293\n",
      "svc F1:  0.9073881673881674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.50      0.67        14\n",
      "          1       0.92      1.00      0.96       134\n",
      "          2       1.00      0.79      0.88        19\n",
      "\n",
      "avg / total       0.94      0.93      0.93       167\n",
      "\n",
      "[  7   7   0   0 134   0   0   4  15]\n",
      "LR Accuracy:  0.9341317365269461\n",
      "LR F1:  0.8365310281818821\n",
      "For name:  l_rocha\n",
      "total sample size before apply threshold:  81\n",
      "Counter({'0000-0001-9402-887X': 24, '0000-0002-4345-6994': 20, '0000-0002-5469-0911': 11, '0000-0001-7832-058X': 8, '0000-0001-8184-8801': 6, '0000-0003-2146-9708': 5, '0000-0002-7219-1518': 5, '0000-0002-5070-9013': 1, '0000-0002-5190-9279': 1})\n",
      "['0000-0002-5469-0911', '0000-0001-9402-887X', '0000-0002-4345-6994']\n",
      "Total sample size after apply threshold:  55\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(55, 169)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(55, 169)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.73      0.80        11\n",
      "          1       0.80      1.00      0.89        24\n",
      "          2       1.00      0.80      0.89        20\n",
      "\n",
      "avg / total       0.89      0.87      0.87        55\n",
      "\n",
      "[ 8  3  0  0 24  0  1  3 16]\n",
      "svc Accuracy:  0.8727272727272727\n",
      "svc F1:  0.8592592592592593\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.73      0.84        11\n",
      "          1       0.83      1.00      0.91        24\n",
      "          2       1.00      0.90      0.95        20\n",
      "\n",
      "avg / total       0.92      0.91      0.91        55\n",
      "\n",
      "[ 8  3  0  0 24  0  0  2 18]\n",
      "LR Accuracy:  0.9090909090909091\n",
      "LR F1:  0.8983780205230056\n",
      "For name:  s_fleming\n",
      "total sample size before apply threshold:  35\n",
      "Counter({'0000-0002-4095-9924': 12, '0000-0001-7205-2051': 11, '0000-0003-2223-3975': 6, '0000-0002-6242-8083': 4, '0000-0001-5385-7233': 2})\n",
      "['0000-0001-7205-2051', '0000-0002-4095-9924']\n",
      "Total sample size after apply threshold:  23\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(23, 62)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(23, 62)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.64      0.78        11\n",
      "          1       0.75      1.00      0.86        12\n",
      "\n",
      "avg / total       0.87      0.83      0.82        23\n",
      "\n",
      "[ 7  4  0 12]\n",
      "svc Accuracy:  0.8260869565217391\n",
      "svc F1:  0.8174603174603174\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.82      0.90        11\n",
      "          1       0.86      1.00      0.92        12\n",
      "\n",
      "avg / total       0.93      0.91      0.91        23\n",
      "\n",
      "[ 9  2  0 12]\n",
      "LR Accuracy:  0.9130434782608695\n",
      "LR F1:  0.9115384615384615\n",
      "For name:  w_tsai\n",
      "total sample size before apply threshold:  113\n",
      "Counter({'0000-0002-2316-5751': 63, '0000-0003-1332-2650': 26, '0000-0002-9437-5131': 22, '0000-0002-4490-3581': 1, '0000-0002-1123-6954': 1})\n",
      "['0000-0003-1332-2650', '0000-0002-9437-5131', '0000-0002-2316-5751']\n",
      "Total sample size after apply threshold:  111\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(111, 95)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(111, 95)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.69      0.77        26\n",
      "          1       1.00      0.95      0.98        22\n",
      "          2       0.87      0.95      0.91        63\n",
      "\n",
      "avg / total       0.89      0.89      0.89       111\n",
      "\n",
      "[18  0  8  0 21  1  3  0 60]\n",
      "svc Accuracy:  0.8918918918918919\n",
      "svc F1:  0.8839308473153104\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.65      0.74        26\n",
      "          1       1.00      0.91      0.95        22\n",
      "          2       0.85      0.95      0.90        63\n",
      "\n",
      "avg / total       0.88      0.87      0.87       111\n",
      "\n",
      "[17  0  9  0 20  2  3  0 60]\n",
      "LR Accuracy:  0.8738738738738738\n",
      "LR F1:  0.8623445917410875\n",
      "For name:  m_rodriguez\n",
      "total sample size before apply threshold:  214\n",
      "Counter({'0000-0001-6328-6497': 195, '0000-0001-8926-2987': 8, '0000-0002-9380-6614': 4, '0000-0002-4476-004X': 3, '0000-0001-6778-1663': 2, '0000-0002-4452-7627': 1, '0000-0002-2640-5888': 1})\n",
      "['0000-0001-6328-6497']\n",
      "Total sample size after apply threshold:  195\n",
      "For name:  r_miranda\n",
      "total sample size before apply threshold:  81\n",
      "Counter({'0000-0002-8467-5464': 69, '0000-0003-4798-314X': 7, '0000-0002-6551-9677': 4, '0000-0003-3222-7368': 1})\n",
      "['0000-0002-8467-5464']\n",
      "Total sample size after apply threshold:  69\n",
      "For name:  j_richardson\n",
      "total sample size before apply threshold:  84\n",
      "Counter({'0000-0001-6521-610X': 70, '0000-0002-9429-151X': 9, '0000-0003-2733-9736': 3, '0000-0002-8895-8365': 2})\n",
      "['0000-0001-6521-610X']\n",
      "Total sample size after apply threshold:  70\n",
      "For name:  a_chin\n",
      "total sample size before apply threshold:  73\n",
      "Counter({'0000-0002-0417-8653': 50, '0000-0002-0332-0048': 19, '0000-0002-8539-754X': 3, '0000-0003-1813-4042': 1})\n",
      "['0000-0002-0332-0048', '0000-0002-0417-8653']\n",
      "Total sample size after apply threshold:  69\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(69, 140)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(69, 140)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.95      0.97        19\n",
      "          1       0.98      1.00      0.99        50\n",
      "\n",
      "avg / total       0.99      0.99      0.99        69\n",
      "\n",
      "[18  1  0 50]\n",
      "svc Accuracy:  0.9855072463768116\n",
      "svc F1:  0.9815359914369814\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.84      0.91        19\n",
      "          1       0.94      1.00      0.97        50\n",
      "\n",
      "avg / total       0.96      0.96      0.96        69\n",
      "\n",
      "[16  3  0 50]\n",
      "LR Accuracy:  0.9565217391304348\n",
      "LR F1:  0.9425797503467406\n",
      "For name:  h_madsen\n",
      "total sample size before apply threshold:  8\n",
      "Counter({'0000-0002-2498-8709': 3, '0000-0001-7103-5380': 3, '0000-0003-2542-3109': 1, '0000-0002-0612-3437': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  m_ferguson\n",
      "total sample size before apply threshold:  168\n",
      "Counter({'0000-0003-1321-8714': 157, '0000-0003-0760-757X': 9, '0000-0002-7780-6462': 1, '0000-0002-5463-1874': 1})\n",
      "['0000-0003-1321-8714']\n",
      "Total sample size after apply threshold:  157\n",
      "For name:  s_mitra\n",
      "total sample size before apply threshold:  48\n",
      "Counter({'0000-0001-7923-8887': 21, '0000-0001-7620-4809': 11, '0000-0002-0800-4626': 10, '0000-0001-6381-5344': 3, '0000-0003-3526-9942': 1, '0000-0001-5744-3935': 1, '0000-0002-3938-6516': 1})\n",
      "['0000-0001-7620-4809', '0000-0001-7923-8887', '0000-0002-0800-4626']\n",
      "Total sample size after apply threshold:  42\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(42, 1698)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(42, 1698)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      1.00      0.85        11\n",
      "          1       1.00      0.86      0.92        21\n",
      "          2       1.00      0.90      0.95        10\n",
      "\n",
      "avg / total       0.93      0.90      0.91        42\n",
      "\n",
      "[11  0  0  3 18  0  1  0  9]\n",
      "svc Accuracy:  0.9047619047619048\n",
      "svc F1:  0.9055330634278004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.55      0.71        11\n",
      "          1       0.78      1.00      0.88        21\n",
      "          2       1.00      0.90      0.95        10\n",
      "\n",
      "avg / total       0.89      0.86      0.85        42\n",
      "\n",
      "[ 6  5  0  0 21  0  0  1  9]\n",
      "LR Accuracy:  0.8571428571428571\n",
      "LR F1:  0.8427502579979361\n",
      "For name:  v_pinto\n",
      "total sample size before apply threshold:  48\n",
      "Counter({'0000-0002-6600-1781': 29, '0000-0002-1152-1667': 11, '0000-0003-3871-9152': 7, '0000-0003-3395-1251': 1})\n",
      "['0000-0002-6600-1781', '0000-0002-1152-1667']\n",
      "Total sample size after apply threshold:  40\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(40, 122)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(40, 122)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        29\n",
      "          1       1.00      1.00      1.00        11\n",
      "\n",
      "avg / total       1.00      1.00      1.00        40\n",
      "\n",
      "[29  0  0 11]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.98        29\n",
      "          1       1.00      0.91      0.95        11\n",
      "\n",
      "avg / total       0.98      0.97      0.97        40\n",
      "\n",
      "[29  0  1 10]\n",
      "LR Accuracy:  0.975\n",
      "LR F1:  0.9677158999192896\n",
      "For name:  m_field\n",
      "total sample size before apply threshold:  126\n",
      "Counter({'0000-0002-4866-2885': 114, '0000-0003-1310-5074': 6, '0000-0002-8350-417X': 5, '0000-0002-6169-6721': 1})\n",
      "['0000-0002-4866-2885']\n",
      "Total sample size after apply threshold:  114\n",
      "For name:  c_jones\n",
      "total sample size before apply threshold:  354\n",
      "Counter({'0000-0002-0026-9494': 194, '0000-0001-7630-7285': 31, '0000-0001-8594-9554': 28, '0000-0003-0541-0431': 23, '0000-0003-3672-6631': 11, '0000-0002-7249-2580': 11, '0000-0001-9096-9728': 9, '0000-0001-6159-1842': 9, '0000-0003-3430-8110': 7, '0000-0001-6275-0235': 6, '0000-0001-7065-1157': 6, '0000-0003-4680-7080': 4, '0000-0002-1698-0408': 3, '0000-0001-9753-0777': 2, '0000-0002-7196-5825': 2, '0000-0001-6136-7111': 2, '0000-0002-6319-2068': 2, '0000-0002-3708-5859': 1, '0000-0003-1523-2368': 1, '0000-0002-0669-7860': 1, '0000-0002-8765-5178': 1})\n",
      "['0000-0003-0541-0431', '0000-0003-3672-6631', '0000-0001-7630-7285', '0000-0002-7249-2580', '0000-0001-8594-9554', '0000-0002-0026-9494']\n",
      "Total sample size after apply threshold:  298\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(298, 558)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(298, 558)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.91      0.95        23\n",
      "          1       0.85      1.00      0.92        11\n",
      "          2       1.00      0.84      0.91        31\n",
      "          3       1.00      0.82      0.90        11\n",
      "          4       1.00      0.89      0.94        28\n",
      "          5       0.95      0.99      0.97       194\n",
      "\n",
      "avg / total       0.96      0.96      0.96       298\n",
      "\n",
      "[ 21   0   0   0   0   2   0  11   0   0   0   0   0   0  26   0   0   5\n",
      "   0   1   0   9   0   1   0   0   0   0  25   3   0   1   0   0   0 193]\n",
      "svc Accuracy:  0.9563758389261745\n",
      "svc F1:  0.9327897159354595\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.87      0.93        23\n",
      "          1       0.91      0.91      0.91        11\n",
      "          2       1.00      0.65      0.78        31\n",
      "          3       1.00      0.55      0.71        11\n",
      "          4       1.00      0.82      0.90        28\n",
      "          5       0.89      1.00      0.94       194\n",
      "\n",
      "avg / total       0.92      0.92      0.91       298\n",
      "\n",
      "[ 20   0   0   0   0   3   0  10   0   0   0   1   0   0  20   0   0  11\n",
      "   0   1   0   6   0   4   0   0   0   0  23   5   0   0   0   0   0 194]\n",
      "LR Accuracy:  0.9161073825503355\n",
      "LR F1:  0.8622046504651794\n",
      "For name:  k_hong\n",
      "total sample size before apply threshold:  127\n",
      "Counter({'0000-0002-4684-6111': 44, '0000-0002-2852-5111': 29, '0000-0001-7325-1036': 20, '0000-0003-3334-817X': 12, '0000-0002-3489-9056': 11, '0000-0002-8308-585X': 6, '0000-0003-1931-0933': 3, '0000-0002-8299-7128': 1, '0000-0001-6213-1848': 1})\n",
      "['0000-0002-3489-9056', '0000-0002-4684-6111', '0000-0002-2852-5111', '0000-0003-3334-817X', '0000-0001-7325-1036']\n",
      "Total sample size after apply threshold:  116\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(116, 245)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(116, 245)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.91      0.95        11\n",
      "          1       0.80      0.89      0.84        44\n",
      "          2       0.96      0.90      0.93        29\n",
      "          3       0.78      0.58      0.67        12\n",
      "          4       0.76      0.80      0.78        20\n",
      "\n",
      "avg / total       0.85      0.84      0.84       116\n",
      "\n",
      "[10  1  0  0  0  0 39  1  1  3  0  3 26  0  0  0  3  0  7  2  0  3  0  1\n",
      " 16]\n",
      "svc Accuracy:  0.8448275862068966\n",
      "svc F1:  0.8333633059832902\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        11\n",
      "          1       0.91      0.93      0.92        44\n",
      "          2       0.97      1.00      0.98        29\n",
      "          3       0.90      0.75      0.82        12\n",
      "          4       0.80      0.80      0.80        20\n",
      "\n",
      "avg / total       0.91      0.91      0.91       116\n",
      "\n",
      "[11  0  0  0  0  0 41  1  0  2  0  0 29  0  0  0  1  0  9  2  0  3  0  1\n",
      " 16]\n",
      "LR Accuracy:  0.9137931034482759\n",
      "LR F1:  0.9045161960492374\n",
      "For name:  t_williams\n",
      "total sample size before apply threshold:  190\n",
      "Counter({'0000-0003-3414-3440': 78, '0000-0002-5857-3851': 42, '0000-0003-1072-0223': 25, '0000-0001-6299-3747': 24, '0000-0003-1710-3914': 9, '0000-0002-3866-1344': 6, '0000-0003-0072-3316': 3, '0000-0002-9319-1701': 2, '0000-0003-3463-9200': 1})\n",
      "['0000-0001-6299-3747', '0000-0003-1072-0223', '0000-0003-3414-3440', '0000-0002-5857-3851']\n",
      "Total sample size after apply threshold:  169\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(169, 443)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(169, 443)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.71      0.83        24\n",
      "          1       1.00      0.88      0.94        25\n",
      "          2       0.84      1.00      0.91        78\n",
      "          3       1.00      0.88      0.94        42\n",
      "\n",
      "avg / total       0.93      0.91      0.91       169\n",
      "\n",
      "[17  0  7  0  0 22  3  0  0  0 78  0  0  0  5 37]\n",
      "svc Accuracy:  0.9112426035502958\n",
      "svc F1:  0.903607016990691\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.75      0.86        24\n",
      "          1       1.00      0.84      0.91        25\n",
      "          2       0.85      1.00      0.92        78\n",
      "          3       1.00      0.90      0.95        42\n",
      "\n",
      "avg / total       0.93      0.92      0.92       169\n",
      "\n",
      "[18  0  6  0  0 21  4  0  0  0 78  0  0  0  4 38]\n",
      "LR Accuracy:  0.9171597633136095\n",
      "LR F1:  0.9094583485568141\n",
      "For name:  j_xavier\n",
      "total sample size before apply threshold:  22\n",
      "Counter({'0000-0002-0702-6700': 12, '0000-0003-1386-4492': 7, '0000-0002-7836-4598': 2, '0000-0002-9669-8532': 1})\n",
      "['0000-0002-0702-6700']\n",
      "Total sample size after apply threshold:  12\n",
      "For name:  b_bhushan\n",
      "total sample size before apply threshold:  187\n",
      "Counter({'0000-0001-7161-6601': 163, '0000-0002-4182-9478': 19, '0000-0002-1716-9764': 3, '0000-0001-5073-0640': 1, '0000-0001-9244-209X': 1})\n",
      "['0000-0001-7161-6601', '0000-0002-4182-9478']\n",
      "Total sample size after apply threshold:  182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(182, 184)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(182, 184)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.99      0.98       163\n",
      "          1       0.92      0.63      0.75        19\n",
      "\n",
      "avg / total       0.95      0.96      0.95       182\n",
      "\n",
      "[162   1   7  12]\n",
      "svc Accuracy:  0.9560439560439561\n",
      "svc F1:  0.8629518072289156\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      1.00      0.96       163\n",
      "          1       1.00      0.37      0.54        19\n",
      "\n",
      "avg / total       0.94      0.93      0.92       182\n",
      "\n",
      "[163   0  12   7]\n",
      "LR Accuracy:  0.9340659340659341\n",
      "LR F1:  0.7514792899408284\n",
      "For name:  r_ellis\n",
      "total sample size before apply threshold:  176\n",
      "Counter({'0000-0003-4931-752X': 158, '0000-0001-7691-5205': 16, '0000-0002-9755-9913': 1, '0000-0003-2355-5407': 1})\n",
      "['0000-0001-7691-5205', '0000-0003-4931-752X']\n",
      "Total sample size after apply threshold:  174\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(174, 499)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(174, 499)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.88      0.93        16\n",
      "          1       0.99      1.00      0.99       158\n",
      "\n",
      "avg / total       0.99      0.99      0.99       174\n",
      "\n",
      "[ 14   2   0 158]\n",
      "svc Accuracy:  0.9885057471264368\n",
      "svc F1:  0.9635220125786164\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.75      0.86        16\n",
      "          1       0.98      1.00      0.99       158\n",
      "\n",
      "avg / total       0.98      0.98      0.98       174\n",
      "\n",
      "[ 12   4   0 158]\n",
      "LR Accuracy:  0.9770114942528736\n",
      "LR F1:  0.9223214285714285\n",
      "For name:  v_saini\n",
      "total sample size before apply threshold:  18\n",
      "Counter({'0000-0002-0258-2871': 11, '0000-0002-9944-0262': 5, '0000-0003-2734-0120': 1, '0000-0002-6796-5881': 1})\n",
      "['0000-0002-0258-2871']\n",
      "Total sample size after apply threshold:  11\n",
      "For name:  a_ellis\n",
      "total sample size before apply threshold:  168\n",
      "Counter({'0000-0001-7456-9214': 47, '0000-0002-0725-2353': 41, '0000-0002-0417-0547': 40, '0000-0002-0053-5641': 37, '0000-0002-8164-1146': 3})\n",
      "['0000-0002-0417-0547', '0000-0001-7456-9214', '0000-0002-0725-2353', '0000-0002-0053-5641']\n",
      "Total sample size after apply threshold:  165\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(165, 379)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(165, 379)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.82      0.90        40\n",
      "          1       1.00      0.94      0.97        47\n",
      "          2       0.71      1.00      0.83        41\n",
      "          3       0.97      0.78      0.87        37\n",
      "\n",
      "avg / total       0.92      0.89      0.89       165\n",
      "\n",
      "[33  0  6  1  0 44  3  0  0  0 41  0  0  0  8 29]\n",
      "svc Accuracy:  0.8909090909090909\n",
      "svc F1:  0.8912742565369839\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.95      0.97        40\n",
      "          1       1.00      0.96      0.98        47\n",
      "          2       0.77      1.00      0.87        41\n",
      "          3       1.00      0.78      0.88        37\n",
      "\n",
      "avg / total       0.94      0.93      0.93       165\n",
      "\n",
      "[38  0  2  0  0 45  2  0  0  0 41  0  0  0  8 29]\n",
      "LR Accuracy:  0.9272727272727272\n",
      "LR F1:  0.9259370370609964\n",
      "For name:  f_reis\n",
      "total sample size before apply threshold:  222\n",
      "Counter({'0000-0002-9258-7472': 111, '0000-0003-3401-9554': 92, '0000-0002-9159-0530': 12, '0000-0003-1546-963X': 4, '0000-0003-2256-4379': 2, '0000-0002-9471-1174': 1})\n",
      "['0000-0002-9159-0530', '0000-0002-9258-7472', '0000-0003-3401-9554']\n",
      "Total sample size after apply threshold:  215\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(215, 395)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(215, 395)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        12\n",
      "          1       0.95      0.95      0.95       111\n",
      "          2       0.95      0.95      0.95        92\n",
      "\n",
      "avg / total       0.95      0.95      0.95       215\n",
      "\n",
      "[ 12   0   0   0 106   5   0   5  87]\n",
      "svc Accuracy:  0.9534883720930233\n",
      "svc F1:  0.9668690429559995\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.92      0.96        12\n",
      "          1       0.95      0.99      0.97       111\n",
      "          2       0.99      0.95      0.97        92\n",
      "\n",
      "avg / total       0.97      0.97      0.97       215\n",
      "\n",
      "[ 11   1   0   0 110   1   0   5  87]\n",
      "LR Accuracy:  0.9674418604651163\n",
      "LR F1:  0.9641171337972717\n",
      "For name:  j_gray\n",
      "total sample size before apply threshold:  112\n",
      "Counter({'0000-0001-6380-2324': 55, '0000-0003-4146-7902': 24, '0000-0003-2338-0301': 17, '0000-0002-7287-0748': 8, '0000-0001-5863-6835': 3, '0000-0001-9972-5156': 2, '0000-0001-6668-5899': 1, '0000-0003-3554-0499': 1, '0000-0001-8572-0020': 1})\n",
      "['0000-0003-4146-7902', '0000-0001-6380-2324', '0000-0003-2338-0301']\n",
      "Total sample size after apply threshold:  96\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(96, 437)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(96, 437)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.96      0.98        24\n",
      "          1       0.82      1.00      0.90        55\n",
      "          2       1.00      0.35      0.52        17\n",
      "\n",
      "avg / total       0.90      0.88      0.85        96\n",
      "\n",
      "[23  1  0  0 55  0  0 11  6]\n",
      "svc Accuracy:  0.875\n",
      "svc F1:  0.8007006263174657\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.92      0.96        24\n",
      "          1       0.81      1.00      0.89        55\n",
      "          2       1.00      0.35      0.52        17\n",
      "\n",
      "avg / total       0.89      0.86      0.84        96\n",
      "\n",
      "[22  2  0  0 55  0  0 11  6]\n",
      "LR Accuracy:  0.8645833333333334\n",
      "LR F1:  0.7908566042182161\n",
      "For name:  r_hughes\n",
      "total sample size before apply threshold:  57\n",
      "Counter({'0000-0001-9910-6566': 30, '0000-0002-4465-4212': 18, '0000-0002-6307-4432': 7, '0000-0002-2875-2103': 2})\n",
      "['0000-0002-4465-4212', '0000-0001-9910-6566']\n",
      "Total sample size after apply threshold:  48\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(48, 171)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(48, 171)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.94      0.97        18\n",
      "          1       0.97      1.00      0.98        30\n",
      "\n",
      "avg / total       0.98      0.98      0.98        48\n",
      "\n",
      "[17  1  0 30]\n",
      "svc Accuracy:  0.9791666666666666\n",
      "svc F1:  0.9775175644028102\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.94      0.97        18\n",
      "          1       0.97      1.00      0.98        30\n",
      "\n",
      "avg / total       0.98      0.98      0.98        48\n",
      "\n",
      "[17  1  0 30]\n",
      "LR Accuracy:  0.9791666666666666\n",
      "LR F1:  0.9775175644028102\n",
      "For name:  a_green\n",
      "total sample size before apply threshold:  169\n",
      "Counter({'0000-0002-2753-4841': 79, '0000-0002-1268-4951': 39, '0000-0003-2058-1204': 35, '0000-0003-0454-1798': 8, '0000-0002-3674-4242': 4, '0000-0001-7666-5584': 2, '0000-0002-1241-4230': 1, '0000-0003-3404-4995': 1})\n",
      "['0000-0002-1268-4951', '0000-0002-2753-4841', '0000-0003-2058-1204']\n",
      "Total sample size after apply threshold:  153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(153, 338)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(153, 338)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.79      0.89        39\n",
      "          1       0.88      1.00      0.93        79\n",
      "          2       1.00      0.91      0.96        35\n",
      "\n",
      "avg / total       0.94      0.93      0.93       153\n",
      "\n",
      "[31  8  0  0 79  0  0  3 32]\n",
      "svc Accuracy:  0.9281045751633987\n",
      "svc F1:  0.9252831363049504\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.79      0.89        39\n",
      "          1       0.87      1.00      0.93        79\n",
      "          2       1.00      0.89      0.94        35\n",
      "\n",
      "avg / total       0.93      0.92      0.92       153\n",
      "\n",
      "[31  8  0  0 79  0  0  4 31]\n",
      "LR Accuracy:  0.9215686274509803\n",
      "LR F1:  0.9181733299380359\n",
      "For name:  c_reis\n",
      "total sample size before apply threshold:  77\n",
      "Counter({'0000-0002-0286-6639': 54, '0000-0002-1046-4031': 19, '0000-0001-6585-3993': 2, '0000-0002-8193-6964': 2})\n",
      "['0000-0002-0286-6639', '0000-0002-1046-4031']\n",
      "Total sample size after apply threshold:  73\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(73, 264)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(73, 264)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99        54\n",
      "          1       1.00      0.95      0.97        19\n",
      "\n",
      "avg / total       0.99      0.99      0.99        73\n",
      "\n",
      "[54  0  1 18]\n",
      "svc Accuracy:  0.9863013698630136\n",
      "svc F1:  0.9818993305231838\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98        54\n",
      "          1       1.00      0.89      0.94        19\n",
      "\n",
      "avg / total       0.97      0.97      0.97        73\n",
      "\n",
      "[54  0  2 17]\n",
      "LR Accuracy:  0.9726027397260274\n",
      "LR F1:  0.9631313131313131\n",
      "For name:  f_scott\n",
      "total sample size before apply threshold:  26\n",
      "Counter({'0000-0002-6021-0419': 15, '0000-0003-0229-3698': 8, '0000-0003-2041-4641': 3})\n",
      "['0000-0002-6021-0419']\n",
      "Total sample size after apply threshold:  15\n",
      "For name:  l_han\n",
      "total sample size before apply threshold:  20\n",
      "Counter({'0000-0003-4180-1288': 8, '0000-0002-0577-9661': 7, '0000-0002-2955-6307': 2, '0000-0003-3436-2811': 2, '0000-0003-3204-6313': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  c_martins\n",
      "total sample size before apply threshold:  121\n",
      "Counter({'0000-0001-8634-6878': 38, '0000-0001-6600-6163': 18, '0000-0002-7901-7600': 17, '0000-0002-8488-5103': 16, '0000-0003-3506-674X': 11, '0000-0001-8710-1856': 4, '0000-0001-8561-5167': 4, '0000-0002-9335-6027': 3, '0000-0003-4341-1005': 3, '0000-0002-8269-9550': 2, '0000-0001-5953-3758': 2, '0000-0002-4886-9261': 1, '0000-0002-7090-6030': 1, '0000-0003-3634-0624': 1})\n",
      "['0000-0001-8634-6878', '0000-0002-8488-5103', '0000-0003-3506-674X', '0000-0001-6600-6163', '0000-0002-7901-7600']\n",
      "Total sample size after apply threshold:  100\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(100, 290)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(100, 290)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.95      0.95        38\n",
      "          1       1.00      1.00      1.00        16\n",
      "          2       0.82      0.82      0.82        11\n",
      "          3       0.82      1.00      0.90        18\n",
      "          4       1.00      0.76      0.87        17\n",
      "\n",
      "avg / total       0.93      0.92      0.92       100\n",
      "\n",
      "[36  0  1  1  0  0 16  0  0  0  1  0  9  1  0  0  0  0 18  0  1  0  1  2\n",
      " 13]\n",
      "svc Accuracy:  0.92\n",
      "svc F1:  0.9064433811802232\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.97      0.91        38\n",
      "          1       1.00      1.00      1.00        16\n",
      "          2       1.00      0.64      0.78        11\n",
      "          3       0.89      0.89      0.89        18\n",
      "          4       0.94      0.88      0.91        17\n",
      "\n",
      "avg / total       0.92      0.91      0.91       100\n",
      "\n",
      "[37  0  0  1  0  0 16  0  0  0  2  0  7  1  1  2  0  0 16  0  2  0  0  0\n",
      " 15]\n",
      "LR Accuracy:  0.91\n",
      "LR F1:  0.8978675645342312\n",
      "For name:  r_schneider\n",
      "total sample size before apply threshold:  158\n",
      "Counter({'0000-0002-6870-6902': 43, '0000-0003-2228-1248': 42, '0000-0002-2626-3111': 29, '0000-0003-0012-4962': 25, '0000-0003-3279-5365': 7, '0000-0003-1400-8401': 5, '0000-0001-9317-2888': 2, '0000-0002-0329-5778': 2, '0000-0001-9628-0809': 1, '0000-0001-9501-8489': 1, '0000-0001-5496-7020': 1})\n",
      "['0000-0003-0012-4962', '0000-0003-2228-1248', '0000-0002-6870-6902', '0000-0002-2626-3111']\n",
      "Total sample size after apply threshold:  139\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(139, 359)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(139, 359)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.92      0.96        25\n",
      "          1       1.00      0.83      0.91        42\n",
      "          2       1.00      0.79      0.88        43\n",
      "          3       0.62      1.00      0.76        29\n",
      "\n",
      "avg / total       0.92      0.87      0.88       139\n",
      "\n",
      "[23  0  0  2  0 35  0  7  0  0 34  9  0  0  0 29]\n",
      "svc Accuracy:  0.8705035971223022\n",
      "svc F1:  0.8784247550694919\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.96      0.98        25\n",
      "          1       1.00      0.88      0.94        42\n",
      "          2       1.00      0.95      0.98        43\n",
      "          3       0.78      1.00      0.88        29\n",
      "\n",
      "avg / total       0.95      0.94      0.94       139\n",
      "\n",
      "[24  0  0  1  0 37  0  5  0  0 41  2  0  0  0 29]\n",
      "LR Accuracy:  0.9424460431654677\n",
      "LR F1:  0.9428197631181356\n",
      "For name:  j_regan\n",
      "total sample size before apply threshold:  27\n",
      "Counter({'0000-0003-2164-9151': 10, '0000-0001-5816-4516': 9, '0000-0001-9987-7942': 7, '0000-0002-3516-5046': 1})\n",
      "['0000-0003-2164-9151']\n",
      "Total sample size after apply threshold:  10\n",
      "For name:  s_brennan\n",
      "total sample size before apply threshold:  31\n",
      "Counter({'0000-0003-1789-8809': 12, '0000-0003-3601-9769': 12, '0000-0002-8719-4367': 6, '0000-0002-7634-9546': 1})\n",
      "['0000-0003-1789-8809', '0000-0003-3601-9769']\n",
      "Total sample size after apply threshold:  24\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(24, 78)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(24, 78)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.83      0.87        12\n",
      "          1       0.85      0.92      0.88        12\n",
      "\n",
      "avg / total       0.88      0.88      0.87        24\n",
      "\n",
      "[10  2  1 11]\n",
      "svc Accuracy:  0.875\n",
      "svc F1:  0.874782608695652\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.92      0.88        12\n",
      "          1       0.91      0.83      0.87        12\n",
      "\n",
      "avg / total       0.88      0.88      0.87        24\n",
      "\n",
      "[11  1  2 10]\n",
      "LR Accuracy:  0.875\n",
      "LR F1:  0.874782608695652\n",
      "For name:  v_patel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total sample size before apply threshold:  27\n",
      "Counter({'0000-0003-2875-4659': 20, '0000-0001-6616-3628': 4, '0000-0002-8949-139X': 2, '0000-0003-2844-2698': 1})\n",
      "['0000-0003-2875-4659']\n",
      "Total sample size after apply threshold:  20\n",
      "For name:  d_johnston\n",
      "total sample size before apply threshold:  29\n",
      "Counter({'0000-0002-2487-1084': 15, '0000-0003-2424-036X': 7, '0000-0003-0065-1105': 6, '0000-0002-6885-5369': 1})\n",
      "['0000-0002-2487-1084']\n",
      "Total sample size after apply threshold:  15\n",
      "For name:  r_gupta\n",
      "total sample size before apply threshold:  188\n",
      "Counter({'0000-0003-2583-563X': 46, '0000-0002-2984-5010': 36, '0000-0001-6841-6676': 28, '0000-0001-7461-2083': 14, '0000-0002-8328-9780': 9, '0000-0002-5537-8057': 8, '0000-0001-9959-3501': 8, '0000-0001-6819-2748': 6, '0000-0002-6608-9867': 6, '0000-0002-4231-9592': 5, '0000-0001-5755-1143': 5, '0000-0002-6257-1285': 4, '0000-0001-9031-1109': 3, '0000-0002-6585-1725': 3, '0000-0002-1414-2830': 1, '0000-0002-9907-5795': 1, '0000-0002-1334-9186': 1, '0000-0002-8634-7501': 1, '0000-0001-9751-1808': 1, '0000-0003-2958-1526': 1, '0000-0002-3345-7862': 1})\n",
      "['0000-0003-2583-563X', '0000-0001-7461-2083', '0000-0002-2984-5010', '0000-0001-6841-6676']\n",
      "Total sample size after apply threshold:  124\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(124, 521)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(124, 521)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      1.00      0.89        46\n",
      "          1       0.91      0.71      0.80        14\n",
      "          2       1.00      0.83      0.91        36\n",
      "          3       1.00      0.93      0.96        28\n",
      "\n",
      "avg / total       0.92      0.90      0.90       124\n",
      "\n",
      "[46  0  0  0  4 10  0  0  5  1 30  0  2  0  0 26]\n",
      "svc Accuracy:  0.9032258064516129\n",
      "svc F1:  0.8913144388872545\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      1.00      0.88        46\n",
      "          1       1.00      0.57      0.73        14\n",
      "          2       1.00      0.86      0.93        36\n",
      "          3       1.00      0.93      0.96        28\n",
      "\n",
      "avg / total       0.92      0.90      0.89       124\n",
      "\n",
      "[46  0  0  0  6  8  0  0  5  0 31  0  2  0  0 26]\n",
      "LR Accuracy:  0.8951612903225806\n",
      "LR F1:  0.8729498251886312\n",
      "For name:  s_reddy\n",
      "total sample size before apply threshold:  51\n",
      "Counter({'0000-0002-9177-0857': 27, '0000-0002-7362-184X': 13, '0000-0002-1458-6853': 7, '0000-0003-2735-9550': 2, '0000-0002-1924-8976': 1, '0000-0002-4726-5714': 1})\n",
      "['0000-0002-9177-0857', '0000-0002-7362-184X']\n",
      "Total sample size after apply threshold:  40\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(40, 144)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(40, 144)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      1.00      0.92        27\n",
      "          1       1.00      0.62      0.76        13\n",
      "\n",
      "avg / total       0.89      0.88      0.87        40\n",
      "\n",
      "[27  0  5  8]\n",
      "svc Accuracy:  0.875\n",
      "svc F1:  0.8385794995964488\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      1.00      0.90        27\n",
      "          1       1.00      0.54      0.70        13\n",
      "\n",
      "avg / total       0.88      0.85      0.84        40\n",
      "\n",
      "[27  0  6  7]\n",
      "LR Accuracy:  0.85\n",
      "LR F1:  0.8\n",
      "For name:  y_yao\n",
      "total sample size before apply threshold:  58\n",
      "Counter({'0000-0001-5827-8716': 19, '0000-0002-4338-2606': 18, '0000-0002-0814-6675': 11, '0000-0002-2943-5994': 3, '0000-0003-4892-052X': 3, '0000-0003-1132-592X': 1, '0000-0001-6502-6226': 1, '0000-0001-9359-2030': 1, '0000-0003-3612-3742': 1})\n",
      "['0000-0002-4338-2606', '0000-0001-5827-8716', '0000-0002-0814-6675']\n",
      "Total sample size after apply threshold:  48\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(48, 65)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(48, 65)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.94      0.85        18\n",
      "          1       1.00      0.89      0.94        19\n",
      "          2       0.78      0.64      0.70        11\n",
      "\n",
      "avg / total       0.86      0.85      0.85        48\n",
      "\n",
      "[17  0  1  1 17  1  4  0  7]\n",
      "svc Accuracy:  0.8541666666666666\n",
      "svc F1:  0.8314814814814815\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.94      0.89        18\n",
      "          1       0.90      1.00      0.95        19\n",
      "          2       0.86      0.55      0.67        11\n",
      "\n",
      "avg / total       0.87      0.88      0.86        48\n",
      "\n",
      "[17  0  1  0 19  0  3  2  6]\n",
      "LR Accuracy:  0.875\n",
      "LR F1:  0.8371345029239766\n",
      "For name:  a_huang\n",
      "total sample size before apply threshold:  58\n",
      "Counter({'0000-0002-5701-4521': 38, '0000-0002-5037-6829': 16, '0000-0002-7848-6755': 2, '0000-0003-1939-9149': 1, '0000-0003-1997-1758': 1})\n",
      "['0000-0002-5701-4521', '0000-0002-5037-6829']\n",
      "Total sample size after apply threshold:  54\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(54, 193)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(54, 193)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      1.00      0.96        38\n",
      "          1       1.00      0.81      0.90        16\n",
      "\n",
      "avg / total       0.95      0.94      0.94        54\n",
      "\n",
      "[38  0  3 13]\n",
      "svc Accuracy:  0.9444444444444444\n",
      "svc F1:  0.9292885202968137\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      1.00      0.94        38\n",
      "          1       1.00      0.69      0.81        16\n",
      "\n",
      "avg / total       0.92      0.91      0.90        54\n",
      "\n",
      "[38  0  5 11]\n",
      "LR Accuracy:  0.9074074074074074\n",
      "LR F1:  0.8765432098765432\n",
      "For name:  d_ghosh\n",
      "total sample size before apply threshold:  23\n",
      "Counter({'0000-0002-6571-304X': 9, '0000-0003-0256-1998': 6, '0000-0003-3266-9262': 6, '0000-0001-9691-1498': 1, '0000-0001-8222-5737': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  r_morgan\n",
      "total sample size before apply threshold:  15\n",
      "Counter({'0000-0003-1664-5316': 8, '0000-0003-0194-0304': 4, '0000-0002-3881-7257': 1, '0000-0002-2842-4441': 1, '0000-0002-7060-7735': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  q_li\n",
      "total sample size before apply threshold:  227\n",
      "Counter({'0000-0002-5941-1985': 36, '0000-0001-9565-0855': 33, '0000-0002-5993-0312': 20, '0000-0002-3934-6004': 20, '0000-0002-3876-0617': 15, '0000-0002-8447-9770': 14, '0000-0001-5015-0750': 14, '0000-0002-2450-3628': 11, '0000-0002-7280-5508': 8, '0000-0002-1644-0508': 7, '0000-0002-5143-6678': 6, '0000-0002-2870-4101': 5, '0000-0002-9522-073X': 5, '0000-0003-3370-471X': 5, '0000-0002-8792-0592': 4, '0000-0002-4822-2863': 3, '0000-0002-7724-1289': 3, '0000-0002-1838-3885': 2, '0000-0002-9128-7005': 2, '0000-0001-5699-9843': 2, '0000-0001-9365-3788': 2, '0000-0002-1116-7215': 2, '0000-0002-1990-8233': 2, '0000-0002-0105-4030': 1, '0000-0002-8633-8859': 1, '0000-0003-0463-1590': 1, '0000-0001-7526-2425': 1, '0000-0002-9023-6185': 1, '0000-0003-3752-4716': 1})\n",
      "['0000-0002-2450-3628', '0000-0002-3876-0617', '0000-0002-8447-9770', '0000-0002-5941-1985', '0000-0002-5993-0312', '0000-0001-5015-0750', '0000-0001-9565-0855', '0000-0002-3934-6004']\n",
      "Total sample size after apply threshold:  163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(163, 448)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(163, 448)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.64      0.82      0.72        11\n",
      "          1       1.00      0.13      0.24        15\n",
      "          2       1.00      0.71      0.83        14\n",
      "          3       0.73      0.92      0.81        36\n",
      "          4       0.89      0.80      0.84        20\n",
      "          5       0.92      0.86      0.89        14\n",
      "          6       0.94      0.94      0.94        33\n",
      "          7       0.54      0.75      0.63        20\n",
      "\n",
      "avg / total       0.83      0.79      0.77       163\n",
      "\n",
      "[ 9  0  0  0  0  0  0  2  0  2  0  6  0  0  1  6  0  0 10  2  1  0  0  1\n",
      "  0  0  0 33  1  1  0  1  1  0  0  1 16  0  0  2  1  0  0  0  0 12  1  0\n",
      "  1  0  0  0  0  0 31  1  2  0  0  3  0  0  0 15]\n",
      "svc Accuracy:  0.7852760736196319\n",
      "svc F1:  0.7373537946544912\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.82      0.86        11\n",
      "          1       1.00      0.13      0.24        15\n",
      "          2       1.00      0.64      0.78        14\n",
      "          3       0.72      0.94      0.82        36\n",
      "          4       0.81      0.85      0.83        20\n",
      "          5       1.00      0.79      0.88        14\n",
      "          6       0.87      1.00      0.93        33\n",
      "          7       0.60      0.75      0.67        20\n",
      "\n",
      "avg / total       0.83      0.80      0.77       163\n",
      "\n",
      "[ 9  0  0  0  0  0  0  2  0  2  0  6  0  0  1  6  0  0  9  2  2  0  1  0\n",
      "  0  0  0 34  1  0  1  0  0  0  0  1 17  0  0  2  0  0  0  1  1 11  1  0\n",
      "  0  0  0  0  0  0 33  0  1  0  0  3  0  0  1 15]\n",
      "LR Accuracy:  0.7975460122699386\n",
      "LR F1:  0.7499794003767688\n",
      "For name:  w_wang\n",
      "total sample size before apply threshold:  765\n",
      "Counter({'0000-0001-9033-0158': 194, '0000-0002-1430-1360': 101, '0000-0001-9093-412X': 39, '0000-0003-0509-2605': 30, '0000-0001-5983-3937': 29, '0000-0002-5369-5446': 28, '0000-0003-4287-1704': 27, '0000-0003-4053-5088': 24, '0000-0001-6022-1567': 21, '0000-0002-4309-9077': 19, '0000-0002-9852-1589': 19, '0000-0003-3987-9270': 16, '0000-0002-1935-6301': 15, '0000-0001-9208-7569': 14, '0000-0003-4163-3173': 14, '0000-0002-5257-7675': 13, '0000-0002-6652-5964': 13, '0000-0002-2269-1952': 13, '0000-0002-4628-1755': 12, '0000-0002-5943-4589': 11, '0000-0003-1319-1988': 10, '0000-0003-3007-1750': 9, '0000-0002-3780-5158': 8, '0000-0002-1083-6720': 7, '0000-0002-7762-7560': 6, '0000-0001-6587-8859': 5, '0000-0002-8330-9913': 5, '0000-0001-7496-4548': 5, '0000-0001-9568-3876': 4, '0000-0002-1260-2098': 4, '0000-0001-8947-4867': 4, '0000-0002-6154-7750': 4, '0000-0001-6109-1645': 4, '0000-0003-1567-2371': 4, '0000-0003-1788-2727': 4, '0000-0003-3941-4860': 4, '0000-0002-8814-525X': 3, '0000-0002-9303-516X': 3, '0000-0002-9865-6811': 2, '0000-0002-3245-9049': 2, '0000-0001-9223-6472': 2, '0000-0001-6818-7711': 2, '0000-0002-2468-5222': 2, '0000-0003-1930-4891': 1, '0000-0002-3116-5954': 1, '0000-0002-1225-4011': 1, '0000-0003-4712-3692': 1, '0000-0001-7511-3497': 1, '0000-0002-3709-021X': 1, '0000-0002-2914-1638': 1, '0000-0003-4426-019X': 1, '0000-0002-6983-2548': 1, '0000-0002-5812-6744': 1})\n",
      "['0000-0002-5369-5446', '0000-0002-4309-9077', '0000-0003-1319-1988', '0000-0003-3987-9270', '0000-0002-4628-1755', '0000-0001-5983-3937', '0000-0002-5257-7675', '0000-0001-9033-0158', '0000-0001-9093-412X', '0000-0003-0509-2605', '0000-0001-9208-7569', '0000-0003-4287-1704', '0000-0002-1935-6301', '0000-0002-1430-1360', '0000-0001-6022-1567', '0000-0002-6652-5964', '0000-0002-9852-1589', '0000-0003-4053-5088', '0000-0002-2269-1952', '0000-0002-5943-4589', '0000-0003-4163-3173']\n",
      "Total sample size after apply threshold:  662\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(662, 1007)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(662, 1007)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.71      0.83        28\n",
      "          1       0.54      0.37      0.44        19\n",
      "          2       0.46      0.60      0.52        10\n",
      "          3       1.00      0.94      0.97        16\n",
      "          4       0.71      0.83      0.77        12\n",
      "          5       0.61      0.76      0.68        29\n",
      "          6       0.50      0.46      0.48        13\n",
      "          7       0.68      0.98      0.81       194\n",
      "          8       0.90      0.67      0.76        39\n",
      "          9       1.00      0.60      0.75        30\n",
      "         10       1.00      0.71      0.83        14\n",
      "         11       1.00      0.81      0.90        27\n",
      "         12       1.00      0.73      0.85        15\n",
      "         13       0.84      0.67      0.75       101\n",
      "         14       0.79      0.52      0.63        21\n",
      "         15       0.60      0.23      0.33        13\n",
      "         16       1.00      0.68      0.81        19\n",
      "         17       0.67      0.58      0.62        24\n",
      "         18       0.60      0.69      0.64        13\n",
      "         19       0.91      0.91      0.91        11\n",
      "         20       1.00      0.79      0.88        14\n",
      "\n",
      "avg / total       0.79      0.76      0.75       662\n",
      "\n",
      "[ 20   0   0   0   0   0   0   7   1   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   7   0   0   0   1   0   8   1   0   0   0   0   1   0\n",
      "   0   0   1   0   0   0   0   0   6   0   0   0   0   2   0   0   0   0\n",
      "   0   2   0   0   0   0   0   0   0   0   0   0  15   0   1   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  10   0\n",
      "   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0\n",
      "   0   0  22   0   6   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   1   0   6   5   0   0   0   0   0   1   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   2   2 190   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   2   0   0   0   4   0   5  26   0   0   0\n",
      "   0   1   1   0   0   0   0   0   0   0   0   1   0   0   2   0   6   0\n",
      "  18   0   0   0   1   0   0   0   1   0   1   0   0   0   0   0   0   1\n",
      "   0   2   0   0  10   0   0   0   0   0   0   1   0   0   0   0   1   1\n",
      "   0   0   1   0   2   0   0   0  22   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   4   0   0   0   0  11   0   0   0   0   0\n",
      "   0   0   0   0   1   4   0   3   1   2  19   0   0   0   0   0  68   0\n",
      "   0   0   2   1   0   0   0   0   0   0   0   0   0   7   0   0   0   0\n",
      "   0   0  11   0   0   1   2   0   0   0   0   0   0   0   1   1   3   1\n",
      "   0   0   0   0   3   0   3   0   1   0   0   0   0   0   0   0   0   0\n",
      "   0   4   0   0   0   0   0   2   0   0  13   0   0   0   0   0   0   1\n",
      "   0   0   0   0   4   0   0   0   0   0   2   0   1   0  14   2   0   0\n",
      "   0   1   0   0   0   0   0   0   0   0   0   0   0   0   2   1   0   0\n",
      "   9   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0\n",
      "   0   0   0   0  10   0   0   0   0   0   0   0   0   2   0   0   0   0\n",
      "   0   0   0   0   0   0   1   0  11]\n",
      "svc Accuracy:  0.7583081570996979\n",
      "svc F1:  0.7218825247623769\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.50      0.67        28\n",
      "          1       0.80      0.21      0.33        19\n",
      "          2       0.33      0.10      0.15        10\n",
      "          3       1.00      0.56      0.72        16\n",
      "          4       1.00      0.58      0.74        12\n",
      "          5       0.73      0.76      0.75        29\n",
      "          6       1.00      0.31      0.47        13\n",
      "          7       0.59      0.99      0.74       194\n",
      "          8       0.88      0.54      0.67        39\n",
      "          9       1.00      0.60      0.75        30\n",
      "         10       1.00      0.43      0.60        14\n",
      "         11       1.00      0.67      0.80        27\n",
      "         12       1.00      0.73      0.85        15\n",
      "         13       0.72      0.75      0.74       101\n",
      "         14       0.83      0.48      0.61        21\n",
      "         15       1.00      0.31      0.47        13\n",
      "         16       0.87      0.68      0.76        19\n",
      "         17       0.77      0.71      0.74        24\n",
      "         18       0.71      0.38      0.50        13\n",
      "         19       1.00      0.91      0.95        11\n",
      "         20       1.00      0.71      0.83        14\n",
      "\n",
      "avg / total       0.78      0.71      0.70       662\n",
      "\n",
      "[ 14   0   0   0   0   0   0  14   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   4   0   0   0   2   0  11   0   0   0   0   0   1   0\n",
      "   0   0   1   0   0   0   0   0   1   0   0   0   0   6   0   0   0   0\n",
      "   0   2   0   0   0   1   0   0   0   0   0   0   9   0   1   0   6   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   7   0\n",
      "   0   2   0   0   0   0   0   3   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0  22   0   5   0   0   0   0   0   2   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   4   6   0   0   0   0   0   3   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0 193   0   0   0   0   0   1   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   2   0  13  21   0   0   0\n",
      "   0   3   0   0   0   0   0   0   0   0   0   0   0   0   1   0   8   0\n",
      "  18   0   0   0   2   0   0   0   1   0   0   0   0   0   0   0   0   1\n",
      "   0   5   0   0   6   0   0   1   0   0   0   1   0   0   0   0   0   1\n",
      "   0   0   0   0   8   0   0   0  18   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   4   0   0   0   0  11   0   0   0   0   0\n",
      "   0   0   0   0   0   1   0   0   0   0  22   1   0   0   0   0  76   0\n",
      "   0   1   0   0   0   0   0   0   0   0   0   0   0   9   0   0   0   0\n",
      "   0   0  10   0   0   0   2   0   0   0   0   0   0   0   0   0   3   1\n",
      "   0   0   0   0   4   0   4   0   1   0   0   0   0   0   0   0   0   0\n",
      "   0   4   0   0   0   0   0   2   0   0  13   0   0   0   0   0   0   0\n",
      "   0   0   0   0   3   1   0   0   0   0   2   0   0   1  17   0   0   0\n",
      "   0   1   0   0   0   0   0   2   0   0   0   0   0   3   2   0   0   0\n",
      "   5   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0\n",
      "   0   0   0   0  10   0   0   0   0   0   0   1   0   3   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0  10]\n",
      "LR Accuracy:  0.7145015105740181\n",
      "LR F1:  0.6587327067854943\n",
      "For name:  r_ross\n",
      "total sample size before apply threshold:  374\n",
      "Counter({'0000-0003-4876-8839': 356, '0000-0002-3987-881X': 17, '0000-0002-7825-9974': 1})\n",
      "['0000-0003-4876-8839', '0000-0002-3987-881X']\n",
      "Total sample size after apply threshold:  373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(373, 428)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(373, 428)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       356\n",
      "          1       1.00      0.94      0.97        17\n",
      "\n",
      "avg / total       1.00      1.00      1.00       373\n",
      "\n",
      "[356   0   1  16]\n",
      "svc Accuracy:  0.9973190348525469\n",
      "svc F1:  0.9841472225763951\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99       356\n",
      "          1       1.00      0.76      0.87        17\n",
      "\n",
      "avg / total       0.99      0.99      0.99       373\n",
      "\n",
      "[356   0   4  13]\n",
      "LR Accuracy:  0.9892761394101877\n",
      "LR F1:  0.9305400372439478\n",
      "For name:  k_yamamoto\n",
      "total sample size before apply threshold:  106\n",
      "Counter({'0000-0002-7935-7015': 93, '0000-0003-0866-3207': 4, '0000-0002-7590-3568': 4, '0000-0001-6642-7961': 2, '0000-0002-6831-5346': 2, '0000-0002-1619-4407': 1})\n",
      "['0000-0002-7935-7015']\n",
      "Total sample size after apply threshold:  93\n",
      "For name:  j_silva\n",
      "total sample size before apply threshold:  268\n",
      "Counter({'0000-0001-9523-9441': 128, '0000-0003-3977-7418': 28, '0000-0002-3696-3955': 22, '0000-0002-6725-5767': 14, '0000-0003-2583-9518': 13, '0000-0001-9959-4272': 8, '0000-0002-5656-0897': 7, '0000-0001-9487-4259': 6, '0000-0002-6041-1763': 6, '0000-0002-1520-0799': 4, '0000-0001-8055-8925': 3, '0000-0001-9708-1043': 3, '0000-0002-7268-6465': 3, '0000-0003-1224-1699': 2, '0000-0002-7211-1661': 2, '0000-0002-7206-0550': 2, '0000-0003-1244-6483': 2, '0000-0003-4180-565X': 2, '0000-0001-9522-6181': 2, '0000-0001-9554-8797': 2, '0000-0002-6455-9618': 1, '0000-0003-2863-8068': 1, '0000-0003-2318-3893': 1, '0000-0002-9230-7135': 1, '0000-0003-4773-6771': 1, '0000-0002-1134-1252': 1, '0000-0002-1956-5779': 1, '0000-0003-1778-3833': 1, '0000-0001-5722-0213': 1})\n",
      "['0000-0002-6725-5767', '0000-0003-2583-9518', '0000-0003-3977-7418', '0000-0002-3696-3955', '0000-0001-9523-9441']\n",
      "Total sample size after apply threshold:  205\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(205, 355)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(205, 355)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.86      0.92        14\n",
      "          1       1.00      0.77      0.87        13\n",
      "          2       0.96      0.96      0.96        28\n",
      "          3       0.82      0.64      0.72        22\n",
      "          4       0.91      0.98      0.95       128\n",
      "\n",
      "avg / total       0.92      0.92      0.92       205\n",
      "\n",
      "[ 12   0   0   1   1   0  10   0   1   2   0   0  27   0   1   0   0   0\n",
      "  14   8   0   0   1   1 126]\n",
      "svc Accuracy:  0.9219512195121952\n",
      "svc F1:  0.8844489987510583\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.93      0.96        14\n",
      "          1       1.00      0.85      0.92        13\n",
      "          2       1.00      0.96      0.98        28\n",
      "          3       1.00      0.50      0.67        22\n",
      "          4       0.90      1.00      0.94       128\n",
      "\n",
      "avg / total       0.93      0.93      0.92       205\n",
      "\n",
      "[ 13   0   0   0   1   0  11   0   0   2   0   0  27   0   1   0   0   0\n",
      "  11  11   0   0   0   0 128]\n",
      "LR Accuracy:  0.926829268292683\n",
      "LR F1:  0.8945527849217886\n",
      "For name:  m_pellegrini\n",
      "total sample size before apply threshold:  64\n",
      "Counter({'0000-0003-3817-4412': 31, '0000-0001-8505-9260': 15, '0000-0001-9355-9564': 14, '0000-0003-4878-4505': 3, '0000-0003-3527-9542': 1})\n",
      "['0000-0001-9355-9564', '0000-0003-3817-4412', '0000-0001-8505-9260']\n",
      "Total sample size after apply threshold:  60\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(60, 249)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(60, 249)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.48      0.79      0.59        14\n",
      "          1       0.96      0.87      0.92        31\n",
      "          2       0.78      0.47      0.58        15\n",
      "\n",
      "avg / total       0.80      0.75      0.76        60\n",
      "\n",
      "[11  1  2  4 27  0  8  0  7]\n",
      "svc Accuracy:  0.75\n",
      "svc F1:  0.6977273884053545\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.57      0.70        14\n",
      "          1       0.76      1.00      0.86        31\n",
      "          2       0.90      0.60      0.72        15\n",
      "\n",
      "avg / total       0.82      0.80      0.79        60\n",
      "\n",
      "[ 8  5  1  0 31  0  1  5  9]\n",
      "LR Accuracy:  0.8\n",
      "LR F1:  0.7589210950080516\n",
      "For name:  s_kwon\n",
      "total sample size before apply threshold:  51\n",
      "Counter({'0000-0002-8490-9101': 17, '0000-0002-0679-1523': 15, '0000-0002-1857-3515': 9, '0000-0002-8215-442X': 5, '0000-0001-5265-862X': 1, '0000-0002-9121-3954': 1, '0000-0003-0249-4190': 1, '0000-0001-9287-4490': 1, '0000-0003-1147-8037': 1})\n",
      "['0000-0002-0679-1523', '0000-0002-8490-9101']\n",
      "Total sample size after apply threshold:  32\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(32, 87)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(32, 87)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.93      0.93        15\n",
      "          1       0.94      0.94      0.94        17\n",
      "\n",
      "avg / total       0.94      0.94      0.94        32\n",
      "\n",
      "[14  1  1 16]\n",
      "svc Accuracy:  0.9375\n",
      "svc F1:  0.9372549019607843\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.93      0.93        15\n",
      "          1       0.94      0.94      0.94        17\n",
      "\n",
      "avg / total       0.94      0.94      0.94        32\n",
      "\n",
      "[14  1  1 16]\n",
      "LR Accuracy:  0.9375\n",
      "LR F1:  0.9372549019607843\n",
      "For name:  m_correa\n",
      "total sample size before apply threshold:  72\n",
      "Counter({'0000-0002-9868-3131': 63, '0000-0003-4856-7578': 7, '0000-0001-8464-2673': 1, '0000-0003-4985-3330': 1})\n",
      "['0000-0002-9868-3131']\n",
      "Total sample size after apply threshold:  63\n",
      "For name:  a_pal\n",
      "total sample size before apply threshold:  14\n",
      "Counter({'0000-0003-3893-1305': 6, '0000-0002-0850-3118': 4, '0000-0001-5425-5218': 3, '0000-0001-7615-7811': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  v_costa\n",
      "total sample size before apply threshold:  141\n",
      "Counter({'0000-0002-0471-2756': 33, '0000-0002-7868-4663': 32, '0000-0002-7294-6933': 27, '0000-0002-2113-7482': 18, '0000-0002-5412-8945': 18, '0000-0001-8188-831X': 7, '0000-0002-1513-0284': 2, '0000-0001-5786-633X': 2, '0000-0003-0122-3567': 1, '0000-0001-8801-5669': 1})\n",
      "['0000-0002-2113-7482', '0000-0002-7294-6933', '0000-0002-5412-8945', '0000-0002-0471-2756', '0000-0002-7868-4663']\n",
      "Total sample size after apply threshold:  128\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(128, 329)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(128, 329)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        18\n",
      "          1       1.00      0.89      0.94        27\n",
      "          2       1.00      0.83      0.91        18\n",
      "          3       1.00      1.00      1.00        33\n",
      "          4       0.84      1.00      0.91        32\n",
      "\n",
      "avg / total       0.96      0.95      0.95       128\n",
      "\n",
      "[18  0  0  0  0  0 24  0  0  3  0  0 15  0  3  0  0  0 33  0  0  0  0  0\n",
      " 32]\n",
      "svc Accuracy:  0.953125\n",
      "svc F1:  0.9529106187929717\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        18\n",
      "          1       1.00      0.93      0.96        27\n",
      "          2       1.00      0.94      0.97        18\n",
      "          3       0.94      1.00      0.97        33\n",
      "          4       0.91      0.94      0.92        32\n",
      "\n",
      "avg / total       0.96      0.96      0.96       128\n",
      "\n",
      "[18  0  0  0  0  0 25  0  0  2  0  0 17  0  1  0  0  0 33  0  0  0  0  2\n",
      " 30]\n",
      "LR Accuracy:  0.9609375\n",
      "LR F1:  0.9653264382676147\n",
      "For name:  j_allen\n",
      "total sample size before apply threshold:  111\n",
      "Counter({'0000-0001-5219-4423': 36, '0000-0002-3829-066X': 27, '0000-0001-9974-4226': 12, '0000-0003-3566-3747': 12, '0000-0003-4740-9404': 11, '0000-0002-3894-4854': 5, '0000-0002-6576-2132': 3, '0000-0002-0950-0429': 3, '0000-0002-3084-7785': 1, '0000-0002-6717-8693': 1})\n",
      "['0000-0002-3829-066X', '0000-0003-4740-9404', '0000-0001-9974-4226', '0000-0003-3566-3747', '0000-0001-5219-4423']\n",
      "Total sample size after apply threshold:  98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(98, 377)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(98, 377)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.68      0.78      0.72        27\n",
      "          1       1.00      0.91      0.95        11\n",
      "          2       1.00      1.00      1.00        12\n",
      "          3       1.00      0.58      0.74        12\n",
      "          4       0.79      0.83      0.81        36\n",
      "\n",
      "avg / total       0.83      0.82      0.82        98\n",
      "\n",
      "[21  0  0  0  6  1 10  0  0  0  0  0 12  0  0  3  0  0  7  2  6  0  0  0\n",
      " 30]\n",
      "svc Accuracy:  0.8163265306122449\n",
      "svc F1:  0.8448343598978807\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.85      0.90        27\n",
      "          1       1.00      0.91      0.95        11\n",
      "          2       1.00      1.00      1.00        12\n",
      "          3       1.00      0.58      0.74        12\n",
      "          4       0.78      0.97      0.86        36\n",
      "\n",
      "avg / total       0.91      0.89      0.89        98\n",
      "\n",
      "[23  0  0  0  4  0 10  0  0  1  0  0 12  0  0  0  0  0  7  5  1  0  0  0\n",
      " 35]\n",
      "LR Accuracy:  0.8877551020408163\n",
      "LR F1:  0.8910762745644067\n",
      "For name:  y_dong\n",
      "total sample size before apply threshold:  76\n",
      "Counter({'0000-0003-0016-9028': 42, '0000-0002-1737-6536': 16, '0000-0003-4550-2322': 9, '0000-0003-1294-4888': 4, '0000-0002-4129-4637': 2, '0000-0001-8595-2868': 2, '0000-0003-1774-1553': 1})\n",
      "['0000-0002-1737-6536', '0000-0003-0016-9028']\n",
      "Total sample size after apply threshold:  58\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(58, 192)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(58, 192)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.56      0.72        16\n",
      "          1       0.86      1.00      0.92        42\n",
      "\n",
      "avg / total       0.90      0.88      0.87        58\n",
      "\n",
      "[ 9  7  0 42]\n",
      "svc Accuracy:  0.8793103448275862\n",
      "svc F1:  0.8215384615384616\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.69      0.81        16\n",
      "          1       0.89      1.00      0.94        42\n",
      "\n",
      "avg / total       0.92      0.91      0.91        58\n",
      "\n",
      "[11  5  0 42]\n",
      "LR Accuracy:  0.9137931034482759\n",
      "LR F1:  0.8793175197669579\n",
      "For name:  m_fitzgerald\n",
      "total sample size before apply threshold:  133\n",
      "Counter({'0000-0003-0183-7761': 81, '0000-0002-4823-8179': 38, '0000-0002-4535-0966': 10, '0000-0002-0176-8973': 4})\n",
      "['0000-0002-4535-0966', '0000-0002-4823-8179', '0000-0003-0183-7761']\n",
      "Total sample size after apply threshold:  129\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(129, 364)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(129, 364)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.80      0.89        10\n",
      "          1       0.97      0.89      0.93        38\n",
      "          2       0.93      0.99      0.96        81\n",
      "\n",
      "avg / total       0.95      0.95      0.94       129\n",
      "\n",
      "[ 8  0  2  0 34  4  0  1 80]\n",
      "svc Accuracy:  0.9457364341085271\n",
      "svc F1:  0.926159856846429\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.80      0.89        10\n",
      "          1       1.00      0.87      0.93        38\n",
      "          2       0.92      1.00      0.96        81\n",
      "\n",
      "avg / total       0.95      0.95      0.94       129\n",
      "\n",
      "[ 8  0  2  0 33  5  0  0 81]\n",
      "LR Accuracy:  0.9457364341085271\n",
      "LR F1:  0.9256820784448087\n",
      "For name:  m_ferreira\n",
      "total sample size before apply threshold:  253\n",
      "Counter({'0000-0002-5293-9090': 80, '0000-0002-6814-6773': 33, '0000-0002-9459-8167': 23, '0000-0001-8362-0819': 16, '0000-0003-2098-066X': 15, '0000-0002-0856-9811': 14, '0000-0002-0075-2400': 10, '0000-0002-2071-9851': 10, '0000-0002-8452-2222': 10, '0000-0002-7909-637X': 6, '0000-0001-6789-3796': 5, '0000-0001-8962-7157': 4, '0000-0001-9609-5099': 4, '0000-0003-1137-9776': 4, '0000-0001-6475-9401': 3, '0000-0001-8316-7022': 3, '0000-0002-3740-6069': 3, '0000-0001-5586-6328': 2, '0000-0002-4294-7003': 2, '0000-0002-4741-8661': 2, '0000-0002-0595-620X': 1, '0000-0002-0159-7422': 1, '0000-0002-2437-7780': 1, '0000-0003-0570-6259': 1})\n",
      "['0000-0002-0075-2400', '0000-0003-2098-066X', '0000-0002-2071-9851', '0000-0002-5293-9090', '0000-0002-0856-9811', '0000-0001-8362-0819', '0000-0002-9459-8167', '0000-0002-6814-6773', '0000-0002-8452-2222']\n",
      "Total sample size after apply threshold:  211\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(211, 625)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(211, 625)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.70      0.78        10\n",
      "          1       1.00      0.93      0.97        15\n",
      "          2       1.00      0.80      0.89        10\n",
      "          3       0.74      1.00      0.85        80\n",
      "          4       0.88      0.50      0.64        14\n",
      "          5       1.00      0.81      0.90        16\n",
      "          6       1.00      0.87      0.93        23\n",
      "          7       1.00      0.73      0.84        33\n",
      "          8       1.00      0.80      0.89        10\n",
      "\n",
      "avg / total       0.89      0.86      0.86       211\n",
      "\n",
      "[ 7  0  0  3  0  0  0  0  0  1 14  0  0  0  0  0  0  0  0  0  8  2  0  0\n",
      "  0  0  0  0  0  0 80  0  0  0  0  0  0  0  0  7  7  0  0  0  0  0  0  0\n",
      "  2  1 13  0  0  0  0  0  0  3  0  0 20  0  0  0  0  0  9  0  0  0 24  0\n",
      "  0  0  0  2  0  0  0  0  8]\n",
      "svc Accuracy:  0.8578199052132701\n",
      "svc F1:  0.8530433120578996\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.70      0.78        10\n",
      "          1       1.00      0.93      0.97        15\n",
      "          2       1.00      0.90      0.95        10\n",
      "          3       0.73      1.00      0.84        80\n",
      "          4       1.00      0.21      0.35        14\n",
      "          5       1.00      0.81      0.90        16\n",
      "          6       1.00      0.87      0.93        23\n",
      "          7       0.96      0.76      0.85        33\n",
      "          8       1.00      0.80      0.89        10\n",
      "\n",
      "avg / total       0.88      0.85      0.84       211\n",
      "\n",
      "[ 7  0  0  3  0  0  0  0  0  1 14  0  0  0  0  0  0  0  0  0  9  1  0  0\n",
      "  0  0  0  0  0  0 80  0  0  0  0  0  0  0  0 11  3  0  0  0  0  0  0  0\n",
      "  2  0 13  0  1  0  0  0  0  3  0  0 20  0  0  0  0  0  8  0  0  0 25  0\n",
      "  0  0  0  2  0  0  0  0  8]\n",
      "LR Accuracy:  0.8483412322274881\n",
      "LR F1:  0.8276489642359114\n",
      "For name:  m_roberts\n",
      "total sample size before apply threshold:  320\n",
      "Counter({'0000-0003-3894-5301': 251, '0000-0002-1441-7363': 26, '0000-0003-0552-7402': 20, '0000-0002-8010-1068': 15, '0000-0002-9396-9720': 3, '0000-0002-0931-9363': 2, '0000-0002-2220-582X': 1, '0000-0003-2693-5093': 1, '0000-0003-2769-7365': 1})\n",
      "['0000-0003-0552-7402', '0000-0002-8010-1068', '0000-0003-3894-5301', '0000-0002-1441-7363']\n",
      "Total sample size after apply threshold:  312\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(312, 564)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(312, 564)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.65      0.79        20\n",
      "          1       1.00      0.80      0.89        15\n",
      "          2       0.92      1.00      0.96       251\n",
      "          3       1.00      0.50      0.67        26\n",
      "\n",
      "avg / total       0.93      0.93      0.92       312\n",
      "\n",
      "[ 13   0   7   0   0  12   3   0   0   0 251   0   0   0  13  13]\n",
      "svc Accuracy:  0.9262820512820513\n",
      "svc F1:  0.8249062049062049\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.35      0.52        20\n",
      "          1       1.00      0.60      0.75        15\n",
      "          2       0.86      1.00      0.92       251\n",
      "          3       1.00      0.15      0.27        26\n",
      "\n",
      "avg / total       0.89      0.87      0.84       312\n",
      "\n",
      "[  7   0  13   0   0   9   6   0   0   0 251   0   0   0  22   4]\n",
      "LR Accuracy:  0.8685897435897436\n",
      "LR F1:  0.6149196848782483\n",
      "For name:  y_lim\n",
      "total sample size before apply threshold:  76\n",
      "Counter({'0000-0002-3484-045X': 21, '0000-0002-8472-289X': 21, '0000-0003-1377-7655': 10, '0000-0002-4181-2916': 8, '0000-0001-5000-5991': 5, '0000-0003-4390-4010': 3, '0000-0003-4050-6332': 2, '0000-0002-3408-8595': 2, '0000-0002-0346-2345': 1, '0000-0002-3279-332X': 1, '0000-0002-4082-3322': 1, '0000-0003-3678-0080': 1})\n",
      "['0000-0002-3484-045X', '0000-0003-1377-7655', '0000-0002-8472-289X']\n",
      "Total sample size after apply threshold:  52\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(52, 101)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(52, 101)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        21\n",
      "          1       1.00      0.80      0.89        10\n",
      "          2       0.91      1.00      0.95        21\n",
      "\n",
      "avg / total       0.96      0.96      0.96        52\n",
      "\n",
      "[21  0  0  0  8  2  0  0 21]\n",
      "svc Accuracy:  0.9615384615384616\n",
      "svc F1:  0.9478114478114478\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        21\n",
      "          1       1.00      0.80      0.89        10\n",
      "          2       0.91      1.00      0.95        21\n",
      "\n",
      "avg / total       0.96      0.96      0.96        52\n",
      "\n",
      "[21  0  0  0  8  2  0  0 21]\n",
      "LR Accuracy:  0.9615384615384616\n",
      "LR F1:  0.9478114478114478\n",
      "For name:  g_miller\n",
      "total sample size before apply threshold:  76\n",
      "Counter({'0000-0002-4743-8187': 26, '0000-0001-8984-1284': 23, '0000-0001-6533-3306': 13, '0000-0003-4527-3814': 11, '0000-0002-1108-0654': 3})\n",
      "['0000-0001-6533-3306', '0000-0001-8984-1284', '0000-0003-4527-3814', '0000-0002-4743-8187']\n",
      "Total sample size after apply threshold:  73\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(73, 239)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(73, 239)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.77      0.87        13\n",
      "          1       0.73      0.96      0.83        23\n",
      "          2       1.00      0.82      0.90        11\n",
      "          3       0.92      0.85      0.88        26\n",
      "\n",
      "avg / total       0.89      0.86      0.87        73\n",
      "\n",
      "[10  2  0  1  0 22  0  1  0  2  9  0  0  4  0 22]\n",
      "svc Accuracy:  0.863013698630137\n",
      "svc F1:  0.8699384741591468\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.85      0.92        13\n",
      "          1       0.85      0.96      0.90        23\n",
      "          2       1.00      0.91      0.95        11\n",
      "          3       0.92      0.92      0.92        26\n",
      "\n",
      "avg / total       0.92      0.92      0.92        73\n",
      "\n",
      "[11  1  0  1  0 22  0  1  0  1 10  0  0  2  0 24]\n",
      "LR Accuracy:  0.9178082191780822\n",
      "LR F1:  0.9225209314495029\n",
      "For name:  x_kong\n",
      "total sample size before apply threshold:  69\n",
      "Counter({'0000-0003-0676-6923': 34, '0000-0002-4475-2162': 10, '0000-0002-1725-4619': 6, '0000-0003-0659-4084': 6, '0000-0002-8554-0369': 6, '0000-0002-2195-268X': 3, '0000-0003-3290-025X': 2, '0000-0003-1039-494X': 1, '0000-0003-2698-3319': 1})\n",
      "['0000-0002-4475-2162', '0000-0003-0676-6923']\n",
      "Total sample size after apply threshold:  44\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(44, 48)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(44, 48)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.80      0.76        10\n",
      "          1       0.94      0.91      0.93        34\n",
      "\n",
      "avg / total       0.89      0.89      0.89        44\n",
      "\n",
      "[ 8  2  3 31]\n",
      "svc Accuracy:  0.8863636363636364\n",
      "svc F1:  0.8436389481165602\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80        10\n",
      "          1       0.94      0.94      0.94        34\n",
      "\n",
      "avg / total       0.91      0.91      0.91        44\n",
      "\n",
      "[ 8  2  2 32]\n",
      "LR Accuracy:  0.9090909090909091\n",
      "LR F1:  0.8705882352941177\n",
      "For name:  w_cao\n",
      "total sample size before apply threshold:  126\n",
      "Counter({'0000-0002-2447-1486': 91, '0000-0002-8952-9159': 27, '0000-0002-5369-9682': 7, '0000-0001-6209-3482': 1})\n",
      "['0000-0002-8952-9159', '0000-0002-2447-1486']\n",
      "Total sample size after apply threshold:  118\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(118, 166)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(118, 166)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.81      0.81        27\n",
      "          1       0.95      0.95      0.95        91\n",
      "\n",
      "avg / total       0.92      0.92      0.92       118\n",
      "\n",
      "[22  5  5 86]\n",
      "svc Accuracy:  0.9152542372881356\n",
      "svc F1:  0.8799348799348798\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.70      0.83        27\n",
      "          1       0.92      1.00      0.96        91\n",
      "\n",
      "avg / total       0.94      0.93      0.93       118\n",
      "\n",
      "[19  8  0 91]\n",
      "LR Accuracy:  0.9322033898305084\n",
      "LR F1:  0.8919908466819222\n",
      "For name:  c_ma\n",
      "total sample size before apply threshold:  126\n",
      "Counter({'0000-0001-8818-6396': 31, '0000-0002-7480-5528': 28, '0000-0001-9245-0356': 18, '0000-0001-7092-7715': 16, '0000-0003-2054-0445': 15, '0000-0001-9612-7898': 9, '0000-0001-6478-5917': 3, '0000-0001-6507-2329': 2, '0000-0002-5936-789X': 2, '0000-0003-1073-4502': 1, '0000-0001-8942-3912': 1})\n",
      "['0000-0001-7092-7715', '0000-0003-2054-0445', '0000-0002-7480-5528', '0000-0001-9245-0356', '0000-0001-8818-6396']\n",
      "Total sample size after apply threshold:  108\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(108, 181)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(108, 181)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.88      0.93        16\n",
      "          1       0.89      0.53      0.67        15\n",
      "          2       0.70      1.00      0.82        28\n",
      "          3       1.00      0.89      0.94        18\n",
      "          4       1.00      0.94      0.97        31\n",
      "\n",
      "avg / total       0.91      0.88      0.88       108\n",
      "\n",
      "[14  1  1  0  0  0  8  7  0  0  0  0 28  0  0  0  0  2 16  0  0  0  2  0\n",
      " 29]\n",
      "svc Accuracy:  0.8796296296296297\n",
      "svc F1:  0.8662745098039217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.88      0.93        16\n",
      "          1       0.90      0.60      0.72        15\n",
      "          2       0.72      1.00      0.84        28\n",
      "          3       0.94      0.89      0.91        18\n",
      "          4       1.00      0.90      0.95        31\n",
      "\n",
      "avg / total       0.90      0.88      0.88       108\n",
      "\n",
      "[14  1  1  0  0  0  9  6  0  0  0  0 28  0  0  0  0  2 16  0  0  0  2  1\n",
      " 28]\n",
      "LR Accuracy:  0.8796296296296297\n",
      "LR F1:  0.8705184971028632\n",
      "For name:  j_chin\n",
      "total sample size before apply threshold:  27\n",
      "Counter({'0000-0003-3932-8639': 13, '0000-0002-1840-325X': 9, '0000-0002-2878-8544': 3, '0000-0001-9809-6976': 1, '0000-0001-7626-6778': 1})\n",
      "['0000-0003-3932-8639']\n",
      "Total sample size after apply threshold:  13\n",
      "For name:  h_kwon\n",
      "total sample size before apply threshold:  35\n",
      "Counter({'0000-0003-4979-8749': 13, '0000-0002-6919-833X': 7, '0000-0002-0960-0198': 5, '0000-0001-6941-4808': 3, '0000-0003-4026-4572': 3, '0000-0002-2936-1358': 1, '0000-0002-8509-3968': 1, '0000-0003-4465-2708': 1, '0000-0001-9772-1354': 1})\n",
      "['0000-0003-4979-8749']\n",
      "Total sample size after apply threshold:  13\n",
      "For name:  s_gao\n",
      "total sample size before apply threshold:  31\n",
      "Counter({'0000-0002-7020-037X': 28, '0000-0002-8919-1338': 1, '0000-0002-3574-6393': 1, '0000-0003-3320-8505': 1})\n",
      "['0000-0002-7020-037X']\n",
      "Total sample size after apply threshold:  28\n",
      "For name:  f_tian\n",
      "total sample size before apply threshold:  17\n",
      "Counter({'0000-0002-1247-6896': 9, '0000-0003-3580-022X': 4, '0000-0002-9680-4518': 1, '0000-0003-0534-9749': 1, '0000-0002-9686-2769': 1, '0000-0001-8985-9679': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  f_martins\n",
      "total sample size before apply threshold:  65\n",
      "Counter({'0000-0003-0960-4620': 19, '0000-0003-4189-1228': 10, '0000-0003-2668-2401': 9, '0000-0002-1812-2300': 8, '0000-0003-2161-459X': 6, '0000-0002-9863-6255': 5, '0000-0002-3277-1809': 4, '0000-0002-0680-3643': 3, '0000-0003-4997-3973': 1})\n",
      "['0000-0003-4189-1228', '0000-0003-0960-4620']\n",
      "Total sample size after apply threshold:  29\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(29, 46)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(29, 46)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      1.00      0.91        10\n",
      "          1       1.00      0.89      0.94        19\n",
      "\n",
      "avg / total       0.94      0.93      0.93        29\n",
      "\n",
      "[10  0  2 17]\n",
      "svc Accuracy:  0.9310344827586207\n",
      "svc F1:  0.9267676767676767\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      1.00      0.95        10\n",
      "          1       1.00      0.95      0.97        19\n",
      "\n",
      "avg / total       0.97      0.97      0.97        29\n",
      "\n",
      "[10  0  1 18]\n",
      "LR Accuracy:  0.9655172413793104\n",
      "LR F1:  0.9626769626769627\n",
      "For name:  s_wolf\n",
      "total sample size before apply threshold:  363\n",
      "Counter({'0000-0003-2972-3440': 173, '0000-0002-7467-7028': 102, '0000-0002-5337-5063': 46, '0000-0003-0832-6315': 15, '0000-0002-3747-8097': 12, '0000-0003-1752-6175': 9, '0000-0003-3921-6629': 3, '0000-0001-7717-6993': 2, '0000-0002-6748-3911': 1})\n",
      "['0000-0002-7467-7028', '0000-0002-5337-5063', '0000-0003-0832-6315', '0000-0003-2972-3440', '0000-0002-3747-8097']\n",
      "Total sample size after apply threshold:  348\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(348, 909)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(348, 909)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.90      0.94       102\n",
      "          1       1.00      0.65      0.79        46\n",
      "          2       1.00      0.53      0.70        15\n",
      "          3       0.84      1.00      0.92       173\n",
      "          4       1.00      0.92      0.96        12\n",
      "\n",
      "avg / total       0.92      0.90      0.90       348\n",
      "\n",
      "[ 92   0   0  10   0   1  30   0  15   0   0   0   8   7   0   0   0   0\n",
      " 173   0   1   0   0   0  11]\n",
      "svc Accuracy:  0.9022988505747126\n",
      "svc F1:  0.8591534045604003\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.92      0.95       102\n",
      "          1       1.00      0.65      0.79        46\n",
      "          2       1.00      0.53      0.70        15\n",
      "          3       0.85      1.00      0.92       173\n",
      "          4       1.00      0.92      0.96        12\n",
      "\n",
      "avg / total       0.92      0.91      0.90       348\n",
      "\n",
      "[ 94   0   0   8   0   1  30   0  15   0   0   0   8   7   0   0   0   0\n",
      " 173   0   1   0   0   0  11]\n",
      "LR Accuracy:  0.9080459770114943\n",
      "LR F1:  0.8622710625412802\n",
      "For name:  m_goldman\n",
      "total sample size before apply threshold:  81\n",
      "Counter({'0000-0002-6786-9320': 74, '0000-0001-6771-169X': 5, '0000-0002-3667-1477': 1, '0000-0001-8908-3356': 1})\n",
      "['0000-0002-6786-9320']\n",
      "Total sample size after apply threshold:  74\n",
      "For name:  d_tang\n",
      "total sample size before apply threshold:  89\n",
      "Counter({'0000-0002-7339-9249': 40, '0000-0001-7136-7481': 39, '0000-0002-4790-9014': 5, '0000-0002-5443-4619': 3, '0000-0002-7615-0246': 2})\n",
      "['0000-0002-7339-9249', '0000-0001-7136-7481']\n",
      "Total sample size after apply threshold:  79\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(79, 197)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(79, 197)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.98        40\n",
      "          1       1.00      0.95      0.97        39\n",
      "\n",
      "avg / total       0.98      0.97      0.97        79\n",
      "\n",
      "[40  0  2 37]\n",
      "svc Accuracy:  0.9746835443037974\n",
      "svc F1:  0.9746469833119384\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99        40\n",
      "          1       1.00      0.97      0.99        39\n",
      "\n",
      "avg / total       0.99      0.99      0.99        79\n",
      "\n",
      "[40  0  1 38]\n",
      "LR Accuracy:  0.9873417721518988\n",
      "LR F1:  0.9873336540003206\n",
      "For name:  m_adams\n",
      "total sample size before apply threshold:  190\n",
      "Counter({'0000-0003-0435-8651': 59, '0000-0001-8989-508X': 46, '0000-0001-6310-1472': 30, '0000-0002-7743-4515': 29, '0000-0003-2849-9096': 12, '0000-0002-5277-5487': 7, '0000-0002-3878-7684': 5, '0000-0002-3602-6849': 1, '0000-0002-4645-2593': 1})\n",
      "['0000-0002-7743-4515', '0000-0001-6310-1472', '0000-0003-2849-9096', '0000-0003-0435-8651', '0000-0001-8989-508X']\n",
      "Total sample size after apply threshold:  176\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(176, 359)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(176, 359)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.76      0.80        29\n",
      "          1       1.00      0.80      0.89        30\n",
      "          2       1.00      1.00      1.00        12\n",
      "          3       1.00      0.86      0.93        59\n",
      "          4       0.68      0.93      0.79        46\n",
      "\n",
      "avg / total       0.89      0.86      0.87       176\n",
      "\n",
      "[22  0  0  0  7  1 24  0  0  5  0  0 12  0  0  0  0  0 51  8  3  0  0  0\n",
      " 43]\n",
      "svc Accuracy:  0.8636363636363636\n",
      "svc F1:  0.8810304883699379\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.79      0.88        29\n",
      "          1       1.00      0.90      0.95        30\n",
      "          2       1.00      1.00      1.00        12\n",
      "          3       1.00      0.88      0.94        59\n",
      "          4       0.74      1.00      0.85        46\n",
      "\n",
      "avg / total       0.93      0.91      0.91       176\n",
      "\n",
      "[23  0  0  0  6  0 27  0  0  3  0  0 12  0  0  0  0  0 52  7  0  0  0  0\n",
      " 46]\n",
      "LR Accuracy:  0.9090909090909091\n",
      "LR F1:  0.924154518891361\n",
      "For name:  t_singh\n",
      "total sample size before apply threshold:  52\n",
      "Counter({'0000-0001-7935-0457': 13, '0000-0001-7420-6739': 11, '0000-0001-7051-6529': 11, '0000-0002-0413-1935': 10, '0000-0003-1007-4540': 2, '0000-0002-5870-6204': 1, '0000-0003-0377-6122': 1, '0000-0002-9740-7776': 1, '0000-0002-7740-4826': 1, '0000-0003-1109-5626': 1})\n",
      "['0000-0002-0413-1935', '0000-0001-7420-6739', '0000-0001-7935-0457', '0000-0001-7051-6529']\n",
      "Total sample size after apply threshold:  45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(45, 89)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(45, 89)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.60      0.75        10\n",
      "          1       1.00      0.64      0.78        11\n",
      "          2       0.80      0.92      0.86        13\n",
      "          3       0.65      1.00      0.79        11\n",
      "\n",
      "avg / total       0.86      0.80      0.80        45\n",
      "\n",
      "[ 6  0  0  4  0  7  3  1  0  0 12  1  0  0  0 11]\n",
      "svc Accuracy:  0.8\n",
      "svc F1:  0.7926587301587302\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.90      0.90        10\n",
      "          1       1.00      0.73      0.84        11\n",
      "          2       0.80      0.92      0.86        13\n",
      "          3       0.83      0.91      0.87        11\n",
      "\n",
      "avg / total       0.88      0.87      0.87        45\n",
      "\n",
      "[ 9  0  0  1  0  8  3  0  0  0 12  1  1  0  0 10]\n",
      "LR Accuracy:  0.8666666666666667\n",
      "LR F1:  0.867203334423014\n",
      "For name:  m_thompson\n",
      "total sample size before apply threshold:  150\n",
      "Counter({'0000-0002-7764-4096': 80, '0000-0001-8958-0336': 29, '0000-0002-4933-009X': 11, '0000-0002-2865-9558': 10, '0000-0002-5649-1203': 6, '0000-0002-1789-312X': 6, '0000-0002-6910-4938': 4, '0000-0002-1194-1506': 1, '0000-0002-8551-4806': 1, '0000-0002-1358-1962': 1, '0000-0001-7006-3646': 1})\n",
      "['0000-0001-8958-0336', '0000-0002-4933-009X', '0000-0002-2865-9558', '0000-0002-7764-4096']\n",
      "Total sample size after apply threshold:  130\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(130, 377)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(130, 377)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.76      0.86        29\n",
      "          1       0.86      0.55      0.67        11\n",
      "          2       1.00      0.80      0.89        10\n",
      "          3       0.86      1.00      0.92        80\n",
      "\n",
      "avg / total       0.90      0.89      0.89       130\n",
      "\n",
      "[22  1  0  6  0  6  0  5  0  0  8  2  0  0  0 80]\n",
      "svc Accuracy:  0.8923076923076924\n",
      "svc F1:  0.8357890362310627\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.62      0.77        29\n",
      "          1       1.00      0.36      0.53        11\n",
      "          2       1.00      0.90      0.95        10\n",
      "          3       0.81      1.00      0.89        80\n",
      "\n",
      "avg / total       0.88      0.85      0.84       130\n",
      "\n",
      "[18  0  0 11  0  4  0  7  0  0  9  1  0  0  0 80]\n",
      "LR Accuracy:  0.8538461538461538\n",
      "LR F1:  0.7851284874494568\n",
      "For name:  s_garcia\n",
      "total sample size before apply threshold:  20\n",
      "Counter({'0000-0002-3143-0527': 16, '0000-0003-4190-6055': 2, '0000-0001-5161-0085': 1, '0000-0001-7317-1423': 1})\n",
      "['0000-0002-3143-0527']\n",
      "Total sample size after apply threshold:  16\n",
      "For name:  e_wang\n",
      "total sample size before apply threshold:  155\n",
      "Counter({'0000-0002-2243-4964': 64, '0000-0002-1180-5854': 53, '0000-0003-3394-2670': 14, '0000-0002-4942-3771': 10, '0000-0001-9335-5457': 6, '0000-0003-1302-9745': 4, '0000-0002-6653-5791': 2, '0000-0002-5178-3530': 1, '0000-0002-1084-7059': 1})\n",
      "['0000-0002-1180-5854', '0000-0003-3394-2670', '0000-0002-4942-3771', '0000-0002-2243-4964']\n",
      "Total sample size after apply threshold:  141\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(141, 382)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(141, 382)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.92      0.96        53\n",
      "          1       1.00      0.86      0.92        14\n",
      "          2       1.00      0.60      0.75        10\n",
      "          3       0.86      1.00      0.93        64\n",
      "\n",
      "avg / total       0.94      0.93      0.93       141\n",
      "\n",
      "[49  0  0  4  0 12  0  2  0  0  6  4  0  0  0 64]\n",
      "svc Accuracy:  0.9290780141843972\n",
      "svc F1:  0.8903493671716178\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.94      0.97        53\n",
      "          1       1.00      0.93      0.96        14\n",
      "          2       1.00      0.70      0.82        10\n",
      "          3       0.90      1.00      0.95        64\n",
      "\n",
      "avg / total       0.96      0.95      0.95       141\n",
      "\n",
      "[50  0  0  3  0 13  0  1  0  0  7  3  0  0  0 64]\n",
      "LR Accuracy:  0.950354609929078\n",
      "LR F1:  0.926378577320896\n",
      "For name:  c_scott\n",
      "total sample size before apply threshold:  162\n",
      "Counter({'0000-0003-1340-0647': 98, '0000-0001-6110-6982': 39, '0000-0001-9363-1829': 21, '0000-0003-0860-4805': 3, '0000-0003-3254-8647': 1})\n",
      "['0000-0001-9363-1829', '0000-0001-6110-6982', '0000-0003-1340-0647']\n",
      "Total sample size after apply threshold:  158\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(158, 578)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(158, 578)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.66      1.00      0.79        21\n",
      "          1       1.00      0.87      0.93        39\n",
      "          2       1.00      0.94      0.97        98\n",
      "\n",
      "avg / total       0.95      0.93      0.94       158\n",
      "\n",
      "[21  0  0  5 34  0  6  0 92]\n",
      "svc Accuracy:  0.930379746835443\n",
      "svc F1:  0.8974602440451088\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.43      0.58        21\n",
      "          1       1.00      0.90      0.95        39\n",
      "          2       0.87      1.00      0.93        98\n",
      "\n",
      "avg / total       0.90      0.90      0.89       158\n",
      "\n",
      "[ 9  0 12  1 35  3  0  0 98]\n",
      "LR Accuracy:  0.8987341772151899\n",
      "LR F1:  0.8185003532809679\n",
      "For name:  m_mukherjee\n",
      "total sample size before apply threshold:  16\n",
      "Counter({'0000-0003-3706-406X': 4, '0000-0002-7924-7211': 4, '0000-0002-3083-436X': 3, '0000-0003-0376-8173': 2, '0000-0002-3615-7574': 2, '0000-0001-9653-0556': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  j_schroeder\n",
      "total sample size before apply threshold:  150\n",
      "Counter({'0000-0002-3283-5972': 146, '0000-0002-4136-843X': 2, '0000-0002-3860-8498': 1, '0000-0002-1975-721X': 1})\n",
      "['0000-0002-3283-5972']\n",
      "Total sample size after apply threshold:  146\n",
      "For name:  a_mayer\n",
      "total sample size before apply threshold:  17\n",
      "Counter({'0000-0002-6859-0612': 10, '0000-0003-3278-1182': 4, '0000-0002-6975-7082': 2, '0000-0002-8765-6373': 1})\n",
      "['0000-0002-6859-0612']\n",
      "Total sample size after apply threshold:  10\n",
      "For name:  e_wright\n",
      "total sample size before apply threshold:  28\n",
      "Counter({'0000-0001-7041-5138': 20, '0000-0002-2390-8017': 3, '0000-0002-2187-7114': 3, '0000-0003-1721-4104': 2})\n",
      "['0000-0001-7041-5138']\n",
      "Total sample size after apply threshold:  20\n",
      "For name:  c_moreno\n",
      "total sample size before apply threshold:  136\n",
      "Counter({'0000-0002-5582-0028': 37, '0000-0003-0541-4846': 35, '0000-0001-6472-6044': 16, '0000-0003-1839-9673': 16, '0000-0002-1660-7072': 13, '0000-0003-2682-211X': 6, '0000-0002-0876-0341': 6, '0000-0003-4816-8040': 6, '0000-0002-9584-2619': 1})\n",
      "['0000-0001-6472-6044', '0000-0002-1660-7072', '0000-0003-1839-9673', '0000-0003-0541-4846', '0000-0002-5582-0028']\n",
      "Total sample size after apply threshold:  117\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(117, 517)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(117, 517)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.81      0.84        16\n",
      "          1       1.00      0.85      0.92        13\n",
      "          2       1.00      0.88      0.93        16\n",
      "          3       1.00      0.86      0.92        35\n",
      "          4       0.79      1.00      0.88        37\n",
      "\n",
      "avg / total       0.91      0.90      0.90       117\n",
      "\n",
      "[13  0  0  0  3  0 11  0  0  2  0  0 14  0  2  2  0  0 30  3  0  0  0  0\n",
      " 37]\n",
      "svc Accuracy:  0.8974358974358975\n",
      "svc F1:  0.8985477962897317\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.81      0.87        16\n",
      "          1       1.00      0.85      0.92        13\n",
      "          2       1.00      0.94      0.97        16\n",
      "          3       0.97      1.00      0.99        35\n",
      "          4       0.90      1.00      0.95        37\n",
      "\n",
      "avg / total       0.95      0.95      0.95       117\n",
      "\n",
      "[13  0  0  1  2  1 11  0  0  1  0  0 15  0  1  0  0  0 35  0  0  0  0  0\n",
      " 37]\n",
      "LR Accuracy:  0.9487179487179487\n",
      "LR F1:  0.9371417420985798\n",
      "For name:  a_moura\n",
      "total sample size before apply threshold:  36\n",
      "Counter({'0000-0003-0339-1230': 15, '0000-0002-2105-7319': 14, '0000-0003-2140-0196': 4, '0000-0002-1513-5448': 3})\n",
      "['0000-0002-2105-7319', '0000-0003-0339-1230']\n",
      "Total sample size after apply threshold:  29\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(29, 68)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(29, 68)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.93      0.93        14\n",
      "          1       0.93      0.93      0.93        15\n",
      "\n",
      "avg / total       0.93      0.93      0.93        29\n",
      "\n",
      "[13  1  1 14]\n",
      "svc Accuracy:  0.9310344827586207\n",
      "svc F1:  0.930952380952381\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        14\n",
      "          1       1.00      1.00      1.00        15\n",
      "\n",
      "avg / total       1.00      1.00      1.00        29\n",
      "\n",
      "[14  0  0 15]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  j_lopez\n",
      "total sample size before apply threshold:  122\n",
      "Counter({'0000-0002-9097-6060': 27, '0000-0003-0842-5348': 23, '0000-0002-5234-1478': 17, '0000-0001-9370-1516': 9, '0000-0001-6884-5577': 6, '0000-0003-3001-1109': 6, '0000-0002-1637-4125': 6, '0000-0003-0821-7530': 4, '0000-0002-5390-6610': 4, '0000-0002-7645-1620': 4, '0000-0002-4104-6262': 3, '0000-0001-5684-4913': 3, '0000-0001-8723-6347': 2, '0000-0003-2213-1186': 2, '0000-0003-0370-4727': 2, '0000-0003-4662-7928': 1, '0000-0003-1028-779X': 1, '0000-0002-4627-2277': 1, '0000-0001-8066-9991': 1})\n",
      "['0000-0003-0842-5348', '0000-0002-9097-6060', '0000-0002-5234-1478']\n",
      "Total sample size after apply threshold:  67\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(67, 329)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(67, 329)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      1.00      0.84        23\n",
      "          1       1.00      0.74      0.85        27\n",
      "          2       1.00      0.88      0.94        17\n",
      "\n",
      "avg / total       0.90      0.87      0.87        67\n",
      "\n",
      "[23  0  0  7 20  0  2  0 15]\n",
      "svc Accuracy:  0.8656716417910447\n",
      "svc F1:  0.8749758220502901\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      1.00      0.94        23\n",
      "          1       1.00      0.85      0.92        27\n",
      "          2       0.94      1.00      0.97        17\n",
      "\n",
      "avg / total       0.95      0.94      0.94        67\n",
      "\n",
      "[23  0  0  3 23  1  0  0 17]\n",
      "LR Accuracy:  0.9402985074626866\n",
      "LR F1:  0.9434013605442177\n",
      "For name:  a_logan\n",
      "total sample size before apply threshold:  26\n",
      "Counter({'0000-0002-1179-5879': 22, '0000-0001-9140-5545': 2, '0000-0002-4403-7329': 1, '0000-0003-3215-5042': 1})\n",
      "['0000-0002-1179-5879']\n",
      "Total sample size after apply threshold:  22\n",
      "For name:  l_williams\n",
      "total sample size before apply threshold:  42\n",
      "Counter({'0000-0002-7860-0319': 21, '0000-0002-0021-5613': 9, '0000-0002-8643-4920': 2, '0000-0002-8577-6339': 2, '0000-0001-8439-5270': 2, '0000-0001-6790-1362': 2, '0000-0002-3964-2356': 1, '0000-0003-2404-1985': 1, '0000-0003-2860-1150': 1, '0000-0002-6317-1718': 1})\n",
      "['0000-0002-7860-0319']\n",
      "Total sample size after apply threshold:  21\n",
      "For name:  h_young\n",
      "total sample size before apply threshold:  109\n",
      "Counter({'0000-0002-0457-8710': 75, '0000-0003-1538-445X': 28, '0000-0002-4249-9060': 5, '0000-0002-8866-7648': 1})\n",
      "['0000-0003-1538-445X', '0000-0002-0457-8710']\n",
      "Total sample size after apply threshold:  103\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(103, 268)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(103, 268)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.82      0.90        28\n",
      "          1       0.94      1.00      0.97        75\n",
      "\n",
      "avg / total       0.95      0.95      0.95       103\n",
      "\n",
      "[23  5  0 75]\n",
      "svc Accuracy:  0.9514563106796117\n",
      "svc F1:  0.9348513598987982\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.61      0.76        28\n",
      "          1       0.87      1.00      0.93        75\n",
      "\n",
      "avg / total       0.91      0.89      0.88       103\n",
      "\n",
      "[17 11  0 75]\n",
      "LR Accuracy:  0.8932038834951457\n",
      "LR F1:  0.8436162870945478\n",
      "For name:  a_vincent\n",
      "total sample size before apply threshold:  79\n",
      "Counter({'0000-0002-4185-3267': 39, '0000-0001-6446-3846': 21, '0000-0002-3760-7266': 12, '0000-0002-0360-6644': 7})\n",
      "['0000-0002-4185-3267', '0000-0002-3760-7266', '0000-0001-6446-3846']\n",
      "Total sample size after apply threshold:  72\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(72, 228)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(72, 228)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.95      0.91        39\n",
      "          1       1.00      0.75      0.86        12\n",
      "          2       0.90      0.90      0.90        21\n",
      "\n",
      "avg / total       0.91      0.90      0.90        72\n",
      "\n",
      "[37  0  2  3  9  0  2  0 19]\n",
      "svc Accuracy:  0.9027777777777778\n",
      "svc F1:  0.8918283362727807\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.95      0.91        39\n",
      "          1       1.00      0.75      0.86        12\n",
      "          2       0.90      0.90      0.90        21\n",
      "\n",
      "avg / total       0.91      0.90      0.90        72\n",
      "\n",
      "[37  0  2  3  9  0  2  0 19]\n",
      "LR Accuracy:  0.9027777777777778\n",
      "LR F1:  0.8918283362727807\n",
      "For name:  a_monteiro\n",
      "total sample size before apply threshold:  132\n",
      "Counter({'0000-0002-8448-4801': 76, '0000-0001-9696-459X': 35, '0000-0001-8182-3380': 7, '0000-0002-2185-0720': 5, '0000-0002-7839-2556': 4, '0000-0002-2322-3624': 2, '0000-0002-3392-2664': 1, '0000-0002-1976-6538': 1, '0000-0003-0499-6522': 1})\n",
      "['0000-0001-9696-459X', '0000-0002-8448-4801']\n",
      "Total sample size after apply threshold:  111\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(111, 942)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(111, 942)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.69      0.79        35\n",
      "          1       0.87      0.97      0.92        76\n",
      "\n",
      "avg / total       0.89      0.88      0.88       111\n",
      "\n",
      "[24 11  2 74]\n",
      "svc Accuracy:  0.8828828828828829\n",
      "svc F1:  0.8530699521433662\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.60      0.75        35\n",
      "          1       0.84      1.00      0.92        76\n",
      "\n",
      "avg / total       0.89      0.87      0.86       111\n",
      "\n",
      "[21 14  0 76]\n",
      "LR Accuracy:  0.8738738738738738\n",
      "LR F1:  0.8328313253012047\n",
      "For name:  d_park\n",
      "total sample size before apply threshold:  156\n",
      "Counter({'0000-0003-2307-8575': 95, '0000-0002-6001-4223': 17, '0000-0001-9209-0493': 14, '0000-0003-0147-2424': 13, '0000-0002-7507-1175': 9, '0000-0002-7325-5480': 2, '0000-0001-9675-7179': 2, '0000-0002-5560-873X': 1, '0000-0003-4991-5247': 1, '0000-0002-1007-8595': 1, '0000-0001-9969-3051': 1})\n",
      "['0000-0001-9209-0493', '0000-0002-6001-4223', '0000-0003-0147-2424', '0000-0003-2307-8575']\n",
      "Total sample size after apply threshold:  139\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(139, 128)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(139, 128)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.86      0.73        14\n",
      "          1       0.93      0.76      0.84        17\n",
      "          2       0.85      0.85      0.85        13\n",
      "          3       0.96      0.94      0.95        95\n",
      "\n",
      "avg / total       0.91      0.90      0.90       139\n",
      "\n",
      "[12  1  0  1  0 13  1  3  2  0 11  0  5  0  1 89]\n",
      "svc Accuracy:  0.8992805755395683\n",
      "svc F1:  0.8397361903710565\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.79      0.85        14\n",
      "          1       0.83      0.59      0.69        17\n",
      "          2       0.90      0.69      0.78        13\n",
      "          3       0.90      0.99      0.94        95\n",
      "\n",
      "avg / total       0.89      0.89      0.89       139\n",
      "\n",
      "[11  2  0  1  0 10  1  6  0  0  9  4  1  0  0 94]\n",
      "LR Accuracy:  0.8920863309352518\n",
      "LR F1:  0.8146044285549533\n",
      "For name:  d_gao\n",
      "total sample size before apply threshold:  23\n",
      "Counter({'0000-0003-1821-2741': 14, '0000-0002-9391-1756': 7, '0000-0001-8725-5740': 1, '0000-0002-2472-7349': 1})\n",
      "['0000-0003-1821-2741']\n",
      "Total sample size after apply threshold:  14\n",
      "For name:  d_quinn\n",
      "total sample size before apply threshold:  145\n",
      "Counter({'0000-0002-1411-0417': 139, '0000-0002-6338-5265': 2, '0000-0003-0321-2255': 2, '0000-0001-7790-7768': 2})\n",
      "['0000-0002-1411-0417']\n",
      "Total sample size after apply threshold:  139\n",
      "For name:  n_dias\n",
      "total sample size before apply threshold:  17\n",
      "Counter({'0000-0002-0498-4612': 13, '0000-0002-6907-6148': 2, '0000-0002-4731-0968': 1, '0000-0002-9019-8406': 1})\n",
      "['0000-0002-0498-4612']\n",
      "Total sample size after apply threshold:  13\n",
      "For name:  k_fisher\n",
      "total sample size before apply threshold:  24\n",
      "Counter({'0000-0002-2751-156X': 11, '0000-0001-7381-9648': 8, '0000-0002-0828-6395': 3, '0000-0002-1774-4431': 1, '0000-0002-5581-8892': 1})\n",
      "['0000-0002-2751-156X']\n",
      "Total sample size after apply threshold:  11\n",
      "For name:  m_schubert\n",
      "total sample size before apply threshold:  84\n",
      "Counter({'0000-0003-0278-4091': 22, '0000-0003-2401-9921': 18, '0000-0002-0994-8805': 14, '0000-0001-6238-663X': 12, '0000-0002-8739-4852': 7, '0000-0002-6862-5221': 6, '0000-0002-2911-8075': 5})\n",
      "['0000-0003-2401-9921', '0000-0003-0278-4091', '0000-0001-6238-663X', '0000-0002-0994-8805']\n",
      "Total sample size after apply threshold:  66\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(66, 416)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(66, 416)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.83      0.91        18\n",
      "          1       0.72      0.95      0.82        22\n",
      "          2       1.00      0.67      0.80        12\n",
      "          3       0.93      0.93      0.93        14\n",
      "\n",
      "avg / total       0.89      0.86      0.86        66\n",
      "\n",
      "[15  3  0  0  0 21  0  1  0  4  8  0  0  1  0 13]\n",
      "svc Accuracy:  0.8636363636363636\n",
      "svc F1:  0.8652979373567609\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        18\n",
      "          1       0.81      1.00      0.90        22\n",
      "          2       1.00      0.67      0.80        12\n",
      "          3       1.00      0.93      0.96        14\n",
      "\n",
      "avg / total       0.94      0.92      0.92        66\n",
      "\n",
      "[18  0  0  0  0 22  0  0  0  4  8  0  0  1  0 13]\n",
      "LR Accuracy:  0.9242424242424242\n",
      "LR F1:  0.915230536659108\n",
      "For name:  j_peters\n",
      "total sample size before apply threshold:  154\n",
      "Counter({'0000-0001-8503-1452': 57, '0000-0002-6725-2814': 36, '0000-0001-8309-3297': 22, '0000-0003-3150-3973': 17, '0000-0003-4592-7275': 13, '0000-0002-5266-8091': 8, '0000-0002-1456-5390': 1})\n",
      "['0000-0001-8309-3297', '0000-0002-6725-2814', '0000-0003-3150-3973', '0000-0001-8503-1452', '0000-0003-4592-7275']\n",
      "Total sample size after apply threshold:  145\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(145, 366)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(145, 366)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.95      0.69        22\n",
      "          1       0.96      0.69      0.81        36\n",
      "          2       1.00      0.82      0.90        17\n",
      "          3       0.95      0.95      0.95        57\n",
      "          4       1.00      0.69      0.82        13\n",
      "\n",
      "avg / total       0.90      0.85      0.86       145\n",
      "\n",
      "[21  1  0  0  0  9 25  0  2  0  3  0 14  0  0  3  0  0 54  0  3  0  0  1\n",
      "  9]\n",
      "svc Accuracy:  0.8482758620689655\n",
      "svc F1:  0.8327504497506446\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.86      0.90        22\n",
      "          1       0.97      0.78      0.86        36\n",
      "          2       1.00      0.82      0.90        17\n",
      "          3       0.81      1.00      0.90        57\n",
      "          4       1.00      0.92      0.96        13\n",
      "\n",
      "avg / total       0.91      0.90      0.90       145\n",
      "\n",
      "[19  1  0  2  0  1 28  0  7  0  0  0 14  3  0  0  0  0 57  0  0  0  0  1\n",
      " 12]\n",
      "LR Accuracy:  0.896551724137931\n",
      "LR F1:  0.905432793605514\n",
      "For name:  e_zimmermann\n",
      "total sample size before apply threshold:  57\n",
      "Counter({'0000-0001-9927-3372': 24, '0000-0001-5854-0542': 16, '0000-0002-1964-2711': 15, '0000-0002-4268-9729': 2})\n",
      "['0000-0002-1964-2711', '0000-0001-5854-0542', '0000-0001-9927-3372']\n",
      "Total sample size after apply threshold:  55\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(55, 239)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(55, 239)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      1.00      0.86        15\n",
      "          1       1.00      0.88      0.93        16\n",
      "          2       1.00      0.88      0.93        24\n",
      "\n",
      "avg / total       0.93      0.91      0.91        55\n",
      "\n",
      "[15  0  0  2 14  0  3  0 21]\n",
      "svc Accuracy:  0.9090909090909091\n",
      "svc F1:  0.9079365079365079\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      1.00      0.94        15\n",
      "          1       1.00      1.00      1.00        16\n",
      "          2       1.00      0.92      0.96        24\n",
      "\n",
      "avg / total       0.97      0.96      0.96        55\n",
      "\n",
      "[15  0  0  0 16  0  2  0 22]\n",
      "LR Accuracy:  0.9636363636363636\n",
      "LR F1:  0.9646739130434782\n",
      "For name:  c_zhang\n",
      "total sample size before apply threshold:  321\n",
      "Counter({'0000-0002-7784-1188': 120, '0000-0003-2349-3138': 52, '0000-0002-1581-5806': 25, '0000-0001-9042-4007': 13, '0000-0002-6502-288X': 10, '0000-0002-5957-2287': 9, '0000-0002-3721-8586': 8, '0000-0002-4067-2798': 7, '0000-0003-3435-0247': 7, '0000-0002-7687-0518': 7, '0000-0001-8663-3674': 6, '0000-0001-8222-4566': 5, '0000-0003-0679-7623': 4, '0000-0003-3212-4270': 4, '0000-0001-6885-1678': 4, '0000-0003-1616-4715': 4, '0000-0001-8206-5171': 4, '0000-0001-6685-0137': 3, '0000-0002-7913-4858': 3, '0000-0002-7167-0840': 3, '0000-0002-1207-4264': 3, '0000-0003-0399-1201': 3, '0000-0002-9461-1755': 2, '0000-0003-4968-8793': 2, '0000-0003-2693-6643': 2, '0000-0003-3871-0342': 2, '0000-0001-5249-141X': 2, '0000-0003-1095-9939': 1, '0000-0002-3065-3497': 1, '0000-0002-1607-5563': 1, '0000-0002-7704-9318': 1, '0000-0001-5552-1960': 1, '0000-0002-1458-8170': 1, '0000-0003-2346-6770': 1})\n",
      "['0000-0002-7784-1188', '0000-0002-1581-5806', '0000-0001-9042-4007', '0000-0003-2349-3138', '0000-0002-6502-288X']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sample size after apply threshold:  220\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(220, 248)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(220, 248)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.98      0.91       120\n",
      "          1       0.86      0.76      0.81        25\n",
      "          2       1.00      0.54      0.70        13\n",
      "          3       1.00      0.94      0.97        52\n",
      "          4       1.00      0.40      0.57        10\n",
      "\n",
      "avg / total       0.91      0.90      0.89       220\n",
      "\n",
      "[118   2   0   0   0   6  19   0   0   0   6   0   7   0   0   2   1   0\n",
      "  49   0   6   0   0   0   4]\n",
      "svc Accuracy:  0.8954545454545455\n",
      "svc F1:  0.7929929843199914\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.99      0.90       120\n",
      "          1       0.94      0.68      0.79        25\n",
      "          2       1.00      0.54      0.70        13\n",
      "          3       1.00      0.96      0.98        52\n",
      "          4       1.00      0.20      0.33        10\n",
      "\n",
      "avg / total       0.90      0.89      0.87       220\n",
      "\n",
      "[119   1   0   0   0   8  17   0   0   0   6   0   7   0   0   2   0   0\n",
      "  50   0   8   0   0   0   2]\n",
      "LR Accuracy:  0.8863636363636364\n",
      "LR F1:  0.7418732260788302\n",
      "For name:  h_shin\n",
      "total sample size before apply threshold:  114\n",
      "Counter({'0000-0001-7615-9809': 34, '0000-0001-7080-6075': 24, '0000-0003-1226-3206': 20, '0000-0002-3353-0310': 13, '0000-0002-6750-118X': 10, '0000-0001-6504-3413': 9, '0000-0002-3398-1074': 2, '0000-0002-1410-9731': 1, '0000-0002-5161-661X': 1})\n",
      "['0000-0002-6750-118X', '0000-0001-7080-6075', '0000-0001-7615-9809', '0000-0003-1226-3206', '0000-0002-3353-0310']\n",
      "Total sample size after apply threshold:  101\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(101, 88)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(101, 88)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.80      0.89        10\n",
      "          1       0.86      0.79      0.83        24\n",
      "          2       1.00      0.94      0.97        34\n",
      "          3       0.76      0.80      0.78        20\n",
      "          4       0.61      0.85      0.71        13\n",
      "\n",
      "avg / total       0.87      0.85      0.86       101\n",
      "\n",
      "[ 8  2  0  0  0  0 19  0  3  2  0  0 32  0  2  0  1  0 16  3  0  0  0  2\n",
      " 11]\n",
      "svc Accuracy:  0.8514851485148515\n",
      "svc F1:  0.8349676078680972\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.80      0.89        10\n",
      "          1       0.81      0.88      0.84        24\n",
      "          2       1.00      0.97      0.99        34\n",
      "          3       0.65      0.75      0.70        20\n",
      "          4       0.73      0.62      0.67        13\n",
      "\n",
      "avg / total       0.85      0.84      0.84       101\n",
      "\n",
      "[ 8  2  0  0  0  0 21  0  2  1  0  0 33  1  0  0  3  0 15  2  0  0  0  5\n",
      "  8]\n",
      "LR Accuracy:  0.8415841584158416\n",
      "LR F1:  0.8156609202051757\n",
      "For name:  r_reis\n",
      "total sample size before apply threshold:  615\n",
      "Counter({'0000-0002-4295-6129': 423, '0000-0002-9639-7940': 113, '0000-0002-9872-9865': 27, '0000-0001-9689-4085': 21, '0000-0002-0681-4721': 10, '0000-0003-0328-1840': 7, '0000-0003-0937-8045': 7, '0000-0003-3746-6894': 4, '0000-0002-6618-2412': 2, '0000-0002-6935-3459': 1})\n",
      "['0000-0002-9639-7940', '0000-0001-9689-4085', '0000-0002-9872-9865', '0000-0002-0681-4721', '0000-0002-4295-6129']\n",
      "Total sample size after apply threshold:  594\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(594, 1136)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(594, 1136)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.84      0.90       113\n",
      "          1       1.00      0.76      0.86        21\n",
      "          2       1.00      0.70      0.83        27\n",
      "          3       0.88      0.70      0.78        10\n",
      "          4       0.93      1.00      0.96       423\n",
      "\n",
      "avg / total       0.95      0.94      0.94       594\n",
      "\n",
      "[ 95   0   0   0  18   0  16   0   1   4   0   0  19   0   8   2   0   0\n",
      "   7   1   0   0   0   0 423]\n",
      "svc Accuracy:  0.9427609427609428\n",
      "svc F1:  0.8676287454830909\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.81      0.89       113\n",
      "          1       0.94      0.71      0.81        21\n",
      "          2       1.00      0.59      0.74        27\n",
      "          3       1.00      0.30      0.46        10\n",
      "          4       0.91      1.00      0.95       423\n",
      "\n",
      "avg / total       0.93      0.92      0.92       594\n",
      "\n",
      "[ 91   1   0   0  21   0  15   0   0   6   0   0  16   0  11   1   0   0\n",
      "   3   6   0   0   0   0 423]\n",
      "LR Accuracy:  0.9225589225589226\n",
      "LR F1:  0.770980398932498\n",
      "For name:  z_ren\n",
      "total sample size before apply threshold:  31\n",
      "Counter({'0000-0001-7606-0331': 25, '0000-0001-7265-065X': 4, '0000-0002-4559-4637': 1, '0000-0003-4208-5076': 1})\n",
      "['0000-0001-7606-0331']\n",
      "Total sample size after apply threshold:  25\n",
      "For name:  m_kumar\n",
      "total sample size before apply threshold:  104\n",
      "Counter({'0000-0003-3769-052X': 22, '0000-0003-1656-1649': 16, '0000-0003-0970-4875': 14, '0000-0001-9173-3872': 10, '0000-0002-0855-3406': 9, '0000-0002-9049-2760': 6, '0000-0002-3554-0563': 4, '0000-0002-4198-5892': 4, '0000-0003-3490-5062': 3, '0000-0002-7630-7389': 2, '0000-0001-6657-1277': 2, '0000-0002-0141-5318': 2, '0000-0001-6745-7425': 2, '0000-0001-5606-401X': 2, '0000-0001-6389-2040': 2, '0000-0002-7936-9892': 1, '0000-0001-5545-3793': 1, '0000-0002-7728-5572': 1, '0000-0001-6578-9741': 1})\n",
      "['0000-0003-1656-1649', '0000-0003-3769-052X', '0000-0003-0970-4875', '0000-0001-9173-3872']\n",
      "Total sample size after apply threshold:  62\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(62, 118)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(62, 118)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.88      0.93        16\n",
      "          1       0.88      1.00      0.94        22\n",
      "          2       1.00      1.00      1.00        14\n",
      "          3       1.00      0.90      0.95        10\n",
      "\n",
      "avg / total       0.96      0.95      0.95        62\n",
      "\n",
      "[14  2  0  0  0 22  0  0  0  0 14  0  0  1  0  9]\n",
      "svc Accuracy:  0.9516129032258065\n",
      "svc F1:  0.9542179917879807\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.88      0.93        16\n",
      "          1       0.92      1.00      0.96        22\n",
      "          2       1.00      1.00      1.00        14\n",
      "          3       1.00      1.00      1.00        10\n",
      "\n",
      "avg / total       0.97      0.97      0.97        62\n",
      "\n",
      "[14  2  0  0  0 22  0  0  0  0 14  0  0  0  0 10]\n",
      "LR Accuracy:  0.967741935483871\n",
      "LR F1:  0.972463768115942\n",
      "For name:  j_wong\n",
      "total sample size before apply threshold:  183\n",
      "Counter({'0000-0003-2953-7728': 59, '0000-0003-2592-3226': 30, '0000-0002-7213-4898': 24, '0000-0001-5572-4143': 21, '0000-0002-8167-540X': 17, '0000-0001-8268-5610': 10, '0000-0001-8080-1294': 8, '0000-0002-9206-3257': 5, '0000-0002-9329-1075': 4, '0000-0003-3897-7725': 4, '0000-0002-6317-2067': 1})\n",
      "['0000-0003-2592-3226', '0000-0001-5572-4143', '0000-0002-8167-540X', '0000-0001-8268-5610', '0000-0003-2953-7728', '0000-0002-7213-4898']\n",
      "Total sample size after apply threshold:  161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(161, 462)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(161, 462)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.77      0.79        30\n",
      "          1       1.00      0.95      0.98        21\n",
      "          2       1.00      0.94      0.97        17\n",
      "          3       1.00      0.90      0.95        10\n",
      "          4       0.86      0.95      0.90        59\n",
      "          5       0.96      0.92      0.94        24\n",
      "\n",
      "avg / total       0.91      0.91      0.91       161\n",
      "\n",
      "[23  0  0  0  7  0  1 20  0  0  0  0  1  0 16  0  0  0  0  0  0  9  1  0\n",
      "  2  0  0  0 56  1  1  0  0  0  1 22]\n",
      "svc Accuracy:  0.906832298136646\n",
      "svc F1:  0.9208624357234325\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.77      0.87        30\n",
      "          1       1.00      0.95      0.98        21\n",
      "          2       1.00      0.94      0.97        17\n",
      "          3       1.00      0.90      0.95        10\n",
      "          4       0.83      0.98      0.90        59\n",
      "          5       0.96      0.92      0.94        24\n",
      "\n",
      "avg / total       0.93      0.92      0.92       161\n",
      "\n",
      "[23  0  0  0  7  0  0 20  0  0  1  0  0  0 16  0  1  0  0  0  0  9  1  0\n",
      "  0  0  0  0 58  1  0  0  0  0  2 22]\n",
      "LR Accuracy:  0.9192546583850931\n",
      "LR F1:  0.9326657823527595\n",
      "For name:  s_turner\n",
      "total sample size before apply threshold:  101\n",
      "Counter({'0000-0003-4859-1068': 40, '0000-0002-8439-4507': 32, '0000-0002-1002-0000': 16, '0000-0001-8692-8210': 5, '0000-0003-2308-158X': 4, '0000-0001-5108-7976': 2, '0000-0003-2541-6072': 1, '0000-0003-2735-3220': 1})\n",
      "['0000-0002-1002-0000', '0000-0003-4859-1068', '0000-0002-8439-4507']\n",
      "Total sample size after apply threshold:  88\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(88, 380)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(88, 380)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.75      0.86        16\n",
      "          1       0.75      1.00      0.86        40\n",
      "          2       1.00      0.72      0.84        32\n",
      "\n",
      "avg / total       0.89      0.85      0.85        88\n",
      "\n",
      "[12  4  0  0 40  0  0  9 23]\n",
      "svc Accuracy:  0.8522727272727273\n",
      "svc F1:  0.8512405157566447\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.75      0.86        16\n",
      "          1       0.78      1.00      0.88        40\n",
      "          2       1.00      0.78      0.88        32\n",
      "\n",
      "avg / total       0.90      0.88      0.87        88\n",
      "\n",
      "[12  4  0  0 40  0  0  7 25]\n",
      "LR Accuracy:  0.875\n",
      "LR F1:  0.8711522395732922\n",
      "For name:  y_yuan\n",
      "total sample size before apply threshold:  67\n",
      "Counter({'0000-0003-1376-0028': 17, '0000-0001-7094-4419': 11, '0000-0003-4284-3973': 10, '0000-0003-4706-7897': 10, '0000-0002-7577-3257': 9, '0000-0003-3020-0700': 4, '0000-0002-1761-9040': 3, '0000-0002-6719-2567': 1, '0000-0002-1823-3174': 1, '0000-0002-2292-7339': 1})\n",
      "['0000-0003-1376-0028', '0000-0003-4284-3973', '0000-0003-4706-7897', '0000-0001-7094-4419']\n",
      "Total sample size after apply threshold:  48\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(48, 139)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(48, 139)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.94      0.82        17\n",
      "          1       1.00      0.80      0.89        10\n",
      "          2       1.00      0.80      0.89        10\n",
      "          3       0.90      0.82      0.86        11\n",
      "\n",
      "avg / total       0.88      0.85      0.86        48\n",
      "\n",
      "[16  0  0  1  2  8  0  0  2  0  8  0  2  0  0  9]\n",
      "svc Accuracy:  0.8541666666666666\n",
      "svc F1:  0.8638583638583639\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.94      0.89        17\n",
      "          1       1.00      0.90      0.95        10\n",
      "          2       1.00      0.90      0.95        10\n",
      "          3       0.91      0.91      0.91        11\n",
      "\n",
      "avg / total       0.92      0.92      0.92        48\n",
      "\n",
      "[16  0  0  1  1  9  0  0  1  0  9  0  1  0  0 10]\n",
      "LR Accuracy:  0.9166666666666666\n",
      "LR F1:  0.9231791600212653\n",
      "For name:  l_liu\n",
      "total sample size before apply threshold:  267\n",
      "Counter({'0000-0003-2732-7399': 36, '0000-0003-1844-338X': 35, '0000-0001-6997-8101': 24, '0000-0003-4934-0367': 17, '0000-0002-6944-5417': 15, '0000-0002-8884-4819': 14, '0000-0003-3631-2148': 13, '0000-0001-5868-4482': 11, '0000-0002-5696-3151': 10, '0000-0002-1450-4950': 9, '0000-0002-5054-2372': 9, '0000-0003-3269-8741': 8, '0000-0002-3710-7042': 8, '0000-0002-8396-0554': 7, '0000-0003-2949-6348': 7, '0000-0002-8924-4890': 6, '0000-0002-4604-4629': 6, '0000-0001-5476-0169': 4, '0000-0002-0493-9272': 4, '0000-0002-6159-7475': 4, '0000-0002-7775-5933': 4, '0000-0003-2741-2542': 2, '0000-0001-8689-1788': 2, '0000-0002-4852-1580': 2, '0000-0002-4811-4897': 1, '0000-0003-1506-2370': 1, '0000-0002-4561-1433': 1, '0000-0001-5056-1517': 1, '0000-0003-2230-2934': 1, '0000-0003-0030-3581': 1, '0000-0002-6506-3462': 1, '0000-0002-2213-8057': 1, '0000-0003-0194-1454': 1, '0000-0002-8468-327X': 1})\n",
      "['0000-0002-8884-4819', '0000-0003-3631-2148', '0000-0001-6997-8101', '0000-0003-4934-0367', '0000-0003-1844-338X', '0000-0002-5696-3151', '0000-0002-6944-5417', '0000-0001-5868-4482', '0000-0003-2732-7399']\n",
      "Total sample size after apply threshold:  175\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(175, 439)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(175, 439)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.43      0.60        14\n",
      "          1       0.80      0.92      0.86        13\n",
      "          2       0.95      0.79      0.86        24\n",
      "          3       0.89      0.94      0.91        17\n",
      "          4       0.66      0.94      0.78        35\n",
      "          5       0.70      0.70      0.70        10\n",
      "          6       0.93      0.93      0.93        15\n",
      "          7       0.80      0.73      0.76        11\n",
      "          8       0.90      0.78      0.84        36\n",
      "\n",
      "avg / total       0.84      0.82      0.81       175\n",
      "\n",
      "[ 6  0  0  0  6  0  0  0  2  0 12  0  0  1  0  0  0  0  0  1 19  0  2  1\n",
      "  1  0  0  0  0  0 16  1  0  0  0  0  0  1  0  1 33  0  0  0  0  0  0  0\n",
      "  0  0  7  0  2  1  0  1  0  0  0  0 14  0  0  0  0  1  1  0  1  0  8  0\n",
      "  0  0  0  0  7  1  0  0 28]\n",
      "svc Accuracy:  0.8171428571428572\n",
      "svc F1:  0.8047327237845237\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.43      0.60        14\n",
      "          1       0.92      0.92      0.92        13\n",
      "          2       0.95      0.75      0.84        24\n",
      "          3       0.89      1.00      0.94        17\n",
      "          4       0.70      0.94      0.80        35\n",
      "          5       0.71      0.50      0.59        10\n",
      "          6       0.93      0.93      0.93        15\n",
      "          7       0.73      0.73      0.73        11\n",
      "          8       0.87      0.92      0.89        36\n",
      "\n",
      "avg / total       0.85      0.83      0.83       175\n",
      "\n",
      "[ 6  0  0  0  6  0  0  0  2  0 12  0  0  1  0  0  0  0  0  0 18  0  3  1\n",
      "  1  0  1  0  0  0 17  0  0  0  0  0  0  1  0  1 33  0  0  0  0  0  0  0\n",
      "  0  0  5  0  3  2  0  0  0  0  1  0 14  0  0  0  0  1  1  0  1  0  8  0\n",
      "  0  0  0  0  3  0  0  0 33]\n",
      "LR Accuracy:  0.8342857142857143\n",
      "LR F1:  0.8055935516936708\n",
      "For name:  a_fonseca\n",
      "total sample size before apply threshold:  91\n",
      "Counter({'0000-0001-6913-5526': 24, '0000-0001-6237-417X': 13, '0000-0003-1395-1406': 7, '0000-0001-8413-9744': 6, '0000-0003-1118-5525': 6, '0000-0001-9624-7208': 5, '0000-0002-7145-2472': 5, '0000-0001-7505-7878': 4, '0000-0002-6382-6833': 4, '0000-0001-6792-8047': 4, '0000-0002-9087-1306': 4, '0000-0002-1715-5469': 3, '0000-0002-6925-1671': 2, '0000-0002-3207-4819': 2, '0000-0002-6661-5185': 1, '0000-0001-7410-269X': 1})\n",
      "['0000-0001-6237-417X', '0000-0001-6913-5526']\n",
      "Total sample size after apply threshold:  37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(37, 91)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(37, 91)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        13\n",
      "          1       1.00      1.00      1.00        24\n",
      "\n",
      "avg / total       1.00      1.00      1.00        37\n",
      "\n",
      "[13  0  0 24]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        13\n",
      "          1       1.00      1.00      1.00        24\n",
      "\n",
      "avg / total       1.00      1.00      1.00        37\n",
      "\n",
      "[13  0  0 24]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  r_francis\n",
      "total sample size before apply threshold:  13\n",
      "Counter({'0000-0002-3995-7040': 6, '0000-0001-8240-4903': 4, '0000-0002-4598-0861': 2, '0000-0003-1580-7934': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  l_castro\n",
      "total sample size before apply threshold:  74\n",
      "Counter({'0000-0001-7697-386X': 47, '0000-0003-3384-3832': 9, '0000-0002-0852-8235': 7, '0000-0003-3048-933X': 4, '0000-0002-1312-0154': 4, '0000-0002-1359-5272': 3})\n",
      "['0000-0001-7697-386X']\n",
      "Total sample size after apply threshold:  47\n",
      "For name:  k_zhou\n",
      "total sample size before apply threshold:  14\n",
      "Counter({'0000-0002-1898-6379': 5, '0000-0002-0351-8812': 3, '0000-0002-2844-1604': 3, '0000-0001-6645-5102': 2, '0000-0001-6442-0475': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  m_macdonald\n",
      "total sample size before apply threshold:  52\n",
      "Counter({'0000-0001-9851-3610': 38, '0000-0001-7946-0023': 11, '0000-0002-6295-6978': 2, '0000-0001-5421-3536': 1})\n",
      "['0000-0001-7946-0023', '0000-0001-9851-3610']\n",
      "Total sample size after apply threshold:  49\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(49, 186)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(49, 186)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.91      0.95        11\n",
      "          1       0.97      1.00      0.99        38\n",
      "\n",
      "avg / total       0.98      0.98      0.98        49\n",
      "\n",
      "[10  1  0 38]\n",
      "svc Accuracy:  0.9795918367346939\n",
      "svc F1:  0.9696969696969696\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.73      0.84        11\n",
      "          1       0.93      1.00      0.96        38\n",
      "\n",
      "avg / total       0.94      0.94      0.94        49\n",
      "\n",
      "[ 8  3  0 38]\n",
      "LR Accuracy:  0.9387755102040817\n",
      "LR F1:  0.9020652898067956\n",
      "For name:  h_guan\n",
      "total sample size before apply threshold:  44\n",
      "Counter({'0000-0002-4683-601X': 17, '0000-0002-4858-3159': 16, '0000-0002-2282-3193': 8, '0000-0001-5425-6974': 3})\n",
      "['0000-0002-4858-3159', '0000-0002-4683-601X']\n",
      "Total sample size after apply threshold:  33\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(33, 82)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(33, 82)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.94      0.91        16\n",
      "          1       0.94      0.88      0.91        17\n",
      "\n",
      "avg / total       0.91      0.91      0.91        33\n",
      "\n",
      "[15  1  2 15]\n",
      "svc Accuracy:  0.9090909090909091\n",
      "svc F1:  0.9090909090909091\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.94      0.94        16\n",
      "          1       0.94      0.94      0.94        17\n",
      "\n",
      "avg / total       0.94      0.94      0.94        33\n",
      "\n",
      "[15  1  1 16]\n",
      "LR Accuracy:  0.9393939393939394\n",
      "LR F1:  0.9393382352941176\n",
      "For name:  t_miller\n",
      "total sample size before apply threshold:  165\n",
      "Counter({'0000-0002-0958-2639': 102, '0000-0003-0731-8006': 42, '0000-0002-1269-1895': 11, '0000-0002-5585-7736': 6, '0000-0002-9749-1656': 3, '0000-0003-4027-7066': 1})\n",
      "['0000-0002-0958-2639', '0000-0002-1269-1895', '0000-0003-0731-8006']\n",
      "Total sample size after apply threshold:  155\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(155, 1391)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(155, 1391)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.99      0.94       102\n",
      "          1       1.00      0.73      0.84        11\n",
      "          2       0.97      0.81      0.88        42\n",
      "\n",
      "avg / total       0.93      0.92      0.92       155\n",
      "\n",
      "[101   0   1   3   8   0   8   0  34]\n",
      "svc Accuracy:  0.9225806451612903\n",
      "svc F1:  0.8897157933065459\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      1.00      0.94       102\n",
      "          1       1.00      0.45      0.62        11\n",
      "          2       1.00      0.81      0.89        42\n",
      "\n",
      "avg / total       0.92      0.91      0.90       155\n",
      "\n",
      "[102   0   0   6   5   0   8   0  34]\n",
      "LR Accuracy:  0.9096774193548387\n",
      "LR F1:  0.8185055528730082\n",
      "For name:  m_kang\n",
      "total sample size before apply threshold:  131\n",
      "Counter({'0000-0003-1595-1717': 38, '0000-0003-3245-144X': 19, '0000-0002-2039-4866': 18, '0000-0002-4778-8240': 13, '0000-0002-1530-7254': 12, '0000-0003-2140-4234': 10, '0000-0002-8795-2973': 8, '0000-0001-5266-2290': 5, '0000-0002-5054-7587': 4, '0000-0003-4946-8512': 2, '0000-0001-6991-0481': 1, '0000-0001-7600-7469': 1})\n",
      "['0000-0003-1595-1717', '0000-0002-4778-8240', '0000-0002-1530-7254', '0000-0003-2140-4234', '0000-0002-2039-4866', '0000-0003-3245-144X']\n",
      "Total sample size after apply threshold:  110\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(110, 175)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(110, 175)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.79      0.78        38\n",
      "          1       0.86      0.92      0.89        13\n",
      "          2       0.80      1.00      0.89        12\n",
      "          3       0.50      0.50      0.50        10\n",
      "          4       0.94      0.83      0.88        18\n",
      "          5       0.94      0.79      0.86        19\n",
      "\n",
      "avg / total       0.82      0.81      0.81       110\n",
      "\n",
      "[30  1  1  5  0  1  0 12  1  0  0  0  0  0 12  0  0  0  5  0  0  5  0  0\n",
      "  2  1  0  0 15  0  2  0  1  0  1 15]\n",
      "svc Accuracy:  0.8090909090909091\n",
      "svc F1:  0.799415725886314\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.84      0.81        38\n",
      "          1       0.86      0.92      0.89        13\n",
      "          2       0.92      0.92      0.92        12\n",
      "          3       0.43      0.30      0.35        10\n",
      "          4       0.88      0.83      0.86        18\n",
      "          5       0.95      0.95      0.95        19\n",
      "\n",
      "avg / total       0.82      0.83      0.82       110\n",
      "\n",
      "[32  1  0  4  1  0  1 12  0  0  0  0  0  0 11  0  0  1  6  0  0  3  1  0\n",
      "  2  1  0  0 15  0  0  0  1  0  0 18]\n",
      "LR Accuracy:  0.8272727272727273\n",
      "LR F1:  0.7955224320833523\n",
      "For name:  z_shi\n",
      "total sample size before apply threshold:  180\n",
      "Counter({'0000-0002-3099-3299': 94, '0000-0002-9624-4960': 25, '0000-0003-2388-6695': 22, '0000-0001-5357-1171': 13, '0000-0002-3928-2960': 12, '0000-0002-3865-0098': 9, '0000-0001-9922-3957': 2, '0000-0002-7798-1121': 1, '0000-0002-8328-0305': 1, '0000-0002-5828-1904': 1})\n",
      "['0000-0002-3099-3299', '0000-0001-5357-1171', '0000-0002-3928-2960', '0000-0002-9624-4960', '0000-0003-2388-6695']\n",
      "Total sample size after apply threshold:  166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(166, 247)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(166, 247)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.95      0.93        94\n",
      "          1       1.00      1.00      1.00        13\n",
      "          2       1.00      0.75      0.86        12\n",
      "          3       0.75      0.84      0.79        25\n",
      "          4       1.00      0.86      0.93        22\n",
      "\n",
      "avg / total       0.92      0.91      0.91       166\n",
      "\n",
      "[89  0  0  5  0  0 13  0  0  0  1  0  9  2  0  4  0  0 21  0  3  0  0  0\n",
      " 19]\n",
      "svc Accuracy:  0.9096385542168675\n",
      "svc F1:  0.9016724256798178\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.99      0.93        94\n",
      "          1       1.00      0.92      0.96        13\n",
      "          2       1.00      0.75      0.86        12\n",
      "          3       0.88      0.88      0.88        25\n",
      "          4       1.00      0.64      0.78        22\n",
      "\n",
      "avg / total       0.91      0.90      0.90       166\n",
      "\n",
      "[93  0  0  1  0  1 12  0  0  0  1  0  9  2  0  3  0  0 22  0  8  0  0  0\n",
      " 14]\n",
      "LR Accuracy:  0.9036144578313253\n",
      "LR F1:  0.880984126984127\n",
      "For name:  t_johnson\n",
      "total sample size before apply threshold:  293\n",
      "Counter({'0000-0001-7147-8237': 174, '0000-0002-5998-3270': 76, '0000-0003-3377-6692': 21, '0000-0002-5372-5457': 17, '0000-0002-9499-3538': 2, '0000-0002-6170-5077': 1, '0000-0002-2724-7017': 1, '0000-0001-6596-6437': 1})\n",
      "['0000-0001-7147-8237', '0000-0003-3377-6692', '0000-0002-5372-5457', '0000-0002-5998-3270']\n",
      "Total sample size after apply threshold:  288\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(288, 2876)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(288, 2876)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.99      0.94       174\n",
      "          1       1.00      0.81      0.89        21\n",
      "          2       0.88      0.88      0.88        17\n",
      "          3       1.00      0.79      0.88        76\n",
      "\n",
      "avg / total       0.93      0.92      0.92       288\n",
      "\n",
      "[173   0   1   0   4  17   0   0   2   0  15   0  15   0   1  60]\n",
      "svc Accuracy:  0.9201388888888888\n",
      "svc F1:  0.899915028940638\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.99      0.91       174\n",
      "          1       1.00      0.62      0.76        21\n",
      "          2       0.88      0.41      0.56        17\n",
      "          3       0.98      0.76      0.86        76\n",
      "\n",
      "avg / total       0.89      0.87      0.86       288\n",
      "\n",
      "[173   0   1   0   7  13   0   1  10   0   7   0  18   0   0  58]\n",
      "LR Accuracy:  0.8715277777777778\n",
      "LR F1:  0.7724310759789663\n",
      "For name:  m_ferretti\n",
      "total sample size before apply threshold:  43\n",
      "Counter({'0000-0003-0280-3598': 29, '0000-0003-2961-6362': 8, '0000-0003-0709-3281': 5, '0000-0002-7578-6699': 1})\n",
      "['0000-0003-0280-3598']\n",
      "Total sample size after apply threshold:  29\n",
      "For name:  b_peng\n",
      "total sample size before apply threshold:  25\n",
      "Counter({'0000-0001-8225-2284': 19, '0000-0003-4183-5939': 4, '0000-0002-5599-6779': 1, '0000-0002-1099-1229': 1})\n",
      "['0000-0001-8225-2284']\n",
      "Total sample size after apply threshold:  19\n",
      "For name:  m_fernandes\n",
      "total sample size before apply threshold:  118\n",
      "Counter({'0000-0001-9391-9574': 60, '0000-0002-1840-616X': 28, '0000-0002-0765-474X': 7, '0000-0001-6533-4309': 6, '0000-0001-7969-2107': 3, '0000-0002-1206-1367': 3, '0000-0001-9239-1202': 3, '0000-0001-7536-2506': 3, '0000-0002-0051-3389': 2, '0000-0002-8009-7513': 2, '0000-0002-9556-7741': 1})\n",
      "['0000-0001-9391-9574', '0000-0002-1840-616X']\n",
      "Total sample size after apply threshold:  88\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(88, 170)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(88, 170)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99        60\n",
      "          1       1.00      0.96      0.98        28\n",
      "\n",
      "avg / total       0.99      0.99      0.99        88\n",
      "\n",
      "[60  0  1 27]\n",
      "svc Accuracy:  0.9886363636363636\n",
      "svc F1:  0.9867768595041322\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      1.00      0.96        60\n",
      "          1       1.00      0.82      0.90        28\n",
      "\n",
      "avg / total       0.95      0.94      0.94        88\n",
      "\n",
      "[60  0  5 23]\n",
      "LR Accuracy:  0.9431818181818182\n",
      "LR F1:  0.9309803921568628\n",
      "For name:  l_cui\n",
      "total sample size before apply threshold:  25\n",
      "Counter({'0000-0001-5549-8780': 18, '0000-0001-5706-9525': 3, '0000-0002-5546-5097': 2, '0000-0002-9818-4543': 1, '0000-0001-5907-0538': 1})\n",
      "['0000-0001-5549-8780']\n",
      "Total sample size after apply threshold:  18\n",
      "For name:  s_monteiro\n",
      "total sample size before apply threshold:  50\n",
      "Counter({'0000-0002-4026-5965': 19, '0000-0002-7069-0591': 14, '0000-0003-0059-9837': 7, '0000-0002-3037-9635': 4, '0000-0001-5040-6170': 2, '0000-0002-8784-7276': 2, '0000-0002-1389-3851': 1, '0000-0003-3507-9911': 1})\n",
      "['0000-0002-4026-5965', '0000-0002-7069-0591']\n",
      "Total sample size after apply threshold:  33\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(33, 69)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(33, 69)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.95      0.97        19\n",
      "          1       0.93      1.00      0.97        14\n",
      "\n",
      "avg / total       0.97      0.97      0.97        33\n",
      "\n",
      "[18  1  0 14]\n",
      "svc Accuracy:  0.9696969696969697\n",
      "svc F1:  0.9692451071761417\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        19\n",
      "          1       1.00      1.00      1.00        14\n",
      "\n",
      "avg / total       1.00      1.00      1.00        33\n",
      "\n",
      "[19  0  0 14]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  m_hsieh\n",
      "total sample size before apply threshold:  35\n",
      "Counter({'0000-0003-3636-6250': 15, '0000-0001-5254-1341': 10, '0000-0002-3396-8427': 5, '0000-0002-7833-847X': 4, '0000-0002-3706-6615': 1})\n",
      "['0000-0003-3636-6250', '0000-0001-5254-1341']\n",
      "Total sample size after apply threshold:  25\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(25, 37)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(25, 37)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      1.00      0.91        15\n",
      "          1       1.00      0.70      0.82        10\n",
      "\n",
      "avg / total       0.90      0.88      0.87        25\n",
      "\n",
      "[15  0  3  7]\n",
      "svc Accuracy:  0.88\n",
      "svc F1:  0.8663101604278074\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      1.00      0.88        15\n",
      "          1       1.00      0.60      0.75        10\n",
      "\n",
      "avg / total       0.87      0.84      0.83        25\n",
      "\n",
      "[15  0  4  6]\n",
      "LR Accuracy:  0.84\n",
      "LR F1:  0.8161764705882353\n",
      "For name:  c_nelson\n",
      "total sample size before apply threshold:  26\n",
      "Counter({'0000-0003-1034-140X': 16, '0000-0003-0195-5610': 6, '0000-0001-5824-2457': 1, '0000-0003-2525-3496': 1, '0000-0001-6287-1598': 1, '0000-0002-4114-1710': 1})\n",
      "['0000-0003-1034-140X']\n",
      "Total sample size after apply threshold:  16\n",
      "For name:  j_barnett\n",
      "total sample size before apply threshold:  23\n",
      "Counter({'0000-0003-0664-4168': 13, '0000-0002-0862-0808': 5, '0000-0001-5381-0064': 4, '0000-0002-4213-4010': 1})\n",
      "['0000-0003-0664-4168']\n",
      "Total sample size after apply threshold:  13\n",
      "For name:  j_tian\n",
      "total sample size before apply threshold:  22\n",
      "Counter({'0000-0002-4008-0469': 10, '0000-0001-6373-6953': 9, '0000-0001-5313-1600': 1, '0000-0002-5896-2515': 1, '0000-0001-9555-0387': 1})\n",
      "['0000-0002-4008-0469']\n",
      "Total sample size after apply threshold:  10\n",
      "For name:  f_costa\n",
      "total sample size before apply threshold:  52\n",
      "Counter({'0000-0002-3097-2834': 22, '0000-0001-5398-3942': 14, '0000-0002-6547-6005': 3, '0000-0001-9368-5640': 3, '0000-0001-8981-7049': 3, '0000-0003-3914-6317': 2, '0000-0001-8729-714X': 2, '0000-0003-0562-2514': 1, '0000-0002-1409-5325': 1, '0000-0001-7572-2014': 1})\n",
      "['0000-0002-3097-2834', '0000-0001-5398-3942']\n",
      "Total sample size after apply threshold:  36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(36, 151)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(36, 151)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.95      0.98        22\n",
      "          1       0.93      1.00      0.97        14\n",
      "\n",
      "avg / total       0.97      0.97      0.97        36\n",
      "\n",
      "[21  1  0 14]\n",
      "svc Accuracy:  0.9722222222222222\n",
      "svc F1:  0.971130713712911\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.95      0.98        22\n",
      "          1       0.93      1.00      0.97        14\n",
      "\n",
      "avg / total       0.97      0.97      0.97        36\n",
      "\n",
      "[21  1  0 14]\n",
      "LR Accuracy:  0.9722222222222222\n",
      "LR F1:  0.971130713712911\n",
      "For name:  a_mccarthy\n",
      "total sample size before apply threshold:  88\n",
      "Counter({'0000-0001-7195-6366': 56, '0000-0002-8979-2926': 26, '0000-0001-6896-0225': 4, '0000-0002-3355-9965': 2})\n",
      "['0000-0002-8979-2926', '0000-0001-7195-6366']\n",
      "Total sample size after apply threshold:  82\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(82, 335)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(82, 335)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.88      0.92        26\n",
      "          1       0.95      0.98      0.96        56\n",
      "\n",
      "avg / total       0.95      0.95      0.95        82\n",
      "\n",
      "[23  3  1 55]\n",
      "svc Accuracy:  0.9512195121951219\n",
      "svc F1:  0.9424561403508771\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.73      0.84        26\n",
      "          1       0.89      1.00      0.94        56\n",
      "\n",
      "avg / total       0.92      0.91      0.91        82\n",
      "\n",
      "[19  7  0 56]\n",
      "LR Accuracy:  0.9146341463414634\n",
      "LR F1:  0.8928104575163398\n",
      "For name:  y_cheng\n",
      "total sample size before apply threshold:  177\n",
      "Counter({'0000-0001-9150-4690': 38, '0000-0001-7112-8835': 29, '0000-0002-4423-4381': 17, '0000-0003-0125-4267': 16, '0000-0002-2583-228X': 15, '0000-0001-9776-395X': 14, '0000-0002-7529-4408': 11, '0000-0001-6874-8187': 9, '0000-0003-2571-4707': 8, '0000-0002-2077-5335': 5, '0000-0003-4912-9879': 3, '0000-0002-5939-0010': 2, '0000-0002-2431-3197': 2, '0000-0002-1468-6686': 2, '0000-0001-5858-6161': 2, '0000-0002-5906-7694': 1, '0000-0002-2352-8647': 1, '0000-0003-1137-2099': 1, '0000-0003-0822-4458': 1})\n",
      "['0000-0001-9776-395X', '0000-0002-4423-4381', '0000-0003-0125-4267', '0000-0002-2583-228X', '0000-0002-7529-4408', '0000-0001-7112-8835', '0000-0001-9150-4690']\n",
      "Total sample size after apply threshold:  140\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(140, 169)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(140, 169)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.79      0.88        14\n",
      "          1       0.25      0.29      0.27        17\n",
      "          2       0.68      0.81      0.74        16\n",
      "          3       0.86      0.80      0.83        15\n",
      "          4       0.77      0.91      0.83        11\n",
      "          5       0.79      0.79      0.79        29\n",
      "          6       0.82      0.74      0.78        38\n",
      "\n",
      "avg / total       0.75      0.73      0.74       140\n",
      "\n",
      "[11  0  2  0  0  1  0  0  5  1  1  1  3  6  0  1 13  0  1  1  0  0  3  0\n",
      " 12  0  0  0  0  0  1  0 10  0  0  0  5  1  0  0 23  0  0  6  1  1  1  1\n",
      " 28]\n",
      "svc Accuracy:  0.7285714285714285\n",
      "svc F1:  0.7321325970587055\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.86      0.92        14\n",
      "          1       0.36      0.24      0.29        17\n",
      "          2       0.86      0.75      0.80        16\n",
      "          3       0.92      0.80      0.86        15\n",
      "          4       0.83      0.91      0.87        11\n",
      "          5       0.69      0.83      0.75        29\n",
      "          6       0.74      0.84      0.79        38\n",
      "\n",
      "avg / total       0.75      0.76      0.75       140\n",
      "\n",
      "[12  0  1  0  0  1  0  0  4  1  0  1  5  6  0  1 12  0  1  2  0  0  1  0\n",
      " 12  0  1  1  0  0  0  0 10  0  1  0  2  0  0  0 24  3  0  3  0  1  0  2\n",
      " 32]\n",
      "LR Accuracy:  0.7571428571428571\n",
      "LR F1:  0.7536603914450705\n",
      "For name:  i_hwang\n",
      "total sample size before apply threshold:  84\n",
      "Counter({'0000-0002-6122-4417': 51, '0000-0002-5388-1919': 14, '0000-0002-4973-6823': 7, '0000-0003-2949-3075': 7, '0000-0002-5720-1765': 2, '0000-0002-1291-8973': 1, '0000-0002-4479-9374': 1, '0000-0002-0533-4638': 1})\n",
      "['0000-0002-6122-4417', '0000-0002-5388-1919']\n",
      "Total sample size after apply threshold:  65\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(65, 63)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(65, 63)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97        51\n",
      "          1       1.00      0.79      0.88        14\n",
      "\n",
      "avg / total       0.96      0.95      0.95        65\n",
      "\n",
      "[51  0  3 11]\n",
      "svc Accuracy:  0.9538461538461539\n",
      "svc F1:  0.9257142857142857\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97        51\n",
      "          1       1.00      0.79      0.88        14\n",
      "\n",
      "avg / total       0.96      0.95      0.95        65\n",
      "\n",
      "[51  0  3 11]\n",
      "LR Accuracy:  0.9538461538461539\n",
      "LR F1:  0.9257142857142857\n",
      "For name:  y_liu\n",
      "total sample size before apply threshold:  965\n",
      "Counter({'0000-0001-8327-3108': 54, '0000-0002-4423-6045': 50, '0000-0002-1862-3121': 46, '0000-0002-2245-4893': 42, '0000-0002-7078-5937': 41, '0000-0002-5247-1678': 38, '0000-0002-6388-9674': 38, '0000-0002-7968-0162': 37, '0000-0001-6677-7961': 28, '0000-0003-0802-3832': 27, '0000-0002-1282-4897': 26, '0000-0002-8417-2488': 23, '0000-0003-1590-0995': 22, '0000-0002-6535-6169': 21, '0000-0002-5880-8649': 21, '0000-0001-9636-990X': 19, '0000-0002-7135-723X': 18, '0000-0002-2885-1670': 18, '0000-0001-6222-5641': 18, '0000-0002-4638-0788': 17, '0000-0001-5304-3459': 17, '0000-0003-1112-4255': 17, '0000-0002-2253-3698': 16, '0000-0003-0180-4142': 15, '0000-0001-5293-5930': 15, '0000-0002-4300-4349': 15, '0000-0002-8320-8725': 14, '0000-0003-4439-8818': 12, '0000-0001-8404-3806': 11, '0000-0001-8181-1080': 11, '0000-0002-3961-2691': 11, '0000-0003-4150-1111': 10, '0000-0001-5198-3674': 10, '0000-0003-1278-7114': 10, '0000-0002-2491-9439': 9, '0000-0003-1618-8813': 9, '0000-0002-8637-661X': 9, '0000-0001-6076-9733': 8, '0000-0003-3205-8963': 8, '0000-0001-6506-5903': 7, '0000-0002-2144-4474': 7, '0000-0003-1420-0276': 7, '0000-0002-8784-8543': 6, '0000-0002-5867-5065': 6, '0000-0001-5960-8166': 5, '0000-0002-4428-3562': 5, '0000-0002-7232-144X': 5, '0000-0001-8903-9101': 4, '0000-0003-2728-4907': 4, '0000-0001-9954-7214': 4, '0000-0001-8118-7775': 4, '0000-0002-6622-6489': 3, '0000-0001-8518-5734': 3, '0000-0002-2883-8329': 3, '0000-0001-6188-993X': 3, '0000-0003-2165-775X': 3, '0000-0002-2923-0729': 2, '0000-0002-0084-863X': 2, '0000-0002-4345-0138': 2, '0000-0002-3483-5909': 2, '0000-0003-3698-4892': 2, '0000-0002-1109-7704': 2, '0000-0003-3687-1337': 2, '0000-0002-8903-2062': 2, '0000-0003-1630-4052': 2, '0000-0001-8672-6301': 2, '0000-0002-8195-7706': 2, '0000-0001-9319-5940': 1, '0000-0002-9005-9166': 1, '0000-0001-5030-2435': 1, '0000-0002-1899-2082': 1, '0000-0001-8371-9579': 1, '0000-0002-2089-2486': 1, '0000-0002-4715-3800': 1, '0000-0003-4912-1746': 1, '0000-0003-4464-1785': 1, '0000-0003-4897-5299': 1, '0000-0003-2706-383X': 1, '0000-0003-4671-698X': 1, '0000-0001-8473-0024': 1, '0000-0002-7571-3123': 1, '0000-0002-6893-6724': 1, '0000-0002-3426-8780': 1, '0000-0002-2515-4431': 1, '0000-0001-5617-661X': 1, '0000-0001-5562-7757': 1, '0000-0002-9754-6370': 1, '0000-0001-9642-1042': 1, '0000-0001-6504-4598': 1, '0000-0003-4861-6915': 1, '0000-0002-8843-1555': 1, '0000-0001-8922-9976': 1, '0000-0002-6003-9121': 1, '0000-0002-9665-7140': 1, '0000-0002-9465-1220': 1, '0000-0002-8591-2599': 1, '0000-0002-0949-6917': 1, '0000-0001-6624-7549': 1, '0000-0003-3703-2498': 1, '0000-0002-8681-4472': 1})\n",
      "['0000-0002-7078-5937', '0000-0001-6677-7961', '0000-0002-8320-8725', '0000-0002-5247-1678', '0000-0002-2245-4893', '0000-0002-4638-0788', '0000-0002-6535-6169', '0000-0002-7968-0162', '0000-0003-4439-8818', '0000-0003-1590-0995', '0000-0002-4423-6045', '0000-0003-0802-3832', '0000-0002-7135-723X', '0000-0002-2253-3698', '0000-0002-2885-1670', '0000-0001-8404-3806', '0000-0003-0180-4142', '0000-0002-1282-4897', '0000-0001-9636-990X', '0000-0001-5293-5930', '0000-0003-4150-1111', '0000-0001-5304-3459', '0000-0002-4300-4349', '0000-0001-6222-5641', '0000-0001-8181-1080', '0000-0002-8417-2488', '0000-0002-6388-9674', '0000-0003-1112-4255', '0000-0001-5198-3674', '0000-0002-1862-3121', '0000-0002-3961-2691', '0000-0001-8327-3108', '0000-0003-1278-7114', '0000-0002-5880-8649']\n",
      "Total sample size after apply threshold:  788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(788, 2628)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(788, 2628)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.37      0.61      0.46        41\n",
      "          1       1.00      0.96      0.98        28\n",
      "          2       0.92      0.79      0.85        14\n",
      "          3       0.75      0.87      0.80        38\n",
      "          4       0.85      0.83      0.84        42\n",
      "          5       0.67      0.71      0.69        17\n",
      "          6       0.63      0.57      0.60        21\n",
      "          7       0.93      0.70      0.80        37\n",
      "          8       0.73      0.92      0.81        12\n",
      "          9       0.40      0.45      0.43        22\n",
      "         10       0.57      0.64      0.60        50\n",
      "         11       0.70      0.85      0.77        27\n",
      "         12       0.87      0.72      0.79        18\n",
      "         13       1.00      0.88      0.93        16\n",
      "         14       0.62      0.72      0.67        18\n",
      "         15       0.70      0.64      0.67        11\n",
      "         16       0.88      1.00      0.94        15\n",
      "         17       1.00      0.69      0.82        26\n",
      "         18       1.00      1.00      1.00        19\n",
      "         19       0.46      0.73      0.56        15\n",
      "         20       1.00      1.00      1.00        10\n",
      "         21       0.92      0.65      0.76        17\n",
      "         22       0.93      0.87      0.90        15\n",
      "         23       0.95      1.00      0.97        18\n",
      "         24       0.56      0.45      0.50        11\n",
      "         25       1.00      0.78      0.88        23\n",
      "         26       0.97      0.89      0.93        38\n",
      "         27       0.82      0.82      0.82        17\n",
      "         28       0.86      0.60      0.71        10\n",
      "         29       0.96      0.93      0.95        46\n",
      "         30       1.00      0.91      0.95        11\n",
      "         31       0.77      0.67      0.71        54\n",
      "         32       1.00      0.40      0.57        10\n",
      "         33       1.00      0.86      0.92        21\n",
      "\n",
      "avg / total       0.80      0.77      0.78       788\n",
      "\n",
      "[25  0  0 ...  0  0 18]\n",
      "svc Accuracy:  0.7703045685279187\n",
      "svc F1:  0.7818218558194472\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.47      0.46      0.47        41\n",
      "          1       1.00      0.96      0.98        28\n",
      "          2       0.83      0.71      0.77        14\n",
      "          3       0.79      1.00      0.88        38\n",
      "          4       0.80      0.86      0.83        42\n",
      "          5       0.80      0.71      0.75        17\n",
      "          6       0.93      0.62      0.74        21\n",
      "          7       0.86      0.84      0.85        37\n",
      "          8       0.79      0.92      0.85        12\n",
      "          9       0.57      0.59      0.58        22\n",
      "         10       0.72      0.68      0.70        50\n",
      "         11       0.73      0.89      0.80        27\n",
      "         12       0.93      0.78      0.85        18\n",
      "         13       1.00      1.00      1.00        16\n",
      "         14       0.52      0.72      0.60        18\n",
      "         15       0.75      0.55      0.63        11\n",
      "         16       1.00      0.93      0.97        15\n",
      "         17       0.95      0.77      0.85        26\n",
      "         18       1.00      1.00      1.00        19\n",
      "         19       0.38      0.67      0.49        15\n",
      "         20       1.00      0.90      0.95        10\n",
      "         21       0.93      0.82      0.87        17\n",
      "         22       0.93      0.93      0.93        15\n",
      "         23       0.82      1.00      0.90        18\n",
      "         24       0.86      0.55      0.67        11\n",
      "         25       1.00      0.78      0.88        23\n",
      "         26       0.95      0.95      0.95        38\n",
      "         27       1.00      0.82      0.90        17\n",
      "         28       0.89      0.80      0.84        10\n",
      "         29       0.91      0.93      0.92        46\n",
      "         30       1.00      0.91      0.95        11\n",
      "         31       0.70      0.78      0.74        54\n",
      "         32       0.86      0.60      0.71        10\n",
      "         33       1.00      0.90      0.95        21\n",
      "\n",
      "avg / total       0.82      0.81      0.81       788\n",
      "\n",
      "[19  0  0 ...  0  0 19]\n",
      "LR Accuracy:  0.8083756345177665\n",
      "LR F1:  0.8161964955060889\n",
      "For name:  m_engel\n",
      "total sample size before apply threshold:  100\n",
      "Counter({'0000-0003-3067-077X': 75, '0000-0002-7031-3825': 19, '0000-0001-7602-1340': 4, '0000-0002-2271-4229': 1, '0000-0002-1474-4161': 1})\n",
      "['0000-0002-7031-3825', '0000-0003-3067-077X']\n",
      "Total sample size after apply threshold:  94\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(94, 201)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(94, 201)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.84      0.91        19\n",
      "          1       0.96      1.00      0.98        75\n",
      "\n",
      "avg / total       0.97      0.97      0.97        94\n",
      "\n",
      "[16  3  0 75]\n",
      "svc Accuracy:  0.9680851063829787\n",
      "svc F1:  0.9473389355742297\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.84      0.91        19\n",
      "          1       0.96      1.00      0.98        75\n",
      "\n",
      "avg / total       0.97      0.97      0.97        94\n",
      "\n",
      "[16  3  0 75]\n",
      "LR Accuracy:  0.9680851063829787\n",
      "LR F1:  0.9473389355742297\n",
      "For name:  w_shi\n",
      "total sample size before apply threshold:  148\n",
      "Counter({'0000-0001-6130-1227': 45, '0000-0002-6336-3912': 40, '0000-0002-9155-613X': 36, '0000-0001-5453-1753': 12, '0000-0002-6458-4776': 6, '0000-0002-1320-2635': 5, '0000-0002-3886-7027': 3, '0000-0001-5683-3800': 1})\n",
      "['0000-0001-5453-1753', '0000-0002-6336-3912', '0000-0002-9155-613X', '0000-0001-6130-1227']\n",
      "Total sample size after apply threshold:  133\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(133, 265)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(133, 265)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.75      0.86        12\n",
      "          1       0.91      1.00      0.95        40\n",
      "          2       0.94      0.92      0.93        36\n",
      "          3       1.00      1.00      1.00        45\n",
      "\n",
      "avg / total       0.96      0.95      0.95       133\n",
      "\n",
      "[ 9  1  2  0  0 40  0  0  0  3 33  0  0  0  0 45]\n",
      "svc Accuracy:  0.9548872180451128\n",
      "svc F1:  0.9347753185781353\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.67      0.80        12\n",
      "          1       0.89      0.97      0.93        40\n",
      "          2       0.97      0.92      0.94        36\n",
      "          3       0.96      1.00      0.98        45\n",
      "\n",
      "avg / total       0.94      0.94      0.94       133\n",
      "\n",
      "[ 8  2  1  1  0 39  0  1  0  3 33  0  0  0  0 45]\n",
      "LR Accuracy:  0.9398496240601504\n",
      "LR F1:  0.9124223602484473\n",
      "For name:  d_matthews\n",
      "total sample size before apply threshold:  57\n",
      "Counter({'0000-0003-4611-8795': 36, '0000-0002-3579-3608': 11, '0000-0003-3562-9549': 9, '0000-0002-0516-7470': 1})\n",
      "['0000-0002-3579-3608', '0000-0003-4611-8795']\n",
      "Total sample size after apply threshold:  47\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(47, 330)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(47, 330)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.36      0.50        11\n",
      "          1       0.83      0.97      0.90        36\n",
      "\n",
      "avg / total       0.83      0.83      0.80        47\n",
      "\n",
      "[ 4  7  1 35]\n",
      "svc Accuracy:  0.8297872340425532\n",
      "svc F1:  0.6987179487179488\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.18      0.31        11\n",
      "          1       0.80      1.00      0.89        36\n",
      "\n",
      "avg / total       0.85      0.81      0.75        47\n",
      "\n",
      "[ 2  9  0 36]\n",
      "LR Accuracy:  0.8085106382978723\n",
      "LR F1:  0.5982905982905984\n",
      "For name:  j_christensen\n",
      "total sample size before apply threshold:  203\n",
      "Counter({'0000-0002-4299-9479': 100, '0000-0003-1414-1886': 53, '0000-0002-7641-8302': 32, '0000-0002-6741-5839': 13, '0000-0002-2689-1169': 1, '0000-0002-9231-8029': 1, '0000-0003-4225-3359': 1, '0000-0003-2370-2702': 1, '0000-0002-2495-8905': 1})\n",
      "['0000-0002-7641-8302', '0000-0002-4299-9479', '0000-0002-6741-5839', '0000-0003-1414-1886']\n",
      "Total sample size after apply threshold:  198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(198, 373)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(198, 373)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.88      0.92        32\n",
      "          1       0.99      0.94      0.96       100\n",
      "          2       0.71      0.38      0.50        13\n",
      "          3       0.73      0.92      0.82        53\n",
      "\n",
      "avg / total       0.90      0.89      0.89       198\n",
      "\n",
      "[28  0  0  4  0 94  0  6  0  0  5  8  1  1  2 49]\n",
      "svc Accuracy:  0.8888888888888888\n",
      "svc F1:  0.7997005044136192\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.84      0.90        32\n",
      "          1       0.99      0.95      0.97       100\n",
      "          2       0.83      0.38      0.53        13\n",
      "          3       0.74      0.94      0.83        53\n",
      "\n",
      "avg / total       0.91      0.89      0.89       198\n",
      "\n",
      "[27  0  0  5  0 95  0  5  0  0  5  8  1  1  1 50]\n",
      "LR Accuracy:  0.8939393939393939\n",
      "LR F1:  0.805537456391865\n",
      "For name:  j_sampaio\n",
      "total sample size before apply threshold:  117\n",
      "Counter({'0000-0003-2335-9991': 61, '0000-0001-8145-5274': 48, '0000-0003-4359-493X': 5, '0000-0002-0460-3664': 3})\n",
      "['0000-0003-2335-9991', '0000-0001-8145-5274']\n",
      "Total sample size after apply threshold:  109\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(109, 252)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(109, 252)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.97      0.94        61\n",
      "          1       0.95      0.88      0.91        48\n",
      "\n",
      "avg / total       0.93      0.93      0.93       109\n",
      "\n",
      "[59  2  6 42]\n",
      "svc Accuracy:  0.926605504587156\n",
      "svc F1:  0.924775707384403\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.97      0.93        61\n",
      "          1       0.95      0.85      0.90        48\n",
      "\n",
      "avg / total       0.92      0.92      0.92       109\n",
      "\n",
      "[59  2  7 41]\n",
      "LR Accuracy:  0.9174311926605505\n",
      "LR F1:  0.9151163796833088\n",
      "For name:  j_dias\n",
      "total sample size before apply threshold:  31\n",
      "Counter({'0000-0002-7613-6241': 9, '0000-0002-1150-4357': 9, '0000-0003-3732-7122': 5, '0000-0003-2517-7905': 3, '0000-0002-0966-0537': 3, '0000-0003-4732-7230': 1, '0000-0002-6271-6501': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  p_nunes\n",
      "total sample size before apply threshold:  36\n",
      "Counter({'0000-0002-4598-685X': 19, '0000-0003-4740-8268': 12, '0000-0002-4641-8846': 4, '0000-0003-1693-1267': 1})\n",
      "['0000-0002-4598-685X', '0000-0003-4740-8268']\n",
      "Total sample size after apply threshold:  31\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(31, 108)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(31, 108)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      1.00      0.95        19\n",
      "          1       1.00      0.83      0.91        12\n",
      "\n",
      "avg / total       0.94      0.94      0.93        31\n",
      "\n",
      "[19  0  2 10]\n",
      "svc Accuracy:  0.9354838709677419\n",
      "svc F1:  0.9295454545454546\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.97        19\n",
      "          1       1.00      0.92      0.96        12\n",
      "\n",
      "avg / total       0.97      0.97      0.97        31\n",
      "\n",
      "[19  0  1 11]\n",
      "LR Accuracy:  0.967741935483871\n",
      "LR F1:  0.9654403567447045\n",
      "For name:  c_bauer\n",
      "total sample size before apply threshold:  7\n",
      "Counter({'0000-0001-9511-2491': 4, '0000-0003-3466-7076': 1, '0000-0001-8288-8290': 1, '0000-0002-3368-6681': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  r_patel\n",
      "total sample size before apply threshold:  182\n",
      "Counter({'0000-0002-1526-4303': 128, '0000-0002-7444-5550': 16, '0000-0002-4712-1921': 9, '0000-0003-1586-5595': 8, '0000-0002-3851-8257': 8, '0000-0001-6344-4141': 4, '0000-0001-7667-5918': 3, '0000-0002-8442-0349': 2, '0000-0001-5330-1438': 2, '0000-0002-5398-2496': 1, '0000-0002-3418-0260': 1})\n",
      "['0000-0002-7444-5550', '0000-0002-1526-4303']\n",
      "Total sample size after apply threshold:  144\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(144, 526)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(144, 526)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.44      0.61        16\n",
      "          1       0.93      1.00      0.97       128\n",
      "\n",
      "avg / total       0.94      0.94      0.93       144\n",
      "\n",
      "[  7   9   0 128]\n",
      "svc Accuracy:  0.9375\n",
      "svc F1:  0.7873666940114848\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.06      0.12        16\n",
      "          1       0.90      1.00      0.94       128\n",
      "\n",
      "avg / total       0.91      0.90      0.85       144\n",
      "\n",
      "[  1  15   0 128]\n",
      "LR Accuracy:  0.8958333333333334\n",
      "LR F1:  0.5311482526589972\n",
      "For name:  a_das\n",
      "total sample size before apply threshold:  74\n",
      "Counter({'0000-0002-0883-1816': 14, '0000-0002-7033-1441': 10, '0000-0003-0740-8140': 8, '0000-0001-5924-4235': 6, '0000-0001-7383-9606': 5, '0000-0002-5196-9589': 5, '0000-0002-7510-1805': 5, '0000-0003-1801-7487': 4, '0000-0002-1733-626X': 3, '0000-0003-0616-9715': 3, '0000-0002-7473-6139': 2, '0000-0003-4305-6007': 2, '0000-0002-2101-9056': 2, '0000-0003-0921-8877': 2, '0000-0001-5884-0852': 1, '0000-0002-0445-0012': 1, '0000-0002-0141-0963': 1})\n",
      "['0000-0002-7033-1441', '0000-0002-0883-1816']\n",
      "Total sample size after apply threshold:  24\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(24, 86)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(24, 86)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        10\n",
      "          1       1.00      1.00      1.00        14\n",
      "\n",
      "avg / total       1.00      1.00      1.00        24\n",
      "\n",
      "[10  0  0 14]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        10\n",
      "          1       1.00      1.00      1.00        14\n",
      "\n",
      "avg / total       1.00      1.00      1.00        24\n",
      "\n",
      "[10  0  0 14]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  c_becker\n",
      "total sample size before apply threshold:  110\n",
      "Counter({'0000-0002-1388-1041': 40, '0000-0002-1716-7208': 27, '0000-0002-6369-2185': 17, '0000-0002-7035-6083': 11, '0000-0002-9179-7996': 7, '0000-0003-3406-4670': 6, '0000-0002-8385-0785': 2})\n",
      "['0000-0002-6369-2185', '0000-0002-7035-6083', '0000-0002-1388-1041', '0000-0002-1716-7208']\n",
      "Total sample size after apply threshold:  95\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(95, 389)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(95, 389)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      1.00      0.77        17\n",
      "          1       1.00      0.82      0.90        11\n",
      "          2       1.00      0.85      0.92        40\n",
      "          3       0.96      0.89      0.92        27\n",
      "\n",
      "avg / total       0.92      0.88      0.89        95\n",
      "\n",
      "[17  0  0  0  2  9  0  0  5  0 34  1  3  0  0 24]\n",
      "svc Accuracy:  0.8842105263157894\n",
      "svc F1:  0.8786807786807787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.94      0.80        17\n",
      "          1       1.00      0.82      0.90        11\n",
      "          2       1.00      0.88      0.93        40\n",
      "          3       0.89      0.93      0.91        27\n",
      "\n",
      "avg / total       0.92      0.89      0.90        95\n",
      "\n",
      "[16  0  0  1  2  9  0  0  3  0 35  2  2  0  0 25]\n",
      "LR Accuracy:  0.8947368421052632\n",
      "LR F1:  0.8856060606060606\n",
      "For name:  k_zhu\n",
      "total sample size before apply threshold:  6\n",
      "Counter({'0000-0001-7664-7204': 3, '0000-0003-4361-1138': 1, '0000-0003-2784-3190': 1, '0000-0003-2293-3568': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  a_machado\n",
      "total sample size before apply threshold:  150\n",
      "Counter({'0000-0002-8132-5610': 54, '0000-0002-5677-7332': 30, '0000-0003-4380-3711': 25, '0000-0001-6200-3686': 16, '0000-0003-0732-1571': 14, '0000-0003-1999-1206': 4, '0000-0003-1947-8605': 4, '0000-0001-8957-661X': 2, '0000-0001-9341-5827': 1})\n",
      "['0000-0002-8132-5610', '0000-0002-5677-7332', '0000-0003-4380-3711', '0000-0003-0732-1571', '0000-0001-6200-3686']\n",
      "Total sample size after apply threshold:  139\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(139, 362)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(139, 362)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.72      0.80        54\n",
      "          1       1.00      0.57      0.72        30\n",
      "          2       0.37      0.88      0.52        25\n",
      "          3       1.00      0.71      0.83        14\n",
      "          4       0.88      0.44      0.58        16\n",
      "\n",
      "avg / total       0.83      0.68      0.71       139\n",
      "\n",
      "[39  0 15  0  0  1 17 11  0  1  3  0 22  0  0  0  0  4 10  0  1  0  8  0\n",
      "  7]\n",
      "svc Accuracy:  0.6834532374100719\n",
      "svc F1:  0.6907272696312569\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.98      0.82        54\n",
      "          1       0.92      0.73      0.81        30\n",
      "          2       0.70      0.56      0.62        25\n",
      "          3       1.00      0.71      0.83        14\n",
      "          4       0.89      0.50      0.64        16\n",
      "\n",
      "avg / total       0.80      0.77      0.76       139\n",
      "\n",
      "[53  0  1  0  0  6 22  1  0  1 11  0 14  0  0  1  0  3 10  0  5  2  1  0\n",
      "  8]\n",
      "LR Accuracy:  0.7697841726618705\n",
      "LR F1:  0.7451509971509972\n",
      "For name:  j_alexander\n",
      "total sample size before apply threshold:  31\n",
      "Counter({'0000-0001-6783-4382': 11, '0000-0003-2226-7913': 10, '0000-0002-2258-5738': 5, '0000-0001-9797-6322': 2, '0000-0002-6492-1621': 2, '0000-0001-7734-9428': 1})\n",
      "['0000-0001-6783-4382', '0000-0003-2226-7913']\n",
      "Total sample size after apply threshold:  21\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(21, 123)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(21, 123)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.73      0.84        11\n",
      "          1       0.77      1.00      0.87        10\n",
      "\n",
      "avg / total       0.89      0.86      0.86        21\n",
      "\n",
      "[ 8  3  0 10]\n",
      "svc Accuracy:  0.8571428571428571\n",
      "svc F1:  0.8558352402745997\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.82      0.90        11\n",
      "          1       0.83      1.00      0.91        10\n",
      "\n",
      "avg / total       0.92      0.90      0.90        21\n",
      "\n",
      "[ 9  2  0 10]\n",
      "LR Accuracy:  0.9047619047619048\n",
      "LR F1:  0.9045454545454545\n",
      "For name:  j_schneider\n",
      "total sample size before apply threshold:  40\n",
      "Counter({'0000-0001-8016-8687': 13, '0000-0002-6028-9956': 7, '0000-0003-1114-618X': 5, '0000-0001-7169-3973': 5, '0000-0003-1176-8309': 3, '0000-0001-5187-6756': 3, '0000-0002-5863-7747': 1, '0000-0001-6093-5404': 1, '0000-0001-5556-0919': 1, '0000-0001-9610-6501': 1})\n",
      "['0000-0001-8016-8687']\n",
      "Total sample size after apply threshold:  13\n",
      "For name:  g_russo\n",
      "total sample size before apply threshold:  58\n",
      "Counter({'0000-0002-8764-7389': 22, '0000-0002-2716-369X': 11, '0000-0003-1493-1087': 7, '0000-0001-9321-1613': 5, '0000-0003-4687-7353': 5, '0000-0001-5001-3027': 4, '0000-0002-4565-3131': 2, '0000-0003-4215-1926': 1, '0000-0002-7779-6225': 1})\n",
      "['0000-0002-8764-7389', '0000-0002-2716-369X']\n",
      "Total sample size after apply threshold:  33\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(33, 175)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(33, 175)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.91      0.95        22\n",
      "          1       0.85      1.00      0.92        11\n",
      "\n",
      "avg / total       0.95      0.94      0.94        33\n",
      "\n",
      "[20  2  0 11]\n",
      "svc Accuracy:  0.9393939393939394\n",
      "svc F1:  0.9345238095238095\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.95      0.95        22\n",
      "          1       0.91      0.91      0.91        11\n",
      "\n",
      "avg / total       0.94      0.94      0.94        33\n",
      "\n",
      "[21  1  1 10]\n",
      "LR Accuracy:  0.9393939393939394\n",
      "LR F1:  0.9318181818181819\n",
      "For name:  j_carvalho\n",
      "total sample size before apply threshold:  136\n",
      "Counter({'0000-0002-3015-7821': 49, '0000-0001-5256-1422': 33, '0000-0003-4495-057X': 22, '0000-0003-2362-0010': 15, '0000-0001-9743-438X': 9, '0000-0001-8091-5419': 3, '0000-0002-4235-1242': 2, '0000-0002-4027-735X': 2, '0000-0002-6263-344X': 1})\n",
      "['0000-0003-4495-057X', '0000-0001-5256-1422', '0000-0003-2362-0010', '0000-0002-3015-7821']\n",
      "Total sample size after apply threshold:  119\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(119, 6019)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(119, 6019)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.86      0.88        22\n",
      "          1       0.86      0.94      0.90        33\n",
      "          2       1.00      1.00      1.00        15\n",
      "          3       1.00      0.96      0.98        49\n",
      "\n",
      "avg / total       0.94      0.94      0.94       119\n",
      "\n",
      "[19  3  0  0  2 31  0  0  0  0 15  0  0  2  0 47]\n",
      "svc Accuracy:  0.9411764705882353\n",
      "svc F1:  0.9403595803842265\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.82      0.86        22\n",
      "          1       0.84      0.94      0.89        33\n",
      "          2       1.00      1.00      1.00        15\n",
      "          3       1.00      0.96      0.98        49\n",
      "\n",
      "avg / total       0.94      0.93      0.93       119\n",
      "\n",
      "[18  4  0  0  2 31  0  0  0  0 15  0  0  2  0 47]\n",
      "LR Accuracy:  0.9327731092436975\n",
      "LR F1:  0.9305059523809524\n",
      "For name:  y_nishikawa\n",
      "total sample size before apply threshold:  21\n",
      "Counter({'0000-0002-0739-8491': 10, '0000-0003-3313-1990': 8, '0000-0002-0088-8447': 2, '0000-0002-1113-6937': 1})\n",
      "['0000-0002-0739-8491']\n",
      "Total sample size after apply threshold:  10\n",
      "For name:  j_ward\n",
      "total sample size before apply threshold:  22\n",
      "Counter({'0000-0001-9870-8936': 11, '0000-0002-4108-4330': 5, '0000-0002-4698-8857': 3, '0000-0002-4196-4653': 1, '0000-0003-0289-117X': 1, '0000-0002-4415-5544': 1})\n",
      "['0000-0001-9870-8936']\n",
      "Total sample size after apply threshold:  11\n",
      "For name:  m_singh\n",
      "total sample size before apply threshold:  133\n",
      "Counter({'0000-0002-8072-1769': 52, '0000-0003-3044-1010': 22, '0000-0002-2884-0074': 21, '0000-0002-8396-5451': 21, '0000-0001-8569-8599': 4, '0000-0002-9124-1859': 4, '0000-0001-8526-2955': 3, '0000-0002-9010-0990': 2, '0000-0002-5783-073X': 1, '0000-0003-0051-336X': 1, '0000-0001-9166-626X': 1, '0000-0002-0034-9726': 1})\n",
      "['0000-0003-3044-1010', '0000-0002-8072-1769', '0000-0002-2884-0074', '0000-0002-8396-5451']\n",
      "Total sample size after apply threshold:  116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(116, 295)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(116, 295)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.77      0.87        22\n",
      "          1       0.88      1.00      0.94        52\n",
      "          2       1.00      0.90      0.95        21\n",
      "          3       1.00      1.00      1.00        21\n",
      "\n",
      "avg / total       0.95      0.94      0.94       116\n",
      "\n",
      "[17  5  0  0  0 52  0  0  0  2 19  0  0  0  0 21]\n",
      "svc Accuracy:  0.9396551724137931\n",
      "svc F1:  0.9396829521829523\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.77      0.87        22\n",
      "          1       0.88      1.00      0.94        52\n",
      "          2       1.00      0.90      0.95        21\n",
      "          3       1.00      1.00      1.00        21\n",
      "\n",
      "avg / total       0.95      0.94      0.94       116\n",
      "\n",
      "[17  5  0  0  0 52  0  0  0  2 19  0  0  0  0 21]\n",
      "LR Accuracy:  0.9396551724137931\n",
      "LR F1:  0.9396829521829523\n",
      "For name:  a_bhattacharyya\n",
      "total sample size before apply threshold:  14\n",
      "Counter({'0000-0002-1646-709X': 8, '0000-0002-5948-3364': 3, '0000-0001-7011-2102': 2, '0000-0003-1077-2082': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  e_morris\n",
      "total sample size before apply threshold:  40\n",
      "Counter({'0000-0001-7046-3623': 29, '0000-0003-1893-7515': 7, '0000-0002-5011-6744': 3, '0000-0002-9913-6041': 1})\n",
      "['0000-0001-7046-3623']\n",
      "Total sample size after apply threshold:  29\n",
      "For name:  m_lewis\n",
      "total sample size before apply threshold:  177\n",
      "Counter({'0000-0002-8430-4479': 63, '0000-0002-5735-5318': 35, '0000-0002-6709-9215': 30, '0000-0001-6042-0865': 13, '0000-0002-2062-6006': 11, '0000-0001-9365-5345': 9, '0000-0002-6241-3690': 8, '0000-0001-5918-3444': 3, '0000-0002-1154-9096': 2, '0000-0002-9703-8456': 1, '0000-0003-4410-5720': 1, '0000-0003-0897-1621': 1})\n",
      "['0000-0002-5735-5318', '0000-0002-8430-4479', '0000-0001-6042-0865', '0000-0002-2062-6006', '0000-0002-6709-9215']\n",
      "Total sample size after apply threshold:  152\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(152, 273)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(152, 273)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      1.00      0.74        35\n",
      "          1       0.98      0.78      0.87        63\n",
      "          2       1.00      0.85      0.92        13\n",
      "          3       1.00      0.36      0.53        11\n",
      "          4       0.89      0.80      0.84        30\n",
      "\n",
      "avg / total       0.87      0.81      0.81       152\n",
      "\n",
      "[35  0  0  0  0 11 49  0  0  3  2  0 11  0  0  7  0  0  4  0  5  1  0  0\n",
      " 24]\n",
      "svc Accuracy:  0.8092105263157895\n",
      "svc F1:  0.7792408011178388\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.68      0.97      0.80        35\n",
      "          1       0.97      0.92      0.94        63\n",
      "          2       1.00      0.85      0.92        13\n",
      "          3       1.00      0.27      0.43        11\n",
      "          4       0.93      0.87      0.90        30\n",
      "\n",
      "avg / total       0.90      0.87      0.86       152\n",
      "\n",
      "[34  1  0  0  0  4 58  0  0  1  1  0 11  0  1  8  0  0  3  0  3  1  0  0\n",
      " 26]\n",
      "LR Accuracy:  0.868421052631579\n",
      "LR F1:  0.796975850054067\n",
      "For name:  v_fernandes\n",
      "total sample size before apply threshold:  55\n",
      "Counter({'0000-0001-6060-9035': 17, '0000-0002-3873-2034': 16, '0000-0003-3979-7523': 15, '0000-0002-9671-3923': 6, '0000-0003-0568-2920': 1})\n",
      "['0000-0003-3979-7523', '0000-0002-3873-2034', '0000-0001-6060-9035']\n",
      "Total sample size after apply threshold:  48\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(48, 132)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(48, 132)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.87      0.93        15\n",
      "          1       0.94      1.00      0.97        16\n",
      "          2       0.94      1.00      0.97        17\n",
      "\n",
      "avg / total       0.96      0.96      0.96        48\n",
      "\n",
      "[13  1  1  0 16  0  0  0 17]\n",
      "svc Accuracy:  0.9583333333333334\n",
      "svc F1:  0.9565656565656565\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.93      0.97        15\n",
      "          1       0.94      1.00      0.97        16\n",
      "          2       1.00      1.00      1.00        17\n",
      "\n",
      "avg / total       0.98      0.98      0.98        48\n",
      "\n",
      "[14  1  0  0 16  0  0  0 17]\n",
      "LR Accuracy:  0.9791666666666666\n",
      "LR F1:  0.9784047370254267\n",
      "For name:  m_pinheiro\n",
      "total sample size before apply threshold:  54\n",
      "Counter({'0000-0001-8228-3435': 30, '0000-0002-6931-1355': 16, '0000-0002-5500-7408': 2, '0000-0003-0758-5526': 2, '0000-0003-2523-245X': 2, '0000-0001-5963-8947': 1, '0000-0001-8234-6790': 1})\n",
      "['0000-0001-8228-3435', '0000-0002-6931-1355']\n",
      "Total sample size after apply threshold:  46\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(46, 234)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(46, 234)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        30\n",
      "          1       1.00      1.00      1.00        16\n",
      "\n",
      "avg / total       1.00      1.00      1.00        46\n",
      "\n",
      "[30  0  0 16]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        30\n",
      "          1       1.00      1.00      1.00        16\n",
      "\n",
      "avg / total       1.00      1.00      1.00        46\n",
      "\n",
      "[30  0  0 16]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  j_petersen\n",
      "total sample size before apply threshold:  41\n",
      "Counter({'0000-0001-9615-1310': 12, '0000-0001-6116-5114': 10, '0000-0001-8612-2508': 5, '0000-0003-0138-0693': 5, '0000-0002-4071-0416': 4, '0000-0002-7715-0088': 2, '0000-0001-6857-982X': 1, '0000-0003-2976-308X': 1, '0000-0003-4939-5149': 1})\n",
      "['0000-0001-6116-5114', '0000-0001-9615-1310']\n",
      "Total sample size after apply threshold:  22\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(22, 51)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(22, 51)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.90      0.95        10\n",
      "          1       0.92      1.00      0.96        12\n",
      "\n",
      "avg / total       0.96      0.95      0.95        22\n",
      "\n",
      "[ 9  1  0 12]\n",
      "svc Accuracy:  0.9545454545454546\n",
      "svc F1:  0.9536842105263159\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.90      0.95        10\n",
      "          1       0.92      1.00      0.96        12\n",
      "\n",
      "avg / total       0.96      0.95      0.95        22\n",
      "\n",
      "[ 9  1  0 12]\n",
      "LR Accuracy:  0.9545454545454546\n",
      "LR F1:  0.9536842105263159\n",
      "For name:  k_shimizu\n",
      "total sample size before apply threshold:  103\n",
      "Counter({'0000-0002-0229-6541': 44, '0000-0003-2454-1795': 37, '0000-0003-1574-5526': 10, '0000-0001-8261-8098': 8, '0000-0002-2796-8666': 4})\n",
      "['0000-0002-0229-6541', '0000-0003-1574-5526', '0000-0003-2454-1795']\n",
      "Total sample size after apply threshold:  91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(91, 186)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(91, 186)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      1.00      0.96        44\n",
      "          1       1.00      0.70      0.82        10\n",
      "          2       1.00      0.97      0.99        37\n",
      "\n",
      "avg / total       0.96      0.96      0.95        91\n",
      "\n",
      "[44  0  0  3  7  0  1  0 36]\n",
      "svc Accuracy:  0.9560439560439561\n",
      "svc F1:  0.9221175069193848\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      1.00      0.96        44\n",
      "          1       1.00      0.70      0.82        10\n",
      "          2       1.00      0.97      0.99        37\n",
      "\n",
      "avg / total       0.96      0.96      0.95        91\n",
      "\n",
      "[44  0  0  3  7  0  1  0 36]\n",
      "LR Accuracy:  0.9560439560439561\n",
      "LR F1:  0.9221175069193848\n",
      "For name:  p_shaw\n",
      "total sample size before apply threshold:  57\n",
      "Counter({'0000-0002-8925-2567': 21, '0000-0003-1076-2669': 18, '0000-0003-3698-1608': 12, '0000-0002-3326-3670': 6})\n",
      "['0000-0002-8925-2567', '0000-0003-1076-2669', '0000-0003-3698-1608']\n",
      "Total sample size after apply threshold:  51\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(51, 187)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(51, 187)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.62      0.76        21\n",
      "          1       0.56      1.00      0.72        18\n",
      "          2       1.00      0.50      0.67        12\n",
      "\n",
      "avg / total       0.85      0.73      0.73        51\n",
      "\n",
      "[13  8  0  0 18  0  0  6  6]\n",
      "svc Accuracy:  0.7254901960784313\n",
      "svc F1:  0.7171241830065359\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.71      0.83        21\n",
      "          1       0.60      1.00      0.75        18\n",
      "          2       1.00      0.50      0.67        12\n",
      "\n",
      "avg / total       0.86      0.76      0.76        51\n",
      "\n",
      "[15  6  0  0 18  0  0  6  6]\n",
      "LR Accuracy:  0.7647058823529411\n",
      "LR F1:  0.7499999999999999\n",
      "For name:  g_coppola\n",
      "total sample size before apply threshold:  142\n",
      "Counter({'0000-0002-9574-0081': 61, '0000-0002-8510-6925': 57, '0000-0003-0147-6142': 16, '0000-0003-2675-783X': 7, '0000-0001-7139-3719': 1})\n",
      "['0000-0002-9574-0081', '0000-0002-8510-6925', '0000-0003-0147-6142']\n",
      "Total sample size after apply threshold:  134\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(134, 482)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(134, 482)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97        61\n",
      "          1       1.00      0.98      0.99        57\n",
      "          2       1.00      0.81      0.90        16\n",
      "\n",
      "avg / total       0.97      0.97      0.97       134\n",
      "\n",
      "[61  0  0  1 56  0  3  0 13]\n",
      "svc Accuracy:  0.9701492537313433\n",
      "svc F1:  0.9519853782899251\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.98        61\n",
      "          1       1.00      0.98      0.99        57\n",
      "          2       1.00      0.88      0.93        16\n",
      "\n",
      "avg / total       0.98      0.98      0.98       134\n",
      "\n",
      "[61  0  0  1 56  0  2  0 14]\n",
      "LR Accuracy:  0.9776119402985075\n",
      "LR F1:  0.9668279252704032\n",
      "For name:  a_sinclair\n",
      "total sample size before apply threshold:  109\n",
      "Counter({'0000-0003-2741-7992': 64, '0000-0001-8510-8691': 31, '0000-0002-2628-1686': 9, '0000-0002-5602-5958': 5})\n",
      "['0000-0003-2741-7992', '0000-0001-8510-8691']\n",
      "Total sample size after apply threshold:  95\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(95, 276)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(95, 276)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.95      0.98        64\n",
      "          1       0.91      1.00      0.95        31\n",
      "\n",
      "avg / total       0.97      0.97      0.97        95\n",
      "\n",
      "[61  3  0 31]\n",
      "svc Accuracy:  0.968421052631579\n",
      "svc F1:  0.964923076923077\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98        64\n",
      "          1       0.97      0.97      0.97        31\n",
      "\n",
      "avg / total       0.98      0.98      0.98        95\n",
      "\n",
      "[63  1  1 30]\n",
      "LR Accuracy:  0.9789473684210527\n",
      "LR F1:  0.9760584677419355\n",
      "For name:  y_pan\n",
      "total sample size before apply threshold:  46\n",
      "Counter({'0000-0001-7709-0508': 15, '0000-0002-6311-2945': 14, '0000-0002-8587-6065': 7, '0000-0002-5547-0849': 3, '0000-0002-1173-1074': 2, '0000-0001-5133-1342': 1, '0000-0002-3945-6377': 1, '0000-0002-0090-1285': 1, '0000-0002-6894-7271': 1, '0000-0002-9195-3776': 1})\n",
      "['0000-0002-6311-2945', '0000-0001-7709-0508']\n",
      "Total sample size after apply threshold:  29\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(29, 62)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(29, 62)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        14\n",
      "          1       1.00      1.00      1.00        15\n",
      "\n",
      "avg / total       1.00      1.00      1.00        29\n",
      "\n",
      "[14  0  0 15]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.93      0.96        14\n",
      "          1       0.94      1.00      0.97        15\n",
      "\n",
      "avg / total       0.97      0.97      0.97        29\n",
      "\n",
      "[13  1  0 15]\n",
      "LR Accuracy:  0.9655172413793104\n",
      "LR F1:  0.965352449223417\n",
      "For name:  m_ramos\n",
      "total sample size before apply threshold:  251\n",
      "Counter({'0000-0002-7554-8324': 187, '0000-0002-8950-2079': 22, '0000-0003-3230-8045': 13, '0000-0002-2157-9774': 8, '0000-0001-6176-5048': 7, '0000-0001-8849-6386': 3, '0000-0001-5224-5665': 3, '0000-0002-2582-7616': 2, '0000-0001-5832-0945': 1, '0000-0001-6594-6591': 1, '0000-0001-6821-3692': 1, '0000-0002-3117-4498': 1, '0000-0003-1133-4164': 1, '0000-0002-9480-782X': 1})\n",
      "['0000-0002-8950-2079', '0000-0003-3230-8045', '0000-0002-7554-8324']\n",
      "Total sample size after apply threshold:  222\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(222, 247)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(222, 247)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.86      0.93        22\n",
      "          1       1.00      1.00      1.00        13\n",
      "          2       0.98      1.00      0.99       187\n",
      "\n",
      "avg / total       0.99      0.99      0.99       222\n",
      "\n",
      "[ 19   0   3   0  13   0   0   0 187]\n",
      "svc Accuracy:  0.9864864864864865\n",
      "svc F1:  0.9729572362036617\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.82      0.90        22\n",
      "          1       1.00      0.77      0.87        13\n",
      "          2       0.96      1.00      0.98       187\n",
      "\n",
      "avg / total       0.97      0.97      0.97       222\n",
      "\n",
      "[ 18   0   4   0  10   3   0   0 187]\n",
      "LR Accuracy:  0.9684684684684685\n",
      "LR F1:  0.9170641713264104\n",
      "For name:  j_tsai\n",
      "total sample size before apply threshold:  153\n",
      "Counter({'0000-0003-2723-6841': 83, '0000-0002-8657-3744': 38, '0000-0002-5227-8894': 16, '0000-0001-5202-722X': 7, '0000-0002-8666-2739': 5, '0000-0002-5332-2818': 2, '0000-0003-1693-9437': 1, '0000-0003-4921-3982': 1})\n",
      "['0000-0003-2723-6841', '0000-0002-8657-3744', '0000-0002-5227-8894']\n",
      "Total sample size after apply threshold:  137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(137, 112)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(137, 112)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.99      0.96        83\n",
      "          1       1.00      0.92      0.96        38\n",
      "          2       0.93      0.81      0.87        16\n",
      "\n",
      "avg / total       0.95      0.95      0.95       137\n",
      "\n",
      "[82  0  1  3 35  0  3  0 13]\n",
      "svc Accuracy:  0.948905109489051\n",
      "svc F1:  0.9282117012470293\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.98      0.95        83\n",
      "          1       0.97      0.92      0.95        38\n",
      "          2       0.93      0.81      0.87        16\n",
      "\n",
      "avg / total       0.94      0.94      0.94       137\n",
      "\n",
      "[81  1  1  3 35  0  3  0 13]\n",
      "LR Accuracy:  0.9416058394160584\n",
      "LR F1:  0.9218512630277336\n",
      "For name:  f_dai\n",
      "total sample size before apply threshold:  34\n",
      "Counter({'0000-0002-7651-8549': 18, '0000-0003-0850-6906': 11, '0000-0002-9229-5576': 4, '0000-0002-2983-4880': 1})\n",
      "['0000-0003-0850-6906', '0000-0002-7651-8549']\n",
      "Total sample size after apply threshold:  29\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(29, 41)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(29, 41)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        11\n",
      "          1       1.00      1.00      1.00        18\n",
      "\n",
      "avg / total       1.00      1.00      1.00        29\n",
      "\n",
      "[11  0  0 18]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        11\n",
      "          1       1.00      1.00      1.00        18\n",
      "\n",
      "avg / total       1.00      1.00      1.00        29\n",
      "\n",
      "[11  0  0 18]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  t_martin\n",
      "total sample size before apply threshold:  83\n",
      "Counter({'0000-0002-4028-4867': 43, '0000-0001-7165-9812': 28, '0000-0002-7872-4194': 7, '0000-0003-2800-5308': 2, '0000-0002-1609-078X': 1, '0000-0002-7302-1190': 1, '0000-0002-6242-6782': 1})\n",
      "['0000-0001-7165-9812', '0000-0002-4028-4867']\n",
      "Total sample size after apply threshold:  71\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(71, 146)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(71, 146)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.64      0.78        28\n",
      "          1       0.81      1.00      0.90        43\n",
      "\n",
      "avg / total       0.89      0.86      0.85        71\n",
      "\n",
      "[18 10  0 43]\n",
      "svc Accuracy:  0.8591549295774648\n",
      "svc F1:  0.8392210144927537\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.64      0.78        28\n",
      "          1       0.81      1.00      0.90        43\n",
      "\n",
      "avg / total       0.89      0.86      0.85        71\n",
      "\n",
      "[18 10  0 43]\n",
      "LR Accuracy:  0.8591549295774648\n",
      "LR F1:  0.8392210144927537\n",
      "For name:  t_o'brien\n",
      "total sample size before apply threshold:  262\n",
      "Counter({'0000-0002-7198-8621': 202, '0000-0002-9161-8070': 39, '0000-0001-9028-5481': 20, '0000-0002-5031-736X': 1})\n",
      "['0000-0002-9161-8070', '0000-0002-7198-8621', '0000-0001-9028-5481']\n",
      "Total sample size after apply threshold:  261\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(261, 987)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(261, 987)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.64      0.78        39\n",
      "          1       0.89      1.00      0.94       202\n",
      "          2       1.00      0.50      0.67        20\n",
      "\n",
      "avg / total       0.92      0.91      0.90       261\n",
      "\n",
      "[ 25  14   0   0 202   0   0  10  10]\n",
      "svc Accuracy:  0.9080459770114943\n",
      "svc F1:  0.7972806334371755\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.56      0.72        39\n",
      "          1       0.87      1.00      0.93       202\n",
      "          2       1.00      0.35      0.52        20\n",
      "\n",
      "avg / total       0.90      0.89      0.87       261\n",
      "\n",
      "[ 22  17   0   0 202   0   0  13   7]\n",
      "LR Accuracy:  0.8850574712643678\n",
      "LR F1:  0.7235685233217404\n",
      "For name:  s_may\n",
      "total sample size before apply threshold:  115\n",
      "Counter({'0000-0003-1813-7745': 59, '0000-0001-5282-3250': 47, '0000-0002-7228-8440': 7, '0000-0001-6762-7500': 2})\n",
      "['0000-0003-1813-7745', '0000-0001-5282-3250']\n",
      "Total sample size after apply threshold:  106\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(106, 366)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(106, 366)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      1.00      0.88        59\n",
      "          1       1.00      0.66      0.79        47\n",
      "\n",
      "avg / total       0.88      0.85      0.84       106\n",
      "\n",
      "[59  0 16 31]\n",
      "svc Accuracy:  0.8490566037735849\n",
      "svc F1:  0.837734404898584\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.98      0.87        59\n",
      "          1       0.97      0.64      0.77        47\n",
      "\n",
      "avg / total       0.86      0.83      0.82       106\n",
      "\n",
      "[58  1 17 30]\n",
      "LR Accuracy:  0.8301886792452831\n",
      "LR F1:  0.817451205510907\n",
      "For name:  z_cai\n",
      "total sample size before apply threshold:  244\n",
      "Counter({'0000-0002-8724-7684': 200, '0000-0002-8937-4943': 27, '0000-0002-9180-675X': 11, '0000-0003-2884-1429': 6})\n",
      "['0000-0002-9180-675X', '0000-0002-8724-7684', '0000-0002-8937-4943']\n",
      "Total sample size after apply threshold:  238\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(238, 221)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(238, 221)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.82      0.86        11\n",
      "          1       0.98      0.99      0.99       200\n",
      "          2       1.00      0.93      0.96        27\n",
      "\n",
      "avg / total       0.98      0.98      0.98       238\n",
      "\n",
      "[  9   2   0   1 199   0   0   2  25]\n",
      "svc Accuracy:  0.9789915966386554\n",
      "svc F1:  0.9354247902634999\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.55      0.71        11\n",
      "          1       0.97      1.00      0.98       200\n",
      "          2       1.00      0.93      0.96        27\n",
      "\n",
      "avg / total       0.97      0.97      0.97       238\n",
      "\n",
      "[  6   5   0   0 200   0   0   2  25]\n",
      "LR Accuracy:  0.9705882352941176\n",
      "LR F1:  0.8834072657602068\n",
      "For name:  a_pereira\n",
      "total sample size before apply threshold:  205\n",
      "Counter({'0000-0003-1378-4273': 47, '0000-0001-9980-441X': 19, '0000-0002-3478-4718': 15, '0000-0002-1053-8715': 14, '0000-0002-7392-2255': 9, '0000-0001-9430-9399': 7, '0000-0002-8587-262X': 7, '0000-0003-2351-1084': 7, '0000-0003-1587-4264': 7, '0000-0003-1344-2118': 7, '0000-0003-3097-7704': 5, '0000-0001-5062-1241': 5, '0000-0002-3897-2732': 5, '0000-0003-3665-7592': 5, '0000-0001-5206-4063': 4, '0000-0001-7616-4683': 4, '0000-0003-4532-6947': 3, '0000-0002-0131-3354': 3, '0000-0002-8573-7364': 3, '0000-0002-4788-0338': 3, '0000-0003-1698-3374': 3, '0000-0002-7616-0444': 3, '0000-0003-2534-1007': 2, '0000-0001-8335-7694': 2, '0000-0003-0824-1063': 2, '0000-0001-7066-1769': 2, '0000-0001-9479-5550': 2, '0000-0003-2291-1350': 2, '0000-0002-5834-9374': 1, '0000-0003-4203-6311': 1, '0000-0002-5468-0932': 1, '0000-0002-2563-6174': 1, '0000-0002-1068-2880': 1, '0000-0003-0038-2064': 1, '0000-0003-3803-2043': 1, '0000-0002-6733-5425': 1})\n",
      "['0000-0002-1053-8715', '0000-0003-1378-4273', '0000-0001-9980-441X', '0000-0002-3478-4718']\n",
      "Total sample size after apply threshold:  95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(95, 305)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(95, 305)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.93      0.96        14\n",
      "          1       0.96      1.00      0.98        47\n",
      "          2       1.00      1.00      1.00        19\n",
      "          3       1.00      0.93      0.97        15\n",
      "\n",
      "avg / total       0.98      0.98      0.98        95\n",
      "\n",
      "[13  1  0  0  0 47  0  0  0  0 19  0  0  1  0 14]\n",
      "svc Accuracy:  0.9789473684210527\n",
      "svc F1:  0.9769117177522351\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.93      0.96        14\n",
      "          1       0.94      1.00      0.97        47\n",
      "          2       1.00      1.00      1.00        19\n",
      "          3       1.00      0.87      0.93        15\n",
      "\n",
      "avg / total       0.97      0.97      0.97        95\n",
      "\n",
      "[13  1  0  0  0 47  0  0  0  0 19  0  0  2  0 13]\n",
      "LR Accuracy:  0.968421052631579\n",
      "LR F1:  0.9651516391207113\n",
      "For name:  d_patel\n",
      "total sample size before apply threshold:  33\n",
      "Counter({'0000-0002-1154-3444': 9, '0000-0002-5744-568X': 8, '0000-0002-2236-7757': 5, '0000-0002-1110-0125': 3, '0000-0002-7198-1163': 2, '0000-0002-3746-8171': 2, '0000-0002-0375-2318': 2, '0000-0002-9592-1990': 2})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  a_james\n",
      "total sample size before apply threshold:  154\n",
      "Counter({'0000-0002-4125-4053': 64, '0000-0002-1411-9307': 37, '0000-0002-0873-3714': 29, '0000-0001-8523-0857': 9, '0000-0002-6174-6696': 4, '0000-0001-8454-6219': 3, '0000-0003-4573-932X': 2, '0000-0001-5655-1213': 2, '0000-0002-0023-4363': 2, '0000-0001-9274-7803': 1, '0000-0002-2002-622X': 1})\n",
      "['0000-0002-4125-4053', '0000-0002-0873-3714', '0000-0002-1411-9307']\n",
      "Total sample size after apply threshold:  130\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(130, 346)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(130, 346)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      1.00      0.91        64\n",
      "          1       1.00      0.90      0.95        29\n",
      "          2       1.00      0.76      0.86        37\n",
      "\n",
      "avg / total       0.92      0.91      0.91       130\n",
      "\n",
      "[64  0  0  3 26  0  9  0 28]\n",
      "svc Accuracy:  0.9076923076923077\n",
      "svc F1:  0.907092907092907\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      1.00      0.91        64\n",
      "          1       0.96      0.90      0.93        29\n",
      "          2       1.00      0.73      0.84        37\n",
      "\n",
      "avg / total       0.91      0.90      0.90       130\n",
      "\n",
      "[64  0  0  3 26  0  9  1 27]\n",
      "LR Accuracy:  0.9\n",
      "LR F1:  0.8955357142857143\n",
      "For name:  c_cao\n",
      "total sample size before apply threshold:  74\n",
      "Counter({'0000-0003-2139-1648': 25, '0000-0003-2830-4383': 20, '0000-0001-8621-8403': 19, '0000-0002-0320-1110': 5, '0000-0002-3407-7837': 4, '0000-0001-6909-5739': 1})\n",
      "['0000-0003-2830-4383', '0000-0003-2139-1648', '0000-0001-8621-8403']\n",
      "Total sample size after apply threshold:  64\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(64, 96)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(64, 96)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.98        20\n",
      "          1       1.00      0.96      0.98        25\n",
      "          2       1.00      1.00      1.00        19\n",
      "\n",
      "avg / total       0.99      0.98      0.98        64\n",
      "\n",
      "[20  0  0  1 24  0  0  0 19]\n",
      "svc Accuracy:  0.984375\n",
      "svc F1:  0.9850671976107517\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.90      0.92        20\n",
      "          1       0.96      0.96      0.96        25\n",
      "          2       0.95      1.00      0.97        19\n",
      "\n",
      "avg / total       0.95      0.95      0.95        64\n",
      "\n",
      "[18  1  1  1 24  0  0  0 19]\n",
      "LR Accuracy:  0.953125\n",
      "LR F1:  0.9524786324786324\n",
      "For name:  c_brown\n",
      "total sample size before apply threshold:  384\n",
      "Counter({'0000-0002-0294-2419': 85, '0000-0002-8959-0101': 60, '0000-0003-2305-846X': 49, '0000-0002-9637-9355': 44, '0000-0003-2506-4871': 33, '0000-0003-0079-7067': 28, '0000-0003-4776-3403': 13, '0000-0002-7271-4091': 12, '0000-0002-0210-1820': 11, '0000-0003-2057-3976': 8, '0000-0002-1559-3238': 8, '0000-0001-6001-2677': 8, '0000-0003-1602-9214': 7, '0000-0003-3060-5652': 6, '0000-0002-7758-6447': 4, '0000-0002-9905-6391': 3, '0000-0003-4780-6485': 2, '0000-0002-9616-2084': 2, '0000-0001-9979-1815': 1})\n",
      "['0000-0002-0294-2419', '0000-0002-0210-1820', '0000-0003-2305-846X', '0000-0002-9637-9355', '0000-0003-0079-7067', '0000-0003-4776-3403', '0000-0003-2506-4871', '0000-0002-8959-0101', '0000-0002-7271-4091']\n",
      "Total sample size after apply threshold:  335\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(335, 1160)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(335, 1160)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.84      0.89        85\n",
      "          1       1.00      0.36      0.53        11\n",
      "          2       0.98      0.90      0.94        49\n",
      "          3       0.96      0.59      0.73        44\n",
      "          4       0.85      0.61      0.71        28\n",
      "          5       1.00      0.23      0.38        13\n",
      "          6       1.00      0.64      0.78        33\n",
      "          7       0.44      1.00      0.62        60\n",
      "          8       1.00      0.42      0.59        12\n",
      "\n",
      "avg / total       0.87      0.75      0.76       335\n",
      "\n",
      "[71  0  0  0  0  0  0 14  0  0  4  0  0  0  0  0  7  0  0  0 44  0  0  0\n",
      "  0  5  0  2  0  0 26  3  0  0 13  0  1  0  0  0 17  0  0 10  0  0  0  0\n",
      "  0  0  3  0 10  0  0  0  1  1  0  0 21 10  0  0  0  0  0  0  0  0 60  0\n",
      "  1  0  0  0  0  0  0  6  5]\n",
      "svc Accuracy:  0.7492537313432835\n",
      "svc F1:  0.6837921036566497\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.92      0.83        85\n",
      "          1       1.00      0.18      0.31        11\n",
      "          2       0.98      0.90      0.94        49\n",
      "          3       1.00      0.73      0.84        44\n",
      "          4       1.00      0.64      0.78        28\n",
      "          5       1.00      0.15      0.27        13\n",
      "          6       1.00      0.79      0.88        33\n",
      "          7       0.54      0.92      0.68        60\n",
      "          8       1.00      0.42      0.59        12\n",
      "\n",
      "avg / total       0.85      0.78      0.77       335\n",
      "\n",
      "[78  0  0  0  0  0  0  7  0  4  2  0  0  0  0  0  5  0  2  0 44  0  0  0\n",
      "  0  3  0  6  0  0 32  0  0  0  6  0  1  0  0  0 18  0  0  9  0  4  0  0\n",
      "  0  0  2  0  7  0  2  0  1  0  0  0 26  4  0  5  0  0  0  0  0  0 55  0\n",
      "  2  0  0  0  0  0  0  5  5]\n",
      "LR Accuracy:  0.7820895522388059\n",
      "LR F1:  0.6792734457019398\n",
      "For name:  y_liang\n",
      "total sample size before apply threshold:  30\n",
      "Counter({'0000-0002-4798-4882': 18, '0000-0002-7224-9687': 5, '0000-0002-7225-7062': 4, '0000-0002-6440-6144': 2, '0000-0001-7756-1621': 1})\n",
      "['0000-0002-4798-4882']\n",
      "Total sample size after apply threshold:  18\n",
      "For name:  y_fan\n",
      "total sample size before apply threshold:  50\n",
      "Counter({'0000-0001-8477-8458': 18, '0000-0001-9677-3777': 11, '0000-0002-1865-4550': 8, '0000-0002-8897-9836': 3, '0000-0002-7919-4148': 3, '0000-0001-8914-4796': 3, '0000-0003-3743-3988': 2, '0000-0002-6551-9394': 1, '0000-0002-4010-9719': 1})\n",
      "['0000-0001-9677-3777', '0000-0001-8477-8458']\n",
      "Total sample size after apply threshold:  29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(29, 123)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(29, 123)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.82      0.90        11\n",
      "          1       0.90      1.00      0.95        18\n",
      "\n",
      "avg / total       0.94      0.93      0.93        29\n",
      "\n",
      "[ 9  2  0 18]\n",
      "svc Accuracy:  0.9310344827586207\n",
      "svc F1:  0.9236842105263159\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.91      0.95        11\n",
      "          1       0.95      1.00      0.97        18\n",
      "\n",
      "avg / total       0.97      0.97      0.97        29\n",
      "\n",
      "[10  1  0 18]\n",
      "LR Accuracy:  0.9655172413793104\n",
      "LR F1:  0.9626769626769627\n",
      "For name:  j_simon\n",
      "total sample size before apply threshold:  93\n",
      "Counter({'0000-0003-0214-3745': 54, '0000-0003-0858-0698': 36, '0000-0003-4824-5667': 1, '0000-0001-6081-4127': 1, '0000-0001-7513-9363': 1})\n",
      "['0000-0003-0214-3745', '0000-0003-0858-0698']\n",
      "Total sample size after apply threshold:  90\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(90, 142)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(90, 142)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.98      0.96        54\n",
      "          1       0.97      0.92      0.94        36\n",
      "\n",
      "avg / total       0.96      0.96      0.96        90\n",
      "\n",
      "[53  1  3 33]\n",
      "svc Accuracy:  0.9555555555555556\n",
      "svc F1:  0.9532467532467532\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.98      0.96        54\n",
      "          1       0.97      0.92      0.94        36\n",
      "\n",
      "avg / total       0.96      0.96      0.96        90\n",
      "\n",
      "[53  1  3 33]\n",
      "LR Accuracy:  0.9555555555555556\n",
      "LR F1:  0.9532467532467532\n",
      "For name:  m_jeong\n",
      "total sample size before apply threshold:  41\n",
      "Counter({'0000-0002-7019-8089': 34, '0000-0003-0669-1386': 3, '0000-0003-2869-1475': 2, '0000-0003-4850-8121': 2})\n",
      "['0000-0002-7019-8089']\n",
      "Total sample size after apply threshold:  34\n",
      "For name:  j_barrett\n",
      "total sample size before apply threshold:  130\n",
      "Counter({'0000-0002-1720-7724': 116, '0000-0002-2222-0579': 9, '0000-0002-7524-6035': 1, '0000-0002-3316-5894': 1, '0000-0002-5573-0401': 1, '0000-0002-4048-1692': 1, '0000-0002-3736-0662': 1})\n",
      "['0000-0002-1720-7724']\n",
      "Total sample size after apply threshold:  116\n",
      "For name:  d_elliott\n",
      "total sample size before apply threshold:  216\n",
      "Counter({'0000-0001-9959-6841': 129, '0000-0002-6081-5442': 59, '0000-0003-1052-7407': 21, '0000-0001-9837-7890': 7})\n",
      "['0000-0002-6081-5442', '0000-0003-1052-7407', '0000-0001-9959-6841']\n",
      "Total sample size after apply threshold:  209\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(209, 398)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(209, 398)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.68      0.81        59\n",
      "          1       0.88      0.67      0.76        21\n",
      "          2       0.84      1.00      0.91       129\n",
      "\n",
      "avg / total       0.89      0.88      0.87       209\n",
      "\n",
      "[ 40   2  17   0  14   7   0   0 129]\n",
      "svc Accuracy:  0.8755980861244019\n",
      "svc F1:  0.8265770606196138\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.63      0.77        59\n",
      "          1       0.91      0.48      0.62        21\n",
      "          2       0.80      1.00      0.89       129\n",
      "\n",
      "avg / total       0.87      0.84      0.83       209\n",
      "\n",
      "[ 37   1  21   0  10  11   0   0 129]\n",
      "LR Accuracy:  0.8421052631578947\n",
      "LR F1:  0.7618295019157086\n",
      "For name:  p_antunes\n",
      "total sample size before apply threshold:  41\n",
      "Counter({'0000-0002-3553-2678': 25, '0000-0003-3324-4151': 10, '0000-0001-9129-3539': 5, '0000-0003-1969-1860': 1})\n",
      "['0000-0002-3553-2678', '0000-0003-3324-4151']\n",
      "Total sample size after apply threshold:  35\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(35, 58)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(35, 58)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.96      0.96        25\n",
      "          1       0.90      0.90      0.90        10\n",
      "\n",
      "avg / total       0.94      0.94      0.94        35\n",
      "\n",
      "[24  1  1  9]\n",
      "svc Accuracy:  0.9428571428571428\n",
      "svc F1:  0.9299999999999999\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98        25\n",
      "          1       1.00      0.90      0.95        10\n",
      "\n",
      "avg / total       0.97      0.97      0.97        35\n",
      "\n",
      "[25  0  1  9]\n",
      "LR Accuracy:  0.9714285714285714\n",
      "LR F1:  0.9638802889576883\n",
      "For name:  x_yuan\n",
      "total sample size before apply threshold:  71\n",
      "Counter({'0000-0002-1632-8460': 38, '0000-0002-8063-9431': 13, '0000-0001-5395-9109': 11, '0000-0002-2891-1354': 5, '0000-0002-6900-6983': 2, '0000-0001-6983-7368': 1, '0000-0001-7280-7207': 1})\n",
      "['0000-0002-1632-8460', '0000-0001-5395-9109', '0000-0002-8063-9431']\n",
      "Total sample size after apply threshold:  62\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(62, 127)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(62, 127)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.92      0.91        38\n",
      "          1       1.00      0.73      0.84        11\n",
      "          2       0.73      0.85      0.79        13\n",
      "\n",
      "avg / total       0.88      0.87      0.87        62\n",
      "\n",
      "[35  0  3  2  8  1  2  0 11]\n",
      "svc Accuracy:  0.8709677419354839\n",
      "svc F1:  0.8456368193210299\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      1.00      0.90        38\n",
      "          1       1.00      0.45      0.62        11\n",
      "          2       0.91      0.77      0.83        13\n",
      "\n",
      "avg / total       0.87      0.85      0.84        62\n",
      "\n",
      "[38  0  0  5  5  1  3  0 10]\n",
      "LR Accuracy:  0.8548387096774194\n",
      "LR F1:  0.7876984126984127\n",
      "For name:  t_kim\n",
      "total sample size before apply threshold:  568\n",
      "Counter({'0000-0003-4982-4441': 109, '0000-0001-5193-1428': 95, '0000-0003-4087-8021': 48, '0000-0003-0806-8969': 39, '0000-0001-6568-2469': 34, '0000-0002-9578-5722': 27, '0000-0001-9827-7531': 27, '0000-0003-2920-9038': 23, '0000-0002-7975-2437': 23, '0000-0001-9802-0568': 22, '0000-0003-3950-7557': 22, '0000-0002-4032-1285': 17, '0000-0001-5328-0913': 15, '0000-0002-2116-4579': 14, '0000-0002-4375-8095': 11, '0000-0001-7071-1455': 10, '0000-0002-5239-3833': 9, '0000-0002-5104-6565': 4, '0000-0002-0691-9072': 4, '0000-0002-9355-7574': 3, '0000-0003-4835-0707': 3, '0000-0002-7683-7259': 2, '0000-0002-6944-4385': 2, '0000-0002-2225-1199': 2, '0000-0002-3594-826X': 1, '0000-0002-6494-1868': 1, '0000-0001-5162-5420': 1})\n",
      "['0000-0001-9802-0568', '0000-0002-4032-1285', '0000-0003-3950-7557', '0000-0001-5328-0913', '0000-0001-6568-2469', '0000-0003-2920-9038', '0000-0002-2116-4579', '0000-0002-4375-8095', '0000-0002-9578-5722', '0000-0001-5193-1428', '0000-0003-4982-4441', '0000-0002-7975-2437', '0000-0003-4087-8021', '0000-0001-7071-1455', '0000-0001-9827-7531', '0000-0003-0806-8969']\n",
      "Total sample size after apply threshold:  536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(536, 770)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(536, 770)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.59      0.60        22\n",
      "          1       0.63      0.71      0.67        17\n",
      "          2       1.00      0.64      0.78        22\n",
      "          3       0.93      0.93      0.93        15\n",
      "          4       0.96      0.71      0.81        34\n",
      "          5       0.57      0.52      0.55        23\n",
      "          6       0.22      0.29      0.25        14\n",
      "          7       0.80      0.73      0.76        11\n",
      "          8       0.95      0.78      0.86        27\n",
      "          9       0.84      0.85      0.85        95\n",
      "         10       0.95      0.94      0.95       109\n",
      "         11       0.94      0.74      0.83        23\n",
      "         12       0.48      0.85      0.61        48\n",
      "         13       0.00      0.00      0.00        10\n",
      "         14       0.37      0.37      0.37        27\n",
      "         15       0.73      0.49      0.58        39\n",
      "\n",
      "avg / total       0.77      0.73      0.74       536\n",
      "\n",
      "[ 13   1   0   0   0   1   0   0   1   1   1   0   1   0   1   2   1  12\n",
      "   0   0   0   0   0   0   0   2   0   0   1   0   1   0   0   0  14   0\n",
      "   0   1   0   0   0   0   0   0   7   0   0   0   0   0   0  14   0   1\n",
      "   0   0   0   0   0   0   0   0   0   0   1   0   0   0  24   1   1   0\n",
      "   0   1   0   0   1   2   3   0   0   0   0   1   0  12   1   1   0   1\n",
      "   0   0   5   1   0   1   0   0   0   0   0   0   4   0   0   0   1   0\n",
      "   2   2   4   1   1   0   0   0   0   0   0   8   0   2   0   0   0   0\n",
      "   0   0   1   0   0   0   0   0   1   0  21   0   0   0   1   0   2   1\n",
      "   2   0   0   0   0   0   1   0   0  81   0   0   9   1   1   0   1   2\n",
      "   0   0   0   0   0   0   0   1 103   0   1   0   0   1   0   2   0   0\n",
      "   0   0   1   0   0   0   0  17   0   2   1   0   0   0   0   0   0   1\n",
      "   1   0   0   2   0   0  41   1   2   0   0   1   0   0   0   0   2   0\n",
      "   0   0   1   1   4   0   1   0   0   0   0   0   1   0   3   0   0   3\n",
      "   0   0   8   1  10   1   1   1   0   0   0   4   3   1   0   2   2   0\n",
      "   5   0   1  19]\n",
      "svc Accuracy:  0.7332089552238806\n",
      "svc F1:  0.6502600667692542\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.41      0.49        22\n",
      "          1       0.69      0.53      0.60        17\n",
      "          2       1.00      0.73      0.84        22\n",
      "          3       0.93      0.87      0.90        15\n",
      "          4       0.90      0.79      0.84        34\n",
      "          5       0.69      0.48      0.56        23\n",
      "          6       0.50      0.29      0.36        14\n",
      "          7       0.73      0.73      0.73        11\n",
      "          8       0.96      0.81      0.88        27\n",
      "          9       0.77      0.91      0.83        95\n",
      "         10       0.92      0.94      0.93       109\n",
      "         11       0.86      0.78      0.82        23\n",
      "         12       0.45      0.85      0.59        48\n",
      "         13       0.50      0.10      0.17        10\n",
      "         14       0.62      0.37      0.47        27\n",
      "         15       0.70      0.67      0.68        39\n",
      "\n",
      "avg / total       0.77      0.75      0.75       536\n",
      "\n",
      "[  9   1   0   0   0   1   0   0   1   2   2   0   3   0   0   3   1   9\n",
      "   0   0   0   0   0   0   0   3   1   0   2   0   1   0   0   0  16   0\n",
      "   0   0   0   0   0   0   0   0   6   0   0   0   0   0   0  13   1   0\n",
      "   0   0   0   0   0   0   1   0   0   0   1   0   0   0  27   0   0   0\n",
      "   0   1   1   0   2   0   2   0   0   0   0   1   0  11   0   1   0   2\n",
      "   1   0   5   0   0   2   0   0   0   0   1   1   4   0   0   2   1   0\n",
      "   3   0   0   2   1   0   0   0   0   0   0   8   0   2   0   0   0   0\n",
      "   0   0   1   0   0   0   0   0   0   0  22   0   0   0   3   0   0   1\n",
      "   1   0   0   0   0   0   0   0   0  86   0   1   6   0   1   0   0   1\n",
      "   0   0   0   0   0   0   0   3 102   1   2   0   0   0   0   0   0   0\n",
      "   1   0   0   1   0   1   0  18   1   1   0   0   0   0   0   0   0   1\n",
      "   1   0   0   2   0   0  41   0   1   2   0   1   0   0   0   0   1   0\n",
      "   0   2   1   0   3   1   0   1   0   0   0   0   0   0   1   0   0   4\n",
      "   1   1  10   0  10   0   1   1   0   0   0   2   1   1   0   1   1   0\n",
      "   4   0   1  26]\n",
      "LR Accuracy:  0.7518656716417911\n",
      "LR F1:  0.6678761805203564\n",
      "For name:  a_cruz\n",
      "total sample size before apply threshold:  80\n",
      "Counter({'0000-0002-0465-4111': 38, '0000-0002-8251-8422': 13, '0000-0002-1662-3072': 10, '0000-0003-0368-9731': 9, '0000-0003-4537-1318': 7, '0000-0002-4591-4362': 3})\n",
      "['0000-0002-8251-8422', '0000-0002-0465-4111', '0000-0002-1662-3072']\n",
      "Total sample size after apply threshold:  61\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(61, 177)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(61, 177)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.92      0.89        13\n",
      "          1       1.00      1.00      1.00        38\n",
      "          2       0.89      0.80      0.84        10\n",
      "\n",
      "avg / total       0.95      0.95      0.95        61\n",
      "\n",
      "[12  0  1  0 38  0  2  0  8]\n",
      "svc Accuracy:  0.9508196721311475\n",
      "svc F1:  0.9103313840155945\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.92      0.86        13\n",
      "          1       1.00      1.00      1.00        38\n",
      "          2       0.88      0.70      0.78        10\n",
      "\n",
      "avg / total       0.94      0.93      0.93        61\n",
      "\n",
      "[12  0  1  0 38  0  3  0  7]\n",
      "LR Accuracy:  0.9344262295081968\n",
      "LR F1:  0.8783068783068783\n",
      "For name:  a_mora\n",
      "total sample size before apply threshold:  84\n",
      "Counter({'0000-0002-0785-5795': 54, '0000-0002-6397-4836': 20, '0000-0003-1344-1131': 5, '0000-0003-1354-4739': 3, '0000-0002-9132-5622': 2})\n",
      "['0000-0002-0785-5795', '0000-0002-6397-4836']\n",
      "Total sample size after apply threshold:  74\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(74, 275)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(74, 275)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.98      0.99        54\n",
      "          1       0.95      1.00      0.98        20\n",
      "\n",
      "avg / total       0.99      0.99      0.99        74\n",
      "\n",
      "[53  1  0 20]\n",
      "svc Accuracy:  0.9864864864864865\n",
      "svc F1:  0.9831319808525188\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        54\n",
      "          1       1.00      1.00      1.00        20\n",
      "\n",
      "avg / total       1.00      1.00      1.00        74\n",
      "\n",
      "[54  0  0 20]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  j_walker\n",
      "total sample size before apply threshold:  253\n",
      "Counter({'0000-0002-8922-083X': 71, '0000-0002-5349-1689': 70, '0000-0002-2050-1641': 64, '0000-0002-2995-0398': 17, '0000-0002-8683-0026': 15, '0000-0001-6034-7514': 9, '0000-0002-9732-5738': 4, '0000-0001-5151-1693': 1, '0000-0003-1349-2633': 1, '0000-0002-8241-9424': 1})\n",
      "['0000-0002-2995-0398', '0000-0002-2050-1641', '0000-0002-8922-083X', '0000-0002-8683-0026', '0000-0002-5349-1689']\n",
      "Total sample size after apply threshold:  237\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(237, 640)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(237, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.88      0.94        17\n",
      "          1       0.77      0.83      0.80        64\n",
      "          2       0.77      0.82      0.79        71\n",
      "          3       1.00      0.87      0.93        15\n",
      "          4       1.00      0.93      0.96        70\n",
      "\n",
      "avg / total       0.87      0.86      0.86       237\n",
      "\n",
      "[15  1  1  0  0  0 53 11  0  0  0 13 58  0  0  0  1  1 13  0  0  1  4  0\n",
      " 65]\n",
      "svc Accuracy:  0.8607594936708861\n",
      "svc F1:  0.8841094841365209\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        17\n",
      "          1       0.89      0.86      0.87        64\n",
      "          2       0.84      0.92      0.88        71\n",
      "          3       1.00      1.00      1.00        15\n",
      "          4       1.00      0.94      0.97        70\n",
      "\n",
      "avg / total       0.92      0.92      0.92       237\n",
      "\n",
      "[17  0  0  0  0  0 55  9  0  0  0  6 65  0  0  0  0  0 15  0  0  1  3  0\n",
      " 66]\n",
      "LR Accuracy:  0.919831223628692\n",
      "LR F1:  0.9443964973376738\n",
      "For name:  j_alves\n",
      "total sample size before apply threshold:  53\n",
      "Counter({'0000-0001-5914-2087': 15, '0000-0001-7221-871X': 13, '0000-0001-7554-2419': 8, '0000-0001-7182-0936': 6, '0000-0002-5736-6519': 4, '0000-0003-3131-9834': 3, '0000-0002-9599-5463': 3, '0000-0002-4355-0921': 1})\n",
      "['0000-0001-5914-2087', '0000-0001-7221-871X']\n",
      "Total sample size after apply threshold:  28\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(28, 120)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(28, 120)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.60      0.72        15\n",
      "          1       0.67      0.92      0.77        13\n",
      "\n",
      "avg / total       0.79      0.75      0.75        28\n",
      "\n",
      "[ 9  6  1 12]\n",
      "svc Accuracy:  0.75\n",
      "svc F1:  0.7470967741935484\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.87      0.93        15\n",
      "          1       0.87      1.00      0.93        13\n",
      "\n",
      "avg / total       0.94      0.93      0.93        28\n",
      "\n",
      "[13  2  0 13]\n",
      "LR Accuracy:  0.9285714285714286\n",
      "LR F1:  0.9285714285714286\n",
      "For name:  j_seo\n",
      "total sample size before apply threshold:  146\n",
      "Counter({'0000-0002-1927-2618': 56, '0000-0003-0242-1805': 47, '0000-0002-5039-2503': 12, '0000-0001-8881-7952': 10, '0000-0001-5095-4046': 6, '0000-0003-3471-7803': 3, '0000-0001-5844-4585': 3, '0000-0002-6582-8162': 2, '0000-0001-5534-2508': 2, '0000-0002-3329-1540': 2, '0000-0002-2878-4551': 1, '0000-0003-2117-5750': 1, '0000-0001-9338-643X': 1})\n",
      "['0000-0003-0242-1805', '0000-0002-5039-2503', '0000-0001-8881-7952', '0000-0002-1927-2618']\n",
      "Total sample size after apply threshold:  125\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(125, 136)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(125, 136)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.89      0.84        47\n",
      "          1       1.00      0.83      0.91        12\n",
      "          2       0.90      0.90      0.90        10\n",
      "          3       0.92      0.86      0.89        56\n",
      "\n",
      "avg / total       0.88      0.87      0.87       125\n",
      "\n",
      "[42  0  1  4  2 10  0  0  1  0  9  0  8  0  0 48]\n",
      "svc Accuracy:  0.872\n",
      "svc F1:  0.8844949494949494\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.91      0.85        47\n",
      "          1       1.00      0.83      0.91        12\n",
      "          2       1.00      0.90      0.95        10\n",
      "          3       0.92      0.86      0.89        56\n",
      "\n",
      "avg / total       0.89      0.88      0.88       125\n",
      "\n",
      "[43  0  0  4  2 10  0  0  1  0  9  0  8  0  0 48]\n",
      "LR Accuracy:  0.88\n",
      "LR F1:  0.8992083418868203\n",
      "For name:  y_tang\n",
      "total sample size before apply threshold:  66\n",
      "Counter({'0000-0003-4888-6771': 34, '0000-0003-2718-544X': 17, '0000-0001-9312-1378': 6, '0000-0002-2649-5270': 5, '0000-0002-8807-9264': 2, '0000-0001-7919-1409': 1, '0000-0003-1096-1764': 1})\n",
      "['0000-0003-4888-6771', '0000-0003-2718-544X']\n",
      "Total sample size after apply threshold:  51\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(51, 173)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(51, 173)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.97      0.94        34\n",
      "          1       0.93      0.82      0.87        17\n",
      "\n",
      "avg / total       0.92      0.92      0.92        51\n",
      "\n",
      "[33  1  3 14]\n",
      "svc Accuracy:  0.9215686274509803\n",
      "svc F1:  0.9089285714285713\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97        34\n",
      "          1       1.00      0.88      0.94        17\n",
      "\n",
      "avg / total       0.96      0.96      0.96        51\n",
      "\n",
      "[34  0  2 15]\n",
      "LR Accuracy:  0.9607843137254902\n",
      "LR F1:  0.9544642857142858\n",
      "For name:  a_norman\n",
      "total sample size before apply threshold:  28\n",
      "Counter({'0000-0002-1282-394X': 16, '0000-0002-4208-2708': 4, '0000-0002-9499-758X': 4, '0000-0002-4332-6049': 3, '0000-0001-6368-521X': 1})\n",
      "['0000-0002-1282-394X']\n",
      "Total sample size after apply threshold:  16\n",
      "For name:  s_tanaka\n",
      "total sample size before apply threshold:  80\n",
      "Counter({'0000-0002-3468-7694': 38, '0000-0001-5157-3317': 24, '0000-0002-1262-3876': 7, '0000-0003-2002-5582': 6, '0000-0002-7101-0690': 4, '0000-0002-2898-9557': 1})\n",
      "['0000-0001-5157-3317', '0000-0002-3468-7694']\n",
      "Total sample size after apply threshold:  62\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(62, 178)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(62, 178)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        24\n",
      "          1       1.00      1.00      1.00        38\n",
      "\n",
      "avg / total       1.00      1.00      1.00        62\n",
      "\n",
      "[24  0  0 38]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98        24\n",
      "          1       1.00      0.97      0.99        38\n",
      "\n",
      "avg / total       0.98      0.98      0.98        62\n",
      "\n",
      "[24  0  1 37]\n",
      "LR Accuracy:  0.9838709677419355\n",
      "LR F1:  0.9831292517006802\n",
      "For name:  c_wen\n",
      "total sample size before apply threshold:  36\n",
      "Counter({'0000-0002-5345-0756': 25, '0000-0002-1181-8786': 6, '0000-0002-7684-8820': 2, '0000-0002-5174-1576': 1, '0000-0002-4445-1589': 1, '0000-0002-2538-0439': 1})\n",
      "['0000-0002-5345-0756']\n",
      "Total sample size after apply threshold:  25\n",
      "For name:  c_myers\n",
      "total sample size before apply threshold:  100\n",
      "Counter({'0000-0002-2776-4823': 92, '0000-0003-1492-9008': 6, '0000-0001-7788-8595': 1, '0000-0001-9860-3931': 1})\n",
      "['0000-0002-2776-4823']\n",
      "Total sample size after apply threshold:  92\n",
      "For name:  v_santos\n",
      "total sample size before apply threshold:  30\n",
      "Counter({'0000-0003-0194-7397': 10, '0000-0002-5370-4867': 9, '0000-0001-8693-0759': 4, '0000-0003-3581-5595': 4, '0000-0002-8518-5408': 1, '0000-0003-1283-7388': 1, '0000-0002-9426-6197': 1})\n",
      "['0000-0003-0194-7397']\n",
      "Total sample size after apply threshold:  10\n",
      "For name:  j_brown\n",
      "total sample size before apply threshold:  290\n",
      "Counter({'0000-0002-2797-5428': 66, '0000-0001-5269-7661': 47, '0000-0002-6936-035X': 27, '0000-0001-8155-677X': 25, '0000-0002-6839-5948': 24, '0000-0002-1447-8633': 15, '0000-0002-0653-4615': 14, '0000-0001-8502-4252': 11, '0000-0002-3155-0334': 10, '0000-0002-2002-3010': 10, '0000-0002-7535-2874': 9, '0000-0002-4681-9586': 8, '0000-0002-4128-4359': 5, '0000-0002-9838-7201': 4, '0000-0001-6486-8667': 3, '0000-0003-3705-0290': 2, '0000-0002-1261-4574': 2, '0000-0002-1100-7457': 2, '0000-0001-6738-9653': 2, '0000-0001-5823-3083': 1, '0000-0002-9125-8474': 1, '0000-0002-2973-1021': 1, '0000-0003-2979-779X': 1})\n",
      "['0000-0002-3155-0334', '0000-0002-1447-8633', '0000-0001-5269-7661', '0000-0002-2797-5428', '0000-0001-8502-4252', '0000-0001-8155-677X', '0000-0002-0653-4615', '0000-0002-6936-035X', '0000-0002-2002-3010', '0000-0002-6839-5948']\n",
      "Total sample size after apply threshold:  249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(249, 569)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(249, 569)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.50      0.67        10\n",
      "          1       0.47      0.53      0.50        15\n",
      "          2       1.00      0.83      0.91        47\n",
      "          3       1.00      0.92      0.96        66\n",
      "          4       1.00      0.45      0.62        11\n",
      "          5       0.55      0.72      0.62        25\n",
      "          6       0.90      0.64      0.75        14\n",
      "          7       1.00      0.93      0.96        27\n",
      "          8       1.00      0.80      0.89        10\n",
      "          9       0.35      0.67      0.46        24\n",
      "\n",
      "avg / total       0.85      0.78      0.80       249\n",
      "\n",
      "[ 5  0  0  0  0  3  1  0  0  1  0  8  0  0  0  1  0  0  0  6  0  1 39  0\n",
      "  0  4  0  0  0  3  0  1  0 61  0  0  0  0  0  4  0  0  0  0  5  1  0  0\n",
      "  0  5  0  4  0  0  0 18  0  0  0  3  0  0  0  0  0  1  9  0  0  4  0  0\n",
      "  0  0  0  0  0 25  0  2  0  0  0  0  0  0  0  0  8  2  0  3  0  0  0  5\n",
      "  0  0  0 16]\n",
      "svc Accuracy:  0.7791164658634538\n",
      "svc F1:  0.7337533194855178\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.50      0.67        10\n",
      "          1       1.00      0.47      0.64        15\n",
      "          2       0.95      0.87      0.91        47\n",
      "          3       0.86      0.95      0.91        66\n",
      "          4       1.00      0.45      0.62        11\n",
      "          5       0.61      0.76      0.68        25\n",
      "          6       0.90      0.64      0.75        14\n",
      "          7       1.00      1.00      1.00        27\n",
      "          8       1.00      0.80      0.89        10\n",
      "          9       0.42      0.71      0.53        24\n",
      "\n",
      "avg / total       0.86      0.81      0.81       249\n",
      "\n",
      "[ 5  0  0  1  0  2  1  0  0  1  0  7  0  1  0  1  0  0  0  6  0  0 41  1\n",
      "  0  4  0  0  0  1  0  0  0 63  0  1  0  0  0  2  0  0  0  1  5  0  0  0\n",
      "  0  5  0  0  1  1  0 19  0  0  0  4  0  0  0  1  0  1  9  0  0  3  0  0\n",
      "  0  0  0  0  0 27  0  0  0  0  0  0  0  1  0  0  8  1  0  0  1  4  0  2\n",
      "  0  0  0 17]\n",
      "LR Accuracy:  0.8072289156626506\n",
      "LR F1:  0.7594326551745617\n",
      "For name:  b_pandey\n",
      "total sample size before apply threshold:  42\n",
      "Counter({'0000-0001-6862-4424': 28, '0000-0002-8453-9665': 11, '0000-0001-7870-6060': 2, '0000-0002-3712-5961': 1})\n",
      "['0000-0001-6862-4424', '0000-0002-8453-9665']\n",
      "Total sample size after apply threshold:  39\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(39, 70)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(39, 70)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.93      0.95        28\n",
      "          1       0.83      0.91      0.87        11\n",
      "\n",
      "avg / total       0.93      0.92      0.92        39\n",
      "\n",
      "[26  2  1 10]\n",
      "svc Accuracy:  0.9230769230769231\n",
      "svc F1:  0.9075098814229249\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      1.00      0.97        28\n",
      "          1       1.00      0.82      0.90        11\n",
      "\n",
      "avg / total       0.95      0.95      0.95        39\n",
      "\n",
      "[28  0  2  9]\n",
      "LR Accuracy:  0.9487179487179487\n",
      "LR F1:  0.9327586206896552\n",
      "For name:  d_morgan\n",
      "total sample size before apply threshold:  86\n",
      "Counter({'0000-0002-2291-1740': 50, '0000-0002-7410-6591': 27, '0000-0001-8725-9477': 7, '0000-0001-7403-4586': 1, '0000-0002-4911-0046': 1})\n",
      "['0000-0002-2291-1740', '0000-0002-7410-6591']\n",
      "Total sample size after apply threshold:  77\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(77, 219)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(77, 219)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      1.00      0.94        50\n",
      "          1       1.00      0.78      0.88        27\n",
      "\n",
      "avg / total       0.93      0.92      0.92        77\n",
      "\n",
      "[50  0  6 21]\n",
      "svc Accuracy:  0.922077922077922\n",
      "svc F1:  0.9091981132075473\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      1.00      0.94        50\n",
      "          1       1.00      0.78      0.88        27\n",
      "\n",
      "avg / total       0.93      0.92      0.92        77\n",
      "\n",
      "[50  0  6 21]\n",
      "LR Accuracy:  0.922077922077922\n",
      "LR F1:  0.9091981132075473\n",
      "For name:  r_smith\n",
      "total sample size before apply threshold:  789\n",
      "Counter({'0000-0002-2381-2349': 587, '0000-0002-5252-9649': 43, '0000-0001-5645-8422': 31, '0000-0002-9174-7681': 19, '0000-0001-8483-6777': 19, '0000-0003-1599-9171': 13, '0000-0003-0245-2265': 13, '0000-0001-9746-1230': 10, '0000-0002-8343-794X': 8, '0000-0002-6881-5690': 8, '0000-0003-4000-2919': 6, '0000-0002-3540-1133': 6, '0000-0003-2502-5098': 6, '0000-0001-9634-2918': 4, '0000-0002-6825-888X': 4, '0000-0003-1209-9653': 3, '0000-0002-9044-9199': 3, '0000-0003-2340-0042': 2, '0000-0002-3794-3788': 2, '0000-0002-7413-4189': 1, '0000-0001-7479-7778': 1})\n",
      "['0000-0003-1599-9171', '0000-0002-9174-7681', '0000-0002-2381-2349', '0000-0001-9746-1230', '0000-0002-5252-9649', '0000-0003-0245-2265', '0000-0001-8483-6777', '0000-0001-5645-8422']\n",
      "Total sample size after apply threshold:  735\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(735, 1464)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(735, 1464)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.23      0.38        13\n",
      "          1       1.00      0.95      0.97        19\n",
      "          2       0.93      0.99      0.96       587\n",
      "          3       1.00      0.70      0.82        10\n",
      "          4       0.88      0.67      0.76        43\n",
      "          5       1.00      0.77      0.87        13\n",
      "          6       1.00      0.84      0.91        19\n",
      "          7       0.95      0.61      0.75        31\n",
      "\n",
      "avg / total       0.93      0.93      0.92       735\n",
      "\n",
      "[  3   0  10   0   0   0   0   0   0  18   1   0   0   0   0   0   0   0\n",
      " 582   0   4   0   0   1   0   0   3   7   0   0   0   0   0   0  14   0\n",
      "  29   0   0   0   0   0   3   0   0  10   0   0   0   0   3   0   0   0\n",
      "  16   0   0   0  12   0   0   0   0  19]\n",
      "svc Accuracy:  0.9306122448979591\n",
      "svc F1:  0.8027042427156563\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.15      0.27        13\n",
      "          1       1.00      0.84      0.91        19\n",
      "          2       0.90      1.00      0.95       587\n",
      "          3       1.00      0.30      0.46        10\n",
      "          4       1.00      0.58      0.74        43\n",
      "          5       1.00      0.62      0.76        13\n",
      "          6       1.00      0.84      0.91        19\n",
      "          7       1.00      0.48      0.65        31\n",
      "\n",
      "avg / total       0.92      0.91      0.90       735\n",
      "\n",
      "[  2   0  11   0   0   0   0   0   0  16   3   0   0   0   0   0   0   0\n",
      " 587   0   0   0   0   0   0   0   7   3   0   0   0   0   0   0  18   0\n",
      "  25   0   0   0   0   0   5   0   0   8   0   0   0   0   3   0   0   0\n",
      "  16   0   0   0  16   0   0   0   0  15]\n",
      "LR Accuracy:  0.9142857142857143\n",
      "LR F1:  0.7069024601023632\n",
      "For name:  a_guerrero\n",
      "total sample size before apply threshold:  57\n",
      "Counter({'0000-0001-5474-1451': 28, '0000-0002-4389-5516': 12, '0000-0001-8602-1248': 9, '0000-0001-6050-8699': 6, '0000-0003-2550-6764': 2})\n",
      "['0000-0002-4389-5516', '0000-0001-5474-1451']\n",
      "Total sample size after apply threshold:  40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(40, 80)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(40, 80)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        12\n",
      "          1       1.00      1.00      1.00        28\n",
      "\n",
      "avg / total       1.00      1.00      1.00        40\n",
      "\n",
      "[12  0  0 28]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        12\n",
      "          1       1.00      1.00      1.00        28\n",
      "\n",
      "avg / total       1.00      1.00      1.00        40\n",
      "\n",
      "[12  0  0 28]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  a_grant\n",
      "total sample size before apply threshold:  45\n",
      "Counter({'0000-0002-1147-2375': 22, '0000-0001-6146-101X': 9, '0000-0001-7205-5869': 7, '0000-0002-7032-3716': 4, '0000-0001-9746-2989': 2, '0000-0002-1553-596X': 1})\n",
      "['0000-0002-1147-2375']\n",
      "Total sample size after apply threshold:  22\n",
      "For name:  v_kumar\n",
      "total sample size before apply threshold:  98\n",
      "Counter({'0000-0003-3522-1121': 18, '0000-0001-6643-7465': 15, '0000-0002-9795-5967': 15, '0000-0001-6477-8274': 9, '0000-0001-5559-0624': 8, '0000-0003-4937-7442': 7, '0000-0003-0910-233X': 7, '0000-0002-7335-0824': 6, '0000-0003-2121-3964': 4, '0000-0002-1583-7749': 3, '0000-0003-1988-2536': 3, '0000-0002-3834-1906': 1, '0000-0002-1513-5835': 1, '0000-0002-3980-1345': 1})\n",
      "['0000-0001-6643-7465', '0000-0003-3522-1121', '0000-0002-9795-5967']\n",
      "Total sample size after apply threshold:  48\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(48, 116)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(48, 116)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.80      0.86        15\n",
      "          1       0.81      0.94      0.87        18\n",
      "          2       1.00      0.93      0.97        15\n",
      "\n",
      "avg / total       0.90      0.90      0.90        48\n",
      "\n",
      "[12  3  0  1 17  0  0  1 14]\n",
      "svc Accuracy:  0.8958333333333334\n",
      "svc F1:  0.8981516567723465\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.87      0.93        15\n",
      "          1       0.84      0.89      0.86        18\n",
      "          2       0.88      0.93      0.90        15\n",
      "\n",
      "avg / total       0.90      0.90      0.90        48\n",
      "\n",
      "[13  2  0  0 16  2  0  1 14]\n",
      "LR Accuracy:  0.8958333333333334\n",
      "LR F1:  0.8988873666293021\n",
      "For name:  p_shah\n",
      "total sample size before apply threshold:  84\n",
      "Counter({'0000-0002-9052-4638': 48, '0000-0001-5497-4765': 12, '0000-0002-1255-9325': 11, '0000-0003-4755-1267': 8, '0000-0002-5839-1687': 3, '0000-0003-1929-9754': 2})\n",
      "['0000-0002-9052-4638', '0000-0001-5497-4765', '0000-0002-1255-9325']\n",
      "Total sample size after apply threshold:  71\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(71, 237)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(71, 237)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97        48\n",
      "          1       1.00      0.75      0.86        12\n",
      "          2       1.00      1.00      1.00        11\n",
      "\n",
      "avg / total       0.96      0.96      0.96        71\n",
      "\n",
      "[48  0  0  3  9  0  0  0 11]\n",
      "svc Accuracy:  0.9577464788732394\n",
      "svc F1:  0.9422799422799423\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97        48\n",
      "          1       1.00      0.75      0.86        12\n",
      "          2       1.00      1.00      1.00        11\n",
      "\n",
      "avg / total       0.96      0.96      0.96        71\n",
      "\n",
      "[48  0  0  3  9  0  0  0 11]\n",
      "LR Accuracy:  0.9577464788732394\n",
      "LR F1:  0.9422799422799423\n",
      "For name:  t_yu\n",
      "total sample size before apply threshold:  134\n",
      "Counter({'0000-0002-0113-2895': 59, '0000-0003-2988-7701': 28, '0000-0002-6737-0618': 17, '0000-0001-5012-9353': 17, '0000-0002-6874-989X': 10, '0000-0002-6724-6043': 1, '0000-0002-1029-6699': 1, '0000-0002-0273-9330': 1})\n",
      "['0000-0002-6874-989X', '0000-0002-0113-2895', '0000-0002-6737-0618', '0000-0003-2988-7701', '0000-0001-5012-9353']\n",
      "Total sample size after apply threshold:  131\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(131, 599)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(131, 599)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        10\n",
      "          1       0.88      0.95      0.91        59\n",
      "          2       0.88      0.88      0.88        17\n",
      "          3       0.95      0.75      0.84        28\n",
      "          4       0.83      0.88      0.86        17\n",
      "\n",
      "avg / total       0.90      0.89      0.89       131\n",
      "\n",
      "[10  0  0  0  0  0 56  0  1  2  0  2 15  0  0  0  4  2 21  1  0  2  0  0\n",
      " 15]\n",
      "svc Accuracy:  0.8931297709923665\n",
      "svc F1:  0.8980129808020768\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        10\n",
      "          1       0.83      1.00      0.91        59\n",
      "          2       0.88      0.88      0.88        17\n",
      "          3       1.00      0.68      0.81        28\n",
      "          4       1.00      0.82      0.90        17\n",
      "\n",
      "avg / total       0.91      0.89      0.89       131\n",
      "\n",
      "[10  0  0  0  0  0 59  0  0  0  0  2 15  0  0  0  7  2 19  0  0  3  0  0\n",
      " 14]\n",
      "LR Accuracy:  0.8931297709923665\n",
      "LR F1:  0.9003563387236527\n",
      "For name:  r_singh\n",
      "total sample size before apply threshold:  197\n",
      "Counter({'0000-0003-4261-7044': 83, '0000-0001-9933-4884': 16, '0000-0001-8094-4703': 15, '0000-0001-6298-8219': 11, '0000-0001-7269-6420': 10, '0000-0001-6358-489X': 8, '0000-0003-3642-0392': 6, '0000-0002-7887-2138': 6, '0000-0002-1195-8738': 6, '0000-0002-2944-2561': 6, '0000-0001-5647-3390': 4, '0000-0001-8068-7428': 4, '0000-0001-7128-5726': 4, '0000-0002-6608-7941': 3, '0000-0002-4842-1336': 3, '0000-0003-0283-3754': 2, '0000-0002-0938-9388': 2, '0000-0001-7049-5473': 2, '0000-0002-1514-5697': 2, '0000-0001-6452-1356': 2, '0000-0003-4022-9945': 1, '0000-0001-8493-4510': 1})\n",
      "['0000-0001-8094-4703', '0000-0001-6298-8219', '0000-0001-7269-6420', '0000-0001-9933-4884', '0000-0003-4261-7044']\n",
      "Total sample size after apply threshold:  135\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(135, 278)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(135, 278)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.87      0.93        15\n",
      "          1       1.00      0.82      0.90        11\n",
      "          2       0.89      0.80      0.84        10\n",
      "          3       1.00      0.88      0.93        16\n",
      "          4       0.92      1.00      0.96        83\n",
      "\n",
      "avg / total       0.94      0.94      0.94       135\n",
      "\n",
      "[13  0  1  0  1  0  9  0  0  2  0  0  8  0  2  0  0  0 14  2  0  0  0  0\n",
      " 83]\n",
      "svc Accuracy:  0.9407407407407408\n",
      "svc F1:  0.9127095194633984\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.87      0.93        15\n",
      "          1       1.00      0.45      0.62        11\n",
      "          2       0.89      0.80      0.84        10\n",
      "          3       1.00      0.88      0.93        16\n",
      "          4       0.88      1.00      0.94        83\n",
      "\n",
      "avg / total       0.92      0.91      0.90       135\n",
      "\n",
      "[13  0  1  0  1  0  5  0  0  6  0  0  8  0  2  0  0  0 14  2  0  0  0  0\n",
      " 83]\n",
      "LR Accuracy:  0.9111111111111111\n",
      "LR F1:  0.8533726264814578\n",
      "For name:  c_baker\n",
      "total sample size before apply threshold:  112\n",
      "Counter({'0000-0001-6861-8964': 49, '0000-0002-4434-3107': 36, '0000-0001-9134-2994': 10, '0000-0002-7622-1251': 6, '0000-0002-9391-2468': 5, '0000-0002-1171-563X': 3, '0000-0002-2675-1078': 2, '0000-0002-6274-0579': 1})\n",
      "['0000-0001-9134-2994', '0000-0001-6861-8964', '0000-0002-4434-3107']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sample size after apply threshold:  95\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(95, 180)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(95, 180)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        10\n",
      "          1       0.87      0.98      0.92        49\n",
      "          2       0.97      0.81      0.88        36\n",
      "\n",
      "avg / total       0.92      0.92      0.91        95\n",
      "\n",
      "[10  0  0  0 48  1  0  7 29]\n",
      "svc Accuracy:  0.9157894736842105\n",
      "svc F1:  0.9339549339549339\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.90      0.95        10\n",
      "          1       0.88      1.00      0.93        49\n",
      "          2       1.00      0.83      0.91        36\n",
      "\n",
      "avg / total       0.94      0.93      0.93        95\n",
      "\n",
      "[ 9  1  0  0 49  0  0  6 30]\n",
      "LR Accuracy:  0.9263157894736842\n",
      "LR F1:  0.9299308878256247\n",
      "For name:  a_cattaneo\n",
      "total sample size before apply threshold:  196\n",
      "Counter({'0000-0002-6975-8923': 127, '0000-0002-9963-848X': 31, '0000-0002-2962-7259': 18, '0000-0002-4500-6540': 12, '0000-0001-5685-3684': 8})\n",
      "['0000-0002-6975-8923', '0000-0002-2962-7259', '0000-0002-4500-6540', '0000-0002-9963-848X']\n",
      "Total sample size after apply threshold:  188\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(188, 606)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(188, 606)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97       127\n",
      "          1       1.00      0.94      0.97        18\n",
      "          2       1.00      0.75      0.86        12\n",
      "          3       1.00      0.87      0.93        31\n",
      "\n",
      "avg / total       0.96      0.96      0.96       188\n",
      "\n",
      "[127   0   0   0   1  17   0   0   3   0   9   0   4   0   0  27]\n",
      "svc Accuracy:  0.9574468085106383\n",
      "svc F1:  0.9322678900462528\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      1.00      0.96       127\n",
      "          1       1.00      0.94      0.97        18\n",
      "          2       1.00      0.75      0.86        12\n",
      "          3       1.00      0.81      0.89        31\n",
      "\n",
      "avg / total       0.95      0.95      0.94       188\n",
      "\n",
      "[127   0   0   0   1  17   0   0   3   0   9   0   6   0   0  25]\n",
      "LR Accuracy:  0.9468085106382979\n",
      "LR F1:  0.9208874458874459\n",
      "For name:  a_ferrari\n",
      "total sample size before apply threshold:  114\n",
      "Counter({'0000-0001-9536-3995': 49, '0000-0002-6166-1350': 18, '0000-0003-1465-2774': 17, '0000-0002-7022-9906': 17, '0000-0002-0387-9984': 9, '0000-0002-6265-4419': 3, '0000-0002-5939-8637': 1})\n",
      "['0000-0003-1465-2774', '0000-0002-7022-9906', '0000-0002-6166-1350', '0000-0001-9536-3995']\n",
      "Total sample size after apply threshold:  101\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(101, 319)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(101, 319)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.82      0.78        17\n",
      "          1       1.00      1.00      1.00        17\n",
      "          2       1.00      0.67      0.80        18\n",
      "          3       0.87      0.94      0.90        49\n",
      "\n",
      "avg / total       0.89      0.88      0.88       101\n",
      "\n",
      "[14  0  0  3  0 17  0  0  2  0 12  4  3  0  0 46]\n",
      "svc Accuracy:  0.8811881188118812\n",
      "svc F1:  0.8699346405228758\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.76      0.87        17\n",
      "          1       1.00      1.00      1.00        17\n",
      "          2       1.00      0.67      0.80        18\n",
      "          3       0.83      1.00      0.91        49\n",
      "\n",
      "avg / total       0.92      0.90      0.90       101\n",
      "\n",
      "[13  0  0  4  0 17  0  0  0  0 12  6  0  0  0 49]\n",
      "LR Accuracy:  0.900990099009901\n",
      "LR F1:  0.8935185185185186\n",
      "For name:  a_murphy\n",
      "total sample size before apply threshold:  178\n",
      "Counter({'0000-0003-4152-4081': 81, '0000-0002-5222-9902': 61, '0000-0002-2547-4750': 20, '0000-0003-4039-9063': 8, '0000-0002-2820-2304': 4, '0000-0002-9983-8641': 2, '0000-0003-2889-503X': 2})\n",
      "['0000-0003-4152-4081', '0000-0002-2547-4750', '0000-0002-5222-9902']\n",
      "Total sample size after apply threshold:  162\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(162, 805)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(162, 805)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.94      0.97        81\n",
      "          1       1.00      0.70      0.82        20\n",
      "          2       0.85      1.00      0.92        61\n",
      "\n",
      "avg / total       0.94      0.93      0.93       162\n",
      "\n",
      "[76  0  5  0 14  6  0  0 61]\n",
      "svc Accuracy:  0.9320987654320988\n",
      "svc F1:  0.902991837029817\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.90      0.95        81\n",
      "          1       1.00      0.70      0.82        20\n",
      "          2       0.81      1.00      0.90        61\n",
      "\n",
      "avg / total       0.93      0.91      0.91       162\n",
      "\n",
      "[73  0  8  0 14  6  0  0 61]\n",
      "LR Accuracy:  0.9135802469135802\n",
      "LR F1:  0.8895467277820218\n",
      "For name:  f_hong\n",
      "total sample size before apply threshold:  41\n",
      "Counter({'0000-0003-1318-2635': 23, '0000-0001-5120-3519': 14, '0000-0003-0060-2063': 2, '0000-0002-4167-6037': 2})\n",
      "['0000-0003-1318-2635', '0000-0001-5120-3519']\n",
      "Total sample size after apply threshold:  37\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(37, 90)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(37, 90)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.91      0.95        23\n",
      "          1       0.88      1.00      0.93        14\n",
      "\n",
      "avg / total       0.95      0.95      0.95        37\n",
      "\n",
      "[21  2  0 14]\n",
      "svc Accuracy:  0.9459459459459459\n",
      "svc F1:  0.9439393939393939\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.96      0.98        23\n",
      "          1       0.93      1.00      0.97        14\n",
      "\n",
      "avg / total       0.97      0.97      0.97        37\n",
      "\n",
      "[22  1  0 14]\n",
      "LR Accuracy:  0.972972972972973\n",
      "LR F1:  0.9716475095785441\n",
      "For name:  m_ferrari\n",
      "total sample size before apply threshold:  150\n",
      "Counter({'0000-0002-3041-2917': 74, '0000-0002-7579-4031': 25, '0000-0002-2986-1272': 22, '0000-0001-6370-605X': 12, '0000-0003-3723-5957': 6, '0000-0001-8535-7348': 5, '0000-0002-7447-6146': 2, '0000-0003-0990-0403': 1, '0000-0003-0283-4263': 1, '0000-0001-7009-6552': 1, '0000-0002-3310-7715': 1})\n",
      "['0000-0002-2986-1272', '0000-0002-3041-2917', '0000-0001-6370-605X', '0000-0002-7579-4031']\n",
      "Total sample size after apply threshold:  133\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(133, 389)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(133, 389)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        22\n",
      "          1       0.96      1.00      0.98        74\n",
      "          2       1.00      0.92      0.96        12\n",
      "          3       1.00      0.92      0.96        25\n",
      "\n",
      "avg / total       0.98      0.98      0.98       133\n",
      "\n",
      "[22  0  0  0  0 74  0  0  0  1 11  0  0  2  0 23]\n",
      "svc Accuracy:  0.9774436090225563\n",
      "svc F1:  0.9737468806987235\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        22\n",
      "          1       0.95      1.00      0.97        74\n",
      "          2       1.00      0.75      0.86        12\n",
      "          3       1.00      0.96      0.98        25\n",
      "\n",
      "avg / total       0.97      0.97      0.97       133\n",
      "\n",
      "[22  0  0  0  0 74  0  0  0  3  9  0  0  1  0 24]\n",
      "LR Accuracy:  0.9699248120300752\n",
      "LR F1:  0.9526047261009667\n",
      "For name:  j_paredes\n",
      "total sample size before apply threshold:  68\n",
      "Counter({'0000-0002-1076-1343': 44, '0000-0002-7788-8939': 9, '0000-0002-0974-8109': 7, '0000-0002-1566-9044': 5, '0000-0002-0620-0770': 3})\n",
      "['0000-0002-1076-1343']\n",
      "Total sample size after apply threshold:  44\n",
      "For name:  z_zhao\n",
      "total sample size before apply threshold:  186\n",
      "Counter({'0000-0003-0654-1193': 79, '0000-0003-2743-9008': 28, '0000-0002-1279-2207': 15, '0000-0002-1876-1284': 15, '0000-0001-6079-1631': 14, '0000-0002-1701-3751': 7, '0000-0002-0862-8471': 6, '0000-0002-2901-5033': 6, '0000-0001-8978-8866': 5, '0000-0002-8679-3130': 4, '0000-0001-8979-844X': 3, '0000-0001-6529-5020': 3, '0000-0002-4577-5470': 1})\n",
      "['0000-0002-1279-2207', '0000-0003-2743-9008', '0000-0001-6079-1631', '0000-0003-0654-1193', '0000-0002-1876-1284']\n",
      "Total sample size after apply threshold:  151\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(151, 194)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(151, 194)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.80      0.89        15\n",
      "          1       0.96      0.86      0.91        28\n",
      "          2       0.69      0.79      0.73        14\n",
      "          3       0.84      0.91      0.87        79\n",
      "          4       0.75      0.60      0.67        15\n",
      "\n",
      "avg / total       0.85      0.85      0.85       151\n",
      "\n",
      "[12  0  0  3  0  0 24  0  4  0  0  0 11  3  0  0  1  3 72  3  0  0  2  4\n",
      "  9]\n",
      "svc Accuracy:  0.847682119205298\n",
      "svc F1:  0.8134553077949302\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.80      0.89        15\n",
      "          1       0.93      0.89      0.91        28\n",
      "          2       0.83      0.36      0.50        14\n",
      "          3       0.78      0.96      0.86        79\n",
      "          4       0.75      0.40      0.52        15\n",
      "\n",
      "avg / total       0.83      0.82      0.80       151\n",
      "\n",
      "[12  0  0  3  0  0 25  0  3  0  0  1  5  7  1  0  1  1 76  1  0  0  0  9\n",
      "  6]\n",
      "LR Accuracy:  0.8211920529801324\n",
      "LR F1:  0.7356951981122947\n",
      "For name:  j_cao\n",
      "total sample size before apply threshold:  39\n",
      "Counter({'0000-0002-3586-2319': 11, '0000-0002-1544-7441': 10, '0000-0001-5938-6604': 8, '0000-0001-5196-8239': 5, '0000-0001-7414-7660': 4, '0000-0001-6171-1170': 1})\n",
      "['0000-0002-3586-2319', '0000-0002-1544-7441']\n",
      "Total sample size after apply threshold:  21\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(21, 309)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(21, 309)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.91      0.95        11\n",
      "          1       0.91      1.00      0.95        10\n",
      "\n",
      "avg / total       0.96      0.95      0.95        21\n",
      "\n",
      "[10  1  0 10]\n",
      "svc Accuracy:  0.9523809523809523\n",
      "svc F1:  0.9523809523809523\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.91      0.95        11\n",
      "          1       0.91      1.00      0.95        10\n",
      "\n",
      "avg / total       0.96      0.95      0.95        21\n",
      "\n",
      "[10  1  0 10]\n",
      "LR Accuracy:  0.9523809523809523\n",
      "LR F1:  0.9523809523809523\n",
      "For name:  d_kuo\n",
      "total sample size before apply threshold:  34\n",
      "Counter({'0000-0002-6461-2562': 17, '0000-0002-3505-0169': 7, '0000-0001-9003-9993': 4, '0000-0001-9300-8551': 3, '0000-0002-7162-174X': 3})\n",
      "['0000-0002-6461-2562']\n",
      "Total sample size after apply threshold:  17\n",
      "For name:  a_andersen\n",
      "total sample size before apply threshold:  18\n",
      "Counter({'0000-0003-0054-1897': 8, '0000-0002-3831-1707': 6, '0000-0001-8169-7273': 3, '0000-0001-9703-3180': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  m_longo\n",
      "total sample size before apply threshold:  44\n",
      "Counter({'0000-0003-1117-1772': 33, '0000-0001-5062-6245': 5, '0000-0002-6364-8184': 3, '0000-0002-2450-4903': 2, '0000-0001-8325-4003': 1})\n",
      "['0000-0003-1117-1772']\n",
      "Total sample size after apply threshold:  33\n",
      "For name:  h_chiang\n",
      "total sample size before apply threshold:  44\n",
      "Counter({'0000-0002-2979-6108': 18, '0000-0001-8781-5146': 14, '0000-0002-2333-9117': 7, '0000-0001-5041-9705': 5})\n",
      "['0000-0002-2979-6108', '0000-0001-8781-5146']\n",
      "Total sample size after apply threshold:  32\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(32, 48)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(32, 48)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.94      0.94        18\n",
      "          1       0.93      0.93      0.93        14\n",
      "\n",
      "avg / total       0.94      0.94      0.94        32\n",
      "\n",
      "[17  1  1 13]\n",
      "svc Accuracy:  0.9375\n",
      "svc F1:  0.9365079365079365\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.89      0.94        18\n",
      "          1       0.88      1.00      0.93        14\n",
      "\n",
      "avg / total       0.95      0.94      0.94        32\n",
      "\n",
      "[16  2  0 14]\n",
      "LR Accuracy:  0.9375\n",
      "LR F1:  0.9372549019607843\n",
      "For name:  m_o'brien\n",
      "total sample size before apply threshold:  34\n",
      "Counter({'0000-0002-8509-3650': 20, '0000-0002-1721-0464': 9, '0000-0003-1096-1991': 4, '0000-0003-4990-3289': 1})\n",
      "['0000-0002-8509-3650']\n",
      "Total sample size after apply threshold:  20\n",
      "For name:  s_ray\n",
      "total sample size before apply threshold:  123\n",
      "Counter({'0000-0002-1051-7260': 75, '0000-0001-5675-1258': 30, '0000-0001-8034-7706': 9, '0000-0002-2414-2930': 5, '0000-0002-4640-708X': 2, '0000-0003-2566-7146': 2})\n",
      "['0000-0002-1051-7260', '0000-0001-5675-1258']\n",
      "Total sample size after apply threshold:  105\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(105, 387)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(105, 387)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99        75\n",
      "          1       1.00      0.97      0.98        30\n",
      "\n",
      "avg / total       0.99      0.99      0.99       105\n",
      "\n",
      "[75  0  1 29]\n",
      "svc Accuracy:  0.9904761904761905\n",
      "svc F1:  0.9882141654506678\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99        75\n",
      "          1       1.00      0.97      0.98        30\n",
      "\n",
      "avg / total       0.99      0.99      0.99       105\n",
      "\n",
      "[75  0  1 29]\n",
      "LR Accuracy:  0.9904761904761905\n",
      "LR F1:  0.9882141654506678\n",
      "For name:  a_cheng\n",
      "total sample size before apply threshold:  636\n",
      "Counter({'0000-0002-9152-6512': 265, '0000-0003-3152-116X': 180, '0000-0003-2345-6951': 71, '0000-0002-1182-7375': 38, '0000-0001-7897-4751': 29, '0000-0003-3862-2967': 25, '0000-0003-2729-606X': 22, '0000-0001-5137-000X': 2, '0000-0002-0977-0381': 2, '0000-0001-5196-3307': 1, '0000-0002-8166-0806': 1})\n",
      "['0000-0003-3862-2967', '0000-0002-1182-7375', '0000-0003-2729-606X', '0000-0003-3152-116X', '0000-0002-9152-6512', '0000-0003-2345-6951', '0000-0001-7897-4751']\n",
      "Total sample size after apply threshold:  630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(630, 2146)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(630, 2146)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.80      0.89        25\n",
      "          1       0.81      0.79      0.80        38\n",
      "          2       0.84      0.73      0.78        22\n",
      "          3       0.87      0.98      0.92       180\n",
      "          4       0.94      0.94      0.94       265\n",
      "          5       0.97      0.83      0.89        71\n",
      "          6       1.00      0.79      0.88        29\n",
      "\n",
      "avg / total       0.91      0.91      0.91       630\n",
      "\n",
      "[ 20   0   0   5   0   0   0   0  30   0   0   8   0   0   0   0  16   6\n",
      "   0   0   0   0   0   1 177   2   0   0   0   6   0   8 249   2   0   0\n",
      "   0   2   4   6  59   0   0   1   0   4   1   0  23]\n",
      "svc Accuracy:  0.9111111111111111\n",
      "svc F1:  0.8725227970951926\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.80      0.89        25\n",
      "          1       0.97      0.74      0.84        38\n",
      "          2       1.00      0.59      0.74        22\n",
      "          3       0.86      0.99      0.93       180\n",
      "          4       0.93      0.98      0.95       265\n",
      "          5       1.00      0.83      0.91        71\n",
      "          6       0.96      0.76      0.85        29\n",
      "\n",
      "avg / total       0.93      0.92      0.92       630\n",
      "\n",
      "[ 20   0   0   5   0   0   0   0  28   0   0  10   0   0   0   0  13   9\n",
      "   0   0   0   0   0   0 179   1   0   0   0   1   0   4 259   0   1   0\n",
      "   0   0   6   6  59   0   0   0   0   4   3   0  22]\n",
      "LR Accuracy:  0.9206349206349206\n",
      "LR F1:  0.8712405089929599\n",
      "For name:  j_savage\n",
      "total sample size before apply threshold:  17\n",
      "Counter({'0000-0002-7756-7166': 6, '0000-0003-0599-0245': 6, '0000-0002-5123-3475': 3, '0000-0002-4737-5673': 2})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  p_matthews\n",
      "total sample size before apply threshold:  329\n",
      "Counter({'0000-0002-1619-8328': 268, '0000-0002-4036-4269': 44, '0000-0002-1362-8003': 10, '0000-0002-2011-9303': 7})\n",
      "['0000-0002-1619-8328', '0000-0002-4036-4269', '0000-0002-1362-8003']\n",
      "Total sample size after apply threshold:  322\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(322, 1264)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(322, 1264)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.99      0.97       268\n",
      "          1       0.97      0.77      0.86        44\n",
      "          2       0.89      0.80      0.84        10\n",
      "\n",
      "avg / total       0.96      0.96      0.95       322\n",
      "\n",
      "[266   1   1  10  34   0   2   0   8]\n",
      "svc Accuracy:  0.9565217391304348\n",
      "svc F1:  0.8924079103959185\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      1.00      0.96       268\n",
      "          1       1.00      0.66      0.79        44\n",
      "          2       1.00      0.50      0.67        10\n",
      "\n",
      "avg / total       0.94      0.94      0.93       322\n",
      "\n",
      "[268   0   0  15  29   0   5   0   5]\n",
      "LR Accuracy:  0.937888198757764\n",
      "LR F1:  0.8084053305300963\n",
      "For name:  i_carvalho\n",
      "total sample size before apply threshold:  39\n",
      "Counter({'0000-0002-2028-777X': 24, '0000-0002-7882-3555': 4, '0000-0002-7569-2019': 3, '0000-0001-7981-4442': 3, '0000-0001-5823-1520': 3, '0000-0002-1811-0588': 2})\n",
      "['0000-0002-2028-777X']\n",
      "Total sample size after apply threshold:  24\n",
      "For name:  j_parsons\n",
      "total sample size before apply threshold:  255\n",
      "Counter({'0000-0002-6875-7566': 212, '0000-0003-1785-3627': 36, '0000-0002-4184-343X': 5, '0000-0002-4856-8610': 1, '0000-0003-1022-6364': 1})\n",
      "['0000-0002-6875-7566', '0000-0003-1785-3627']\n",
      "Total sample size after apply threshold:  248\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(248, 342)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(248, 342)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.99       212\n",
      "          1       1.00      0.83      0.91        36\n",
      "\n",
      "avg / total       0.98      0.98      0.97       248\n",
      "\n",
      "[212   0   6  30]\n",
      "svc Accuracy:  0.9758064516129032\n",
      "svc F1:  0.947568710359408\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98       212\n",
      "          1       1.00      0.75      0.86        36\n",
      "\n",
      "avg / total       0.97      0.96      0.96       248\n",
      "\n",
      "[212   0   9  27]\n",
      "LR Accuracy:  0.9637096774193549\n",
      "LR F1:  0.9181788188716595\n",
      "For name:  s_oliveira\n",
      "total sample size before apply threshold:  143\n",
      "Counter({'0000-0003-4984-4805': 48, '0000-0002-6011-2122': 25, '0000-0001-7919-4191': 23, '0000-0001-8240-0013': 17, '0000-0002-6914-5529': 8, '0000-0002-7322-1184': 8, '0000-0003-0649-2694': 4, '0000-0002-7654-1909': 4, '0000-0002-3504-5749': 3, '0000-0002-8901-9757': 2, '0000-0002-3840-6781': 1})\n",
      "['0000-0003-4984-4805', '0000-0001-7919-4191', '0000-0002-6011-2122', '0000-0001-8240-0013']\n",
      "Total sample size after apply threshold:  113\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(113, 405)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(113, 405)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      1.00      0.95        48\n",
      "          1       1.00      0.91      0.95        23\n",
      "          2       1.00      1.00      1.00        25\n",
      "          3       1.00      0.82      0.90        17\n",
      "\n",
      "avg / total       0.96      0.96      0.96       113\n",
      "\n",
      "[48  0  0  0  2 21  0  0  0  0 25  0  3  0  0 14]\n",
      "svc Accuracy:  0.9557522123893806\n",
      "svc F1:  0.9520665776255045\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      1.00      0.96        48\n",
      "          1       1.00      0.91      0.95        23\n",
      "          2       0.96      1.00      0.98        25\n",
      "          3       1.00      0.82      0.90        17\n",
      "\n",
      "avg / total       0.96      0.96      0.95       113\n",
      "\n",
      "[48  0  0  0  1 21  1  0  0  0 25  0  3  0  0 14]\n",
      "LR Accuracy:  0.9557522123893806\n",
      "LR F1:  0.9495408544649532\n",
      "For name:  h_kang\n",
      "total sample size before apply threshold:  47\n",
      "Counter({'0000-0003-3431-0827': 25, '0000-0001-9671-0944': 6, '0000-0001-8697-4292': 6, '0000-0003-2844-5880': 2, '0000-0002-4952-5524': 1, '0000-0001-6876-4021': 1, '0000-0001-5550-241X': 1, '0000-0001-9073-5833': 1, '0000-0002-0309-7448': 1, '0000-0002-6771-2112': 1, '0000-0001-5036-5612': 1, '0000-0001-6293-0121': 1})\n",
      "['0000-0003-3431-0827']\n",
      "Total sample size after apply threshold:  25\n",
      "For name:  s_vogt\n",
      "total sample size before apply threshold:  93\n",
      "Counter({'0000-0002-8034-5513': 68, '0000-0003-3466-9995': 9, '0000-0002-9009-5183': 8, '0000-0002-0393-5712': 8})\n",
      "['0000-0002-8034-5513']\n",
      "Total sample size after apply threshold:  68\n",
      "For name:  d_garcia\n",
      "total sample size before apply threshold:  60\n",
      "Counter({'0000-0002-8552-1475': 32, '0000-0003-3356-4454': 24, '0000-0002-2820-9151': 2, '0000-0001-6669-9457': 1, '0000-0001-6777-9184': 1})\n",
      "['0000-0003-3356-4454', '0000-0002-8552-1475']\n",
      "Total sample size after apply threshold:  56\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(56, 128)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(56, 128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.92      0.94        24\n",
      "          1       0.94      0.97      0.95        32\n",
      "\n",
      "avg / total       0.95      0.95      0.95        56\n",
      "\n",
      "[22  2  1 31]\n",
      "svc Accuracy:  0.9464285714285714\n",
      "svc F1:  0.9450081833060556\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.96      0.98        24\n",
      "          1       0.97      1.00      0.98        32\n",
      "\n",
      "avg / total       0.98      0.98      0.98        56\n",
      "\n",
      "[23  1  0 32]\n",
      "LR Accuracy:  0.9821428571428571\n",
      "LR F1:  0.9816693944353518\n",
      "For name:  w_xie\n",
      "total sample size before apply threshold:  115\n",
      "Counter({'0000-0002-2768-3572': 44, '0000-0003-2410-2135': 17, '0000-0003-0493-062X': 15, '0000-0003-4655-6496': 10, '0000-0003-4504-8609': 7, '0000-0003-1762-7224': 6, '0000-0002-5500-8195': 6, '0000-0003-2546-2415': 5, '0000-0003-1501-896X': 2, '0000-0002-4887-3711': 1, '0000-0003-3856-9887': 1, '0000-0002-9983-7948': 1})\n",
      "['0000-0003-2410-2135', '0000-0003-4655-6496', '0000-0002-2768-3572', '0000-0003-0493-062X']\n",
      "Total sample size after apply threshold:  86\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(86, 146)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(86, 146)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.65      0.63        17\n",
      "          1       0.75      0.60      0.67        10\n",
      "          2       0.87      0.89      0.88        44\n",
      "          3       0.87      0.87      0.87        15\n",
      "\n",
      "avg / total       0.80      0.80      0.80        86\n",
      "\n",
      "[11  1  4  1  3  6  1  0  3  1 39  1  1  0  1 13]\n",
      "svc Accuracy:  0.8023255813953488\n",
      "svc F1:  0.759577314071696\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.59      0.71        17\n",
      "          1       1.00      0.50      0.67        10\n",
      "          2       0.75      1.00      0.85        44\n",
      "          3       0.91      0.67      0.77        15\n",
      "\n",
      "avg / total       0.84      0.80      0.79        86\n",
      "\n",
      "[10  0  6  1  0  5  5  0  0  0 44  0  1  0  4 10]\n",
      "LR Accuracy:  0.8023255813953488\n",
      "LR F1:  0.7511380205554963\n",
      "For name:  m_cruz\n",
      "total sample size before apply threshold:  141\n",
      "Counter({'0000-0001-9759-5466': 57, '0000-0001-9846-6754': 46, '0000-0003-1822-0514': 30, '0000-0002-4767-530X': 3, '0000-0001-8152-3054': 3, '0000-0003-3311-7582': 2})\n",
      "['0000-0001-9846-6754', '0000-0001-9759-5466', '0000-0003-1822-0514']\n",
      "Total sample size after apply threshold:  133\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(133, 293)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(133, 293)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98        46\n",
      "          1       0.98      0.98      0.98        57\n",
      "          2       0.93      0.93      0.93        30\n",
      "\n",
      "avg / total       0.97      0.97      0.97       133\n",
      "\n",
      "[45  0  1  0 56  1  1  1 28]\n",
      "svc Accuracy:  0.9699248120300752\n",
      "svc F1:  0.9646834477498093\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.98      0.94        46\n",
      "          1       0.98      1.00      0.99        57\n",
      "          2       0.96      0.80      0.87        30\n",
      "\n",
      "avg / total       0.95      0.95      0.95       133\n",
      "\n",
      "[45  0  1  0 57  0  5  1 24]\n",
      "LR Accuracy:  0.9473684210526315\n",
      "LR F1:  0.9338438735177865\n",
      "For name:  w_xu\n",
      "total sample size before apply threshold:  126\n",
      "Counter({'0000-0002-2884-3101': 43, '0000-0002-7085-7814': 20, '0000-0003-4019-5140': 19, '0000-0001-8006-2399': 16, '0000-0002-5976-4991': 6, '0000-0002-3014-756X': 6, '0000-0002-2084-2630': 5, '0000-0003-3681-5052': 2, '0000-0003-0164-4652': 2, '0000-0001-7294-8229': 2, '0000-0001-7598-1489': 2, '0000-0002-5442-8569': 1, '0000-0003-0030-8606': 1, '0000-0001-5588-9300': 1})\n",
      "['0000-0002-2884-3101', '0000-0002-7085-7814', '0000-0001-8006-2399', '0000-0003-4019-5140']\n",
      "Total sample size after apply threshold:  98\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(98, 240)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(98, 240)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.98      0.89        43\n",
      "          1       0.94      0.75      0.83        20\n",
      "          2       1.00      0.81      0.90        16\n",
      "          3       1.00      0.95      0.97        19\n",
      "\n",
      "avg / total       0.91      0.90      0.90        98\n",
      "\n",
      "[42  1  0  0  5 15  0  0  3  0 13  0  1  0  0 18]\n",
      "svc Accuracy:  0.8979591836734694\n",
      "svc F1:  0.8991187629302082\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      1.00      0.91        43\n",
      "          1       1.00      0.75      0.86        20\n",
      "          2       1.00      0.81      0.90        16\n",
      "          3       1.00      0.95      0.97        19\n",
      "\n",
      "avg / total       0.92      0.91      0.91        98\n",
      "\n",
      "[43  0  0  0  5 15  0  0  3  0 13  0  1  0  0 18]\n",
      "LR Accuracy:  0.9081632653061225\n",
      "LR F1:  0.9079826780371245\n",
      "For name:  k_roy\n",
      "total sample size before apply threshold:  131\n",
      "Counter({'0000-0003-4486-8074': 110, '0000-0001-9623-0617': 17, '0000-0002-3694-3663': 2, '0000-0001-7537-0792': 1, '0000-0001-8799-6207': 1})\n",
      "['0000-0003-4486-8074', '0000-0001-9623-0617']\n",
      "Total sample size after apply threshold:  127\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(127, 107)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(127, 107)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99       110\n",
      "          1       1.00      0.88      0.94        17\n",
      "\n",
      "avg / total       0.98      0.98      0.98       127\n",
      "\n",
      "[110   0   2  15]\n",
      "svc Accuracy:  0.984251968503937\n",
      "svc F1:  0.9642454954954954\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99       110\n",
      "          1       1.00      0.88      0.94        17\n",
      "\n",
      "avg / total       0.98      0.98      0.98       127\n",
      "\n",
      "[110   0   2  15]\n",
      "LR Accuracy:  0.984251968503937\n",
      "LR F1:  0.9642454954954954\n",
      "For name:  b_white\n",
      "total sample size before apply threshold:  47\n",
      "Counter({'0000-0002-4293-6128': 29, '0000-0002-0684-5210': 7, '0000-0003-3365-939X': 7, '0000-0002-7477-9956': 3, '0000-0003-4191-3511': 1})\n",
      "['0000-0002-4293-6128']\n",
      "Total sample size after apply threshold:  29\n",
      "For name:  p_graham\n",
      "total sample size before apply threshold:  89\n",
      "Counter({'0000-0002-3745-0940': 33, '0000-0003-2890-2447': 27, '0000-0001-7133-1358': 26, '0000-0002-1600-1601': 3})\n",
      "['0000-0001-7133-1358', '0000-0002-3745-0940', '0000-0003-2890-2447']\n",
      "Total sample size after apply threshold:  86\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(86, 198)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(86, 198)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.81      0.89        26\n",
      "          1       0.94      0.97      0.96        33\n",
      "          2       0.84      0.96      0.90        27\n",
      "\n",
      "avg / total       0.93      0.92      0.92        86\n",
      "\n",
      "[21  1  4  0 32  1  0  1 26]\n",
      "svc Accuracy:  0.9186046511627907\n",
      "svc F1:  0.9151308753371806\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.92      0.96        26\n",
      "          1       0.97      0.97      0.97        33\n",
      "          2       0.90      0.96      0.93        27\n",
      "\n",
      "avg / total       0.96      0.95      0.95        86\n",
      "\n",
      "[24  0  2  0 32  1  0  1 26]\n",
      "LR Accuracy:  0.9534883720930233\n",
      "LR F1:  0.9527561327561328\n",
      "For name:  d_rubin\n",
      "total sample size before apply threshold:  43\n",
      "Counter({'0000-0002-6388-7724': 19, '0000-0002-0483-9458': 13, '0000-0003-1639-6989': 9, '0000-0001-5057-4369': 2})\n",
      "['0000-0002-0483-9458', '0000-0002-6388-7724']\n",
      "Total sample size after apply threshold:  32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(32, 79)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(32, 79)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.92      0.96        13\n",
      "          1       0.95      1.00      0.97        19\n",
      "\n",
      "avg / total       0.97      0.97      0.97        32\n",
      "\n",
      "[12  1  0 19]\n",
      "svc Accuracy:  0.96875\n",
      "svc F1:  0.9671794871794872\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.92      0.96        13\n",
      "          1       0.95      1.00      0.97        19\n",
      "\n",
      "avg / total       0.97      0.97      0.97        32\n",
      "\n",
      "[12  1  0 19]\n",
      "LR Accuracy:  0.96875\n",
      "LR F1:  0.9671794871794872\n",
      "For name:  b_ryan\n",
      "total sample size before apply threshold:  31\n",
      "Counter({'0000-0002-6703-3718': 15, '0000-0001-7213-3273': 11, '0000-0002-5018-2952': 3, '0000-0003-3881-8556': 2})\n",
      "['0000-0001-7213-3273', '0000-0002-6703-3718']\n",
      "Total sample size after apply threshold:  26\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(26, 99)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(26, 99)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      1.00      0.96        11\n",
      "          1       1.00      0.93      0.97        15\n",
      "\n",
      "avg / total       0.96      0.96      0.96        26\n",
      "\n",
      "[11  0  1 14]\n",
      "svc Accuracy:  0.9615384615384616\n",
      "svc F1:  0.9610194902548725\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      1.00      0.96        11\n",
      "          1       1.00      0.93      0.97        15\n",
      "\n",
      "avg / total       0.96      0.96      0.96        26\n",
      "\n",
      "[11  0  1 14]\n",
      "LR Accuracy:  0.9615384615384616\n",
      "LR F1:  0.9610194902548725\n",
      "For name:  j_kim\n",
      "total sample size before apply threshold:  2116\n",
      "Counter({'0000-0003-1835-9436': 200, '0000-0003-3477-1172': 146, '0000-0003-1232-5307': 124, '0000-0001-6537-0350': 78, '0000-0003-0934-3344': 73, '0000-0001-7964-106X': 56, '0000-0003-2337-6935': 52, '0000-0003-2068-7287': 51, '0000-0002-3573-638X': 46, '0000-0003-4085-293X': 41, '0000-0002-6349-6950': 41, '0000-0002-6931-8581': 38, '0000-0002-4171-3803': 38, '0000-0003-0373-5080': 36, '0000-0002-1299-4300': 36, '0000-0002-8383-8524': 33, '0000-0002-0087-1151': 32, '0000-0002-3500-7494': 32, '0000-0002-4687-6732': 31, '0000-0001-5979-5774': 30, '0000-0001-9660-6303': 29, '0000-0002-1903-8354': 28, '0000-0002-5390-8763': 27, '0000-0003-0767-1918': 26, '0000-0002-4747-9763': 25, '0000-0003-0103-7457': 24, '0000-0003-4035-0438': 23, '0000-0003-2841-147X': 23, '0000-0003-0693-1415': 23, '0000-0002-3566-3379': 19, '0000-0003-4978-1867': 18, '0000-0002-9570-4216': 18, '0000-0001-5080-7097': 17, '0000-0002-1672-5730': 17, '0000-0002-9159-0733': 16, '0000-0001-8208-8568': 16, '0000-0002-5329-6605': 16, '0000-0003-0578-0635': 16, '0000-0001-5204-3369': 16, '0000-0002-3729-8774': 15, '0000-0002-6152-2924': 15, '0000-0001-6417-864X': 15, '0000-0001-6426-9074': 15, '0000-0002-0195-1460': 14, '0000-0001-5951-8013': 14, '0000-0002-8218-0062': 13, '0000-0003-1519-3274': 12, '0000-0001-9881-2784': 12, '0000-0003-0530-3425': 12, '0000-0002-1376-9498': 12, '0000-0001-5096-4068': 12, '0000-0003-4217-3228': 11, '0000-0003-4438-1872': 11, '0000-0001-9840-4780': 11, '0000-0001-7649-4244': 11, '0000-0001-7842-2172': 10, '0000-0001-9595-2765': 10, '0000-0003-4157-9365': 10, '0000-0003-4802-010X': 9, '0000-0001-6188-7571': 9, '0000-0002-0484-9189': 8, '0000-0003-0448-1684': 8, '0000-0002-8580-8134': 8, '0000-0002-0359-2887': 8, '0000-0002-7040-7397': 8, '0000-0001-6603-6768': 8, '0000-0002-7419-021X': 7, '0000-0002-4490-3610': 7, '0000-0001-7819-2784': 7, '0000-0002-3849-649X': 6, '0000-0001-8984-2914': 6, '0000-0002-6575-452X': 6, '0000-0003-0462-6521': 5, '0000-0002-2713-1006': 5, '0000-0002-1810-5383': 5, '0000-0002-0066-534X': 4, '0000-0002-1076-1095': 4, '0000-0003-0340-4169': 4, '0000-0002-8321-026X': 4, '0000-0001-7340-2770': 4, '0000-0001-5228-4939': 4, '0000-0001-6210-4540': 4, '0000-0003-1222-0054': 3, '0000-0002-7425-1828': 3, '0000-0003-1522-9038': 3, '0000-0001-7409-6306': 3, '0000-0002-5810-1512': 3, '0000-0002-3502-7604': 3, '0000-0001-8087-7977': 3, '0000-0001-9302-0040': 3, '0000-0002-3010-1641': 3, '0000-0001-6201-9602': 3, '0000-0003-3172-3212': 3, '0000-0002-3512-5837': 2, '0000-0003-3889-2289': 2, '0000-0002-2124-0818': 2, '0000-0002-5678-2019': 2, '0000-0001-7353-9259': 2, '0000-0001-5235-2612': 2, '0000-0003-4074-877X': 2, '0000-0002-3984-0686': 2, '0000-0002-2679-8802': 2, '0000-0002-9423-438X': 2, '0000-0002-8908-0902': 2, '0000-0001-6746-7447': 2, '0000-0001-5794-975X': 2, '0000-0001-5402-7725': 1, '0000-0002-1273-6096': 1, '0000-0002-3531-489X': 1, '0000-0002-5886-8545': 1, '0000-0003-1834-4867': 1, '0000-0001-8641-7904': 1, '0000-0002-7918-1072': 1, '0000-0001-8371-2852': 1, '0000-0001-7176-409X': 1, '0000-0002-5409-2743': 1, '0000-0001-8616-1654': 1, '0000-0001-6886-2449': 1, '0000-0002-5201-9841': 1, '0000-0002-4966-1980': 1, '0000-0002-0947-876X': 1, '0000-0001-5104-4634': 1, '0000-0002-8663-798X': 1, '0000-0001-7565-068X': 1, '0000-0003-3530-9342': 1, '0000-0003-4907-4716': 1, '0000-0002-7689-6822': 1, '0000-0001-8986-8436': 1, '0000-0002-6944-473X': 1, '0000-0002-8416-3872': 1, '0000-0001-5086-0277': 1, '0000-0002-1384-6799': 1, '0000-0003-0812-6663': 1, '0000-0002-2156-9875': 1, '0000-0002-1094-3761': 1, '0000-0001-7282-0559': 1, '0000-0003-4677-0513': 1, '0000-0002-1418-3309': 1, '0000-0002-3365-8007': 1, '0000-0002-6143-8810': 1, '0000-0003-2479-0548': 1, '0000-0002-2556-7404': 1, '0000-0001-5494-4582': 1, '0000-0002-1764-1045': 1, '0000-0002-0872-4906': 1, '0000-0002-1368-6684': 1, '0000-0002-8237-3956': 1, '0000-0003-4856-6305': 1, '0000-0002-3423-5401': 1, '0000-0002-6204-5170': 1, '0000-0003-3155-0569': 1, '0000-0002-0341-7085': 1, '0000-0002-2938-3995': 1, '0000-0001-6600-9647': 1, '0000-0003-4184-363X': 1, '0000-0002-9011-4209': 1, '0000-0003-0461-6438': 1, '0000-0002-5065-5916': 1, '0000-0001-9078-6892': 1, '0000-0003-2304-6549': 1, '0000-0003-4491-0308': 1, '0000-0001-5182-0242': 1, '0000-0002-0708-9242': 1, '0000-0002-1690-9396': 1, '0000-0002-0824-8532': 1, '0000-0001-9661-5015': 1, '0000-0001-9061-3350': 1, '0000-0002-6214-3889': 1, '0000-0002-4478-6127': 1})\n",
      "['0000-0003-1519-3274', '0000-0001-9660-6303', '0000-0002-5390-8763', '0000-0002-9159-0733', '0000-0001-7842-2172', '0000-0001-7964-106X', '0000-0002-0195-1460', '0000-0001-9881-2784', '0000-0001-5080-7097', '0000-0003-0103-7457', '0000-0003-1232-5307', '0000-0003-4217-3228', '0000-0002-4687-6732', '0000-0002-0087-1151', '0000-0003-0934-3344', '0000-0002-3500-7494', '0000-0003-4978-1867', '0000-0002-3573-638X', '0000-0003-4438-1872', '0000-0002-3729-8774', '0000-0002-6931-8581', '0000-0002-9570-4216', '0000-0001-5979-5774', '0000-0002-8218-0062', '0000-0003-0767-1918', '0000-0002-1903-8354', '0000-0002-6152-2924', '0000-0001-5951-8013', '0000-0001-8208-8568', '0000-0002-4171-3803', '0000-0001-6417-864X', '0000-0001-9595-2765', '0000-0003-2068-7287', '0000-0002-5329-6605', '0000-0003-4085-293X', '0000-0003-0373-5080', '0000-0003-4035-0438', '0000-0002-4747-9763', '0000-0001-6537-0350', '0000-0003-0530-3425', '0000-0002-1376-9498', '0000-0003-4157-9365', '0000-0002-8383-8524', '0000-0003-2841-147X', '0000-0002-3566-3379', '0000-0003-0693-1415', '0000-0001-9840-4780', '0000-0003-1835-9436', '0000-0001-7649-4244', '0000-0003-2337-6935', '0000-0003-0578-0635', '0000-0002-1672-5730', '0000-0002-6349-6950', '0000-0001-5204-3369', '0000-0002-1299-4300', '0000-0001-6426-9074', '0000-0001-5096-4068', '0000-0003-3477-1172']\n",
      "Total sample size after apply threshold:  1846\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1846, 1737)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(1846, 1737)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.83      0.91        12\n",
      "          1       0.79      0.66      0.72        29\n",
      "          2       0.78      0.93      0.85        27\n",
      "          3       0.57      0.50      0.53        16\n",
      "          4       1.00      0.60      0.75        10\n",
      "          5       0.31      0.75      0.44        56\n",
      "          6       1.00      0.93      0.96        14\n",
      "          7       0.55      0.92      0.69        12\n",
      "          8       0.65      0.65      0.65        17\n",
      "          9       0.90      0.75      0.82        24\n",
      "         10       0.71      0.73      0.72       124\n",
      "         11       0.29      0.18      0.22        11\n",
      "         12       0.96      0.77      0.86        31\n",
      "         13       0.47      0.44      0.45        32\n",
      "         14       0.67      0.77      0.71        73\n",
      "         15       0.36      0.38      0.37        32\n",
      "         16       0.00      0.00      0.00        18\n",
      "         17       0.86      0.83      0.84        46\n",
      "         18       0.67      0.18      0.29        11\n",
      "         19       0.55      0.73      0.63        15\n",
      "         20       1.00      0.53      0.69        38\n",
      "         21       0.68      0.83      0.75        18\n",
      "         22       0.62      0.70      0.66        30\n",
      "         23       0.56      0.69      0.62        13\n",
      "         24       0.73      0.42      0.54        26\n",
      "         25       0.95      0.75      0.84        28\n",
      "         26       0.75      0.40      0.52        15\n",
      "         27       0.67      0.57      0.62        14\n",
      "         28       0.33      0.19      0.24        16\n",
      "         29       0.70      0.68      0.69        38\n",
      "         30       1.00      1.00      1.00        15\n",
      "         31       1.00      0.60      0.75        10\n",
      "         32       0.82      0.71      0.76        51\n",
      "         33       0.70      0.44      0.54        16\n",
      "         34       0.97      0.93      0.95        41\n",
      "         35       0.85      0.78      0.81        36\n",
      "         36       1.00      0.83      0.90        23\n",
      "         37       0.74      0.68      0.71        25\n",
      "         38       0.47      0.36      0.41        78\n",
      "         39       0.78      0.58      0.67        12\n",
      "         40       0.32      0.58      0.41        12\n",
      "         41       0.80      0.80      0.80        10\n",
      "         42       0.72      0.64      0.68        33\n",
      "         43       1.00      0.52      0.69        23\n",
      "         44       0.94      0.89      0.92        19\n",
      "         45       0.73      0.70      0.71        23\n",
      "         46       0.55      0.55      0.55        11\n",
      "         47       0.75      0.80      0.77       200\n",
      "         48       0.73      0.73      0.73        11\n",
      "         49       0.37      0.25      0.30        52\n",
      "         50       0.91      0.62      0.74        16\n",
      "         51       0.25      0.24      0.24        17\n",
      "         52       0.81      0.71      0.75        41\n",
      "         53       0.29      0.38      0.32        16\n",
      "         54       1.00      0.81      0.89        36\n",
      "         55       0.80      0.53      0.64        15\n",
      "         56       1.00      0.92      0.96        12\n",
      "         57       0.52      0.71      0.60       146\n",
      "\n",
      "avg / total       0.69      0.66      0.66      1846\n",
      "\n",
      "[ 10   0   1 ...   0   0 103]\n",
      "svc Accuracy:  0.6608884073672806\n",
      "svc F1:  0.6508496106389788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gao137/intel/intelpython3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/gao137/intel/intelpython3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.67      0.80        12\n",
      "          1       0.80      0.69      0.74        29\n",
      "          2       0.71      0.89      0.79        27\n",
      "          3       0.67      0.50      0.57        16\n",
      "          4       1.00      0.60      0.75        10\n",
      "          5       0.46      0.66      0.54        56\n",
      "          6       0.92      0.86      0.89        14\n",
      "          7       0.60      0.75      0.67        12\n",
      "          8       0.85      0.65      0.73        17\n",
      "          9       1.00      0.75      0.86        24\n",
      "         10       0.64      0.76      0.69       124\n",
      "         11       0.67      0.18      0.29        11\n",
      "         12       0.97      0.94      0.95        31\n",
      "         13       0.65      0.47      0.55        32\n",
      "         14       0.66      0.77      0.71        73\n",
      "         15       0.62      0.31      0.42        32\n",
      "         16       0.00      0.00      0.00        18\n",
      "         17       0.75      0.89      0.81        46\n",
      "         18       0.00      0.00      0.00        11\n",
      "         19       0.78      0.47      0.58        15\n",
      "         20       1.00      0.71      0.83        38\n",
      "         21       0.80      0.67      0.73        18\n",
      "         22       0.78      0.70      0.74        30\n",
      "         23       0.70      0.54      0.61        13\n",
      "         24       0.77      0.38      0.51        26\n",
      "         25       1.00      0.82      0.90        28\n",
      "         26       0.86      0.40      0.55        15\n",
      "         27       1.00      0.57      0.73        14\n",
      "         28       0.67      0.12      0.21        16\n",
      "         29       0.57      0.66      0.61        38\n",
      "         30       1.00      1.00      1.00        15\n",
      "         31       1.00      0.60      0.75        10\n",
      "         32       0.91      0.76      0.83        51\n",
      "         33       0.83      0.31      0.45        16\n",
      "         34       1.00      0.95      0.97        41\n",
      "         35       0.86      0.86      0.86        36\n",
      "         36       1.00      0.87      0.93        23\n",
      "         37       0.67      0.56      0.61        25\n",
      "         38       0.50      0.41      0.45        78\n",
      "         39       1.00      0.50      0.67        12\n",
      "         40       0.67      0.17      0.27        12\n",
      "         41       1.00      0.80      0.89        10\n",
      "         42       0.85      0.70      0.77        33\n",
      "         43       0.94      0.74      0.83        23\n",
      "         44       0.85      0.89      0.87        19\n",
      "         45       0.71      0.65      0.68        23\n",
      "         46       0.41      0.64      0.50        11\n",
      "         47       0.62      0.83      0.71       200\n",
      "         48       0.45      0.45      0.45        11\n",
      "         49       0.51      0.37      0.43        52\n",
      "         50       0.83      0.62      0.71        16\n",
      "         51       0.56      0.29      0.38        17\n",
      "         52       0.77      0.80      0.79        41\n",
      "         53       0.38      0.19      0.25        16\n",
      "         54       1.00      0.86      0.93        36\n",
      "         55       0.73      0.53      0.62        15\n",
      "         56       0.85      0.92      0.88        12\n",
      "         57       0.46      0.82      0.58       146\n",
      "\n",
      "avg / total       0.70      0.68      0.67      1846\n",
      "\n",
      "[  8   0   2 ...   0   0 119]\n",
      "LR Accuracy:  0.6798483206933911\n",
      "LR F1:  0.6518303683473154\n",
      "For name:  a_duarte\n",
      "total sample size before apply threshold:  373\n",
      "Counter({'0000-0002-4868-4099': 194, '0000-0002-0616-4650': 36, '0000-0002-9255-3635': 34, '0000-0002-0223-7867': 30, '0000-0003-0800-0112': 19, '0000-0003-2181-0187': 13, '0000-0003-4001-0871': 12, '0000-0003-3333-5977': 10, '0000-0001-9036-0170': 6, '0000-0001-6849-6004': 5, '0000-0001-5578-1586': 4, '0000-0003-0218-1952': 4, '0000-0002-6774-4886': 3, '0000-0002-5911-6521': 2, '0000-0001-8369-368X': 1})\n",
      "['0000-0003-4001-0871', '0000-0003-0800-0112', '0000-0003-2181-0187', '0000-0002-0223-7867', '0000-0002-0616-4650', '0000-0002-4868-4099', '0000-0002-9255-3635', '0000-0003-3333-5977']\n",
      "Total sample size after apply threshold:  348\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(348, 573)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(348, 573)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      1.00      0.92        12\n",
      "          1       0.94      0.84      0.89        19\n",
      "          2       0.92      0.92      0.92        13\n",
      "          3       1.00      0.93      0.97        30\n",
      "          4       0.94      0.89      0.91        36\n",
      "          5       0.91      0.99      0.95       194\n",
      "          6       1.00      0.71      0.83        34\n",
      "          7       1.00      0.60      0.75        10\n",
      "\n",
      "avg / total       0.93      0.93      0.92       348\n",
      "\n",
      "[ 12   0   0   0   0   0   0   0   0  16   0   0   0   3   0   0   0   0\n",
      "  12   0   0   1   0   0   0   0   0  28   0   2   0   0   0   0   1   0\n",
      "  32   3   0   0   1   1   0   0   0 192   0   0   1   0   0   0   1   8\n",
      "  24   0   0   0   0   0   1   3   0   6]\n",
      "svc Accuracy:  0.9252873563218391\n",
      "svc F1:  0.8922805881857605\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.67      0.80        12\n",
      "          1       1.00      0.63      0.77        19\n",
      "          2       0.92      0.85      0.88        13\n",
      "          3       1.00      0.93      0.97        30\n",
      "          4       1.00      0.83      0.91        36\n",
      "          5       0.85      1.00      0.92       194\n",
      "          6       1.00      0.76      0.87        34\n",
      "          7       1.00      0.50      0.67        10\n",
      "\n",
      "avg / total       0.92      0.90      0.90       348\n",
      "\n",
      "[  8   0   0   0   0   4   0   0   0  12   0   0   0   7   0   0   0   0\n",
      "  11   0   0   2   0   0   0   0   0  28   0   2   0   0   0   0   1   0\n",
      "  30   5   0   0   0   0   0   0   0 194   0   0   0   0   0   0   0   8\n",
      "  26   0   0   0   0   0   0   5   0   5]\n",
      "LR Accuracy:  0.9022988505747126\n",
      "LR F1:  0.8479687792613608\n",
      "For name:  a_correia\n",
      "total sample size before apply threshold:  136\n",
      "Counter({'0000-0002-5115-1429': 81, '0000-0003-0408-6262': 26, '0000-0002-0119-9790': 11, '0000-0002-2831-025X': 7, '0000-0003-2414-0131': 4, '0000-0003-3000-9324': 4, '0000-0002-8946-8579': 2, '0000-0002-2172-6631': 1})\n",
      "['0000-0003-0408-6262', '0000-0002-5115-1429', '0000-0002-0119-9790']\n",
      "Total sample size after apply threshold:  118\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(118, 196)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(118, 196)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.96      0.96        26\n",
      "          1       0.91      0.95      0.93        81\n",
      "          2       0.57      0.36      0.44        11\n",
      "\n",
      "avg / total       0.89      0.90      0.89       118\n",
      "\n",
      "[25  1  0  1 77  3  0  7  4]\n",
      "svc Accuracy:  0.8983050847457628\n",
      "svc F1:  0.7778979164521335\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.96      0.98        26\n",
      "          1       0.90      0.98      0.93        81\n",
      "          2       0.60      0.27      0.37        11\n",
      "\n",
      "avg / total       0.89      0.91      0.89       118\n",
      "\n",
      "[25  1  0  0 79  2  0  8  3]\n",
      "LR Accuracy:  0.9067796610169492\n",
      "LR F1:  0.7634344664887651\n",
      "For name:  a_reynolds\n",
      "total sample size before apply threshold:  40\n",
      "Counter({'0000-0002-0836-746X': 23, '0000-0001-9534-8699': 7, '0000-0002-6768-5716': 5, '0000-0002-9919-4161': 3, '0000-0003-0554-8107': 1, '0000-0002-6364-6250': 1})\n",
      "['0000-0002-0836-746X']\n",
      "Total sample size after apply threshold:  23\n",
      "For name:  g_qin\n",
      "total sample size before apply threshold:  15\n",
      "Counter({'0000-0001-6770-1096': 7, '0000-0002-2212-1597': 6, '0000-0002-3524-2013': 1, '0000-0002-3437-3716': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  m_tang\n",
      "total sample size before apply threshold:  86\n",
      "Counter({'0000-0001-6460-8136': 36, '0000-0003-2861-682X': 19, '0000-0002-3483-0219': 10, '0000-0002-8756-8445': 6, '0000-0001-5858-5126': 4, '0000-0001-8117-9695': 3, '0000-0003-2876-9199': 3, '0000-0002-2416-4101': 2, '0000-0001-7479-6206': 1, '0000-0001-9726-9943': 1, '0000-0001-7321-6927': 1})\n",
      "['0000-0003-2861-682X', '0000-0001-6460-8136', '0000-0002-3483-0219']\n",
      "Total sample size after apply threshold:  65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(65, 165)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(65, 165)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.79      0.88        19\n",
      "          1       0.82      1.00      0.90        36\n",
      "          2       1.00      0.60      0.75        10\n",
      "\n",
      "avg / total       0.90      0.88      0.87        65\n",
      "\n",
      "[15  4  0  0 36  0  0  4  6]\n",
      "svc Accuracy:  0.8769230769230769\n",
      "svc F1:  0.8441176470588235\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.84      0.91        19\n",
      "          1       0.84      1.00      0.91        36\n",
      "          2       1.00      0.60      0.75        10\n",
      "\n",
      "avg / total       0.91      0.89      0.89        65\n",
      "\n",
      "[16  3  0  0 36  0  0  4  6]\n",
      "LR Accuracy:  0.8923076923076924\n",
      "LR F1:  0.8585593731163351\n",
      "For name:  a_baranov\n",
      "total sample size before apply threshold:  42\n",
      "Counter({'0000-0002-9976-8532': 20, '0000-0002-9112-0838': 14, '0000-0003-3987-8112': 7, '0000-0001-8810-9972': 1})\n",
      "['0000-0002-9112-0838', '0000-0002-9976-8532']\n",
      "Total sample size after apply threshold:  34\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(34, 95)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(34, 95)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      1.00      0.93        14\n",
      "          1       1.00      0.90      0.95        20\n",
      "\n",
      "avg / total       0.95      0.94      0.94        34\n",
      "\n",
      "[14  0  2 18]\n",
      "svc Accuracy:  0.9411764705882353\n",
      "svc F1:  0.9403508771929825\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      1.00      0.97        14\n",
      "          1       1.00      0.95      0.97        20\n",
      "\n",
      "avg / total       0.97      0.97      0.97        34\n",
      "\n",
      "[14  0  1 19]\n",
      "LR Accuracy:  0.9705882352941176\n",
      "LR F1:  0.9699381078691424\n",
      "For name:  r_gray\n",
      "total sample size before apply threshold:  162\n",
      "Counter({'0000-0001-9694-4206': 83, '0000-0002-9858-0191': 48, '0000-0002-2203-2703': 19, '0000-0001-9668-6497': 6, '0000-0002-5890-1819': 6})\n",
      "['0000-0001-9694-4206', '0000-0002-2203-2703', '0000-0002-9858-0191']\n",
      "Total sample size after apply threshold:  150\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(150, 332)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(150, 332)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.98      0.92        83\n",
      "          1       1.00      0.68      0.81        19\n",
      "          2       0.95      0.85      0.90        48\n",
      "\n",
      "avg / total       0.91      0.90      0.90       150\n",
      "\n",
      "[81  0  2  6 13  0  7  0 41]\n",
      "svc Accuracy:  0.9\n",
      "svc F1:  0.8762843794623456\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.99      0.91        83\n",
      "          1       1.00      0.58      0.73        19\n",
      "          2       0.98      0.85      0.91        48\n",
      "\n",
      "avg / total       0.91      0.89      0.89       150\n",
      "\n",
      "[82  0  1  8 11  0  7  0 41]\n",
      "LR Accuracy:  0.8933333333333333\n",
      "LR F1:  0.8518518518518517\n",
      "For name:  r_nunes\n",
      "total sample size before apply threshold:  46\n",
      "Counter({'0000-0001-7425-5717': 28, '0000-0002-1377-9899': 13, '0000-0001-8633-4404': 3, '0000-0002-9014-0570': 2})\n",
      "['0000-0002-1377-9899', '0000-0001-7425-5717']\n",
      "Total sample size after apply threshold:  41\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(41, 117)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(41, 117)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      1.00      0.93        13\n",
      "          1       1.00      0.93      0.96        28\n",
      "\n",
      "avg / total       0.96      0.95      0.95        41\n",
      "\n",
      "[13  0  2 26]\n",
      "svc Accuracy:  0.9512195121951219\n",
      "svc F1:  0.9457671957671958\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.69      0.82        13\n",
      "          1       0.88      1.00      0.93        28\n",
      "\n",
      "avg / total       0.91      0.90      0.90        41\n",
      "\n",
      "[ 9  4  0 28]\n",
      "LR Accuracy:  0.9024390243902439\n",
      "LR F1:  0.8757575757575757\n",
      "For name:  s_huang\n",
      "total sample size before apply threshold:  441\n",
      "Counter({'0000-0002-0590-3474': 119, '0000-0002-3239-1072': 85, '0000-0001-5688-3410': 48, '0000-0001-9517-2515': 36, '0000-0001-8261-7079': 28, '0000-0001-6244-1555': 23, '0000-0001-8622-4838': 19, '0000-0003-4815-1863': 17, '0000-0001-7797-3626': 17, '0000-0003-2976-5798': 9, '0000-0002-2030-7574': 8, '0000-0003-1372-0480': 5, '0000-0001-5933-3115': 4, '0000-0002-8436-1991': 4, '0000-0003-1878-9348': 4, '0000-0003-1023-118X': 3, '0000-0001-9751-4523': 2, '0000-0003-4273-9682': 1, '0000-0001-7426-5181': 1, '0000-0002-8547-5309': 1, '0000-0002-1127-8898': 1, '0000-0002-6124-4178': 1, '0000-0002-7906-8467': 1, '0000-0002-8072-4388': 1, '0000-0002-4315-4494': 1, '0000-0003-3200-2206': 1, '0000-0003-3391-539X': 1})\n",
      "['0000-0001-8622-4838', '0000-0002-0590-3474', '0000-0001-5688-3410', '0000-0003-4815-1863', '0000-0001-7797-3626', '0000-0001-6244-1555', '0000-0002-3239-1072', '0000-0001-9517-2515', '0000-0001-8261-7079']\n",
      "Total sample size after apply threshold:  392\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(392, 332)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(392, 332)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.68      0.68      0.68        19\n",
      "          1       0.88      0.93      0.91       119\n",
      "          2       0.93      0.81      0.87        48\n",
      "          3       1.00      0.59      0.74        17\n",
      "          4       0.85      0.65      0.73        17\n",
      "          5       0.88      0.61      0.72        23\n",
      "          6       0.75      0.89      0.81        85\n",
      "          7       0.78      0.78      0.78        36\n",
      "          8       1.00      1.00      1.00        28\n",
      "\n",
      "avg / total       0.85      0.84      0.84       392\n",
      "\n",
      "[ 13   1   2   0   0   0   3   0   0   1 111   0   0   0   1   6   0   0\n",
      "   2   5  39   0   0   1   1   0   0   0   0   0  10   2   0   4   1   0\n",
      "   0   0   0   0  11   0   5   1   0   1   3   1   0   0  14   2   2   0\n",
      "   1   4   0   0   0   0  76   4   0   1   2   0   0   0   0   5  28   0\n",
      "   0   0   0   0   0   0   0   0  28]\n",
      "svc Accuracy:  0.8418367346938775\n",
      "svc F1:  0.8044038262623943\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.47      0.58        19\n",
      "          1       0.90      0.95      0.92       119\n",
      "          2       0.95      0.83      0.89        48\n",
      "          3       0.92      0.71      0.80        17\n",
      "          4       1.00      0.76      0.87        17\n",
      "          5       0.69      0.78      0.73        23\n",
      "          6       0.77      0.93      0.84        85\n",
      "          7       0.81      0.69      0.75        36\n",
      "          8       1.00      0.96      0.98        28\n",
      "\n",
      "avg / total       0.86      0.86      0.85       392\n",
      "\n",
      "[  9   3   2   0   0   2   3   0   0   0 113   0   0   0   2   4   0   0\n",
      "   1   3  40   0   0   3   1   0   0   0   0   0  12   0   0   2   3   0\n",
      "   0   0   0   1  13   0   2   1   0   1   1   0   0   0  18   3   0   0\n",
      "   0   3   0   0   0   1  79   2   0   1   2   0   0   0   0   8  25   0\n",
      "   0   1   0   0   0   0   0   0  27]\n",
      "LR Accuracy:  0.8571428571428571\n",
      "LR F1:  0.8184833554021769\n",
      "For name:  c_reid\n",
      "total sample size before apply threshold:  60\n",
      "Counter({'0000-0002-1152-8551': 33, '0000-0003-2739-1585': 13, '0000-0001-8572-1162': 11, '0000-0001-5916-8172': 2, '0000-0001-8117-662X': 1})\n",
      "['0000-0001-8572-1162', '0000-0003-2739-1585', '0000-0002-1152-8551']\n",
      "Total sample size after apply threshold:  57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(57, 153)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(57, 153)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.45      0.62        11\n",
      "          1       1.00      0.77      0.87        13\n",
      "          2       0.79      1.00      0.88        33\n",
      "\n",
      "avg / total       0.88      0.84      0.83        57\n",
      "\n",
      "[ 5  0  6  0 10  3  0  0 33]\n",
      "svc Accuracy:  0.8421052631578947\n",
      "svc F1:  0.7915217391304349\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.55      0.71        11\n",
      "          1       1.00      0.69      0.82        13\n",
      "          2       0.79      1.00      0.88        33\n",
      "\n",
      "avg / total       0.88      0.84      0.83        57\n",
      "\n",
      "[ 6  0  5  0  9  4  0  0 33]\n",
      "LR Accuracy:  0.8421052631578947\n",
      "LR F1:  0.8013547237076648\n",
      "For name:  h_lu\n",
      "total sample size before apply threshold:  108\n",
      "Counter({'0000-0003-1720-6526': 20, '0000-0002-8340-2739': 19, '0000-0003-2180-3091': 17, '0000-0003-4025-3160': 9, '0000-0001-9732-0833': 6, '0000-0002-1440-9902': 6, '0000-0002-3940-3283': 5, '0000-0002-0017-4276': 5, '0000-0003-3604-7145': 5, '0000-0002-6708-0223': 5, '0000-0002-0349-2181': 4, '0000-0002-9090-258X': 3, '0000-0002-5177-3391': 2, '0000-0002-6881-660X': 1, '0000-0002-9443-4031': 1})\n",
      "['0000-0002-8340-2739', '0000-0003-1720-6526', '0000-0003-2180-3091']\n",
      "Total sample size after apply threshold:  56\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(56, 126)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(56, 126)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.95      0.97        19\n",
      "          1       0.95      1.00      0.98        20\n",
      "          2       0.94      0.94      0.94        17\n",
      "\n",
      "avg / total       0.97      0.96      0.96        56\n",
      "\n",
      "[18  0  1  0 20  0  0  1 16]\n",
      "svc Accuracy:  0.9642857142857143\n",
      "svc F1:  0.9632530665529231\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.95      0.97        19\n",
      "          1       0.95      1.00      0.98        20\n",
      "          2       0.94      0.94      0.94        17\n",
      "\n",
      "avg / total       0.97      0.96      0.96        56\n",
      "\n",
      "[18  0  1  0 20  0  0  1 16]\n",
      "LR Accuracy:  0.9642857142857143\n",
      "LR F1:  0.9632530665529231\n",
      "For name:  j_cordeiro\n",
      "total sample size before apply threshold:  30\n",
      "Counter({'0000-0003-4656-6045': 14, '0000-0003-4605-1615': 9, '0000-0003-0902-9638': 5, '0000-0001-7876-0219': 1, '0000-0002-2118-1192': 1})\n",
      "['0000-0003-4656-6045']\n",
      "Total sample size after apply threshold:  14\n",
      "For name:  c_yu\n",
      "total sample size before apply threshold:  335\n",
      "Counter({'0000-0001-5664-9392': 252, '0000-0002-2136-2444': 26, '0000-0002-8648-8419': 16, '0000-0002-8453-5023': 11, '0000-0002-1742-2344': 10, '0000-0001-8062-9498': 6, '0000-0003-0084-6746': 5, '0000-0002-2934-2122': 3, '0000-0002-0635-3718': 2, '0000-0003-3712-5491': 2, '0000-0003-1555-8683': 1, '0000-0002-4304-2177': 1})\n",
      "['0000-0002-1742-2344', '0000-0001-5664-9392', '0000-0002-2136-2444', '0000-0002-8648-8419', '0000-0002-8453-5023']\n",
      "Total sample size after apply threshold:  315\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(315, 324)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(315, 324)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.90      0.95        10\n",
      "          1       0.95      0.98      0.97       252\n",
      "          2       0.86      0.69      0.77        26\n",
      "          3       0.87      0.81      0.84        16\n",
      "          4       1.00      1.00      1.00        11\n",
      "\n",
      "avg / total       0.94      0.95      0.94       315\n",
      "\n",
      "[  9   1   0   0   0   0 247   3   2   0   0   8  18   0   0   0   3   0\n",
      "  13   0   0   0   0   0  11]\n",
      "svc Accuracy:  0.946031746031746\n",
      "svc F1:  0.903753488703849\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.80      0.89        10\n",
      "          1       0.94      1.00      0.97       252\n",
      "          2       0.95      0.73      0.83        26\n",
      "          3       1.00      0.62      0.77        16\n",
      "          4       1.00      1.00      1.00        11\n",
      "\n",
      "avg / total       0.95      0.95      0.95       315\n",
      "\n",
      "[  8   2   0   0   0   0 251   1   0   0   0   7  19   0   0   0   6   0\n",
      "  10   0   0   0   0   0  11]\n",
      "LR Accuracy:  0.9492063492063492\n",
      "LR F1:  0.8906637167506734\n",
      "For name:  d_simpson\n",
      "total sample size before apply threshold:  27\n",
      "Counter({'0000-0002-0500-9675': 11, '0000-0002-1189-0833': 11, '0000-0002-8105-2552': 2, '0000-0002-9768-7413': 2, '0000-0001-9538-3208': 1})\n",
      "['0000-0002-0500-9675', '0000-0002-1189-0833']\n",
      "Total sample size after apply threshold:  22\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(22, 52)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(22, 52)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.91      0.83        11\n",
      "          1       0.89      0.73      0.80        11\n",
      "\n",
      "avg / total       0.83      0.82      0.82        22\n",
      "\n",
      "[10  1  3  8]\n",
      "svc Accuracy:  0.8181818181818182\n",
      "svc F1:  0.8166666666666667\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.91      0.91        11\n",
      "          1       0.91      0.91      0.91        11\n",
      "\n",
      "avg / total       0.91      0.91      0.91        22\n",
      "\n",
      "[10  1  1 10]\n",
      "LR Accuracy:  0.9090909090909091\n",
      "LR F1:  0.9090909090909091\n",
      "For name:  c_pereira\n",
      "total sample size before apply threshold:  258\n",
      "Counter({'0000-0001-7874-1894': 55, '0000-0002-6630-5056': 31, '0000-0002-8392-9581': 29, '0000-0002-9724-1382': 26, '0000-0002-8167-6912': 25, '0000-0002-0804-9197': 14, '0000-0003-3421-8676': 11, '0000-0003-0093-771X': 9, '0000-0002-7133-0600': 8, '0000-0002-2146-1223': 8, '0000-0001-8050-5102': 7, '0000-0001-9224-1917': 7, '0000-0001-8111-1455': 6, '0000-0003-1389-9214': 5, '0000-0001-5514-4544': 4, '0000-0003-1788-4562': 4, '0000-0002-2899-9640': 3, '0000-0003-3406-3985': 3, '0000-0002-1805-7115': 2, '0000-0001-6153-7555': 1})\n",
      "['0000-0002-8167-6912', '0000-0001-7874-1894', '0000-0002-9724-1382', '0000-0002-8392-9581', '0000-0002-0804-9197', '0000-0003-3421-8676', '0000-0002-6630-5056']\n",
      "Total sample size after apply threshold:  191\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(191, 459)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(191, 459)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.92      0.96        25\n",
      "          1       0.82      1.00      0.90        55\n",
      "          2       1.00      0.88      0.94        26\n",
      "          3       0.89      0.86      0.88        29\n",
      "          4       1.00      0.86      0.92        14\n",
      "          5       1.00      0.82      0.90        11\n",
      "          6       1.00      0.94      0.97        31\n",
      "\n",
      "avg / total       0.93      0.92      0.92       191\n",
      "\n",
      "[23  1  0  1  0  0  0  0 55  0  0  0  0  0  0  1 23  2  0  0  0  0  4  0\n",
      " 25  0  0  0  0  2  0  0 12  0  0  0  2  0  0  0  9  0  0  2  0  0  0  0\n",
      " 29]\n",
      "svc Accuracy:  0.9214659685863874\n",
      "svc F1:  0.9236692514284915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.92      0.96        25\n",
      "          1       0.89      1.00      0.94        55\n",
      "          2       1.00      0.88      0.94        26\n",
      "          3       0.87      0.93      0.90        29\n",
      "          4       1.00      0.93      0.96        14\n",
      "          5       1.00      0.82      0.90        11\n",
      "          6       1.00      0.97      0.98        31\n",
      "\n",
      "avg / total       0.95      0.94      0.94       191\n",
      "\n",
      "[23  1  0  1  0  0  0  0 55  0  0  0  0  0  0  1 23  2  0  0  0  0  2  0\n",
      " 27  0  0  0  0  0  0  1 13  0  0  0  2  0  0  0  9  0  0  1  0  0  0  0\n",
      " 30]\n",
      "LR Accuracy:  0.9424083769633508\n",
      "LR F1:  0.9405499005783382\n",
      "For name:  h_wang\n",
      "total sample size before apply threshold:  848\n",
      "Counter({'0000-0002-0211-9000': 91, '0000-0003-0477-2908': 62, '0000-0002-5051-4929': 44, '0000-0002-7528-7494': 44, '0000-0002-9887-5555': 39, '0000-0001-5836-4120': 31, '0000-0002-8796-0367': 30, '0000-0002-7752-6217': 30, '0000-0003-1708-8734': 28, '0000-0002-6107-5095': 27, '0000-0001-8238-1641': 27, '0000-0003-4623-1878': 26, '0000-0001-9570-3611': 25, '0000-0001-9199-0721': 25, '0000-0002-7959-7377': 24, '0000-0003-2420-3147': 23, '0000-0002-8066-475X': 19, '0000-0003-1625-3400': 18, '0000-0002-1483-5135': 17, '0000-0001-6590-7736': 15, '0000-0002-9634-8778': 15, '0000-0001-7964-0809': 14, '0000-0001-6507-5503': 13, '0000-0003-4107-2062': 11, '0000-0002-3355-2448': 11, '0000-0002-6567-9144': 10, '0000-0003-4414-3372': 10, '0000-0002-6859-5683': 9, '0000-0002-2565-5543': 8, '0000-0003-0388-510X': 7, '0000-0003-1086-5318': 7, '0000-0002-4858-8195': 5, '0000-0002-2325-0120': 5, '0000-0003-4453-8059': 5, '0000-0001-9243-3935': 5, '0000-0002-9567-8249': 4, '0000-0003-4957-0509': 4, '0000-0002-0769-600X': 4, '0000-0002-1994-4402': 4, '0000-0002-3857-5737': 4, '0000-0001-7988-6120': 3, '0000-0002-0600-2555': 3, '0000-0003-1688-7948': 3, '0000-0001-9530-6587': 3, '0000-0002-9863-0144': 3, '0000-0002-9216-9342': 3, '0000-0002-6938-9507': 3, '0000-0001-9107-6120': 3, '0000-0003-4791-7994': 2, '0000-0001-6529-5629': 2, '0000-0001-5623-1148': 2, '0000-0002-5696-156X': 1, '0000-0002-8415-9793': 1, '0000-0002-3221-2820': 1, '0000-0001-6192-853X': 1, '0000-0002-7167-2483': 1, '0000-0002-0173-0545': 1, '0000-0001-8556-3504': 1, '0000-0001-7815-610X': 1, '0000-0002-7775-3268': 1, '0000-0002-3394-1531': 1, '0000-0002-9100-321X': 1, '0000-0001-9900-8528': 1, '0000-0002-8465-0996': 1, '0000-0001-5228-9270': 1, '0000-0001-9355-1319': 1, '0000-0002-2367-5591': 1, '0000-0001-5388-6691': 1, '0000-0002-2516-6774': 1})\n",
      "['0000-0002-0211-9000', '0000-0001-7964-0809', '0000-0003-2420-3147', '0000-0001-9570-3611', '0000-0001-9199-0721', '0000-0002-8066-475X', '0000-0002-8796-0367', '0000-0003-4107-2062', '0000-0002-3355-2448', '0000-0001-6590-7736', '0000-0002-6567-9144', '0000-0001-5836-4120', '0000-0002-7959-7377', '0000-0003-4623-1878', '0000-0003-1625-3400', '0000-0002-7752-6217', '0000-0003-4414-3372', '0000-0001-6507-5503', '0000-0003-0477-2908', '0000-0002-6107-5095', '0000-0003-1708-8734', '0000-0002-1483-5135', '0000-0002-9887-5555', '0000-0002-5051-4929', '0000-0002-9634-8778', '0000-0002-7528-7494', '0000-0001-8238-1641']\n",
      "Total sample size after apply threshold:  729\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(729, 2302)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(729, 2302)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.81      0.84        91\n",
      "          1       0.56      0.36      0.43        14\n",
      "          2       0.71      0.43      0.54        23\n",
      "          3       0.67      0.48      0.56        25\n",
      "          4       0.59      0.76      0.67        25\n",
      "          5       0.43      0.53      0.48        19\n",
      "          6       0.75      0.80      0.77        30\n",
      "          7       0.19      0.27      0.22        11\n",
      "          8       0.69      0.82      0.75        11\n",
      "          9       1.00      1.00      1.00        15\n",
      "         10       1.00      0.90      0.95        10\n",
      "         11       0.42      0.97      0.59        31\n",
      "         12       1.00      0.79      0.88        24\n",
      "         13       0.95      0.81      0.88        26\n",
      "         14       0.80      0.89      0.84        18\n",
      "         15       0.55      0.73      0.63        30\n",
      "         16       1.00      0.90      0.95        10\n",
      "         17       0.45      0.38      0.42        13\n",
      "         18       1.00      0.79      0.88        62\n",
      "         19       0.67      0.44      0.53        27\n",
      "         20       0.67      0.50      0.57        28\n",
      "         21       0.83      0.88      0.86        17\n",
      "         22       0.81      0.54      0.65        39\n",
      "         23       0.95      0.82      0.88        44\n",
      "         24       0.58      0.47      0.52        15\n",
      "         25       0.56      0.80      0.66        44\n",
      "         26       0.70      0.70      0.70        27\n",
      "\n",
      "avg / total       0.75      0.71      0.72       729\n",
      "\n",
      "[74  0  1  0  1  1  4  0  1  0  0  1  0  1  0  2  0  0  0  0  0  0  0  0\n",
      "  0  5  0  1  5  0  0  0  0  0  1  0  0  0  2  0  0  0  0  0  1  0  0  0\n",
      "  2  0  0  0  2  0  1  0 10  0  1  1  0  0  0  0  0  5  0  0  0  0  0  0\n",
      "  0  0  1  1  2  0  0  0  1  0  1  0 12  3  0  0  3  0  0  0  1  0  0  0\n",
      "  1  0  1  0  0  1  0  0  0  0  1  1  0  0  0  1 19  0  0  1  0  0  0  1\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  1  0  1  0  0 10  0  0  0\n",
      "  0  0  1  0  0  1  2  0  3  0  0  0  0  0  0  0  0  0  1  0  0  0  1  0\n",
      " 24  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  1  0  0  2  0  1  0  0\n",
      "  1  2  0  0  3  0  0  0  1  0  0  0  0  0  0  0  0  1  0  0  0  0  2  0\n",
      "  0  0  1  0  0  0  0  0  9  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0 15  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  9  1  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0 30  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  3\n",
      " 19  0  0  1  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  1  0\n",
      "  0  0  0  0 21  0  0  0  1  0  0  0  0  0  0  0  3  0  0  0  0  0  0  1\n",
      "  0  0  0  0  0  0  0  0 16  0  0  0  0  0  0  0  0  0  0  1  0  0  1  1\n",
      "  0  0  1  0  0  0  0  0  4  0  0  0 22  0  0  0  0  0  0  0  0  0  0  1\n",
      "  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  9  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  4  0  0  0  0  0  3  0  0  0  1  0  5  0  0  0\n",
      "  0  0  0  0  0  0  1  0  0  2  1  0  1  0  0  0  0  5  0  0  0  1  0  0\n",
      " 49  0  0  0  1  0  0  0  1  1  0  0  0  0  2  0  1  0  0  0  3  0  0  1\n",
      "  1  0  0  0 12  1  0  0  0  2  0  3  1  0  0  1  0  1  0  2  1  0  0  1\n",
      "  0  0  0  1  0  0  0  2 14  0  0  0  0  4  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  2  0  0  0  0  0 15  0  0  0  0  0  0  1  0  0  1  2\n",
      "  2  0  2  0  0  0  0  0  1  3  0  0  0  1  0  0 21  0  1  3  1  0  0  0\n",
      "  0  0  0  0  0  0  0  0  4  0  0  1  1  0  0  0  0  0  0  0 36  2  0  0\n",
      "  2  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  1  1  0  0  2\n",
      "  7  0  0  1  0  0  0  2  0  1  3  0  0  0  0  0  0  0  0  0  0  0  0  2\n",
      "  0  0  0  0 35  0  0  0  0  1  1  0  0  1  0  0  0  1  0  0  0  2  0  0\n",
      "  0  2  0  0  0  0  0  0 19]\n",
      "svc Accuracy:  0.7133058984910837\n",
      "svc F1:  0.6905285542773707\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.88      0.88        91\n",
      "          1       0.43      0.21      0.29        14\n",
      "          2       0.69      0.48      0.56        23\n",
      "          3       0.82      0.56      0.67        25\n",
      "          4       0.57      0.80      0.67        25\n",
      "          5       0.50      0.68      0.58        19\n",
      "          6       0.74      0.93      0.82        30\n",
      "          7       0.00      0.00      0.00        11\n",
      "          8       0.71      0.91      0.80        11\n",
      "          9       0.88      0.93      0.90        15\n",
      "         10       1.00      1.00      1.00        10\n",
      "         11       0.50      0.90      0.64        31\n",
      "         12       0.96      0.92      0.94        24\n",
      "         13       1.00      0.81      0.89        26\n",
      "         14       0.93      0.78      0.85        18\n",
      "         15       0.71      0.80      0.75        30\n",
      "         16       1.00      0.80      0.89        10\n",
      "         17       0.88      0.54      0.67        13\n",
      "         18       0.96      0.84      0.90        62\n",
      "         19       0.74      0.52      0.61        27\n",
      "         20       0.52      0.50      0.51        28\n",
      "         21       0.94      0.88      0.91        17\n",
      "         22       0.66      0.69      0.68        39\n",
      "         23       0.95      0.89      0.92        44\n",
      "         24       0.60      0.40      0.48        15\n",
      "         25       0.64      0.80      0.71        44\n",
      "         26       0.82      0.85      0.84        27\n",
      "\n",
      "avg / total       0.77      0.76      0.75       729\n",
      "\n",
      "[80  0  0  0  0  1  3  0  0  1  0  2  0  0  0  1  0  0  0  0  1  0  2  0\n",
      "  0  0  0  0  3  0  0  1  2  1  0  0  0  0  1  0  0  0  2  0  0  0  1  1\n",
      "  0  0  0  0  2  0  2  0 11  0  1  0  0  0  1  0  0  4  0  0  0  1  0  0\n",
      "  0  0  1  0  2  0  0  0  0  0  1  0 14  2  0  0  1  0  0  0  0  0  0  0\n",
      "  1  0  0  1  0  3  0  0  0  0  2  0  0  0  0  1 20  0  0  0  0  0  0  1\n",
      "  0  0  0  1  0  0  0  0  0  0  0  0  0  2  0  3  0  1  0  0 13  0  0  0\n",
      "  0  0  0  0  0  1  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0\n",
      " 28  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  1  0  0  0  0  0  0  0\n",
      "  1  2  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  4  1\n",
      "  0  0  1  0  0  0  0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0 14  0  0  0  0  0  0  0  0  0  0  1\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0 28  0  0  0\n",
      "  0  0  0  1  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1\n",
      " 22  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  1  0  0  0\n",
      "  0  0  0  1 21  0  0  0  0  0  0  1  0  0  0  0  2  0  0  0  0  0  0  1\n",
      "  0  0  0  0  0  0  0  0 14  0  0  0  0  1  0  0  1  0  0  1  0  0  1  2\n",
      "  0  0  0  0  0  0  0  0  2  0  0  0 24  0  0  0  0  0  1  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0  8  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  4  0  0  0  0  0  2  0  0  0  0  0  7  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  0  0  0  1  0  0\n",
      " 52  0  0  0  1  0  0  0  1  1  0  0  0  0  2  0  0  0  0  0  2  0  0  0\n",
      "  1  0  0  0 14  2  0  1  1  1  0  2  1  0  1  1  2  1  0  1  2  0  0  0\n",
      "  0  0  0  0  0  0  0  1 14  0  0  0  0  4  0  0  2  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0 15  0  0  0  0  0  1  0  0  0  1  1\n",
      "  2  0  1  1  0  0  0  0  0  0  0  0  0  1  0  0 27  0  1  2  1  0  0  0\n",
      "  0  0  0  1  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0 39  2  0  0\n",
      "  2  0  0  0  0  0  1  0  0  0  0  1  0  0  0  0  0  0  0  1  0  0  2  1\n",
      "  6  1  0  0  0  0  0  5  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  2\n",
      "  0  1  0  0 35  0  0  0  0  0  1  0  0  0  0  0  0  1  0  0  0  1  0  1\n",
      "  0  0  0  0  0  0  0  0 23]\n",
      "LR Accuracy:  0.757201646090535\n",
      "LR F1:  0.7160674242229329\n",
      "For name:  a_tan\n",
      "total sample size before apply threshold:  200\n",
      "Counter({'0000-0003-2955-8369': 97, '0000-0002-9158-7243': 60, '0000-0001-5313-8650': 18, '0000-0002-9225-0247': 18, '0000-0002-8484-7107': 3, '0000-0003-2902-4025': 3, '0000-0001-6459-6171': 1})\n",
      "['0000-0002-9158-7243', '0000-0001-5313-8650', '0000-0003-2955-8369', '0000-0002-9225-0247']\n",
      "Total sample size after apply threshold:  193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(193, 644)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(193, 644)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.97      0.98        60\n",
      "          1       1.00      0.94      0.97        18\n",
      "          2       0.97      1.00      0.98        97\n",
      "          3       1.00      1.00      1.00        18\n",
      "\n",
      "avg / total       0.98      0.98      0.98       193\n",
      "\n",
      "[58  0  2  0  0 17  1  0  0  0 97  0  0  0  0 18]\n",
      "svc Accuracy:  0.9844559585492227\n",
      "svc F1:  0.9848127481225648\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.98      0.99        60\n",
      "          1       1.00      0.72      0.84        18\n",
      "          2       0.94      1.00      0.97        97\n",
      "          3       1.00      1.00      1.00        18\n",
      "\n",
      "avg / total       0.97      0.97      0.97       193\n",
      "\n",
      "[59  0  1  0  0 13  5  0  0  0 97  0  0  0  0 18]\n",
      "LR Accuracy:  0.9689119170984456\n",
      "LR F1:  0.9500765790187042\n",
      "For name:  m_aguilar\n",
      "total sample size before apply threshold:  108\n",
      "Counter({'0000-0002-1935-6619': 59, '0000-0001-7395-5754': 18, '0000-0002-2586-859X': 14, '0000-0002-8084-6991': 8, '0000-0002-5150-1871': 8, '0000-0002-9953-7400': 1})\n",
      "['0000-0002-1935-6619', '0000-0001-7395-5754', '0000-0002-2586-859X']\n",
      "Total sample size after apply threshold:  91\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(91, 171)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(91, 171)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        59\n",
      "          1       1.00      0.89      0.94        18\n",
      "          2       0.88      1.00      0.93        14\n",
      "\n",
      "avg / total       0.98      0.98      0.98        91\n",
      "\n",
      "[59  0  0  0 16  2  0  0 14]\n",
      "svc Accuracy:  0.978021978021978\n",
      "svc F1:  0.9581699346405229\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.98        59\n",
      "          1       1.00      0.94      0.97        18\n",
      "          2       1.00      0.93      0.96        14\n",
      "\n",
      "avg / total       0.98      0.98      0.98        91\n",
      "\n",
      "[59  0  0  1 17  0  1  0 13]\n",
      "LR Accuracy:  0.978021978021978\n",
      "LR F1:  0.9725749559082892\n",
      "For name:  a_bianchi\n",
      "total sample size before apply threshold:  73\n",
      "Counter({'0000-0002-1082-3911': 38, '0000-0001-6583-1671': 23, '0000-0001-9340-6971': 8, '0000-0002-4571-0511': 2, '0000-0003-4925-5269': 2})\n",
      "['0000-0001-6583-1671', '0000-0002-1082-3911']\n",
      "Total sample size after apply threshold:  61\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(61, 142)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(61, 142)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      1.00      0.96        23\n",
      "          1       1.00      0.95      0.97        38\n",
      "\n",
      "avg / total       0.97      0.97      0.97        61\n",
      "\n",
      "[23  0  2 36]\n",
      "svc Accuracy:  0.9672131147540983\n",
      "svc F1:  0.9656531531531531\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98        23\n",
      "          1       1.00      0.97      0.99        38\n",
      "\n",
      "avg / total       0.98      0.98      0.98        61\n",
      "\n",
      "[23  0  1 37]\n",
      "LR Accuracy:  0.9836065573770492\n",
      "LR F1:  0.9826950354609929\n",
      "For name:  p_rossi\n",
      "total sample size before apply threshold:  200\n",
      "Counter({'0000-0003-2620-7918': 116, '0000-0003-4796-327X': 39, '0000-0002-6316-338X': 36, '0000-0002-3995-8836': 8, '0000-0002-6472-3588': 1})\n",
      "['0000-0002-6316-338X', '0000-0003-2620-7918', '0000-0003-4796-327X']\n",
      "Total sample size after apply threshold:  191\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(191, 732)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(191, 732)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        36\n",
      "          1       0.97      0.99      0.98       116\n",
      "          2       0.97      0.92      0.95        39\n",
      "\n",
      "avg / total       0.98      0.98      0.98       191\n",
      "\n",
      "[ 36   0   0   0 115   1   0   3  36]\n",
      "svc Accuracy:  0.9790575916230366\n",
      "svc F1:  0.9767581346528714\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.97      0.99        36\n",
      "          1       0.96      1.00      0.98       116\n",
      "          2       1.00      0.90      0.95        39\n",
      "\n",
      "avg / total       0.97      0.97      0.97       191\n",
      "\n",
      "[ 35   1   0   0 116   0   0   4  35]\n",
      "LR Accuracy:  0.9738219895287958\n",
      "LR F1:  0.9702547974967302\n",
      "For name:  y_yang\n",
      "total sample size before apply threshold:  665\n",
      "Counter({'0000-0002-8633-0873': 115, '0000-0002-6266-9864': 97, '0000-0003-1391-8040': 73, '0000-0001-8839-8161': 50, '0000-0002-6782-2813': 43, '0000-0001-7896-1184': 39, '0000-0002-3598-7218': 35, '0000-0003-4275-0515': 26, '0000-0002-0007-6481': 16, '0000-0003-3711-2842': 12, '0000-0003-0195-9478': 11, '0000-0001-8572-5155': 10, '0000-0001-8971-4648': 8, '0000-0003-2442-3713': 8, '0000-0002-0491-8295': 8, '0000-0002-7540-3301': 8, '0000-0002-2767-9354': 8, '0000-0002-5982-1706': 8, '0000-0001-7139-1254': 6, '0000-0001-6417-3654': 6, '0000-0001-5769-1795': 6, '0000-0003-0298-8641': 5, '0000-0002-5599-0975': 5, '0000-0002-1707-0633': 5, '0000-0003-1536-343X': 4, '0000-0002-8514-8228': 4, '0000-0001-9306-3227': 3, '0000-0002-5033-6210': 3, '0000-0003-3428-1587': 3, '0000-0002-7856-2009': 3, '0000-0002-5808-0109': 3, '0000-0002-8565-6214': 3, '0000-0002-1837-3628': 3, '0000-0002-6976-7416': 2, '0000-0002-3487-8730': 2, '0000-0001-8274-6196': 2, '0000-0002-2222-0202': 2, '0000-0002-7653-0601': 2, '0000-0003-0576-6032': 2, '0000-0003-1599-635X': 2, '0000-0002-6306-1324': 1, '0000-0001-9436-964X': 1, '0000-0001-5796-7990': 1, '0000-0002-3519-5472': 1, '0000-0002-8404-8305': 1, '0000-0002-7982-7988': 1, '0000-0003-2902-3989': 1, '0000-0001-5726-9420': 1, '0000-0002-0100-1078': 1, '0000-0002-1105-3824': 1, '0000-0002-6891-0655': 1, '0000-0002-1080-5086': 1, '0000-0003-4505-8954': 1, '0000-0002-1416-4925': 1})\n",
      "['0000-0001-8572-5155', '0000-0002-0007-6481', '0000-0002-3598-7218', '0000-0002-6782-2813', '0000-0003-0195-9478', '0000-0002-6266-9864', '0000-0001-7896-1184', '0000-0003-3711-2842', '0000-0002-8633-0873', '0000-0003-1391-8040', '0000-0001-8839-8161', '0000-0003-4275-0515']\n",
      "Total sample size after apply threshold:  527\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(527, 417)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(527, 417)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.80      0.76        10\n",
      "          1       0.73      0.69      0.71        16\n",
      "          2       0.93      0.80      0.86        35\n",
      "          3       0.65      0.81      0.72        43\n",
      "          4       0.88      0.64      0.74        11\n",
      "          5       0.84      0.86      0.85        97\n",
      "          6       0.73      0.69      0.71        39\n",
      "          7       1.00      0.58      0.74        12\n",
      "          8       0.85      0.90      0.87       115\n",
      "          9       0.90      0.88      0.89        73\n",
      "         10       0.76      0.76      0.76        50\n",
      "         11       0.96      0.88      0.92        26\n",
      "\n",
      "avg / total       0.83      0.82      0.82       527\n",
      "\n",
      "[  8   0   0   0   0   0   1   0   0   0   0   1   0  11   1   1   0   2\n",
      "   1   0   0   0   0   0   0   1  28   0   0   2   0   0   1   2   1   0\n",
      "   0   1   0  35   0   0   2   0   0   0   5   0   0   0   0   1   7   2\n",
      "   0   0   1   0   0   0   0   1   1   1   0  83   0   0   9   2   0   0\n",
      "   0   1   0   6   0   0  27   0   1   0   4   0   0   0   0   3   0   0\n",
      "   2   7   0   0   0   0   1   0   0   0   0   6   1   0 103   3   1   0\n",
      "   0   0   0   0   0   4   1   0   4  64   0   0   1   0   0   7   1   0\n",
      "   2   0   1   0  38   0   1   0   0   0   0   0   0   0   1   0   1  23]\n",
      "svc Accuracy:  0.8235294117647058\n",
      "svc F1:  0.7939741394984359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.70      0.78        10\n",
      "          1       0.86      0.75      0.80        16\n",
      "          2       0.90      0.77      0.83        35\n",
      "          3       0.83      0.88      0.85        43\n",
      "          4       1.00      0.55      0.71        11\n",
      "          5       0.87      0.90      0.88        97\n",
      "          6       0.76      0.64      0.69        39\n",
      "          7       1.00      0.75      0.86        12\n",
      "          8       0.85      0.92      0.88       115\n",
      "          9       0.91      0.85      0.88        73\n",
      "         10       0.67      0.84      0.74        50\n",
      "         11       0.96      0.92      0.94        26\n",
      "\n",
      "avg / total       0.85      0.84      0.84       527\n",
      "\n",
      "[  7   0   0   0   0   0   1   0   0   1   0   1   0  12   0   0   0   0\n",
      "   3   0   0   0   1   0   0   0  27   0   0   3   0   0   2   2   1   0\n",
      "   0   0   0  38   0   0   0   0   0   0   5   0   0   1   0   0   6   1\n",
      "   0   0   1   0   2   0   0   1   1   0   0  87   0   0   5   2   1   0\n",
      "   0   0   0   7   0   1  25   0   0   0   6   0   0   0   0   0   0   0\n",
      "   0   9   0   0   3   0   1   0   1   0   0   5   0   0 106   1   1   0\n",
      "   0   0   1   0   0   3   1   0   6  62   0   0   0   0   0   1   0   0\n",
      "   3   0   4   0  42   0   0   0   0   0   0   0   0   0   1   0   1  24]\n",
      "LR Accuracy:  0.8444022770398482\n",
      "LR F1:  0.8208752698502556\n",
      "For name:  s_hsieh\n",
      "total sample size before apply threshold:  166\n",
      "Counter({'0000-0002-3188-8482': 88, '0000-0002-1879-4086': 65, '0000-0002-2716-4609': 9, '0000-0002-8475-7486': 3, '0000-0001-7617-8559': 1})\n",
      "['0000-0002-3188-8482', '0000-0002-1879-4086']\n",
      "Total sample size after apply threshold:  153\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(153, 137)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(153, 137)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.91      0.93        88\n",
      "          1       0.88      0.94      0.91        65\n",
      "\n",
      "avg / total       0.92      0.92      0.92       153\n",
      "\n",
      "[80  8  4 61]\n",
      "svc Accuracy:  0.9215686274509803\n",
      "svc F1:  0.9203401596667824\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.95      0.95        88\n",
      "          1       0.94      0.94      0.94        65\n",
      "\n",
      "avg / total       0.95      0.95      0.95       153\n",
      "\n",
      "[84  4  4 61]\n",
      "LR Accuracy:  0.9477124183006536\n",
      "LR F1:  0.9465034965034965\n",
      "For name:  c_baptista\n",
      "total sample size before apply threshold:  19\n",
      "Counter({'0000-0002-1263-7880': 7, '0000-0002-8158-4743': 7, '0000-0003-4664-6766': 2, '0000-0002-7807-0995': 2, '0000-0002-9966-0708': 1})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  d_kavanagh\n",
      "total sample size before apply threshold:  178\n",
      "Counter({'0000-0001-9072-8828': 113, '0000-0003-4718-0072': 58, '0000-0003-1531-6617': 4, '0000-0003-2854-7270': 3})\n",
      "['0000-0003-4718-0072', '0000-0001-9072-8828']\n",
      "Total sample size after apply threshold:  171\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(171, 448)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(171, 448)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.84      0.91        58\n",
      "          1       0.93      0.99      0.96       113\n",
      "\n",
      "avg / total       0.94      0.94      0.94       171\n",
      "\n",
      "[ 49   9   1 112]\n",
      "svc Accuracy:  0.9415204678362573\n",
      "svc F1:  0.9323361823361824\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.81      0.90        58\n",
      "          1       0.91      1.00      0.95       113\n",
      "\n",
      "avg / total       0.94      0.94      0.93       171\n",
      "\n",
      "[ 47  11   0 113]\n",
      "LR Accuracy:  0.935672514619883\n",
      "LR F1:  0.9244122965641952\n",
      "For name:  l_wang\n",
      "total sample size before apply threshold:  828\n",
      "Counter({'0000-0001-9783-4383': 98, '0000-0003-3870-3388': 64, '0000-0002-5947-306X': 63, '0000-0002-5773-1627': 56, '0000-0002-5859-2526': 53, '0000-0002-5126-1046': 48, '0000-0002-4344-8791': 40, '0000-0001-8927-6772': 31, '0000-0001-5813-9505': 31, '0000-0002-1709-9401': 30, '0000-0003-3463-0740': 27, '0000-0003-1382-9195': 25, '0000-0003-3075-6872': 22, '0000-0001-9556-2361': 19, '0000-0002-4809-3109': 17, '0000-0002-1919-9107': 17, '0000-0002-4747-0419': 17, '0000-0002-6156-9028': 16, '0000-0001-7302-4714': 15, '0000-0001-7124-2718': 14, '0000-0001-8412-2985': 9, '0000-0002-8208-7079': 7, '0000-0003-4276-0051': 7, '0000-0002-4165-4022': 6, '0000-0002-7300-9271': 6, '0000-0002-0933-2808': 5, '0000-0001-9355-1167': 5, '0000-0002-4930-8618': 5, '0000-0001-7383-934X': 5, '0000-0001-7324-2682': 5, '0000-0003-1117-1326': 5, '0000-0002-7579-0233': 5, '0000-0002-2753-0947': 4, '0000-0002-1869-9871': 4, '0000-0002-0543-5519': 4, '0000-0002-5371-2138': 4, '0000-0001-5601-7539': 3, '0000-0003-0881-9689': 3, '0000-0001-6223-5962': 3, '0000-0003-0968-1247': 3, '0000-0001-8752-6635': 3, '0000-0001-8905-3456': 2, '0000-0001-6222-7807': 2, '0000-0002-1835-7601': 2, '0000-0001-5038-694X': 2, '0000-0002-2448-8149': 2, '0000-0001-8573-1213': 2, '0000-0003-2746-8992': 1, '0000-0001-7309-4325': 1, '0000-0002-8151-5182': 1, '0000-0002-9062-6183': 1, '0000-0002-5678-2369': 1, '0000-0001-5646-9746': 1, '0000-0001-7587-8924': 1, '0000-0002-0350-8534': 1, '0000-0002-8673-5221': 1, '0000-0003-2766-0845': 1, '0000-0002-5534-5466': 1, '0000-0002-9760-7436': 1})\n",
      "['0000-0002-6156-9028', '0000-0003-3463-0740', '0000-0001-9556-2361', '0000-0001-7302-4714', '0000-0002-4809-3109', '0000-0003-3075-6872', '0000-0002-1919-9107', '0000-0002-4344-8791', '0000-0002-5859-2526', '0000-0003-3870-3388', '0000-0003-1382-9195', '0000-0001-8927-6772', '0000-0002-5773-1627', '0000-0001-9783-4383', '0000-0002-4747-0419', '0000-0001-7124-2718', '0000-0002-1709-9401', '0000-0002-5947-306X', '0000-0001-5813-9505', '0000-0002-5126-1046']\n",
      "Total sample size after apply threshold:  703\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(703, 985)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(703, 985)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.62      0.69        16\n",
      "          1       0.49      0.85      0.62        27\n",
      "          2       0.91      0.53      0.67        19\n",
      "          3       0.29      0.40      0.33        15\n",
      "          4       1.00      0.94      0.97        17\n",
      "          5       0.89      0.77      0.83        22\n",
      "          6       1.00      0.71      0.83        17\n",
      "          7       0.42      0.62      0.51        40\n",
      "          8       0.87      0.74      0.80        53\n",
      "          9       1.00      0.88      0.93        64\n",
      "         10       0.81      0.84      0.82        25\n",
      "         11       0.89      0.81      0.85        31\n",
      "         12       0.95      0.95      0.95        56\n",
      "         13       0.65      0.83      0.73        98\n",
      "         14       1.00      0.88      0.94        17\n",
      "         15       0.50      0.07      0.12        14\n",
      "         16       0.61      0.47      0.53        30\n",
      "         17       0.87      0.75      0.80        63\n",
      "         18       0.74      0.74      0.74        31\n",
      "         19       0.89      0.83      0.86        48\n",
      "\n",
      "avg / total       0.79      0.76      0.76       703\n",
      "\n",
      "[10  0  0  0  0  0  0  4  1  0  0  0  0  1  0  0  0  0  0  0  1 23  0  0\n",
      "  0  0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  1 10  0  0  0  0  3\n",
      "  0  0  1  0  0  4  0  0  0  0  0  0  0  1  0  6  0  0  0  0  0  0  0  0\n",
      "  0  1  0  0  2  3  0  2  0  0  0  1 16  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0 17  0  1  1  0  1  0  0  1  0  0  1  0  0  0\n",
      "  0  1  0  0  0  0 12  0  0  0  0  0  0  2  0  0  0  0  2  0  0  2  0  0\n",
      "  0  0  0 25  1  0  0  0  0  9  0  0  0  2  1  0  1  5  0  0  0  0  0  0\n",
      " 39  0  0  1  0  5  0  0  1  0  0  1  0  0  0  1  0  0  0  3  0 56  0  0\n",
      "  0  3  0  0  0  0  1  0  1  0  0  0  0  0  0  1  0  0 21  0  0  1  0  0\n",
      "  0  0  1  0  0  1  0  1  0  0  0  1  0  0  0 25  0  2  0  0  0  0  1  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0 53  0  0  0  2  0  1  0  0  5  1  1\n",
      "  0  0  0  7  1  0  0  1  0 81  0  1  0  0  0  0  0  0  0  0  0  0  0  2\n",
      "  0  0  0  0  0  0 15  0  0  0  0  0  0  3  0  1  0  0  0  3  0  0  1  1\n",
      "  0  3  0  1  0  1  0  0  0  1  0  5  0  1  0  1  0  0  1  0  2  3  0  0\n",
      " 14  0  0  2  0  2  0  2  0  0  0  5  1  0  1  0  0  2  0  0  2 47  1  0\n",
      "  0  1  0  1  0  0  0  3  0  0  0  0  0  3  0  0  0  0 23  0  0  1  0  2\n",
      "  0  1  0  0  1  0  0  0  1  0  0  0  1  1  0 40]\n",
      "svc Accuracy:  0.7596017069701281\n",
      "svc F1:  0.7257823518464978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.50      0.64        16\n",
      "          1       0.58      0.78      0.67        27\n",
      "          2       1.00      0.26      0.42        19\n",
      "          3       0.73      0.53      0.62        15\n",
      "          4       1.00      0.94      0.97        17\n",
      "          5       1.00      0.91      0.95        22\n",
      "          6       1.00      0.76      0.87        17\n",
      "          7       0.47      0.50      0.48        40\n",
      "          8       0.86      0.79      0.82        53\n",
      "          9       1.00      0.95      0.98        64\n",
      "         10       0.76      0.76      0.76        25\n",
      "         11       0.93      0.87      0.90        31\n",
      "         12       0.92      0.96      0.94        56\n",
      "         13       0.60      0.91      0.72        98\n",
      "         14       1.00      0.82      0.90        17\n",
      "         15       1.00      0.07      0.13        14\n",
      "         16       0.74      0.67      0.70        30\n",
      "         17       0.81      0.81      0.81        63\n",
      "         18       0.81      0.71      0.76        31\n",
      "         19       0.87      0.83      0.85        48\n",
      "\n",
      "avg / total       0.81      0.78      0.78       703\n",
      "\n",
      "[ 8  0  0  0  0  0  0  4  1  0  0  0  0  3  0  0  0  0  0  0  1 21  0  0\n",
      "  0  0  0  0  0  0  0  0  0  5  0  0  0  0  0  0  0  2  5  0  0  0  0  3\n",
      "  0  0  1  0  0  8  0  0  0  0  0  0  0  0  0  8  0  0  0  0  0  0  0  0\n",
      "  0  1  0  0  2  3  0  1  0  0  0  0 16  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  1  0  0  0  0  0  0  0 20  0  0  0  0  0  0  0  1  0  0  0  0  0  1\n",
      "  0  1  0  0  0  0 13  0  0  0  0  0  0  1  0  0  0  0  2  0  0  1  0  0\n",
      "  0  0  0 20  0  0  1  0  1 12  0  0  0  3  2  0  0  2  0  0  0  0  0  1\n",
      " 42  0  0  0  0  7  0  0  0  0  0  1  0  0  0  0  0  0  0  1  1 61  0  0\n",
      "  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0 19  0  0  2  0  0\n",
      "  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0 27  0  3  0  0  1  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  1 54  0  0  0  1  0  0  0  0  2  0  1\n",
      "  0  0  0  3  2  0  0  0  0 89  0  0  0  1  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  2 14  0  0  1  0  0  0  3  0  0  0  0  0  2  0  0  1  1\n",
      "  0  4  0  1  0  1  0  1  0  1  0  1  0  0  0  1  1  0  0  0  3  1  0  0\n",
      " 20  0  0  2  0  1  0  0  0  0  0  3  0  0  3  0  0  4  0  0  1 51  0  0\n",
      "  0  1  0  0  0  0  0  2  2  0  0  0  0  4  0  0  0  0 22  0  0  1  0  1\n",
      "  0  0  0  0  0  0  0  0  1  1  0  0  2  2  0 40]\n",
      "LR Accuracy:  0.7837837837837838\n",
      "LR F1:  0.7443109861424326\n",
      "For name:  m_pinho\n",
      "total sample size before apply threshold:  97\n",
      "Counter({'0000-0002-7132-8842': 42, '0000-0001-8173-0379': 26, '0000-0002-4645-1638': 13, '0000-0002-4298-0014': 6, '0000-0003-4502-667X': 4, '0000-0003-3142-4351': 4, '0000-0002-8045-2546': 1, '0000-0002-7993-5161': 1})\n",
      "['0000-0001-8173-0379', '0000-0002-4645-1638', '0000-0002-7132-8842']\n",
      "Total sample size after apply threshold:  81\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(81, 266)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(81, 266)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98        26\n",
      "          1       1.00      0.54      0.70        13\n",
      "          2       0.87      0.98      0.92        42\n",
      "\n",
      "avg / total       0.92      0.91      0.91        81\n",
      "\n",
      "[26  0  0  0  7  6  1  0 41]\n",
      "svc Accuracy:  0.9135802469135802\n",
      "svc F1:  0.8674934633594799\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.96      0.94        26\n",
      "          1       1.00      0.54      0.70        13\n",
      "          2       0.87      0.98      0.92        42\n",
      "\n",
      "avg / total       0.91      0.90      0.89        81\n",
      "\n",
      "[25  0  1  1  7  5  1  0 41]\n",
      "LR Accuracy:  0.9012345679012346\n",
      "LR F1:  0.8549148470072786\n",
      "For name:  m_bergman\n",
      "total sample size before apply threshold:  36\n",
      "Counter({'0000-0003-2589-2976': 20, '0000-0002-4529-4925': 13, '0000-0002-7033-5362': 2, '0000-0002-7018-1578': 1})\n",
      "['0000-0002-4529-4925', '0000-0003-2589-2976']\n",
      "Total sample size after apply threshold:  33\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(33, 60)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(33, 60)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.69      0.82        13\n",
      "          1       0.83      1.00      0.91        20\n",
      "\n",
      "avg / total       0.90      0.88      0.87        33\n",
      "\n",
      "[ 9  4  0 20]\n",
      "svc Accuracy:  0.8787878787878788\n",
      "svc F1:  0.8636363636363635\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.77      0.87        13\n",
      "          1       0.87      1.00      0.93        20\n",
      "\n",
      "avg / total       0.92      0.91      0.91        33\n",
      "\n",
      "[10  3  0 20]\n",
      "LR Accuracy:  0.9090909090909091\n",
      "LR F1:  0.8998988877654197\n",
      "For name:  j_castro\n",
      "total sample size before apply threshold:  39\n",
      "Counter({'0000-0001-6169-3822': 15, '0000-0002-0382-553X': 10, '0000-0001-8984-475X': 7, '0000-0003-0794-3178': 3, '0000-0002-1939-7859': 2, '0000-0002-7468-5220': 1, '0000-0003-0868-1894': 1})\n",
      "['0000-0001-6169-3822', '0000-0002-0382-553X']\n",
      "Total sample size after apply threshold:  25\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(25, 53)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(25, 53)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        15\n",
      "          1       1.00      1.00      1.00        10\n",
      "\n",
      "avg / total       1.00      1.00      1.00        25\n",
      "\n",
      "[15  0  0 10]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        15\n",
      "          1       1.00      1.00      1.00        10\n",
      "\n",
      "avg / total       1.00      1.00      1.00        25\n",
      "\n",
      "[15  0  0 10]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  n_hall\n",
      "total sample size before apply threshold:  115\n",
      "Counter({'0000-0003-2808-0009': 102, '0000-0003-0100-0291': 5, '0000-0003-1503-5989': 4, '0000-0001-7465-5470': 2, '0000-0001-7082-1523': 1, '0000-0002-0216-512X': 1})\n",
      "['0000-0003-2808-0009']\n",
      "Total sample size after apply threshold:  102\n",
      "For name:  d_schneider\n",
      "total sample size before apply threshold:  93\n",
      "Counter({'0000-0002-2124-8385': 40, '0000-0001-9659-6731': 33, '0000-0002-2867-2613': 12, '0000-0002-0163-6137': 5, '0000-0002-5276-3304': 3})\n",
      "['0000-0002-2867-2613', '0000-0001-9659-6731', '0000-0002-2124-8385']\n",
      "Total sample size after apply threshold:  85\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(85, 311)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(85, 311)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.83      0.91        12\n",
      "          1       0.86      0.97      0.91        33\n",
      "          2       0.97      0.93      0.95        40\n",
      "\n",
      "avg / total       0.94      0.93      0.93        85\n",
      "\n",
      "[10  2  0  0 32  1  0  3 37]\n",
      "svc Accuracy:  0.9294117647058824\n",
      "svc F1:  0.9240315240315241\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.75      0.86        12\n",
      "          1       0.86      0.97      0.91        33\n",
      "          2       0.97      0.95      0.96        40\n",
      "\n",
      "avg / total       0.94      0.93      0.93        85\n",
      "\n",
      "[ 9  3  0  0 32  1  0  2 38]\n",
      "LR Accuracy:  0.9294117647058824\n",
      "LR F1:  0.9111512959614224\n",
      "For name:  n_kumar\n",
      "total sample size before apply threshold:  156\n",
      "Counter({'0000-0002-9876-2884': 25, '0000-0002-4197-5133': 24, '0000-0003-4665-9401': 22, '0000-0003-0898-908X': 15, '0000-0001-6275-8501': 12, '0000-0003-0170-888X': 11, '0000-0003-4504-4704': 7, '0000-0003-2805-779X': 7, '0000-0002-1546-1921': 5, '0000-0003-4445-877X': 4, '0000-0003-2380-9489': 4, '0000-0001-7364-6601': 4, '0000-0002-1064-1659': 3, '0000-0003-3531-7414': 3, '0000-0002-7123-2111': 2, '0000-0003-3709-0823': 2, '0000-0002-6064-4161': 1, '0000-0002-2009-3158': 1, '0000-0002-3020-3947': 1, '0000-0002-6871-1840': 1, '0000-0001-5932-8500': 1, '0000-0002-2427-647X': 1})\n",
      "['0000-0002-4197-5133', '0000-0003-0898-908X', '0000-0001-6275-8501', '0000-0003-4665-9401', '0000-0003-0170-888X', '0000-0002-9876-2884']\n",
      "Total sample size after apply threshold:  109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(109, 181)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(109, 181)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.88      0.93        24\n",
      "          1       0.54      0.93      0.68        15\n",
      "          2       0.88      0.58      0.70        12\n",
      "          3       1.00      0.91      0.95        22\n",
      "          4       0.83      0.91      0.87        11\n",
      "          5       1.00      0.88      0.94        25\n",
      "\n",
      "avg / total       0.91      0.86      0.87       109\n",
      "\n",
      "[21  3  0  0  0  0  0 14  1  0  0  0  0  3  7  0  2  0  0  2  0 20  0  0\n",
      "  0  1  0  0 10  0  0  3  0  0  0 22]\n",
      "svc Accuracy:  0.8623853211009175\n",
      "svc F1:  0.8457294241899734\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.92      0.96        24\n",
      "          1       0.67      0.93      0.78        15\n",
      "          2       0.90      0.75      0.82        12\n",
      "          3       1.00      0.95      0.98        22\n",
      "          4       0.83      0.91      0.87        11\n",
      "          5       0.96      0.88      0.92        25\n",
      "\n",
      "avg / total       0.92      0.90      0.90       109\n",
      "\n",
      "[22  2  0  0  0  0  0 14  1  0  0  0  0  1  9  0  2  0  0  1  0 21  0  0\n",
      "  0  0  0  0 10  1  0  3  0  0  0 22]\n",
      "LR Accuracy:  0.8990825688073395\n",
      "LR F1:  0.885909567532419\n",
      "For name:  i_martins\n",
      "total sample size before apply threshold:  54\n",
      "Counter({'0000-0002-9284-8599': 12, '0000-0002-0136-1671': 11, '0000-0002-8521-2613': 8, '0000-0002-5362-9801': 7, '0000-0001-6797-2558': 7, '0000-0002-3412-9377': 6, '0000-0003-0897-8807': 1, '0000-0003-4328-7286': 1, '0000-0003-3291-0079': 1})\n",
      "['0000-0002-0136-1671', '0000-0002-9284-8599']\n",
      "Total sample size after apply threshold:  23\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(23, 118)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(23, 118)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.91      0.95        11\n",
      "          1       0.92      1.00      0.96        12\n",
      "\n",
      "avg / total       0.96      0.96      0.96        23\n",
      "\n",
      "[10  1  0 12]\n",
      "svc Accuracy:  0.9565217391304348\n",
      "svc F1:  0.9561904761904763\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.91      0.95        11\n",
      "          1       0.92      1.00      0.96        12\n",
      "\n",
      "avg / total       0.96      0.96      0.96        23\n",
      "\n",
      "[10  1  0 12]\n",
      "LR Accuracy:  0.9565217391304348\n",
      "LR F1:  0.9561904761904763\n",
      "For name:  j_qiu\n",
      "total sample size before apply threshold:  58\n",
      "Counter({'0000-0002-1541-9627': 41, '0000-0002-7633-6227': 8, '0000-0002-9886-3570': 3, '0000-0001-9220-4219': 2, '0000-0002-1059-627X': 1, '0000-0002-7628-5431': 1, '0000-0002-6155-8548': 1, '0000-0002-1275-4171': 1})\n",
      "['0000-0002-1541-9627']\n",
      "Total sample size after apply threshold:  41\n",
      "For name:  m_antunes\n",
      "total sample size before apply threshold:  27\n",
      "Counter({'0000-0001-5545-2520': 7, '0000-0001-5888-2278': 6, '0000-0002-8913-6136': 6, '0000-0002-1257-2829': 5, '0000-0001-8216-8066': 3})\n",
      "[]\n",
      "Total sample size after apply threshold:  0\n",
      "For name:  m_andersen\n",
      "total sample size before apply threshold:  399\n",
      "Counter({'0000-0002-3894-4811': 222, '0000-0003-4694-486X': 58, '0000-0003-4794-6808': 39, '0000-0001-7029-2860': 21, '0000-0003-1125-1553': 14, '0000-0002-0234-0266': 11, '0000-0001-8275-9472': 11, '0000-0003-4977-3031': 8, '0000-0003-3845-4465': 7, '0000-0002-4833-1867': 3, '0000-0002-4654-3946': 2, '0000-0002-6803-0981': 2, '0000-0002-8164-278X': 1})\n",
      "['0000-0003-1125-1553', '0000-0002-0234-0266', '0000-0003-4794-6808', '0000-0001-7029-2860', '0000-0001-8275-9472', '0000-0003-4694-486X', '0000-0002-3894-4811']\n",
      "Total sample size after apply threshold:  376\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(376, 1019)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(376, 1019)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.71      0.74        14\n",
      "          1       1.00      0.73      0.84        11\n",
      "          2       0.96      0.69      0.81        39\n",
      "          3       0.77      0.48      0.59        21\n",
      "          4       0.78      0.64      0.70        11\n",
      "          5       0.92      0.84      0.88        58\n",
      "          6       0.87      0.99      0.92       222\n",
      "\n",
      "avg / total       0.88      0.88      0.87       376\n",
      "\n",
      "[ 10   0   1   0   2   0   1   0   8   0   2   0   0   1   0   0  27   0\n",
      "   0   2  10   0   0   0  10   0   1  10   2   0   0   0   7   0   2   0\n",
      "   0   0   0   0  49   9   1   0   0   1   0   1 219]\n",
      "svc Accuracy:  0.8776595744680851\n",
      "svc F1:  0.7834264232948984\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.79      0.88        14\n",
      "          1       1.00      0.55      0.71        11\n",
      "          2       0.89      0.64      0.75        39\n",
      "          3       0.91      0.48      0.62        21\n",
      "          4       1.00      0.45      0.62        11\n",
      "          5       0.98      0.74      0.84        58\n",
      "          6       0.82      1.00      0.90       222\n",
      "\n",
      "avg / total       0.87      0.86      0.85       376\n",
      "\n",
      "[ 11   0   1   0   0   0   2   0   6   0   0   0   0   5   0   0  25   0\n",
      "   0   1  13   0   0   0  10   0   0  11   0   0   0   0   5   0   6   0\n",
      "   0   2   1   0  43  12   0   0   0   0   0   0 222]\n",
      "LR Accuracy:  0.8563829787234043\n",
      "LR F1:  0.7608423976899045\n",
      "For name:  l_xiao\n",
      "total sample size before apply threshold:  302\n",
      "Counter({'0000-0001-8532-2727': 267, '0000-0002-4631-2443': 24, '0000-0003-0178-9384': 5, '0000-0003-4088-6101': 5, '0000-0002-0391-6909': 1})\n",
      "['0000-0001-8532-2727', '0000-0002-4631-2443']\n",
      "Total sample size after apply threshold:  291\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(291, 712)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(291, 712)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.98       267\n",
      "          1       0.94      0.67      0.78        24\n",
      "\n",
      "avg / total       0.97      0.97      0.97       291\n",
      "\n",
      "[266   1   8  16]\n",
      "svc Accuracy:  0.9690721649484536\n",
      "svc F1:  0.88192597267932\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.97       267\n",
      "          1       1.00      0.42      0.59        24\n",
      "\n",
      "avg / total       0.95      0.95      0.94       291\n",
      "\n",
      "[267   0  14  10]\n",
      "LR Accuracy:  0.9518900343642611\n",
      "LR F1:  0.7813439244310864\n",
      "For name:  m_hartmann\n",
      "total sample size before apply threshold:  88\n",
      "Counter({'0000-0001-8069-5284': 28, '0000-0001-6937-5677': 25, '0000-0002-8207-3806': 21, '0000-0001-6046-0365': 10, '0000-0002-4774-2787': 4})\n",
      "['0000-0001-6937-5677', '0000-0001-8069-5284', '0000-0002-8207-3806', '0000-0001-6046-0365']\n",
      "Total sample size after apply threshold:  84\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(84, 304)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(84, 304)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.84      0.91        25\n",
      "          1       1.00      0.86      0.92        28\n",
      "          2       0.64      1.00      0.78        21\n",
      "          3       1.00      0.60      0.75        10\n",
      "\n",
      "avg / total       0.91      0.86      0.86        84\n",
      "\n",
      "[21  0  4  0  0 24  4  0  0  0 21  0  0  0  4  6]\n",
      "svc Accuracy:  0.8571428571428571\n",
      "svc F1:  0.8409745447788926\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.88      0.94        25\n",
      "          1       1.00      0.96      0.98        28\n",
      "          2       0.78      1.00      0.88        21\n",
      "          3       1.00      0.80      0.89        10\n",
      "\n",
      "avg / total       0.94      0.93      0.93        84\n",
      "\n",
      "[22  0  3  0  0 27  1  0  0  0 21  0  0  0  2  8]\n",
      "LR Accuracy:  0.9285714285714286\n",
      "LR F1:  0.9204693208682571\n",
      "For name:  k_nielsen\n",
      "total sample size before apply threshold:  194\n",
      "Counter({'0000-0002-5848-0911': 89, '0000-0002-7217-2114': 59, '0000-0001-7956-1748': 20, '0000-0002-9155-2972': 12, '0000-0002-4643-5697': 11, '0000-0002-5510-7767': 2, '0000-0002-4944-9453': 1})\n",
      "['0000-0002-4643-5697', '0000-0002-9155-2972', '0000-0001-7956-1748', '0000-0002-5848-0911', '0000-0002-7217-2114']\n",
      "Total sample size after apply threshold:  191\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(191, 539)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(191, 539)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.64      0.64      0.64        11\n",
      "          1       1.00      0.83      0.91        12\n",
      "          2       0.94      0.75      0.83        20\n",
      "          3       0.96      0.89      0.92        89\n",
      "          4       0.78      0.95      0.85        59\n",
      "\n",
      "avg / total       0.89      0.87      0.88       191\n",
      "\n",
      "[ 7  0  0  2  2  0 10  0  0  2  0  0 15  0  5  2  0  1 79  7  2  0  0  1\n",
      " 56]\n",
      "svc Accuracy:  0.8743455497382199\n",
      "svc F1:  0.8315452638072165\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.36      0.50        11\n",
      "          1       1.00      0.83      0.91        12\n",
      "          2       0.93      0.70      0.80        20\n",
      "          3       0.86      0.92      0.89        89\n",
      "          4       0.83      0.93      0.88        59\n",
      "\n",
      "avg / total       0.87      0.86      0.86       191\n",
      "\n",
      "[ 4  0  0  6  1  0 10  0  1  1  0  0 14  2  4  1  0  1 82  5  0  0  0  4\n",
      " 55]\n",
      "LR Accuracy:  0.8638743455497382\n",
      "LR F1:  0.7960790513833992\n",
      "For name:  m_sousa\n",
      "total sample size before apply threshold:  211\n",
      "Counter({'0000-0002-3009-3290': 117, '0000-0001-9424-4150': 28, '0000-0002-4524-2260': 28, '0000-0003-2305-4813': 9, '0000-0003-4957-7831': 8, '0000-0002-5269-3342': 5, '0000-0002-5397-4672': 4, '0000-0002-9946-4926': 4, '0000-0003-2238-1070': 4, '0000-0003-1687-942X': 2, '0000-0003-2575-3263': 1, '0000-0003-0012-7493': 1})\n",
      "['0000-0002-3009-3290', '0000-0001-9424-4150', '0000-0002-4524-2260']\n",
      "Total sample size after apply threshold:  173\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(173, 317)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(173, 317)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.99      0.96       117\n",
      "          1       0.96      0.82      0.88        28\n",
      "          2       0.96      0.86      0.91        28\n",
      "\n",
      "avg / total       0.94      0.94      0.94       173\n",
      "\n",
      "[116   1   0   4  23   1   4   0  24]\n",
      "svc Accuracy:  0.9421965317919075\n",
      "svc F1:  0.9176437878778754\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97       117\n",
      "          1       1.00      0.86      0.92        28\n",
      "          2       1.00      0.89      0.94        28\n",
      "\n",
      "avg / total       0.96      0.96      0.96       173\n",
      "\n",
      "[117   0   0   4  24   0   3   0  25]\n",
      "LR Accuracy:  0.9595375722543352\n",
      "LR F1:  0.9458091687794968\n",
      "For name:  a_coelho\n",
      "total sample size before apply threshold:  128\n",
      "Counter({'0000-0002-6143-4203': 72, '0000-0003-2780-5821': 15, '0000-0002-7196-4179': 11, '0000-0002-3286-0262': 11, '0000-0003-3077-3859': 6, '0000-0002-7277-2267': 5, '0000-0002-2883-415X': 4, '0000-0003-3527-9091': 2, '0000-0002-9767-8891': 1, '0000-0002-2919-5468': 1})\n",
      "['0000-0003-2780-5821', '0000-0002-7196-4179', '0000-0002-6143-4203', '0000-0002-3286-0262']\n",
      "Total sample size after apply threshold:  109\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(109, 309)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(109, 309)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.53      0.64        15\n",
      "          1       0.86      0.55      0.67        11\n",
      "          2       0.84      0.94      0.89        72\n",
      "          3       0.91      0.91      0.91        11\n",
      "\n",
      "avg / total       0.84      0.84      0.83       109\n",
      "\n",
      "[ 8  0  7  0  0  6  5  0  2  1 68  1  0  0  1 10]\n",
      "svc Accuracy:  0.8440366972477065\n",
      "svc F1:  0.7761616161616162\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.47      0.64        15\n",
      "          1       1.00      0.55      0.71        11\n",
      "          2       0.85      0.99      0.91        72\n",
      "          3       0.92      1.00      0.96        11\n",
      "\n",
      "avg / total       0.89      0.87      0.86       109\n",
      "\n",
      "[ 7  0  8  0  0  6  5  0  0  0 71  1  0  0  0 11]\n",
      "LR Accuracy:  0.8715596330275229\n",
      "LR F1:  0.8022560346729144\n",
      "For name:  r_sanz\n",
      "total sample size before apply threshold:  42\n",
      "Counter({'0000-0003-2830-0892': 30, '0000-0001-6626-4146': 8, '0000-0002-2381-933X': 3, '0000-0001-8211-7306': 1})\n",
      "['0000-0003-2830-0892']\n",
      "Total sample size after apply threshold:  30\n",
      "For name:  m_ferrara\n",
      "total sample size before apply threshold:  103\n",
      "Counter({'0000-0003-2304-7576': 92, '0000-0002-4751-8478': 5, '0000-0001-5291-1373': 4, '0000-0002-1567-4281': 2})\n",
      "['0000-0003-2304-7576']\n",
      "Total sample size after apply threshold:  92\n",
      "For name:  c_hui\n",
      "total sample size before apply threshold:  44\n",
      "Counter({'0000-0002-3660-8160': 34, '0000-0002-3698-5572': 5, '0000-0002-2886-4957': 4, '0000-0002-0260-193X': 1})\n",
      "['0000-0002-3660-8160']\n",
      "Total sample size after apply threshold:  34\n",
      "For name:  l_bruno\n",
      "total sample size before apply threshold:  42\n",
      "Counter({'0000-0001-8260-4729': 22, '0000-0002-6745-0466': 10, '0000-0003-4703-2088': 7, '0000-0002-0875-7716': 2, '0000-0002-3054-2180': 1})\n",
      "['0000-0002-6745-0466', '0000-0001-8260-4729']\n",
      "Total sample size after apply threshold:  32\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(32, 108)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(32, 108)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      1.00      0.87        10\n",
      "          1       1.00      0.86      0.93        22\n",
      "\n",
      "avg / total       0.93      0.91      0.91        32\n",
      "\n",
      "[10  0  3 19]\n",
      "svc Accuracy:  0.90625\n",
      "svc F1:  0.8981972428419936\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.80      0.84        10\n",
      "          1       0.91      0.95      0.93        22\n",
      "\n",
      "avg / total       0.91      0.91      0.90        32\n",
      "\n",
      "[ 8  2  1 21]\n",
      "LR Accuracy:  0.90625\n",
      "LR F1:  0.887719298245614\n",
      "For name:  s_nielsen\n",
      "total sample size before apply threshold:  290\n",
      "Counter({'0000-0003-2417-0787': 108, '0000-0001-6391-7455': 72, '0000-0001-5341-1055': 44, '0000-0003-4175-3829': 21, '0000-0002-5777-6542': 13, '0000-0002-7780-7131': 11, '0000-0003-4309-5153': 7, '0000-0002-9754-0630': 4, '0000-0002-9214-2932': 4, '0000-0002-0458-3739': 3, '0000-0003-2770-9899': 1, '0000-0002-8449-476X': 1, '0000-0003-2064-8050': 1})\n",
      "['0000-0001-5341-1055', '0000-0003-4175-3829', '0000-0002-5777-6542', '0000-0002-7780-7131', '0000-0001-6391-7455', '0000-0003-2417-0787']\n",
      "Total sample size after apply threshold:  269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(269, 526)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(269, 526)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.80      0.89        44\n",
      "          1       1.00      0.86      0.92        21\n",
      "          2       1.00      0.62      0.76        13\n",
      "          3       0.90      0.82      0.86        11\n",
      "          4       0.98      0.86      0.92        72\n",
      "          5       0.79      0.99      0.88       108\n",
      "\n",
      "avg / total       0.91      0.89      0.89       269\n",
      "\n",
      "[ 35   0   0   0   0   9   0  18   0   0   0   3   0   0   8   0   0   5\n",
      "   0   0   0   9   0   2   0   0   0   1  62   9   0   0   0   0   1 107]\n",
      "svc Accuracy:  0.8884758364312267\n",
      "svc F1:  0.8712295743706902\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.82      0.90        44\n",
      "          1       1.00      0.90      0.95        21\n",
      "          2       1.00      0.62      0.76        13\n",
      "          3       1.00      0.73      0.84        11\n",
      "          4       0.97      0.85      0.90        72\n",
      "          5       0.79      0.99      0.88       108\n",
      "\n",
      "avg / total       0.91      0.89      0.89       269\n",
      "\n",
      "[ 36   0   0   0   0   8   0  19   0   0   0   2   0   0   8   0   0   5\n",
      "   0   0   0   8   1   2   0   0   0   0  61  11   0   0   0   0   1 107]\n",
      "LR Accuracy:  0.8884758364312267\n",
      "LR F1:  0.8730620274967253\n",
      "For name:  b_russell\n",
      "total sample size before apply threshold:  84\n",
      "Counter({'0000-0003-2333-4348': 61, '0000-0003-1282-9978': 20, '0000-0002-8740-6040': 1, '0000-0002-3345-8478': 1, '0000-0002-8848-4841': 1})\n",
      "['0000-0003-1282-9978', '0000-0003-2333-4348']\n",
      "Total sample size after apply threshold:  81\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(81, 354)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(81, 354)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.95      0.97        20\n",
      "          1       0.98      1.00      0.99        61\n",
      "\n",
      "avg / total       0.99      0.99      0.99        81\n",
      "\n",
      "[19  1  0 61]\n",
      "svc Accuracy:  0.9876543209876543\n",
      "svc F1:  0.9831144465290806\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.95      0.97        20\n",
      "          1       0.98      1.00      0.99        61\n",
      "\n",
      "avg / total       0.99      0.99      0.99        81\n",
      "\n",
      "[19  1  0 61]\n",
      "LR Accuracy:  0.9876543209876543\n",
      "LR F1:  0.9831144465290806\n",
      "For name:  y_wu\n",
      "total sample size before apply threshold:  612\n",
      "Counter({'0000-0002-3611-0258': 120, '0000-0002-1720-7863': 65, '0000-0003-3456-3373': 43, '0000-0002-2573-8736': 39, '0000-0002-1751-461X': 35, '0000-0001-5579-2197': 33, '0000-0002-2985-219X': 23, '0000-0002-8621-4098': 23, '0000-0003-0365-5590': 23, '0000-0001-9359-1863': 23, '0000-0003-0253-1625': 17, '0000-0001-9142-456X': 14, '0000-0002-0833-1205': 12, '0000-0003-3191-3163': 11, '0000-0002-4459-087X': 10, '0000-0002-7919-1107': 10, '0000-0002-9460-3579': 9, '0000-0002-9289-1271': 9, '0000-0002-3612-7818': 8, '0000-0001-5035-4577': 8, '0000-0002-3805-6515': 7, '0000-0002-5163-0884': 6, '0000-0003-1028-1785': 6, '0000-0003-3511-4270': 5, '0000-0001-7857-0247': 5, '0000-0003-3970-3160': 5, '0000-0001-7247-7404': 5, '0000-0002-1457-3681': 4, '0000-0002-8937-4417': 4, '0000-0003-0878-7605': 4, '0000-0002-8858-1289': 4, '0000-0001-5083-8950': 4, '0000-0003-4542-1741': 3, '0000-0002-1509-1721': 3, '0000-0003-2874-8267': 2, '0000-0002-3269-7143': 2, '0000-0001-7876-261X': 2, '0000-0001-9263-5369': 1, '0000-0003-3357-3051': 1, '0000-0002-8966-1459': 1, '0000-0003-0118-2152': 1, '0000-0001-5480-1741': 1, '0000-0001-5466-2446': 1})\n",
      "['0000-0001-9142-456X', '0000-0001-5579-2197', '0000-0002-3611-0258', '0000-0003-3191-3163', '0000-0002-4459-087X', '0000-0002-2985-219X', '0000-0002-0833-1205', '0000-0002-8621-4098', '0000-0002-7919-1107', '0000-0003-3456-3373', '0000-0002-1751-461X', '0000-0002-1720-7863', '0000-0003-0365-5590', '0000-0001-9359-1863', '0000-0002-2573-8736', '0000-0003-0253-1625']\n",
      "Total sample size after apply threshold:  501\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(501, 748)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(501, 748)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        14\n",
      "          1       0.45      0.73      0.56        33\n",
      "          2       0.75      0.82      0.78       120\n",
      "          3       1.00      0.91      0.95        11\n",
      "          4       1.00      0.60      0.75        10\n",
      "          5       0.60      0.65      0.63        23\n",
      "          6       0.86      0.50      0.63        12\n",
      "          7       0.50      0.39      0.44        23\n",
      "          8       0.60      0.30      0.40        10\n",
      "          9       0.53      0.63      0.57        43\n",
      "         10       0.91      0.83      0.87        35\n",
      "         11       0.77      0.75      0.76        65\n",
      "         12       0.90      0.78      0.84        23\n",
      "         13       0.65      0.57      0.60        23\n",
      "         14       0.93      0.72      0.81        39\n",
      "         15       1.00      0.88      0.94        17\n",
      "\n",
      "avg / total       0.75      0.73      0.73       501\n",
      "\n",
      "[14  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 24  2  0  0  1  0  1\n",
      "  0  1  1  3  0  0  0  0  0  6 98  0  0  2  1  2  1  6  0  0  2  2  0  0\n",
      "  0  1  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0  2  1  0  6  0  0  0\n",
      "  0  1  0  0  0  0  0  0  0  1  1  0  0 15  0  0  0  4  0  0  0  2  0  0\n",
      "  0  0  2  0  0  0  6  0  0  2  0  0  0  2  0  0  0  1  2  0  0  0  0  9\n",
      "  0  2  1  8  0  0  0  0  0  1  5  0  0  0  0  0  3  0  0  0  0  0  1  0\n",
      "  0  1  7  0  0  5  0  0  0 27  0  2  0  0  1  0  0  3  0  0  0  0  0  2\n",
      "  0  0 29  1  0  0  0  0  0  5  2  0  0  1  0  4  0  3  1 49  0  0  0  0\n",
      "  0  0  4  0  0  0  0  0  1  0  0  0 18  0  0  0  0  5  1  0  0  1  0  0\n",
      "  0  3  0  0  0 13  0  0  0  3  5  0  0  0  0  0  0  2  0  0  0  1 28  0\n",
      "  0  0  1  0  0  0  0  0  0  0  0  1  0  0  0 15]\n",
      "svc Accuracy:  0.7265469061876247\n",
      "svc F1:  0.7204865397683612\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        14\n",
      "          1       0.48      0.64      0.55        33\n",
      "          2       0.72      0.89      0.80       120\n",
      "          3       1.00      0.91      0.95        11\n",
      "          4       1.00      0.60      0.75        10\n",
      "          5       0.81      0.74      0.77        23\n",
      "          6       1.00      0.50      0.67        12\n",
      "          7       0.45      0.22      0.29        23\n",
      "          8       0.50      0.10      0.17        10\n",
      "          9       0.56      0.56      0.56        43\n",
      "         10       0.84      0.89      0.86        35\n",
      "         11       0.74      0.83      0.78        65\n",
      "         12       1.00      0.78      0.88        23\n",
      "         13       0.70      0.61      0.65        23\n",
      "         14       0.97      0.82      0.89        39\n",
      "         15       1.00      0.82      0.90        17\n",
      "\n",
      "avg / total       0.76      0.75      0.74       501\n",
      "\n",
      "[ 14   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  21\n",
      "   1   0   0   0   0   1   0   3   3   4   0   0   0   0   0   7 107   0\n",
      "   0   2   0   0   0   4   0   0   0   0   0   0   0   1   0  10   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   1   3   0   6   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   2   0   0  17   0   0   0   2\n",
      "   0   0   0   2   0   0   0   0   1   0   0   0   6   0   0   2   0   1\n",
      "   0   2   0   0   0   5   2   0   0   0   0   5   0   0   2   9   0   0\n",
      "   0   0   0   1   7   0   0   0   0   0   1   1   0   0   0   0   0   0\n",
      "   0   0  13   0   0   1   0   0   0  24   0   2   0   2   1   0   0   1\n",
      "   0   0   0   0   0   1   1   0  31   1   0   0   0   0   0   3   1   0\n",
      "   0   0   0   4   0   2   1  54   0   0   0   0   0   0   5   0   0   0\n",
      "   0   0   0   0   0   0  18   0   0   0   0   2   3   0   0   1   0   0\n",
      "   0   3   0   0   0  14   0   0   0   2   3   0   0   0   0   0   0   2\n",
      "   0   0   0   0  32   0   0   0   1   0   0   0   0   0   0   0   0   2\n",
      "   0   0   0  14]\n",
      "LR Accuracy:  0.7465069860279441\n",
      "LR F1:  0.7166711495365727\n",
      "For name:  j_soto\n",
      "total sample size before apply threshold:  64\n",
      "Counter({'0000-0003-0234-9188': 39, '0000-0001-6702-2878': 21, '0000-0001-5521-0900': 3, '0000-0001-8961-9106': 1})\n",
      "['0000-0001-6702-2878', '0000-0003-0234-9188']\n",
      "Total sample size after apply threshold:  60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(60, 255)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(60, 255)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        21\n",
      "          1       1.00      1.00      1.00        39\n",
      "\n",
      "avg / total       1.00      1.00      1.00        60\n",
      "\n",
      "[21  0  0 39]\n",
      "svc Accuracy:  1.0\n",
      "svc F1:  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        21\n",
      "          1       1.00      1.00      1.00        39\n",
      "\n",
      "avg / total       1.00      1.00      1.00        60\n",
      "\n",
      "[21  0  0 39]\n",
      "LR Accuracy:  1.0\n",
      "LR F1:  1.0\n",
      "For name:  r_mckay\n",
      "total sample size before apply threshold:  53\n",
      "Counter({'0000-0001-7781-1539': 31, '0000-0003-2723-5371': 17, '0000-0002-5602-6985': 4, '0000-0001-8042-2462': 1})\n",
      "['0000-0001-7781-1539', '0000-0003-2723-5371']\n",
      "Total sample size after apply threshold:  48\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(48, 119)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(48, 119)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      1.00      0.94        31\n",
      "          1       1.00      0.76      0.87        17\n",
      "\n",
      "avg / total       0.93      0.92      0.91        48\n",
      "\n",
      "[31  0  4 13]\n",
      "svc Accuracy:  0.9166666666666666\n",
      "svc F1:  0.903030303030303\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      1.00      0.94        31\n",
      "          1       1.00      0.76      0.87        17\n",
      "\n",
      "avg / total       0.93      0.92      0.91        48\n",
      "\n",
      "[31  0  4 13]\n",
      "LR Accuracy:  0.9166666666666666\n",
      "LR F1:  0.903030303030303\n",
      "For name:  d_sharma\n",
      "total sample size before apply threshold:  60\n",
      "Counter({'0000-0001-7612-3486': 32, '0000-0002-0082-1285': 16, '0000-0003-4463-1480': 4, '0000-0001-7379-4233': 4, '0000-0001-5818-025X': 2, '0000-0002-2971-5013': 1, '0000-0001-5557-9388': 1})\n",
      "['0000-0001-7612-3486', '0000-0002-0082-1285']\n",
      "Total sample size after apply threshold:  48\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(48, 68)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(48, 68)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.97      0.98        32\n",
      "          1       0.94      1.00      0.97        16\n",
      "\n",
      "avg / total       0.98      0.98      0.98        48\n",
      "\n",
      "[31  1  0 16]\n",
      "svc Accuracy:  0.9791666666666666\n",
      "svc F1:  0.976911976911977\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.97      0.91        32\n",
      "          1       0.92      0.69      0.79        16\n",
      "\n",
      "avg / total       0.88      0.88      0.87        48\n",
      "\n",
      "[31  1  5 11]\n",
      "LR Accuracy:  0.875\n",
      "LR F1:  0.8487394957983194\n",
      "For name:  a_wilson\n",
      "total sample size before apply threshold:  252\n",
      "Counter({'0000-0002-5045-2051': 61, '0000-0003-3679-9232': 48, '0000-0002-5016-4164': 36, '0000-0003-1098-8457': 35, '0000-0002-2000-2914': 27, '0000-0002-7696-1671': 10, '0000-0003-1325-8513': 9, '0000-0003-2352-5232': 7, '0000-0001-5865-6537': 7, '0000-0003-3362-7806': 4, '0000-0001-5775-6085': 3, '0000-0002-6473-7234': 2, '0000-0003-1461-6212': 2, '0000-0002-1015-3786': 1})\n",
      "['0000-0003-3679-9232', '0000-0002-5045-2051', '0000-0002-7696-1671', '0000-0002-5016-4164', '0000-0003-1098-8457', '0000-0002-2000-2914']\n",
      "Total sample size after apply threshold:  217\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(217, 629)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(217, 629)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.71      0.82        48\n",
      "          1       0.59      0.67      0.63        61\n",
      "          2       1.00      0.70      0.82        10\n",
      "          3       0.94      0.86      0.90        36\n",
      "          4       0.47      0.77      0.59        35\n",
      "          5       1.00      0.56      0.71        27\n",
      "\n",
      "avg / total       0.78      0.71      0.73       217\n",
      "\n",
      "[34  6  0  0  8  0  1 41  0  1 18  0  0  2  7  0  1  0  0  3  0 31  2  0\n",
      "  0  7  0  1 27  0  0 11  0  0  1 15]\n",
      "svc Accuracy:  0.7142857142857143\n",
      "svc F1:  0.7447589465557082\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.83      0.90        48\n",
      "          1       0.61      0.97      0.75        61\n",
      "          2       1.00      0.50      0.67        10\n",
      "          3       0.91      0.81      0.85        36\n",
      "          4       1.00      0.66      0.79        35\n",
      "          5       1.00      0.70      0.83        27\n",
      "\n",
      "avg / total       0.87      0.81      0.81       217\n",
      "\n",
      "[40  8  0  0  0  0  1 59  0  1  0  0  0  5  5  0  0  0  0  7  0 29  0  0\n",
      "  0 11  0  1 23  0  0  7  0  1  0 19]\n",
      "LR Accuracy:  0.8064516129032258\n",
      "LR F1:  0.7974183492445355\n",
      "For name:  f_marini\n",
      "total sample size before apply threshold:  65\n",
      "Counter({'0000-0001-8266-1117': 37, '0000-0002-9495-2349': 12, '0000-0003-0747-5060': 12, '0000-0003-3252-7758': 4})\n",
      "['0000-0002-9495-2349', '0000-0003-0747-5060', '0000-0001-8266-1117']\n",
      "Total sample size after apply threshold:  61\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(61, 213)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(61, 213)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.92      0.96        12\n",
      "          1       1.00      0.83      0.91        12\n",
      "          2       0.93      1.00      0.96        37\n",
      "\n",
      "avg / total       0.95      0.95      0.95        61\n",
      "\n",
      "[11  0  1  0 10  2  0  0 37]\n",
      "svc Accuracy:  0.9508196721311475\n",
      "svc F1:  0.9422172030867683\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.83      0.91        12\n",
      "          1       1.00      0.83      0.91        12\n",
      "          2       0.90      1.00      0.95        37\n",
      "\n",
      "avg / total       0.94      0.93      0.93        61\n",
      "\n",
      "[10  0  2  0 10  2  0  0 37]\n",
      "LR Accuracy:  0.9344262295081968\n",
      "LR F1:  0.9222999222999223\n",
      "For name:  h_tsai\n",
      "total sample size before apply threshold:  93\n",
      "Counter({'0000-0002-9393-7155': 36, '0000-0003-1310-9980': 15, '0000-0002-4070-0058': 14, '0000-0002-9661-5848': 8, '0000-0002-7395-1603': 7, '0000-0003-2097-0170': 6, '0000-0003-1174-5473': 1, '0000-0001-8242-4939': 1, '0000-0001-6444-8814': 1, '0000-0002-4480-0240': 1, '0000-0001-8972-7174': 1, '0000-0003-3467-0507': 1, '0000-0003-3840-7853': 1})\n",
      "['0000-0002-4070-0058', '0000-0003-1310-9980', '0000-0002-9393-7155']\n",
      "Total sample size after apply threshold:  65\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(65, 72)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(65, 72)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.93      0.93        14\n",
      "          1       0.91      0.67      0.77        15\n",
      "          2       0.85      0.94      0.89        36\n",
      "\n",
      "avg / total       0.88      0.88      0.87        65\n",
      "\n",
      "[13  0  1  0 10  5  1  1 34]\n",
      "svc Accuracy:  0.8769230769230769\n",
      "svc F1:  0.8641796799691536\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.79      0.85        14\n",
      "          1       0.89      0.53      0.67        15\n",
      "          2       0.77      0.94      0.85        36\n",
      "\n",
      "avg / total       0.83      0.82      0.81        65\n",
      "\n",
      "[11  0  3  0  8  7  1  1 34]\n",
      "LR Accuracy:  0.8153846153846154\n",
      "LR F1:  0.7876068376068376\n",
      "For name:  s_o'brien\n",
      "total sample size before apply threshold:  32\n",
      "Counter({'0000-0001-7353-8301': 27, '0000-0003-3133-8920': 2, '0000-0001-8965-178X': 2, '0000-0002-2052-0762': 1})\n",
      "['0000-0001-7353-8301']\n",
      "Total sample size after apply threshold:  27\n",
      "For name:  c_webb\n",
      "total sample size before apply threshold:  35\n",
      "Counter({'0000-0003-1031-3249': 16, '0000-0002-4094-2524': 15, '0000-0002-2538-6953': 3, '0000-0003-4164-9634': 1})\n",
      "['0000-0002-4094-2524', '0000-0003-1031-3249']\n",
      "Total sample size after apply threshold:  31\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(31, 149)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(31, 149)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.73      0.79        15\n",
      "          1       0.78      0.88      0.82        16\n",
      "\n",
      "avg / total       0.81      0.81      0.81        31\n",
      "\n",
      "[11  4  2 14]\n",
      "svc Accuracy:  0.8064516129032258\n",
      "svc F1:  0.8046218487394958\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.69      0.73      0.71        15\n",
      "          1       0.73      0.69      0.71        16\n",
      "\n",
      "avg / total       0.71      0.71      0.71        31\n",
      "\n",
      "[11  4  5 11]\n",
      "LR Accuracy:  0.7096774193548387\n",
      "LR F1:  0.7096774193548386\n",
      "For name:  c_adams\n",
      "total sample size before apply threshold:  69\n",
      "Counter({'0000-0003-2100-4417': 43, '0000-0001-5602-2741': 20, '0000-0002-7333-9908': 4, '0000-0003-1628-4020': 1, '0000-0002-0667-8088': 1})\n",
      "['0000-0003-2100-4417', '0000-0001-5602-2741']\n",
      "Total sample size after apply threshold:  63\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(63, 132)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(63, 132)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.95      0.98        43\n",
      "          1       0.91      1.00      0.95        20\n",
      "\n",
      "avg / total       0.97      0.97      0.97        63\n",
      "\n",
      "[41  2  0 20]\n",
      "svc Accuracy:  0.9682539682539683\n",
      "svc F1:  0.9642857142857143\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.98      0.99        43\n",
      "          1       0.95      1.00      0.98        20\n",
      "\n",
      "avg / total       0.98      0.98      0.98        63\n",
      "\n",
      "[42  1  0 20]\n",
      "LR Accuracy:  0.9841269841269841\n",
      "LR F1:  0.981922525107604\n",
      "For name:  c_peng\n",
      "total sample size before apply threshold:  103\n",
      "Counter({'0000-0003-3666-9833': 79, '0000-0003-3332-184X': 10, '0000-0001-7943-9873': 7, '0000-0001-6090-2944': 5, '0000-0002-0800-1417': 2})\n",
      "['0000-0003-3666-9833', '0000-0003-3332-184X']\n",
      "Total sample size after apply threshold:  89\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(89, 144)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(89, 144)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.96      0.98        79\n",
      "          1       0.77      1.00      0.87        10\n",
      "\n",
      "avg / total       0.97      0.97      0.97        89\n",
      "\n",
      "[76  3  0 10]\n",
      "svc Accuracy:  0.9662921348314607\n",
      "svc F1:  0.9251051893408135\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.97      0.99        79\n",
      "          1       0.83      1.00      0.91        10\n",
      "\n",
      "avg / total       0.98      0.98      0.98        89\n",
      "\n",
      "[77  2  0 10]\n",
      "LR Accuracy:  0.9775280898876404\n",
      "LR F1:  0.9481351981351982\n",
      "For name:  k_kobayashi\n",
      "total sample size before apply threshold:  35\n",
      "Counter({'0000-0003-2951-1341': 17, '0000-0002-9475-6807': 8, '0000-0002-4642-1690': 7, '0000-0002-4163-9498': 2, '0000-0001-8842-469X': 1})\n",
      "['0000-0003-2951-1341']\n",
      "Total sample size after apply threshold:  17\n",
      "For name:  s_larsen\n",
      "total sample size before apply threshold:  68\n",
      "Counter({'0000-0002-5170-4337': 31, '0000-0002-0838-9378': 11, '0000-0003-4570-9489': 10, '0000-0001-9522-7678': 9, '0000-0002-5134-3332': 4, '0000-0002-9056-6061': 2, '0000-0001-5836-370X': 1})\n",
      "['0000-0002-0838-9378', '0000-0002-5170-4337', '0000-0003-4570-9489']\n",
      "Total sample size after apply threshold:  52\n",
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "(52, 186)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "1\n",
      "(52, 186)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.82      0.90        11\n",
      "          1       1.00      0.97      0.98        31\n",
      "          2       0.77      1.00      0.87        10\n",
      "\n",
      "avg / total       0.96      0.94      0.94        52\n",
      "\n",
      "[ 9  0  2  0 30  1  0  0 10]\n",
      "svc Accuracy:  0.9423076923076923\n",
      "svc F1:  0.9177239249227845\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        11\n",
      "          1       0.97      1.00      0.98        31\n",
      "          2       1.00      0.90      0.95        10\n",
      "\n",
      "avg / total       0.98      0.98      0.98        52\n",
      "\n",
      "[11  0  0  0 31  0  0  1  9]\n",
      "LR Accuracy:  0.9807692307692307\n",
      "LR F1:  0.977165135059872\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# load the file\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "fileDir = \"../Data/\"+Dataset+\"/canopies_labeled/\"\n",
    "listfiles = os.listdir(fileDir)\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "# collect statistic to output\n",
    "allname = []\n",
    "num_class = []\n",
    "per_class_count = []\n",
    "\n",
    "all_svcLinear_accuracy = []\n",
    "all_svcLinear_f1 = []\n",
    "all_LR_accuracy = []\n",
    "all_LR_f1 = []\n",
    "\n",
    "\n",
    "# read all file in labeled group\n",
    "for file in listfiles:\n",
    "    # group name\n",
    "    temp = file.split(\"_\")\n",
    "    name = temp[1]+\"_\"+temp[-1]\n",
    "    print(\"For name: \",name)\n",
    "    allname.append(name)\n",
    "    # read needed content in labeled file\n",
    "    labeled_data_part = read_labeled_file(fileDir+file)\n",
    "    print(\"total sample size before apply threshold: \",len(labeled_data_part))\n",
    "    # count number of paper each author write based on author ID\n",
    "    paperCounter = collections.Counter(labeled_data_part[\"authorID\"])\n",
    "    print(paperCounter)\n",
    "    # collect per class statistic\n",
    "    for k in list(paperCounter):\n",
    "        if paperCounter[k] < threshold:\n",
    "            del paperCounter[k]\n",
    "    temp =list(paperCounter.keys())\n",
    "    print(temp)\n",
    "    per_class_count.append(paperCounter)\n",
    "    num_class.append(len(paperCounter))\n",
    "    # remove samples that are smaller than threshold\n",
    "    labeled_data_part = labeled_data_part[labeled_data_part.authorID.isin(temp)]\n",
    "    print(\"Total sample size after apply threshold: \",len(labeled_data_part))\n",
    "    # if only have one class or no class pass the threshold, not applicable\n",
    "    if(len(paperCounter)==0) or (len(paperCounter)==1):\n",
    "        all_svcLinear_accuracy.append(\"Not applicable\")\n",
    "        all_svcLinear_f1.append(\"Not applicable\")\n",
    "        all_LR_accuracy.append(\"Not applicable\")\n",
    "        all_LR_f1.append(\"Not applicable\")\n",
    "    else:\n",
    "        # convert author id to label\n",
    "        gather_label = []\n",
    "        for index, record in labeled_data_part.iterrows():\n",
    "            gather_label.append(temp.index(record[\"authorID\"]))\n",
    "        labeled_data_part[\"label\"] = gather_label\n",
    "        # shuffle the data\n",
    "        labeled_data = labeled_data_part.sample(frac=1).reset_index(drop=True)\n",
    "        # extract true label and it's corresponeding pid for matching\n",
    "        label = labeled_data[\"label\"]\n",
    "        pid = labeled_data[\"paperID\"]\n",
    "        reorder_idx = labeled_data.index\n",
    "        # list of different data field\n",
    "        part_collection = []\n",
    "        # select feature wanted to fit to clustering/classification algorithm\n",
    "        # data part 1, co-author matrix\n",
    "        data_part_co_author = co_author_to_vector(labeled_data[\"co-author\"], emb_type=coauthor_emb_type)\n",
    "        print(data_part_co_author.shape)\n",
    "        part_collection.append(data_part_co_author)\n",
    "        # data part 2.1, venue_id that author attend\n",
    "        data_part_venue = venue_to_vector(labeled_data[\"venue_id\"], emb_type=venue_emb_type)\n",
    "        print(data_part_venue.shape)\n",
    "        part_collection.append(data_part_venue)\n",
    "        # data part 2.2 year that author attend\n",
    "        data_part_year = year_to_vector(labeled_data[\"publish_year\"], emb_type=year_emb_type)\n",
    "        print(data_part_year.shape)\n",
    "        part_collection.append(data_part_year)\n",
    "        # data part 3.1, extract textual embedding\n",
    "        data_part_textual = extract_embedding(all_textual_embedding, pid)\n",
    "        print(data_part_textual.shape)\n",
    "        part_collection.append(data_part_textual)\n",
    "        # data part 4, read citation embedding \n",
    "        data_part_citation = extract_embedding(all_citation_embedding, pid)\n",
    "        print(data_part_citation.shape)\n",
    "        part_collection.append(data_part_citation)\n",
    "        # merge different part of data data together by concatenate it all together\n",
    "        # remove empty emb (when emb set off)\n",
    "        part_collection = [part for part in part_collection if len(part)!=0]\n",
    "        print(len(part_collection))\n",
    "        if len(part_collection)>1:\n",
    "            combinedata = np.concatenate(part_collection,axis=1)\n",
    "        elif len(part_collection)==1:\n",
    "            if isinstance(part_collection[0], pd.DataFrame):\n",
    "                combinedata = part_collection[0].values\n",
    "            else:\n",
    "                combinedata = part_collection[0]\n",
    "        else:\n",
    "            print(\"No data available\")\n",
    "            break\n",
    "        print(combinedata.shape)\n",
    "        # using converted feature vector to train classifier\n",
    "        # using SVM with linear kernal\n",
    "        # use 10 fold cv\n",
    "        clf = SVC(decision_function_shape='ovr', kernel='linear')\n",
    "        svcaccuracy, svcmarcof1 = k_fold_cv(combinedata, label, clf, k=10)\n",
    "        print(\"svc Accuracy: \",svcaccuracy)\n",
    "        print(\"svc F1: \", svcmarcof1)\n",
    "        all_svcLinear_accuracy.append(svcaccuracy)\n",
    "        all_svcLinear_f1.append(svcmarcof1)\n",
    "        # using logistic regression\n",
    "        clf = LogisticRegression(multi_class='ovr')\n",
    "        LRaccuracy, LRmarcof1 = k_fold_cv(combinedata, label, clf, k=10)\n",
    "        print(\"LR Accuracy: \",LRaccuracy)\n",
    "        print(\"LR F1: \", LRmarcof1)\n",
    "        all_LR_accuracy.append(LRaccuracy)\n",
    "        all_LR_f1.append(LRmarcof1)\n",
    "\n",
    "# write evaluation result to excel\n",
    "output = pd.DataFrame({'Name Group':allname,\"Class number\":num_class, \"per_class_size\":per_class_count,\n",
    "                       \"svc(linear) accuracy\":all_svcLinear_accuracy, \"svc(linear) macro f1\": all_svcLinear_f1, \n",
    "                       \"logistic regression accuracy\":all_LR_accuracy, \"logistic regression macro f1\": all_LR_f1})\n",
    "\n",
    "savePath = \"../result/\"+Dataset+\"/author_only/\"\n",
    "if not os.path.exists(savePath):\n",
    "    os.makedirs(savePath)\n",
    "filename = \"2004_name_clf_\"+coauthor_emb_type+\"_threshold=\"+str(threshold)+\".csv\"\n",
    "output.to_csv(savePath+filename, encoding='utf-8',index=False)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T03:49:52.142337Z",
     "start_time": "2018-11-30T03:49:52.019572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n",
      "784\n",
      "0.9082657784303335\n",
      "0.9132005972509546\n"
     ]
    }
   ],
   "source": [
    "# accuracy\n",
    "from statistics import mean \n",
    "cleaned_svcLinear_accuracy = [x for x in all_svcLinear_accuracy if isinstance(x, float)]\n",
    "cleaned_lr_accuracy = [x for x in all_LR_accuracy if isinstance(x, float)]\n",
    "print(len(cleaned_svcLinear_accuracy))\n",
    "print(len(cleaned_lr_accuracy))\n",
    "print(mean(cleaned_svcLinear_accuracy))\n",
    "print(mean(cleaned_lr_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T03:49:52.259227Z",
     "start_time": "2018-11-30T03:49:52.153793Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n",
      "784\n",
      "0.8868844839633812\n",
      "0.8808230121485386\n"
     ]
    }
   ],
   "source": [
    "# f1\n",
    "from statistics import mean \n",
    "# remove string from result\n",
    "cleaned_svcLinear_f1 = [x for x in all_svcLinear_f1 if isinstance(x, float)]\n",
    "cleaned_lr_f1 = [x for x in all_LR_f1 if isinstance(x, float)]\n",
    "print(len(cleaned_svcLinear_f1))\n",
    "print(len(cleaned_lr_f1))\n",
    "print(mean(cleaned_svcLinear_f1))\n",
    "print(mean(cleaned_lr_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
