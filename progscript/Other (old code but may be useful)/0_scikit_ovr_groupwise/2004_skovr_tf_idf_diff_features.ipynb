{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-17T00:32:45.093Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import com_func\n",
    "\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "# parameters\n",
    "threshold = 30\n",
    "cutoff = 3\n",
    "\n",
    "pp_textual = \"tf_idf\"\n",
    "\n",
    "Dataset = \"pubmed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-17T00:32:45.740Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# load text information\n",
    "Dataset = \"pubmed\"\n",
    "raw_filepath = \"../../Data\"+\"/\"+Dataset+\"/id_textual_combined.txt\"\n",
    "all_text_content = []\n",
    "with open(raw_filepath, 'r', encoding = 'utf8') as f:\n",
    "    # items[0] is paper ID, items[1] is title, items[2] is abstract\n",
    "    for line in f:\n",
    "        items = line.split(\"\\t\")\n",
    "        # lower case all character\n",
    "        paperID = items[0]\n",
    "        title = items[1].lower()\n",
    "        keywords = items[2].lower()\n",
    "        mesh = items[3].lower()\n",
    "        abstract = items[4].lower()\n",
    "        # keyword and mesh\n",
    "        key_mesh = keywords+\" \"+mesh\n",
    "        # title and abstract\n",
    "        title_abstract = title+\" \"+abstract\n",
    "        # title keywords mesh\n",
    "        title_key_mesh = title+\" \"+key_mesh\n",
    "        # abstract keywords mesh\n",
    "        abstract_key_mesh = abstract+\" \"+key_mesh\n",
    "        # all feature combined\n",
    "        content = title+\" \"+keywords+\" \"+mesh+\" \"+abstract\n",
    "        paper_text_content = {\"paperID\": paperID, \"title\":title, \"keywords_mesh\":key_mesh, \"abstract\": abstract,\n",
    "                              \"title_abstract\":title_abstract,\"title_key_mesh\":title_key_mesh, \n",
    "                              \"abstract_key_mesh\":abstract_key_mesh, \"combine_textual\":content}\n",
    "        all_text_content.append(paper_text_content)\n",
    "print(\"Total \", len(all_text_content), \" paper have text information\")\n",
    "# convert to dataframe so it's easy to process\n",
    "all_text_content = pd.DataFrame(all_text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-17T00:32:46.989Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_labeled_file(infile):\n",
    "    LabeledRecords_original = []\n",
    "    with open(infile, 'r', encoding = 'utf8') as f:\n",
    "        for line in f:\n",
    "            read_data = line.split(\"\\t\")\n",
    "            # get ride of bad formated lines\n",
    "            if(len(read_data)==13):\n",
    "                paper_detail = {\"paperID\": read_data[0], \"authorID\":read_data[1], \n",
    "                                \"co-author\": read_data[5], \"venue_id\": read_data[7]}\n",
    "                LabeledRecords_original.append(paper_detail)\n",
    "        f.close()\n",
    "    return pd.DataFrame(LabeledRecords_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-17T00:32:47.395Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# document relation wrt textual content\n",
    "# convert raw text to numerical feature vectors\n",
    "# bow(Bags of words) are used with uni-gram setting\n",
    "def raw_text_to_vector(raw_textual_content, emb_type=\"off\", stopword=True):\n",
    "    cleaned_token, sample_size= com_func.clean_batch_of_raw(raw_textual_content, stopword=stopword)\n",
    "    average_sample_size = sum(sample_size)/len(sample_size)\n",
    "    print(\"Minimal sample size: \", min(sample_size))\n",
    "    print(\"maximal sample size: \", max(sample_size))\n",
    "    while True:\n",
    "        if emb_type == \"tf_idf\":\n",
    "            # using tf-idf\n",
    "            tfidf_vectorizer = TfidfVectorizer(use_idf=True, smooth_idf=True, sublinear_tf=True, tokenizer=dummy,\n",
    "                                               preprocessor=dummy, stop_words = None,min_df=cutoff)\n",
    "            result_vector = tfidf_vectorizer.fit_transform(cleaned_token).toarray()\n",
    "            #print(len(tfidf_vectorizer.vocabulary_))\n",
    "            #print(tfidf_vectorizer.get_feature_names())\n",
    "            break\n",
    "        elif emb_type == \"tf\":\n",
    "            # Document-Term frequence Matrix\n",
    "            count_vectorizer = CountVectorizer(tokenizer=dummy,preprocessor=dummy, min_df=cutoff)\n",
    "            result_vector = count_vectorizer.fit_transform(cleaned_token).toarray()\n",
    "            break\n",
    "        elif emb_type == \"lsa\":\n",
    "            # use lsa\n",
    "            result_vector = LSA(cleaned_token, dim=100)\n",
    "            break\n",
    "        elif emb_type == \"off\":\n",
    "            result_vector = pd.DataFrame()\n",
    "            break\n",
    "        else:\n",
    "            print(\"Embedding type not available, selecting default setting\")\n",
    "            emb_type=\"off\"\n",
    "    return result_vector, average_sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-17T00:32:47.630Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_score, recall_score,f1_score,accuracy_score\n",
    "# cross validation\n",
    "def k_fold_cv(data, label, clf, k=10):\n",
    "    kf = KFold(n_splits=k, shuffle=False)\n",
    "    allTrueLabel = []\n",
    "    allPredLabel = []\n",
    "    for train_index, test_index in kf.split(data):\n",
    "        # print(\"TRAIN:\", train_index, \" \\n TEST:\", test_index)\n",
    "        # split train and test\n",
    "        data_train, data_test = data[train_index], data[test_index]\n",
    "        label_train, label_test = label[train_index], label[test_index]\n",
    "        # fit data to clf\n",
    "        clf.fit(data_train, label_train)\n",
    "        # get predicted label\n",
    "        label_pred = clf.predict(data_test)\n",
    "        allTrueLabel.extend(label_test)\n",
    "        allPredLabel.extend(label_pred)\n",
    "        # print(\"True positive: {tp}, False positive: {fp}, False negative: {fn}, True negative: {tn}\"\n",
    "        # .format(tp=round_tp, fp=round_fp, fn=round_fn, tn=round_tn))\n",
    "\n",
    "    accuracy = accuracy_score(allTrueLabel, allPredLabel)\n",
    "    f1 = f1_score(allTrueLabel, allPredLabel,average='macro')\n",
    "    \n",
    "    # accumulate statistic for entire model f1\n",
    "    cnf_matrix = confusion_matrix(allTrueLabel, allPredLabel)\n",
    "    TP = np.diag(cnf_matrix)\n",
    "    FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)  \n",
    "    FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "    TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "#     print(cnf_matrix)\n",
    "#     print(\"TP: \",TP, \"TN: \",TN, \"FP: \",FP,\"FN: \",FN)\n",
    "\n",
    "    return accuracy, f1, TP.sum(), TN.sum(), FP.sum(), FN.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-17T00:32:48.136Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_csv_df(savePath, filename, df):\n",
    "    if not os.path.exists(savePath):\n",
    "        os.makedirs(savePath)\n",
    "    # Give the filename you wish to save the file to\n",
    "    pathfile = os.path.normpath(os.path.join(savePath,filename))\n",
    "\n",
    "    # Use this function to search for any files which match your filename\n",
    "    files_present = os.path.isfile(pathfile) \n",
    "    # if no matching files, write to csv, if there are matching files, print statement\n",
    "    if not files_present:\n",
    "        df.to_csv(pathfile, encoding='utf-8',index=False)\n",
    "    else:\n",
    "        overwrite = input(\"WARNING: \" + pathfile + \" already exists! Do you want to overwrite <y/n>? \\n \")\n",
    "        if overwrite == 'y':\n",
    "            df.to_csv(pathfile, encoding='utf-8',index=False)\n",
    "        elif overwrite == 'n':\n",
    "            new_filename = input(\"Type new filename: \\n \")\n",
    "            write_csv_df(savePath,new_filename,df)\n",
    "        else:\n",
    "            print(\"Not a valid input. Data is NOT saved!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-17T00:32:48.748Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the file\n",
    "import io\n",
    "import collections\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "fileDir = \"../../Data/\"+Dataset+\"/canopies_labeled/\"\n",
    "listfiles = os.listdir(fileDir)\n",
    "\n",
    "# model f1\n",
    "modelMNBf1, modelSVCf1, modelLRf1 = ([] for i in range(3))\n",
    "\n",
    "# list of features\n",
    "all_features = [\"title_abstract\", \"combine_textual\"]\n",
    "\n",
    "for feature in all_features:\n",
    "    # collect statistic to output\n",
    "    allname, num_class, per_class_count, average_textual_size = ([] for i in range(4))\n",
    "\n",
    "    all_mnb_accuracy, all_mnb_f1,all_svcLinear_accuracy = ([] for i in range(3))\n",
    "    all_svcLinear_f1, all_LR_accuracy, all_LR_f1 = ([] for i in range(3))\n",
    "    \n",
    "    # collect overall tp, tn, fp, fn\n",
    "    mnbTP=mnbTN=mnbFP=mnbFN = 0\n",
    "    svcTP=svcTN=svcFP=svcFN = 0\n",
    "    lrTP=lrTN=lrFP=lrFN = 0\n",
    "    \n",
    "    # read all file in labeled group\n",
    "    for file in listfiles:\n",
    "        # group name\n",
    "        temp = file.split(\"_\")\n",
    "        name = temp[1]+\"_\"+temp[-1]\n",
    "        print(\"For name: \",name)\n",
    "        # read needed content in labeled file\n",
    "        labeled_data_part = read_labeled_file(fileDir+file)\n",
    "        print(\"total sample size before apply threshold: \",len(labeled_data_part))\n",
    "        # count number of paper each author write based on author ID\n",
    "        paperCounter = collections.Counter(labeled_data_part[\"authorID\"])\n",
    "        print(paperCounter)\n",
    "        # collect per class statistic\n",
    "        for k in list(paperCounter):\n",
    "            if paperCounter[k] < threshold:\n",
    "                del paperCounter[k]\n",
    "        temp =list(paperCounter.keys())\n",
    "        print(temp)\n",
    "        # remove samples that are smaller than threshold\n",
    "        labeled_data_part = labeled_data_part[labeled_data_part.authorID.isin(temp)]\n",
    "        print(\"Total sample size after apply threshold: \",len(labeled_data_part))\n",
    "        # if only have one class or no class pass the threshold, not applicable\n",
    "        if(len(paperCounter)==0) or (len(paperCounter)==1):\n",
    "            print(name, \" pass\")\n",
    "        else:\n",
    "            allname.append(name)\n",
    "            num_class.append(len(paperCounter))\n",
    "            per_class_count.append(paperCounter)\n",
    "            # convert author id to label\n",
    "            gather_label = []\n",
    "            for index, record in labeled_data_part.iterrows():\n",
    "                gather_label.append(temp.index(record[\"authorID\"]))\n",
    "            labeled_data_part[\"label\"] = gather_label\n",
    "            # merge title and abstract from all raw data to labeled dataset\n",
    "            labeled_data = pd.merge(left=labeled_data_part,right=all_text_content, how='left', left_on='paperID', right_on='paperID')\n",
    "            # shuffle the data\n",
    "            labeled_data = labeled_data.sample(frac=1).reset_index(drop=True)\n",
    "            # extract true label and pid\n",
    "            label = labeled_data[\"label\"]\n",
    "            pid = labeled_data[\"paperID\"]\n",
    "            # list of different data field\n",
    "            part_collection = []\n",
    "            # select feature wanted to fit to clustering/classification algorithm\n",
    "            # data part 3, textual information\n",
    "            data_part_textual, avg_textual_size = raw_text_to_vector(labeled_data[feature], emb_type=pp_textual)\n",
    "            average_textual_size.append(avg_textual_size)\n",
    "            print(data_part_textual.shape)\n",
    "            part_collection.append(data_part_textual)\n",
    "            # merge different part of data data together by concatenate it all together\n",
    "            # remove empty emb (when emb set off)\n",
    "            part_collection = [part for part in part_collection if len(part)!=0]\n",
    "            if len(part_collection)>1:\n",
    "                combinedata = np.concatenate(part_collection,axis=1)\n",
    "            else:\n",
    "                combinedata = part_collection[0]\n",
    "            print(combinedata.shape)\n",
    "            # using converted feature vector to train classifier\n",
    "            # using Multinomial naive bayes\n",
    "            clf = MultinomialNB()\n",
    "            # use 10 fold cv\n",
    "            mnbaccuracy, mnbmarcof1, tp, tn, fp, fn = k_fold_cv(combinedata, label, clf, k=10)\n",
    "            mnbTP+=tp\n",
    "            mnbTN+=tn\n",
    "            mnbFP+=fp\n",
    "            mnbFN+=fn\n",
    "            print(\"MNB Accuracy: \",mnbaccuracy)\n",
    "            print(\"MNB F1: \", mnbmarcof1)\n",
    "            all_mnb_accuracy.append(mnbaccuracy)\n",
    "            all_mnb_f1.append(mnbmarcof1)\n",
    "            # using SVM with linear kernal\n",
    "            clf = SVC(decision_function_shape='ovr', kernel='linear')\n",
    "            svcaccuracy, svcmarcof1, tp, tn, fp, fn = k_fold_cv(combinedata, label, clf, k=10)\n",
    "            svcTP+=tp\n",
    "            svcTN+=tn\n",
    "            svcFP+=fp\n",
    "            svcFN+=fn\n",
    "            print(\"svc Accuracy: \",svcaccuracy)\n",
    "            print(\"svc F1: \", svcmarcof1)\n",
    "            all_svcLinear_accuracy.append(svcaccuracy)\n",
    "            all_svcLinear_f1.append(svcmarcof1)\n",
    "            # using logistic regression\n",
    "            clf = LogisticRegression(multi_class='ovr')\n",
    "            LRaccuracy, LRmarcof1, tp, tn, fp, fn = k_fold_cv(combinedata, label, clf, k=10)\n",
    "            lrTP+=tp\n",
    "            lrTN+=tn\n",
    "            lrFP+=fp\n",
    "            lrFN+=fn\n",
    "            print(\"LR Accuracy: \",LRaccuracy)\n",
    "            print(\"LR F1: \", LRmarcof1)\n",
    "            all_LR_accuracy.append(LRaccuracy)\n",
    "            all_LR_f1.append(LRmarcof1)\n",
    "    # print f1 for entire model\n",
    "    print(\"mnb: TP: \",mnbTP, \"TN: \",mnbTN, \"FP: \",mnbFP,\"FN: \",mnbFN)\n",
    "    print(\"svc: TP: \",svcTP, \"TN: \",svcTN, \"FP: \",svcFP,\"FN: \",svcFN)\n",
    "    print(\"lr: TP: \",lrTP, \"TN: \",lrTN, \"FP: \",lrFP,\"FN: \",lrFN)\n",
    "    mnbF1 = 2*mnbTP / (2*mnbTP + mnbFP + mnbFN)\n",
    "    svcF1 = 2*svcTP / (2*svcTP + svcFP + svcFN)\n",
    "    lrF1 = 2*lrTP / (2*lrTP + lrFP + lrFN)\n",
    "    modelMNBf1.append(mnbF1)\n",
    "    modelSVCf1.append(svcF1)\n",
    "    modelLRf1.append(lrF1)\n",
    "    # write evaluation result to excel\n",
    "    output = pd.DataFrame({'Name Group':allname,\"Class number\":num_class,\"average term in sample\":average_textual_size,\n",
    "                           \"per_class_size\":per_class_count,\"mnb accuracy\":all_mnb_accuracy, \"mnb macro f1\": all_mnb_f1,\n",
    "                           \"svc(linear) accuracy\":all_svcLinear_accuracy, \"svc(linear) macro f1\": all_svcLinear_f1, \n",
    "                           \"logistic regression accuracy\":all_LR_accuracy, \"logistic regression macro f1\": all_LR_f1})\n",
    "\n",
    "    savePath = \"../../result/\"+Dataset+\"/skovr/\"+feature+\"/\"\n",
    "    filename = \"feature=\"+feature+\"_textual=\"+pp_textual+\"_threshold=\"+str(threshold)+\".csv\"\n",
    "    write_csv_df(savePath, filename, output)\n",
    "    print(feature, \" Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-17T00:32:49.559Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(all_features)\n",
    "print(\"mnb: \", modelMNBf1)\n",
    "print(\"svc: \", modelSVCf1)\n",
    "print(\"lr: \", modelLRf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T01:24:15.579122Z",
     "start_time": "2018-12-14T01:24:15.557894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289\n",
      "289\n",
      "289\n",
      "0.9778910513144647\n",
      "0.9572108961110675\n",
      "0.977951254354412\n"
     ]
    }
   ],
   "source": [
    "# accuracy\n",
    "from statistics import mean \n",
    "cleaned_mnb_accuracy = [x for x in all_mnb_accuracy if isinstance(x, float)]\n",
    "cleaned_svcLinear_accuracy = [x for x in all_svcLinear_accuracy if isinstance(x, float)]\n",
    "cleaned_lr_accuracy = [x for x in all_LR_accuracy if isinstance(x, float)]\n",
    "print(len(cleaned_mnb_accuracy))\n",
    "print(len(cleaned_svcLinear_accuracy))\n",
    "print(len(cleaned_lr_accuracy))\n",
    "print(mean(cleaned_mnb_accuracy))\n",
    "print(mean(cleaned_svcLinear_accuracy))\n",
    "print(mean(cleaned_lr_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T01:24:16.477091Z",
     "start_time": "2018-12-14T01:24:16.455026Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289\n",
      "289\n",
      "289\n",
      "0.9747506103432456\n",
      "0.9517279039263197\n",
      "0.9747488300982716\n"
     ]
    }
   ],
   "source": [
    "# f1\n",
    "from statistics import mean \n",
    "# remove string from result\n",
    "cleaned_mnb_f1 = [x for x in all_mnb_f1 if isinstance(x, float)]\n",
    "cleaned_svcLinear_f1 = [x for x in all_svcLinear_f1 if isinstance(x, float)]\n",
    "cleaned_lr_f1 = [x for x in all_LR_f1 if isinstance(x, float)]\n",
    "print(len(cleaned_mnb_f1))\n",
    "print(len(cleaned_svcLinear_f1))\n",
    "print(len(cleaned_lr_f1))\n",
    "print(mean(cleaned_mnb_f1))\n",
    "print(mean(cleaned_svcLinear_f1))\n",
    "print(mean(cleaned_lr_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-11-29T02:46:19.893Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(all_LR_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-11-29T02:46:20.400Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = [x for x in all_mnb_f1 if isinstance(x, float)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
