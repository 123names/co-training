{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One classifier each name: OCEN (Author group)\n",
    "This method simply ignored the records below threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T20:41:54.244922Z",
     "start_time": "2019-01-09T20:41:52.662375Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "\n",
    "import com_func\n",
    "\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "\n",
    "# parameters\n",
    "threshold = 100\n",
    "cutoff = 3\n",
    "\n",
    "pp_textual = [\"tf\",\"tf_idf\",\"lsa\"]\n",
    "\n",
    "Dataset = \"pubmed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T20:41:55.477693Z",
     "start_time": "2019-01-09T20:41:54.250410Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gensim\n",
    "# read trained rec to rec textual graph\n",
    "def read_textual_embedding(Dataset = \"pubmed\", emb_type = \"off\"):\n",
    "    textual_emb = []\n",
    "    emb_pid = []\n",
    "    while True:\n",
    "        if emb_type == \"tf\":\n",
    "            modelSaveDir = \"../../Data/\"+Dataset+\"/models/tf/textual_sample=140k/\"\n",
    "            with open(modelSaveDir+'tf_features.pickle', \"rb\") as input_file:\n",
    "                vec = pickle.load(input_file)\n",
    "            with open(modelSaveDir+'feature_pid.pickle', \"rb\") as input_file:\n",
    "                allPaperid = pickle.load(input_file)\n",
    "            textual_emb = vec.toarray()\n",
    "            emb_pid = allPaperid\n",
    "            break\n",
    "        elif emb_type == \"tfidf\":\n",
    "            modelSaveDir = \"../../Data/\"+Dataset+\"/models/tf_idf/textual_sample=140k/\"\n",
    "            with open(modelSaveDir+'tf_idf_trained_features.pickle', \"rb\") as input_file:\n",
    "                vec = pickle.load(input_file)\n",
    "            with open(modelSaveDir+'feature_pid.pickle', \"rb\") as input_file:\n",
    "                allPaperid = pickle.load(input_file)\n",
    "            textual_emb = vec.toarray()\n",
    "            emb_pid = allPaperid\n",
    "            break\n",
    "        elif emb_type == \"lsa\":\n",
    "            modelSaveDir = \"../../Data/\"+Dataset+\"/models/lsa/textual_sample=140k/\"\n",
    "            with open(modelSaveDir+'lsa_Matrix.pickle', \"rb\") as input_file:\n",
    "                vec = pickle.load(input_file)\n",
    "            with open(modelSaveDir+'feature_pid.pickle', \"rb\") as input_file:\n",
    "                allPaperid = pickle.load(input_file)\n",
    "            textual_emb = vec\n",
    "            emb_pid = allPaperid\n",
    "            break\n",
    "        elif emb_type == \"pv_dm\":\n",
    "            modelSaveDir = \"../../Data/\"+Dataset+\"/models/doc2v/textual_sample=140k/\"\n",
    "            model = gensim.models.Doc2Vec.load(modelSaveDir+\"pv_dm/Doc2Vec(dmm,d100,n5,w5,mc2,s0.001,t24)\")\n",
    "            allPaperTags = model.docvecs.offset2doctag\n",
    "            for pid in allPaperTags:\n",
    "                vectorRepresentation = model.docvecs[pid].tolist()\n",
    "                vectorRepresentation = [float(i) for i in vectorRepresentation]\n",
    "                textual_emb.append(vectorRepresentation)\n",
    "            emb_pid = allPaperTags\n",
    "            break\n",
    "        elif emb_type == \"pv_dbow\":\n",
    "            modelSaveDir = \"../../Data/\"+Dataset+\"/models/doc2v/textual_sample=140k/\"\n",
    "            model = gensim.models.Doc2Vec.load(modelSaveDir+\"pv_dbow/Doc2Vec(dbow,d100,n5,mc2,s0.001,t24)\")\n",
    "\n",
    "            allPaperTags = model.docvecs.offset2doctag\n",
    "            for pid in allPaperTags:\n",
    "                vectorRepresentation = model.docvecs[pid].tolist()\n",
    "                vectorRepresentation = [float(i) for i in vectorRepresentation]\n",
    "                textual_emb.append(vectorRepresentation)\n",
    "            emb_pid = allPaperTags\n",
    "            break\n",
    "        elif emb_type == \"off\":\n",
    "            break\n",
    "        else:\n",
    "            print(\"Embedding type not available, selecting default setting\")\n",
    "            emb_type=\"off\"\n",
    "    print(\"Total textual vector records:\",len(textual_emb))\n",
    "    print(\"Vector dimension: \", len(textual_emb[0]))\n",
    "    return textual_emb, emb_pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T20:41:55.583427Z",
     "start_time": "2019-01-09T20:41:55.482958Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read trained rec to rec node2vec citation graph\n",
    "def read_citation_embedding(Dataset = \"pubmed\", emb_type = \"off\"):\n",
    "    citation_emb = []\n",
    "    emb_pid = []\n",
    "    while True:\n",
    "        if emb_type == \"n2v\":\n",
    "            citation_emb_dir = \"../../Data/\"+Dataset+\"/vectors/\"+emb_type+\"/extracted_labeled_n2v.txt\"\n",
    "            with open(citation_emb_dir, 'r', encoding = 'utf8') as f:\n",
    "                for line in f:\n",
    "                    read_data = line.split(\" \")\n",
    "                    if(len(read_data)==101):\n",
    "                        emb_pid.append(read_data[0])\n",
    "                        citation_emb.append(read_data[1:])\n",
    "            f.close()\n",
    "            break\n",
    "        elif emb_type == \"off\":\n",
    "            break\n",
    "        else:\n",
    "            print(\"Embedding type not available, selecting default setting\")\n",
    "            emb_type=\"off\"\n",
    "    print(\"Total citation vector records:\",len(citation_emb))\n",
    "    print(\"Vector dimension: \", len(citation_emb[0]))\n",
    "    return citation_emb, emb_pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T20:41:55.921800Z",
     "start_time": "2019-01-09T20:41:55.588344Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "def read_labeled_file(infile):\n",
    "    LabeledRecords_original = []\n",
    "    with open(infile, 'r', encoding = 'utf8') as f:\n",
    "        for line in f:\n",
    "            read_data = line.split(\"\\t\")\n",
    "            # get ride of bad formated lines\n",
    "            if(len(read_data)==13 or len(read_data)==12):\n",
    "                paper_detail = {\"paperID\": read_data[0], \"authorID\":read_data[1], \n",
    "                                \"co-author\": read_data[5], \"venue_id\": read_data[7]}\n",
    "                LabeledRecords_original.append(paper_detail)\n",
    "            else:\n",
    "                print(len(read_data))\n",
    "        f.close()\n",
    "    return pd.DataFrame(LabeledRecords_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T20:41:55.946622Z",
     "start_time": "2019-01-09T20:41:55.924008Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_embedding(all_embedding, all_embedding_pid, wanted_pid_list):\n",
    "    extracted_emb = []\n",
    "    wanted_pid_list = wanted_pid_list.values.tolist()\n",
    "    # only if embedding exist\n",
    "    if len(all_embedding)>0:\n",
    "        # loop through wanted pid list to keep input order\n",
    "        for wanted_pid in wanted_pid_list:\n",
    "            # if wanted paper in all pretrained embeddings\n",
    "            if wanted_pid in all_embedding_pid:\n",
    "                emb_idx = all_embedding_pid.index(wanted_pid)\n",
    "                extracted_emb.append(all_embedding[emb_idx])\n",
    "            # if wanted paper not in all pretrained embeddings, fill missing sample with 0's\n",
    "            else:\n",
    "                print(\"Missing Sample: \", wanted_pid)\n",
    "                temp = [0] * len(all_embedding[0])\n",
    "                extracted_emb.append(temp)\n",
    "                \n",
    "    extracted_emb = pd.DataFrame(extracted_emb)\n",
    "    return extracted_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T20:41:56.730808Z",
     "start_time": "2019-01-09T20:41:56.549911Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_score, recall_score,f1_score,accuracy_score\n",
    "# cross validation\n",
    "def k_fold_cv(data, label, clf, k=10):\n",
    "    kf = KFold(n_splits=k, shuffle=False)\n",
    "    allTrueLabel = []\n",
    "    allPredLabel = []\n",
    "    for train_index, test_index in kf.split(data):\n",
    "        # print(\"TRAIN:\", train_index, \" \\n TEST:\", test_index)\n",
    "        # split train and test\n",
    "        data_train, data_test = data[train_index], data[test_index]\n",
    "        label_train, label_test = label[train_index], label[test_index]\n",
    "        # fit data to clf\n",
    "        clf.fit(data_train, label_train)\n",
    "        # get predicted label\n",
    "        label_pred = clf.predict(data_test)\n",
    "        allTrueLabel.extend(label_test)\n",
    "        allPredLabel.extend(label_pred)\n",
    "\n",
    "    accuracy = accuracy_score(allTrueLabel, allPredLabel)\n",
    "    f1 = f1_score(allTrueLabel, allPredLabel,average='macro')\n",
    "    \n",
    "    print(metrics.classification_report(allTrueLabel, allPredLabel))\n",
    "    print(metrics.confusion_matrix(allTrueLabel, allPredLabel).ravel())\n",
    "    \n",
    "    # accumulate statistic for entire model f1\n",
    "    cnf_matrix = confusion_matrix(allTrueLabel, allPredLabel)\n",
    "    TP = np.diag(cnf_matrix)\n",
    "    FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)  \n",
    "    FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "    TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "#     print(cnf_matrix)\n",
    "#     print(\"TP: \",TP, \"TN: \",TN, \"FP: \",FP,\"FN: \",FN)\n",
    "\n",
    "    return accuracy, f1, TP.sum(), TN.sum(), FP.sum(), FN.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T20:41:57.580582Z",
     "start_time": "2019-01-09T20:41:57.517673Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_csv_df(savePath, filename, df):\n",
    "    if not os.path.exists(savePath):\n",
    "        os.makedirs(savePath)\n",
    "    # Give the filename you wish to save the file to\n",
    "    pathfile = os.path.normpath(os.path.join(savePath,filename))\n",
    "\n",
    "    # Use this function to search for any files which match your filename\n",
    "    files_present = os.path.isfile(pathfile) \n",
    "    # if no matching files, write to csv, if there are matching files, print statement\n",
    "    if not files_present:\n",
    "        df.to_csv(pathfile, encoding='utf-8',index=False)\n",
    "    else:\n",
    "        overwrite = input(\"WARNING: \" + pathfile + \" already exists! Do you want to overwrite <y/n>? \\n \")\n",
    "        if overwrite == 'y':\n",
    "            df.to_csv(pathfile, encoding='utf-8',index=False)\n",
    "        elif overwrite == 'n':\n",
    "            new_filename = input(\"Type new filename: \\n \")\n",
    "            write_csv_df(savePath,new_filename,df)\n",
    "        else:\n",
    "            print(\"Not a valid input. Data is NOT saved!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T20:42:16.582173Z",
     "start_time": "2019-01-09T20:41:58.617279Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total textual vector records: 135796\n",
      "Vector dimension:  50000\n",
      "Total citation vector records: 124922\n",
      "Vector dimension:  100\n"
     ]
    }
   ],
   "source": [
    "# read pretrained embeddings\n",
    "all_textual_embedding, all_textual_emb_pid = read_textual_embedding(emb_type = \"tf\")\n",
    "all_citation_embedding, all_citation_emb_pid = read_citation_embedding(emb_type = \"n2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T20:42:16.590999Z",
     "start_time": "2019-01-09T20:42:16.584116Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "8077\n",
      "['0.074837', '0.437304', '0.157833', '0.179944', '-0.0696371', '-0.0925071', '-0.37209', '0.16441', '0.257381', '0.482553', '0.420752', '0.294299', '0.48322', '0.310536', '0.451489', '-0.0321524', '-0.266308', '-0.507235', '0.302519', '-0.192578', '-0.196128', '-0.716089', '0.118927', '0.130549', '0.0538411', '-0.36721', '0.320577', '0.107628', '0.437685', '0.261019', '-0.134182', '0.467584', '-0.433934', '-0.337566', '-0.112999', '0.131627', '0.185436', '-0.0716854', '0.222004', '-0.296244', '0.0662622', '0.209887', '-0.177259', '-0.202866', '0.206727', '-0.0535898', '-0.0832955', '0.00406953', '-0.13292', '-0.0853675', '-0.241761', '-0.327425', '-0.46692', '0.0485383', '0.00806723', '0.0284221', '0.115838', '-0.255672', '-0.770949', '0.0873891', '0.00681434', '0.0626846', '-0.0590345', '0.299776', '-0.173271', '-0.00270774', '-0.498401', '-0.222046', '0.321921', '0.0837049', '-0.0501312', '-0.284909', '0.274566', '0.0670506', '0.0773459', '0.24957', '-0.0768505', '0.0357878', '-0.197779', '-0.110859', '-0.0586628', '-0.371421', '-0.331327', '-0.184969', '0.347994', '-0.535585', '0.136484', '0.606065', '-0.34836', '-0.153024', '0.264854', '-0.347494', '0.0979302', '0.352819', '0.116963', '-0.428671', '-0.203673', '0.340799', '-0.153595', '0.333619\\n']\n",
      "8077\n"
     ]
    }
   ],
   "source": [
    "print(all_textual_embedding[0])\n",
    "print(all_textual_emb_pid[0])\n",
    "print(all_citation_embedding[0])\n",
    "print(all_citation_emb_pid[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T04:07:05.250782Z",
     "start_time": "2018-12-21T04:06:34.855274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For name:  j_read\n",
      "total sample size before apply threshold:  136\n",
      "Counter({'0000-0002-5159-1192': 57, '0000-0002-9029-5185': 39, '0000-0002-9697-0962': 31, '0000-0002-4739-9245': 3, '0000-0003-0605-5259': 3, '0000-0003-4316-7006': 1, '0000-0002-0784-0091': 1, '0000-0002-3888-6631': 1})\n",
      "['0000-0002-9697-0962', '0000-0002-9029-5185', '0000-0002-5159-1192']\n",
      "Total sample size after apply threshold:  127\n",
      "(127, 50000)\n",
      "Missing Sample:  20516165\n",
      "Missing Sample:  12803940\n",
      "Missing Sample:  22427696\n",
      "Missing Sample:  17233514\n",
      "Missing Sample:  16946369\n",
      "Missing Sample:  23482297\n",
      "Missing Sample:  25091803\n",
      "Missing Sample:  27990325\n",
      "Missing Sample:  26963178\n",
      "Missing Sample:  24071587\n",
      "(127, 100)\n",
      "2\n",
      "(127, 50100)\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "0000-0002-5159-1192       0.93      0.98      0.96        57\n",
      "0000-0002-9029-5185       0.97      0.97      0.97        39\n",
      "0000-0002-9697-0962       1.00      0.90      0.95        31\n",
      "\n",
      "          micro avg       0.96      0.96      0.96       127\n",
      "          macro avg       0.97      0.95      0.96       127\n",
      "       weighted avg       0.96      0.96      0.96       127\n",
      "\n",
      "[56  1  0  1 38  0  3  0 28]\n",
      "svc Accuracy:  0.9606299212598425\n",
      "svc F1:  0.9602588246656042\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "0000-0002-5159-1192       0.95      1.00      0.97        57\n",
      "0000-0002-9029-5185       0.97      1.00      0.99        39\n",
      "0000-0002-9697-0962       1.00      0.87      0.93        31\n",
      "\n",
      "          micro avg       0.97      0.97      0.97       127\n",
      "          macro avg       0.97      0.96      0.96       127\n",
      "       weighted avg       0.97      0.97      0.97       127\n",
      "\n",
      "[57  0  0  0 39  0  3  1 27]\n",
      "LR Accuracy:  0.968503937007874\n",
      "LR F1:  0.9642450764231646\n",
      "svc: TP:  122 TN:  249 FP:  5 FN:  5\n",
      "lr: TP:  123 TN:  250 FP:  4 FN:  4\n"
     ]
    }
   ],
   "source": [
    "# load the file\n",
    "import io\n",
    "import collections\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "fileDir = \"../../Data/\"+Dataset+\"/canopies_labeled/\"\n",
    "listfiles = os.listdir(fileDir)\n",
    "\n",
    "# entire model f1\n",
    "modelSVCf1, modelLRf1 = ([] for i in range(2))\n",
    "\n",
    "# collect statistic to output\n",
    "allname, num_class, per_class_count = ([] for i in range(3))\n",
    "\n",
    "all_svcLinear_accuracy, all_svcLinear_f1, all_LR_accuracy, all_LR_f1 = ([] for i in range(4))\n",
    "\n",
    "# collect overall tp, tn, fp, fn\n",
    "svcTP=svcTN=svcFP=svcFN = 0\n",
    "lrTP=lrTN=lrFP=lrFN = 0\n",
    "# read all file in labeled group\n",
    "for file in listfiles:\n",
    "    # group name\n",
    "    temp = file.split(\"_\")\n",
    "    name = temp[1]+\"_\"+temp[-1]\n",
    "    print(\"For name: \",name)\n",
    "    # read needed content in labeled file\n",
    "    labeled_data = read_labeled_file(fileDir+file)\n",
    "    print(\"total sample size before apply threshold: \",len(labeled_data))\n",
    "    # count number of paper each author write based on author ID\n",
    "    paperCounter = collections.Counter(labeled_data[\"authorID\"])\n",
    "    print(paperCounter)\n",
    "    # collect per class statistic\n",
    "    for k in list(paperCounter):\n",
    "        if paperCounter[k] < threshold:\n",
    "            del paperCounter[k]\n",
    "    temp =list(paperCounter.keys())\n",
    "    print(temp)\n",
    "    # remove samples that are smaller than threshold\n",
    "    labeled_data = labeled_data[labeled_data.authorID.isin(temp)]\n",
    "    print(\"Total sample size after apply threshold: \",len(labeled_data))\n",
    "    # if only have one class or no class pass the threshold, not applicable\n",
    "    if(len(paperCounter)==0) or (len(paperCounter)==1):\n",
    "        print(name, \" pass\")\n",
    "    else:\n",
    "        allname.append(name)\n",
    "        num_class.append(len(paperCounter))\n",
    "        per_class_count.append(paperCounter)\n",
    "        # shuffle the data\n",
    "        labeled_data = labeled_data.sample(frac=1).reset_index(drop=True)\n",
    "        # extract true label and pid\n",
    "        label = labeled_data[\"authorID\"]\n",
    "        pid = labeled_data[\"paperID\"]\n",
    "        # list of different data field\n",
    "        part_collection = []\n",
    "        # select feature wanted to fit to clustering/classification algorithm\n",
    "        # data part, textual information\n",
    "        data_part_textual = extract_embedding(all_textual_embedding, all_textual_emb_pid, pid)\n",
    "        print(data_part_textual.shape)\n",
    "        part_collection.append(data_part_textual)\n",
    "        # data part, citation information\n",
    "        data_part_citation = extract_embedding(all_citation_embedding, all_citation_emb_pid, pid)\n",
    "        print(data_part_citation.shape)\n",
    "        part_collection.append(data_part_citation)\n",
    "        # merge different part of data data together by concatenate it all together\n",
    "        # remove empty emb (when emb set off)\n",
    "        part_collection = [part for part in part_collection if len(part)!=0]\n",
    "        print(len(part_collection))\n",
    "        if len(part_collection)>1:\n",
    "            combinedata = np.concatenate(part_collection,axis=1)\n",
    "        elif len(part_collection)==1:\n",
    "            if isinstance(part_collection[0], pd.DataFrame):\n",
    "                combinedata = part_collection[0].values\n",
    "            else:\n",
    "                combinedata = part_collection[0]\n",
    "        else:\n",
    "            print(\"No data available\")\n",
    "            break\n",
    "        print(combinedata.shape)\n",
    "        # using converted feature vector to train classifier\n",
    "        # using SVM with linear kernal\n",
    "        clf = SVC(decision_function_shape='ovr', kernel='linear')\n",
    "        svcaccuracy, svcmarcof1, tp, tn, fp, fn = k_fold_cv(combinedata, label, clf, k=10)\n",
    "        svcTP+=tp\n",
    "        svcTN+=tn\n",
    "        svcFP+=fp\n",
    "        svcFN+=fn\n",
    "        print(\"svc Accuracy: \",svcaccuracy)\n",
    "        print(\"svc F1: \", svcmarcof1)\n",
    "        all_svcLinear_accuracy.append(svcaccuracy)\n",
    "        all_svcLinear_f1.append(svcmarcof1)\n",
    "        # using logistic regression\n",
    "        clf = LogisticRegression(multi_class='ovr')\n",
    "        LRaccuracy, LRmarcof1, tp, tn, fp, fn = k_fold_cv(combinedata, label, clf, k=10)\n",
    "        lrTP+=tp\n",
    "        lrTN+=tn\n",
    "        lrFP+=fp\n",
    "        lrFN+=fn\n",
    "        print(\"LR Accuracy: \",LRaccuracy)\n",
    "        print(\"LR F1: \", LRmarcof1)\n",
    "        all_LR_accuracy.append(LRaccuracy)\n",
    "        all_LR_f1.append(LRmarcof1)\n",
    "    break\n",
    "# print f1 for entire model\n",
    "print(\"svc: TP: \",svcTP, \"TN: \",svcTN, \"FP: \",svcFP,\"FN: \",svcFN)\n",
    "print(\"lr: TP: \",lrTP, \"TN: \",lrTN, \"FP: \",lrFP,\"FN: \",lrFN)\n",
    "svcF1 = 2*svcTP / (2*svcTP + svcFP + svcFN)\n",
    "lrF1 = 2*lrTP / (2*lrTP + lrFP + lrFN)\n",
    "modelSVCf1.append(svcF1)\n",
    "modelLRf1.append(lrF1)\n",
    "# # write evaluation result to excel\n",
    "# output = pd.DataFrame({'Name Group':allname,\"Class number\":num_class,\"per_class_size\":per_class_count, \n",
    "#                        \"svc(linear) accuracy\":all_svcLinear_accuracy, \"svc(linear) macro f1\": all_svcLinear_f1, \n",
    "#                        \"logistic regression accuracy\":all_LR_accuracy, \"logistic regression macro f1\": all_LR_f1})\n",
    "\n",
    "# savePath = \"../../result/\"+Dataset+\"/skovr/\"\n",
    "# filename = \"textual=\"+emb+\"_threshold=\"+str(threshold)+\".csv\"\n",
    "# write_csv_df(savePath, filename, output)\n",
    "# print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T05:13:38.648417Z",
     "start_time": "2018-12-14T05:13:38.641968Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tf', 'tf_idf', 'lsa']\n",
      "svc:  [0.9464991405128569, 0.9731343812578639, 0.9706356660582325]\n",
      "lr:  [0.9699976962200287, 0.9314359637774903, 0.9415017101135941]\n"
     ]
    }
   ],
   "source": [
    "print(pp_textual)\n",
    "print(\"svc: \", modelSVCf1)\n",
    "print(\"lr: \", modelLRf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T01:26:32.525744Z",
     "start_time": "2018-12-14T01:26:32.507072Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "578\n",
      "578\n",
      "0.9798807689357634\n",
      "0.9496564529342066\n"
     ]
    }
   ],
   "source": [
    "# accuracy\n",
    "from statistics import mean \n",
    "cleaned_svcLinear_accuracy = [x for x in all_svcLinear_accuracy if isinstance(x, float)]\n",
    "cleaned_lr_accuracy = [x for x in all_LR_accuracy if isinstance(x, float)]\n",
    "print(len(cleaned_svcLinear_accuracy))\n",
    "print(len(cleaned_lr_accuracy))\n",
    "print(mean(cleaned_svcLinear_accuracy))\n",
    "print(mean(cleaned_lr_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T01:26:32.881233Z",
     "start_time": "2018-12-14T01:26:32.856794Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "578\n",
      "578\n",
      "0.9764067884953497\n",
      "0.9288583090243283\n"
     ]
    }
   ],
   "source": [
    "# f1\n",
    "from statistics import mean \n",
    "# remove string from result\n",
    "cleaned_svcLinear_f1 = [x for x in all_svcLinear_f1 if isinstance(x, float)]\n",
    "cleaned_lr_f1 = [x for x in all_LR_f1 if isinstance(x, float)]\n",
    "print(len(cleaned_svcLinear_f1))\n",
    "print(len(cleaned_lr_f1))\n",
    "print(mean(cleaned_svcLinear_f1))\n",
    "print(mean(cleaned_lr_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-11-29T21:26:31.930Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%who"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
