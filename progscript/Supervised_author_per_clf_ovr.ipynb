{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vector records: 3149758\n",
      "['22516865', '0.0422827', '-0.0443959', '-0.00631522', '-0.0297457', '-0.203585', '0.0791923', '-0.156993', '0.102914', '-0.112826', '0.0301516', '-0.118142', '-0.0447324', '0.0131143', '0.127706', '0.107417', '-0.0843709', '-0.0808756', '-0.0504176', '0.132817', '0.0157529', '-0.0383218', '0.165365', '-0.0619297', '-0.0345336', '0.110633', '0.0860297', '-0.0835764', '0.193302', '0.015641', '0.0323645', '0.00893256', '0.151895', '0.0534986', '-0.18639', '-0.0558457', '-0.0603591', '-0.00530632', '-0.000749852', '0.0772172', '0.0646557', '-0.106445', '-0.028214', '0.0400324', '0.11686', '-0.153841', '-0.064435', '-0.0270715', '-0.0234374', '0.0588498', '-0.0750828', '-0.107259', '-0.0351021', '-0.0166648', '-0.135551', '0.1277', '-0.118618', '-0.0417162', '0.0543777', '0.129501', '-0.152003', '0.0624739', '0.175164', '0.0680417', '-0.0896099', '0.171857', '-0.0123408', '-0.0424403', '-0.017759', '0.0631562', '-0.0947245', '-0.0297129', '-0.0952609', '0.0229094', '-0.0183544', '-0.0643149', '-0.0840294', '0.00101607', '-0.00613327', '-0.134319', '-0.0389319', '0.0865807', '-0.0275466', '-0.00559333', '0.028664', '-0.108864', '-0.125772', '-0.171528', '-0.0725501', '-0.0918486', '-0.207318', '-0.0379387', '-0.000273051', '0.0290756', '-0.105057', '-0.121686', '0.127923', '-0.128233', '-0.11645', '-0.365793', '0.0843131\\n']\n"
     ]
    }
   ],
   "source": [
    "# load the vector files\n",
    "import sys\n",
    "import io\n",
    "setting = \"p2v\"\n",
    "\n",
    "vectorFilesDir = \"../Data/vectors/\"+setting+\"/\"+setting+\".txt\"\n",
    "allPaperVectors = []\n",
    "\n",
    "with open(vectorFilesDir, 'r', encoding = 'utf8') as f:\n",
    "    for line in f:\n",
    "        read_data = line.split(\" \")\n",
    "        paper_Vectors = read_data\n",
    "        allPaperVectors.append(paper_Vectors)\n",
    "f.close()\n",
    "        \n",
    "print(\"Total vector records:\",len(allPaperVectors))\n",
    "print(allPaperVectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chung-may yang.txt', 'chung-may yang0.txt', 'chung-may yang1.txt', 'david g lloyd.txt', 'david g lloyd0.txt', 'david g lloyd1.txt', 'jeong hwan kim.txt', 'jeong hwan kim0.txt', 'jeong hwan kim1.txt', 'kevin m. ryan.txt', 'kevin m. ryan0.txt', 'kevin m. ryan1.txt', 'lei wang.txt', 'lei wang0.txt', 'lei wang1.txt', 'michael wagner.txt', 'michael wagner0.txt', 'michael wagner1.txt']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# collect data\n",
    "fileDir = \"../Data/filteredSameNameAuthor/filter=30/\"\n",
    "fileList = os.listdir(fileDir)\n",
    "fileList.sort()\n",
    "print(fileList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove author(positive sample) from other(negative sample)\n",
    "import random\n",
    "def extractNegativeSample(positiveSample, allSample):\n",
    "    negativeSample = [x for x in allSample if x not in positiveSample]\n",
    "    print(\"Total negative sample size:\", len(negativeSample))\n",
    "    return negativeSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# collect class vectors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def extractVectors(author_pids, NegativeSample_pid, allPaperVectors):\n",
    "    # extract class one vectors\n",
    "    author_features = []\n",
    "    for pid in author_pids:\n",
    "         for paper_Vectors in allPaperVectors:\n",
    "            if(paper_Vectors[0] == pid):\n",
    "                author_features.append(paper_Vectors)\n",
    "    print(\"Positive sample size: \", len(author_features))\n",
    "    classOne = pd.DataFrame(author_features)\n",
    "    classOne[\"label\"] = 0\n",
    "    # extract class two vectors\n",
    "    other_features = []\n",
    "    for pid in NegativeSample_pid:\n",
    "        for paper_Vectors in allPaperVectors:\n",
    "            if(paper_Vectors[0] == pid):\n",
    "                other_features.append(paper_Vectors)\n",
    "    print(\"Negative sample size: \", len(other_features))\n",
    "    classTwo = pd.DataFrame(other_features)\n",
    "    classTwo[\"label\"] = 1\n",
    "    return classOne, classTwo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combine data from different class get all data\n",
    "def combineClassesData(classOne,classTwo):\n",
    "    combinedData = pd.concat([classOne, classTwo])\n",
    "    combinedData = combinedData.sample(frac=1).reset_index(drop=True)\n",
    "    # take the paper id out\n",
    "    paperID = combinedData[0]\n",
    "    # split data and label\n",
    "    data = combinedData.drop([0,'label'], axis=1)\n",
    "    label = combinedData['label']\n",
    "    print(\"Total sample size and shape: \",data.shape)\n",
    "    return data, label, paperID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import (precision_score, recall_score,f1_score,accuracy_score)\n",
    "# cross validation\n",
    "def k_fold_cv(author, data, label, classifier, k=10):\n",
    "    kf = KFold(n_splits=k, shuffle=False)\n",
    "    allTrueLabel = []\n",
    "    allPredLabel = []\n",
    "    for counter,(train_index, test_index) in enumerate(kf.split(data)):\n",
    "        # print(\"TRAIN:\", train_index, \" \\n TEST:\", test_index)\n",
    "        # split train and test\n",
    "        data_train, data_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        label_train, test_true_label = label.iloc[train_index], label.iloc[test_index]\n",
    "        # fit data to svm\n",
    "        classifier.fit(data_train, label_train)\n",
    "        # get predicted label\n",
    "        label_pred = classifier.predict(data_test)\n",
    "        allTrueLabel.extend(test_true_label)\n",
    "        allPredLabel.extend(label_pred)\n",
    "#         # get predict proba\n",
    "#         proba = classifier.predict_proba(data_test)\n",
    "        # find out which sample cause the issue\n",
    "        print(\"Pred: \",label_pred)\n",
    "        print(\"True: \", test_true_label.values.tolist())\n",
    "        print(\"Mislabeled sample: \",end='')\n",
    "        for i in range(len(test_true_label)):\n",
    "            if(label_pred[i]!=test_true_label[test_index[i]]):\n",
    "                print(paperID[test_index[i]]+\",\",end='')\n",
    "        print()\n",
    "        # print(\"True positive: {tp}, False positive: {fp}, False negative: {fn}, True negative: {tn}\"\n",
    "        # .format(tp=round_tp, fp=round_fp, fn=round_fn, tn=round_tn))\n",
    "\n",
    "    accuracy = accuracy_score(allTrueLabel, allPredLabel)\n",
    "    f1 = f1_score(allTrueLabel, allPredLabel,average='binary')\n",
    "    precision = precision_score(allTrueLabel, allPredLabel)\n",
    "    recall = recall_score(allTrueLabel, allPredLabel)\n",
    "    tn,fp,fn,tp = metrics.confusion_matrix(allTrueLabel, allPredLabel).ravel()\n",
    "    \n",
    "    print(\"Author: \", author)\n",
    "    print(\"Classifier: \",classifier)\n",
    "    print(metrics.classification_report(allTrueLabel, allPredLabel))\n",
    "    print(metrics.confusion_matrix(allTrueLabel, allPredLabel).ravel())\n",
    "    print(\"Accuracy: \",accuracy)\n",
    "    print(\"F1: \", f1)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    \n",
    "    return accuracy, f1, precision, recall, tn, fp, fn, tp\n",
    "    # return ppv, npv, specificity, sensitivity, accuracy, f1proba = linear_svc.predict_proba(allDatas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chung-may yang0\n",
      "42\n",
      "113\n",
      "Total negative sample size: 71\n",
      "71\n",
      "Positive sample size:  42\n",
      "Negative sample size:  71\n",
      "(42, 102)\n",
      "(71, 102)\n",
      "Total sample size and shape:  (113, 100)\n",
      "Pred:  [1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "True:  [1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0]\n",
      "Mislabeled sample: 8282044,26732884,15389271,16858434,20039856,24296307,\n",
      "Pred:  [1 0 1 1 1 1 1 1 1 1 1 0]\n",
      "True:  [1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0]\n",
      "Mislabeled sample: 27084002,26582311,15621160,1547971,\n",
      "Pred:  [1 1 1 0 1 1 1 0 1 1 1 1]\n",
      "True:  [1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "Mislabeled sample: 20688740,26311257,24107645,\n",
      "Pred:  [1 1 1 1 1 1 1 1 1 1 1]\n",
      "True:  [1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1]\n",
      "Mislabeled sample: 16139013,26047532,16159720,\n",
      "Pred:  [1 1 1 1 1 1 1 1 1 1 1]\n",
      "True:  [0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1]\n",
      "Mislabeled sample: 14767657,19369181,17721499,26891760,\n",
      "Pred:  [1 1 1 1 1 1 1 1 1 1 1]\n",
      "True:  [1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1]\n",
      "Mislabeled sample: 24743637,18347622,19394702,24669373,25572579,\n",
      "Pred:  [1 1 0 1 1 1 1 1 1 1 1]\n",
      "True:  [1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1]\n",
      "Mislabeled sample: 26868376,9798216,26803488,25061523,\n",
      "Pred:  [1 1 1 1 1 1 1 1 1 1 1]\n",
      "True:  [0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1]\n",
      "Mislabeled sample: 17013703,24020891,26200509,10704565,18682973,\n",
      "Pred:  [1 0 1 1 1 1 1 1 1 1 1]\n",
      "True:  [1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0]\n",
      "Mislabeled sample: 16397616,24126678,15846382,20673589,22967867,\n",
      "Pred:  [1 1 1 1 1 1 1 1 1 1 0]\n",
      "True:  [1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
      "Mislabeled sample: 20847676,26404862,\n",
      "Author:  chung-may yang0\n",
      "Classifier:  LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.10      0.16        42\n",
      "          1       0.64      0.96      0.77        71\n",
      "\n",
      "avg / total       0.62      0.64      0.54       113\n",
      "\n",
      "[ 4 38  3 68]\n",
      "Accuracy:  0.6371681415929203\n",
      "F1:  0.7683615819209039\n",
      "Precision:  0.6415094339622641\n",
      "Recall:  0.9577464788732394\n",
      "chung-may yang1\n",
      "71\n",
      "113\n",
      "Total negative sample size: 42\n",
      "42\n",
      "Positive sample size:  71\n",
      "Negative sample size:  42\n",
      "(71, 102)\n",
      "(42, 102)\n",
      "Total sample size and shape:  (113, 100)\n",
      "Pred:  [0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "True:  [0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1]\n",
      "Mislabeled sample: 10704565,18347622,20039856,26732884,\n",
      "Pred:  [0 1 0 0 1 0 0 1 0 0 0 0]\n",
      "True:  [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "Mislabeled sample: 20673589,20688740,16397616,1547971,\n",
      "Pred:  [0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "True:  [0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0]\n",
      "Mislabeled sample: 17721499,20847676,26311257,16159720,\n",
      "Pred:  [0 0 0 0 0 0 1 0 1 0 0]\n",
      "True:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Mislabeled sample: 16226531,24107645,24126678,25572579,\n",
      "Pred:  [0 0 0 0 0 0 0 0 0 0 0]\n",
      "True:  [0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1]\n",
      "Mislabeled sample: 15389271,22967867,18682973,24669373,19394702,27084002,\n",
      "Pred:  [0 0 0 0 0 0 0 0 0 0 0]\n",
      "True:  [0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1]\n",
      "Mislabeled sample: 26891760,25061523,14767657,26200509,\n",
      "Pred:  [0 0 0 0 0 0 0 0 0 0 0]\n",
      "True:  [0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1]\n",
      "Mislabeled sample: 17013703,26582311,26404862,26047532,\n",
      "Pred:  [0 0 0 0 0 0 0 0 0 0 0]\n",
      "True:  [1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1]\n",
      "Mislabeled sample: 15846382,19369181,8282044,24020891,\n",
      "Pred:  [0 0 0 0 0 0 0 0 0 0 0]\n",
      "True:  [0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0]\n",
      "Mislabeled sample: 24296307,26803488,16139013,15621160,\n",
      "Pred:  [0 0 0 0 1 0 0 0 0 0 0]\n",
      "True:  [1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0]\n",
      "Mislabeled sample: 24743637,16858434,9798216,26868376,\n",
      "Author:  chung-may yang1\n",
      "Classifier:  LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.64      0.94      0.76        71\n",
      "          1       0.50      0.10      0.16        42\n",
      "\n",
      "avg / total       0.59      0.63      0.54       113\n",
      "\n",
      "[67  4 38  4]\n",
      "Accuracy:  0.6283185840707964\n",
      "F1:  0.16\n",
      "Precision:  0.5\n",
      "Recall:  0.09523809523809523\n",
      "david g lloyd0\n",
      "50\n",
      "154\n",
      "Total negative sample size: 104\n",
      "104\n",
      "Positive sample size:  50\n",
      "Negative sample size:  104\n",
      "(50, 102)\n",
      "(104, 102)\n",
      "Total sample size and shape:  (154, 100)\n",
      "Pred:  [1 0 1 0 1 0 1 1 1 1 0 0 1 1 0 0]\n",
      "True:  [1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0]\n",
      "Mislabeled sample: \n",
      "Pred:  [1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1]\n",
      "True:  [1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1]\n",
      "Mislabeled sample: \n",
      "Pred:  [1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0]\n",
      "True:  [1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0]\n",
      "Mislabeled sample: 24451633,\n",
      "Pred:  [1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0]\n",
      "True:  [1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0]\n",
      "Mislabeled sample: \n",
      "Pred:  [1 1 1 1 1 1 1 1 1 0 1 1 0 1 1]\n",
      "True:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1]\n",
      "Mislabeled sample: \n",
      "Pred:  [1 1 1 1 1 1 0 0 0 1 0 1 1 1 1]\n",
      "True:  [1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1]\n",
      "Mislabeled sample: \n",
      "Pred:  [0 1 0 0 1 1 0 1 1 1 1 0 1 1 1]\n",
      "True:  [0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1]\n",
      "Mislabeled sample: \n",
      "Pred:  [1 0 1 1 1 0 0 0 0 1 1 0 1 0 0]\n",
      "True:  [1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0]\n",
      "Mislabeled sample: \n",
      "Pred:  [1 1 1 1 1 1 0 0 1 1 0 0 0 0 1]\n",
      "True:  [1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1]\n",
      "Mislabeled sample: 19074587,\n",
      "Pred:  [1 1 1 1 1 1 1 0 0 1 1 0 0 1 1]\n",
      "True:  [1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1]\n",
      "Mislabeled sample: \n",
      "Author:  david g lloyd0\n",
      "Classifier:  LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.96      0.98        50\n",
      "          1       0.98      1.00      0.99       104\n",
      "\n",
      "avg / total       0.99      0.99      0.99       154\n",
      "\n",
      "[ 48   2   0 104]\n",
      "Accuracy:  0.987012987012987\n",
      "F1:  0.9904761904761905\n",
      "Precision:  0.9811320754716981\n",
      "Recall:  1.0\n",
      "david g lloyd1\n",
      "104\n",
      "154\n",
      "Total negative sample size: 50\n",
      "50\n",
      "Positive sample size:  104\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-82f2f20a85ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNegativeSample_pid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0;31m# collect all vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                     \u001b[0mclassOne\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassTwo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextractVectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauthor_pids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNegativeSample_pid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mallPaperVectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassOne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassTwo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-065baf8606c9>\u001b[0m in \u001b[0;36mextractVectors\u001b[0;34m(author_pids, NegativeSample_pid, allPaperVectors)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mother_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mNegativeSample_pid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpaper_Vectors\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallPaperVectors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaper_Vectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mother_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaper_Vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# loop through files in directory |\n",
    "# add name to name list\n",
    "# author as positive sample, other as all samples\n",
    "name_list = []\n",
    "# create name list for all authors have same name\n",
    "for file in fileList:\n",
    "    if not file.startswith('.'):\n",
    "        if not re.match(r'\\D*\\d+.txt$', file):\n",
    "            # fix the coding issue\n",
    "            name_list.append(file.encode(\"utf-8\", \"surrogateescape\").decode('utf8','surrogateescape')[:-4])\n",
    "# print(name_list)\n",
    "\n",
    "# loop through all the author and gather result\n",
    "allauthor = []\n",
    "authorSampleSize = []\n",
    "allSampleSize = []\n",
    "allaccuracy = []\n",
    "allf1 = []\n",
    "allprecision = []\n",
    "allrecall = []\n",
    "alltn = []\n",
    "allfp = []\n",
    "allfn = []\n",
    "alltp = []\n",
    "\n",
    "for name in name_list:\n",
    "    other_pids = []\n",
    "    # read other sample\n",
    "    with open((fileDir+name+\".txt\").encode('utf-8'), 'r', encoding = 'utf8') as f:\n",
    "        for line in f:\n",
    "            other_pids.extend(line.strip().split(\" \"))\n",
    "#     print(name)\n",
    "    for file in fileList:\n",
    "        file=file.encode(\"utf-8\", \"surrogateescape\").decode('utf8','surrogateescape')\n",
    "        if not file.startswith('.'):\n",
    "            if re.match(r'\\D*\\d+.txt$', file):\n",
    "                if name in file:\n",
    "                    print(os.path.splitext(file)[0])\n",
    "                    # add author to list for final output\n",
    "                    allauthor.append(os.path.splitext(file)[0])\n",
    "                    author_pids = []\n",
    "                    # read author sample\n",
    "                    with open((fileDir+os.path.splitext(file)[0]+\".txt\").encode('utf-8'), 'r', encoding = 'utf8') as f:\n",
    "                        for line in f:\n",
    "                            author_pids.extend(line.strip().split(\" \"))\n",
    "                    # print properties\n",
    "                    authorSampleSize.append(len(author_pids))\n",
    "                    allSampleSize.append(len(other_pids))\n",
    "                    print(len(author_pids))\n",
    "                    print(len(other_pids))\n",
    "                    # remove author(positive sample) from other(all sample) to create negative sample\n",
    "                    NegativeSample_pid = extractNegativeSample(author_pids, other_pids)\n",
    "                    print(len(NegativeSample_pid))\n",
    "                    # collect all vector\n",
    "                    classOne, classTwo = extractVectors(author_pids,NegativeSample_pid,allPaperVectors)\n",
    "                    print(classOne.shape)\n",
    "                    print(classTwo.shape)\n",
    "                    # combine data from different class get all data\n",
    "                    data, label, paperID = combineClassesData(classOne, classTwo)\n",
    "#                     # PCA transform\n",
    "#                     pca = PCA(n_components=2)\n",
    "#                     pca_transformed = pd.DataFrame(pca.fit_transform(X=data, y=label))\n",
    "#                     pca_transformed[\"label\"] = label\n",
    "#                     # 10 fold cv SVM linear kernel\n",
    "#                     linear_svc = svm.SVC(kernel='linear', class_weight='balanced', probability=True,cache_size=4000)\n",
    "#                     accuracy, f1, precision, recall, tn, fp, fn, tp= k_fold_cv(os.path.splitext(file)[0],data, label, linear_svc,10)\n",
    "#                     # 10 fold cv SVM rbf kernel\n",
    "#                     rbf_svc = svm.SVC(kernel='rbf')\n",
    "#                     accuracy, f1, precision, recall, tn, fp, fn, tp = k_fold_cv(os.path.splitext(file)[0],data, label, rbf_svc,10)\n",
    "                    # 10 fold cv logistic regression\n",
    "                    logistic = linear_model.LogisticRegression()\n",
    "                    accuracy, f1, precision, recall, tn, fp, fn, tp = k_fold_cv(os.path.splitext(file)[0],data, label, logistic,10)\n",
    "                    allaccuracy.append(accuracy)\n",
    "                    allf1.append(f1)\n",
    "                    allprecision.append(precision)\n",
    "                    allrecall.append(recall)\n",
    "                    alltn.append(tn)\n",
    "                    alltp.append(tp)\n",
    "                    allfn.append(fn)\n",
    "                    allfp.append(fp)\n",
    "# write evaluation result to excel\n",
    "output = pd.DataFrame({'author':allauthor,\"AuthorSampleSize\":authorSampleSize,\n",
    "                       \"accuracy\":allaccuracy,\"f1\":allf1, \"precision\":allprecision,\n",
    "                      \"recall\":allrecall, \"AllSameNameSampleCount\":allSampleSize,\n",
    "                      \"True positive\": alltp, \"True negative\":alltn,\n",
    "                      \"False positive\": allfp, \"False negative\": allfn})\n",
    "filename = \"logistic_regression_\"+setting+\"_filter=10.csv\"\n",
    "output.to_csv(\"../result/\"+filename, encoding='utf-8',index=False)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n",
      "117\n"
     ]
    }
   ],
   "source": [
    "# hard code to read the file one by one\n",
    "# store the features for classification\n",
    "author_pids = []\n",
    "other_pids = []\n",
    "name = \"lei wang0\"\n",
    "# author as positive sample, other as all samples\n",
    "with open(fileDir+name+\".txt\", 'r', encoding = 'utf8') as f:\n",
    "    for line in f:\n",
    "        author_pids.extend(line.strip().split(\" \"))\n",
    "\n",
    "with open(fileDir+\"lei wang.txt\", 'r', encoding = 'utf8') as f:\n",
    "    for line in f:\n",
    "        other_pids.extend(line.strip().split(\" \"))\n",
    "        \n",
    "# size of each class\n",
    "print(len(author_pids))\n",
    "print(len(other_pids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total negative sample size: 64\n",
      "Choicen negative sample  64\n"
     ]
    }
   ],
   "source": [
    "# extract negative Sample\n",
    "NegativeSample_pid = extractNegativeSample(author_pids, other_pids)\n",
    "print(\"Choicen negative sample \", len(NegativeSample_pid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive sample size:  53\n",
      "Negative sample size:  64\n",
      "(53, 102)\n",
      "(64, 102)\n"
     ]
    }
   ],
   "source": [
    "classOne, classTwo = extractVectors(author_pids,NegativeSample_pid,allPaperVectors)\n",
    "print(classOne.shape)\n",
    "print(classTwo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sample size and shape:  (117, 100)\n"
     ]
    }
   ],
   "source": [
    "data, label, paperID = combineClassesData(classOne, classTwo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred:  [1 1 1 1 0 0 1 1 1 0 1 1]\n",
      "True:  [1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1]\n",
      "Mislabeled sample: \n",
      "Pred:  [1 0 0 0 0 1 1 1 0 1 0 0]\n",
      "True:  [1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0]\n",
      "Mislabeled sample: \n",
      "Pred:  [1 0 1 1 1 1 0 0 1 1 1 0]\n",
      "True:  [1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0]\n",
      "Mislabeled sample: \n",
      "Pred:  [1 1 1 1 0 1 0 1 1 0 1 1]\n",
      "True:  [1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1]\n",
      "Mislabeled sample: \n",
      "Pred:  [1 0 1 1 0 0 0 0 1 0 0 1]\n",
      "True:  [1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1]\n",
      "Mislabeled sample: \n",
      "Pred:  [0 1 1 0 1 0 0 1 0 1 0 1]\n",
      "True:  [0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1]\n",
      "Mislabeled sample: 22751827,\n",
      "Pred:  [1 1 1 0 1 0 0 0 0 0 1 0]\n",
      "True:  [1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0]\n",
      "Mislabeled sample: \n",
      "Pred:  [1 0 1 0 1 0 1 0 0 1 1]\n",
      "True:  [1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1]\n",
      "Mislabeled sample: \n",
      "Pred:  [1 1 0 1 1 1 0 1 0 0 1]\n",
      "True:  [1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1]\n",
      "Mislabeled sample: \n",
      "Pred:  [1 0 0 1 0 0 0 0 0 0 1]\n",
      "True:  [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1]\n",
      "Mislabeled sample: \n",
      "Author:  david g lloyd1\n",
      "Classifier:  SVC(C=1.0, cache_size=4000, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99        53\n",
      "          1       1.00      0.98      0.99        64\n",
      "\n",
      "avg / total       0.99      0.99      0.99       117\n",
      "\n",
      "[53  0  1 63]\n",
      "Accuracy:  0.9914529914529915\n",
      "F1:  0.9921259842519685\n",
      "Precision:  1.0\n",
      "Recall:  0.984375\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "# create linear SVM model\n",
    "linear_svc = svm.SVC(kernel='linear', class_weight='balanced', probability=True,cache_size=4000)\n",
    "\n",
    "accuracy, f1, precision, recall, tn, fp, fn, tp= k_fold_cv(os.path.splitext(file)[0],data, label, linear_svc,10)\n",
    "#print(linear_svc)\n",
    "# # fit model and do 10-fold cv\n",
    "# k_fold_cv(name,data, label, linear_svc,10)\n",
    "\n",
    "# # get number of support vectors for each class\n",
    "# print(linear_svc.n_support_)\n",
    "\n",
    "# '''\n",
    "# # compute the distance to decision boundry (Not same as confidence measure)\n",
    "# Distance = linear_svc.decision_function(allDatas)\n",
    "\n",
    "# # computer the confidence measure (Platt scaling: transforming the outputs of a \n",
    "# # classification model into a probability distribution over classes)\n",
    "# # P(class/input) = 1 / (1 + exp(A * f(input) + B))\n",
    "# # P(class/input) is the probability that “input” belongs to “class” \n",
    "# # and f(input) is the signed distance of the input datapoint from the boundary,\n",
    "# # which is basically the output of “decision_function”. \n",
    "\n",
    "# proba = linear_svc.predict_proba(allDatas)\n",
    "\n",
    "# '''\n",
    "\n",
    "# # create rbf SVM model with C=10 where (C*Error) is added into minimize function\n",
    "# # C big means error matter more\n",
    "# rbf_svc = svm.SVC(kernel='rbf',probability=True)\n",
    "\n",
    "# # fit model and do 10-fold cv\n",
    "# k_fold_cv(name,data, label, rbf_svc)\n",
    "\n",
    "# # get number of support vectors for each class\n",
    "# print(rbf_svc.n_support_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
