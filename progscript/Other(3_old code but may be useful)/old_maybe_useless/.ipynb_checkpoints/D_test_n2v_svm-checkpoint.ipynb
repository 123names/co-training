{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vector records: 8602916\n",
      "['22516865', '0.0109272', '0.126011', '0.186979', '0.0496719', '0.0373553', '0.0458918', '-0.119893', '0.217118', '0.0524591', '0.237477', '0.191269', '-0.0277055', '0.0290957', '-0.0366833', '0.118964', '0.0654807', '-0.0335345', '-0.0900123', '0.128621', '0.0561669', '-0.087823', '-0.0882296', '0.0740289', '0.082104', '0.0269581', '-0.0346502', '0.0153376', '0.104666', '0.0908716', '-0.085694', '-0.111344', '0.0787209', '-0.17003', '-0.103366', '-0.0832094', '-0.210496', '0.153037', '-0.0342884', '0.0698413', '-0.0719641', '-0.0535707', '0.172399', '0.106226', '-0.0593672', '-0.0348048', '-0.0863189', '-0.0801566', '-0.0665761', '0.0673258', '0.0306541', '-0.0896316', '-0.00800971', '-0.174798', '-0.0252528', '0.0098563', '0.0230368', '0.0282268', '-0.0366493', '-0.131323', '0.0318188', '-0.00778704', '-0.0608064', '-0.0860078', '0.215632', '0.0209927', '-0.0953191', '-0.191736', '-0.0741615', '0.151972', '-0.0522046', '-0.11081', '0.134878', '0.090797', '0.0160238', '0.113017', '0.196071', '-0.0598695', '-0.181981', '-0.0217668', '0.165394', '0.0724198', '0.0967185', '-0.115696', '-0.104803', '0.231757', '-0.0871117', '-0.0397107', '0.137358', '-0.083055', '-0.226604', '0.0186784', '-0.0608683', '-0.0177875', '0.082095', '-0.00222851', '-0.137642', '-0.0848053', '0.165064', '-0.022831', '0.240943\\n']\n"
     ]
    }
   ],
   "source": [
    "# extract different view of data\n",
    "# view two, node2vec\n",
    "setting = \"n2v\"\n",
    "Dataset = \"pubmed\"\n",
    "viewTwoFilesDir = \"../Data/\"+Dataset+\"/vectors/\"+setting+\"/data=Meta-alg=N2V-l2=1.0-n2v_p=0.85-iteration=100-no_self_predict=1-idx=0.emb\"\n",
    "viewTwoVectors = []\n",
    "\n",
    "with open(viewTwoFilesDir, 'r', encoding = 'utf8') as f:\n",
    "    for line in f:\n",
    "        read_data = line.split(\" \")\n",
    "        if(len(read_data[0])<=8):\n",
    "            paper_Vectors = read_data\n",
    "            viewTwoVectors.append(paper_Vectors)\n",
    "f.close()\n",
    "viewTwoVectors = viewTwoVectors[1:]\n",
    "print(\"Total vector records:\",len(viewTwoVectors))\n",
    "print(viewTwoVectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alfredo martinez.txt', 'alfredo martinez0.txt', 'alfredo martinez1.txt', 'amit patel.txt', 'amit patel0.txt', 'amit patel1.txt', 'ana castro.txt', 'ana castro0.txt', 'ana castro1.txt', 'ana castro2.txt', 'anna ferrari.txt', 'anna ferrari0.txt', 'anna ferrari1.txt', 'bin liu.txt', 'bin liu0.txt', 'bin liu1.txt', 'carmen moreno.txt', 'carmen moreno0.txt', 'carmen moreno1.txt', 'carmen torres.txt', 'carmen torres0.txt', 'carmen torres1.txt', 'chao liu.txt', 'chao liu0.txt', 'chao liu1.txt', 'cheng luo.txt', 'cheng luo0.txt', 'cheng luo1.txt', 'chung-may yang.txt', 'chung-may yang0.txt', 'chung-may yang1.txt', 'david g lloyd.txt', 'david g lloyd0.txt', 'david g lloyd1.txt', 'fang liu.txt', 'fang liu0.txt', 'fang liu1.txt', 'feng liu.txt', 'feng liu0.txt', 'feng liu1.txt', 'feng xu.txt', 'feng xu0.txt', 'feng xu1.txt', 'francisco esteves.txt', 'francisco esteves0.txt', 'francisco esteves1.txt', 'francisco j blanco.txt', 'francisco j blanco0.txt', 'francisco j blanco1.txt', 'giovanni volpe.txt', 'giovanni volpe0.txt', 'giovanni volpe1.txt', 'hao song.txt', 'hao song0.txt', 'hao song1.txt', 'hong yang.txt', 'hong yang0.txt', 'hong yang1.txt', 'jacob john.txt', 'jacob john0.txt', 'jacob john1.txt', 'jeong hwan kim.txt', 'jeong hwan kim0.txt', 'jeong hwan kim1.txt', 'jeremy m brown.txt', 'jeremy m brown0.txt', 'jeremy m brown1.txt', 'jie zhang.txt', 'jie zhang0.txt', 'jie zhang1.txt', 'jin young kim.txt', 'jin young kim0.txt', 'jin young kim1.txt', 'john f marshall.txt', 'john f marshall0.txt', 'john f marshall1.txt', 'jong hee chang.txt', 'jong hee chang0.txt', 'jong hee chang1.txt', 'jun chen.txt', 'jun chen0.txt', 'jun chen1.txt', 'jun chen2.txt', 'jun zhang.txt', 'jun zhang0.txt', 'jun zhang1.txt', 'kevin m. ryan.txt', 'kevin m. ryan0.txt', 'kevin m. ryan1.txt', 'kyung su kim.txt', 'kyung su kim0.txt', 'kyung su kim1.txt', 'lei wang.txt', 'lei wang0.txt', 'lei wang1.txt', 'lei wang2.txt', 'lei wang3.txt', 'lin yang.txt', 'lin yang0.txt', 'lin yang1.txt', 'lu\\udcc3\\udcads alves.txt', 'lu\\udcc3\\udcads alves0.txt', 'lu\\udcc3\\udcads alves1.txt', 'marco ferrari.txt', 'marco ferrari0.txt', 'marco ferrari1.txt', 'marta crespo.txt', 'marta crespo0.txt', 'marta crespo1.txt', 'martin wagner.txt', 'martin wagner0.txt', 'martin wagner1.txt', 'michael wagner.txt', 'michael wagner0.txt', 'michael wagner1.txt', 'michael wagner2.txt', 'mikael svensson.txt', 'mikael svensson0.txt', 'mikael svensson1.txt', 'pei-ming yang.txt', 'pei-ming yang0.txt', 'pei-ming yang1.txt', 'peng zhang.txt', 'peng zhang0.txt', 'peng zhang1.txt', 'peng zhang2.txt', 'peng zhang3.txt', 'qian wang.txt', 'qian wang0.txt', 'qian wang1.txt', 'qiang wang.txt', 'qiang wang0.txt', 'qiang wang1.txt', 'qin li.txt', 'qin li0.txt', 'qin li1.txt', 'richard w morris.txt', 'richard w morris0.txt', 'richard w morris1.txt', 'robert j young.txt', 'robert j young0.txt', 'robert j young1.txt', 'sebastian wolf.txt', 'sebastian wolf0.txt', 'sebastian wolf1.txt', 'vineet gupta.txt', 'vineet gupta0.txt', 'vineet gupta1.txt', 'vivek gupta.txt', 'vivek gupta0.txt', 'vivek gupta1.txt', 'vivek kumar.txt', 'vivek kumar0.txt', 'vivek kumar1.txt', 'wei lu.txt', 'wei lu0.txt', 'wei lu1.txt', 'wei wang.txt', 'wei wang0.txt', 'wei wang1.txt', 'wei wang2.txt', 'wei wang3.txt', 'wei wang4.txt', 'wei xu.txt', 'wei xu0.txt', 'wei xu1.txt', 'xin li.txt', 'xin li0.txt', 'xin li1.txt', 'yang wang.txt', 'yang wang0.txt', 'yang wang1.txt', 'yang zhao.txt', 'yang zhao0.txt', 'yang zhao1.txt', 'ying liu.txt', 'ying liu0.txt', 'ying liu1.txt', 'ying liu2.txt', 'ying zhang.txt', 'ying zhang0.txt', 'ying zhang1.txt', 'yong liu.txt', 'yong liu0.txt', 'yong liu1.txt', 'yong wang.txt', 'yong wang0.txt', 'yong wang1.txt', 'yongsheng liu.txt', 'yongsheng liu0.txt', 'yongsheng liu1.txt', 'yu zhang.txt', 'yu zhang0.txt', 'yu zhang1.txt', 'yu-jun zhao.txt', 'yu-jun zhao0.txt', 'yu-jun zhao1.txt']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "# collect pid\n",
    "fileDir = \"../Data/\"+Dataset+\"/filteredSameNameAuthor/filter=10/\"\n",
    "fileList = os.listdir(fileDir)\n",
    "fileList.sort()\n",
    "print(fileList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove author(positive sample) from other(negative sample)\n",
    "import random\n",
    "def extractNegativeSample(positiveSample, allSample):\n",
    "    negativeSample = [x for x in allSample if x not in positiveSample]\n",
    "    print(\"Total negative sample size:\", len(negativeSample))\n",
    "    return negativeSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# collect class vectors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def extractVectors(authors_pids, allPaperVectors):\n",
    "    appended_data = []\n",
    "    for label, author in enumerate(authors_pids):\n",
    "        data = []\n",
    "        labels = []\n",
    "        for paper_Vectors in allPaperVectors:\n",
    "            if paper_Vectors[0] in author:\n",
    "                data.append(paper_Vectors)\n",
    "                labels.append(label)\n",
    "        print(\"Class \",label,\" sample size: \", len(data))\n",
    "        # create df save one author data \n",
    "        authordf = pd.DataFrame(data)\n",
    "        authordf['label'] = labels\n",
    "        appended_data.append(authordf)\n",
    "    # add all together\n",
    "    dataset = pd.concat(appended_data, axis=0,ignore_index=True)\n",
    "    # shuffle it\n",
    "    dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "    # print shape for confirmation\n",
    "    print(dataset.shape)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import (precision_score, recall_score,f1_score,accuracy_score)\n",
    "# cross validation\n",
    "def k_fold_cv(data, label, paperID, classifier, k=10):\n",
    "    kf = KFold(n_splits=k, shuffle=False)\n",
    "    allTrueLabel = []\n",
    "    allPredLabel = []\n",
    "    for counter,(train_index, test_index) in enumerate(kf.split(data)):\n",
    "        # print(\"TRAIN:\", train_index, \" \\n TEST:\", test_index)\n",
    "        # split train and test\n",
    "        data_train, data_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        label_train, test_true_label = label.iloc[train_index], label.iloc[test_index]\n",
    "        # fit data to svm\n",
    "        classifier.fit(data_train, label_train)\n",
    "        # get predicted label\n",
    "        label_pred = classifier.predict(data_test)\n",
    "        allTrueLabel.extend(test_true_label)\n",
    "        allPredLabel.extend(label_pred)\n",
    "#         # get predict proba\n",
    "#         proba = classifier.predict_proba(data_test)\n",
    "        # find out which sample cause the issue\n",
    "        print(\"Pred: \",label_pred)\n",
    "        print(\"True: \", test_true_label.values.tolist())\n",
    "        print(\"Mislabeled sample: \",end='')\n",
    "        for i in range(len(test_true_label)):\n",
    "            if(label_pred[i]!=test_true_label[test_index[i]]):\n",
    "                print(paperID[test_index[i]]+\",\",end='')\n",
    "        print()\n",
    "        # print(\"True positive: {tp}, False positive: {fp}, False negative: {fn}, True negative: {tn}\"\n",
    "        # .format(tp=round_tp, fp=round_fp, fn=round_fn, tn=round_tn))\n",
    "\n",
    "    accuracy = accuracy_score(allTrueLabel, allPredLabel)\n",
    "    f1 = f1_score(allTrueLabel, allPredLabel,average='binary')\n",
    "    precision = precision_score(allTrueLabel, allPredLabel)\n",
    "    recall = recall_score(allTrueLabel, allPredLabel)\n",
    "    tn,fp,fn,tp = metrics.confusion_matrix(allTrueLabel, allPredLabel).ravel()\n",
    "    \n",
    "    print(\"Classifier: \",classifier)\n",
    "    print(metrics.classification_report(allTrueLabel, allPredLabel))\n",
    "    print(metrics.confusion_matrix(allTrueLabel, allPredLabel).ravel())\n",
    "    print(\"Accuracy: \",accuracy)\n",
    "    print(\"F1: \", f1)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    \n",
    "    return accuracy, f1, precision, recall, tn, fp, fn, tp\n",
    "    # return ppv, npv, specificity, sensitivity, accuracy, f1proba = linear_svc.predict_proba(allDatas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['24856736', '24553547', '23704754', '24981010', '24495814', '23632268', '22664745', '23648213', '23932734', '23603152', '23648567', '22867838', '22975022', '22867824', '23380094', '22683079', '23187987', '22487663', '22748697', '22980485', '21824465', '22227383', '22391544', '22463971', '21803673', '22533576', '22424993', '22657731', '22932993', '22322281', '20542398', '22372785', '21633052', '21908139', '21129911', '21610354', '20511643', '18996670', '21236547', '18722743', '19913985', '20599213', '20154545', '20837252', '19336669', '19542400', '19111959', '18638308', '19119441', '18022782', '18756061', '18821148', '18370984', '18657674', '17590258']\n",
      "23528677\n"
     ]
    }
   ],
   "source": [
    "# hard code to read the file one by one\n",
    "# store the features for classification\n",
    "author_pids = []\n",
    "other_pids = []\n",
    "# author as positive sample, other as all samples\n",
    "with open(fileDir+\"kyung su kim1.txt\", 'r', encoding = 'utf8') as f:\n",
    "    for line in f:\n",
    "        author_pids.extend(line.strip().split(\" \"))\n",
    "\n",
    "with open(fileDir+\"kyung su kim.txt\", 'r', encoding = 'utf8') as f:\n",
    "    for line in f:\n",
    "        other_pids.extend(line.strip().split(\" \"))\n",
    "        \n",
    "print(author_pids)\n",
    "print(other_pids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total negative sample size: 14\n",
      "Choicen negative sample  14\n"
     ]
    }
   ],
   "source": [
    "# extract negative Sample\n",
    "NegativeSample_pid = extractNegativeSample(author_pids, other_pids)\n",
    "print(\"Choicen negative sample \", len(NegativeSample_pid))\n",
    "all_author = []\n",
    "all_author.append(author_pids)\n",
    "all_author.append(NegativeSample_pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class  0  sample size:  55\n",
      "Class  1  sample size:  14\n",
      "(69, 102)\n",
      "(69, 102)\n"
     ]
    }
   ],
   "source": [
    "data = extractVectors(all_author,viewTwoVectors)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69, 100)\n"
     ]
    }
   ],
   "source": [
    "label = data['label']\n",
    "pid = data[0]\n",
    "data = data.drop([0,'label'], axis=1)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred:  [1 0 0 1 0 0 0]\n",
      "True:  [0, 0, 0, 0, 0, 0, 0]\n",
      "Mislabeled sample: 24553547,21236547,\n",
      "Pred:  [1 0 1 0 0 0 0]\n",
      "True:  [1, 1, 0, 0, 0, 0, 0]\n",
      "Mislabeled sample: 20825813,23648567,\n",
      "Pred:  [1 0 0 0 1 0 0]\n",
      "True:  [0, 0, 1, 0, 0, 1, 0]\n",
      "Mislabeled sample: 18821148,19097734,23380094,18660392,\n",
      "Pred:  [0 0 1 0 0 0 0]\n",
      "True:  [0, 0, 0, 0, 0, 0, 0]\n",
      "Mislabeled sample: 22463971,\n",
      "Pred:  [0 0 0 1 1 0 0]\n",
      "True:  [1, 0, 1, 0, 0, 0, 0]\n",
      "Mislabeled sample: 22101201,20825912,23932734,22980485,\n",
      "Pred:  [0 0 0 1 1 1 1]\n",
      "True:  [0, 0, 0, 0, 0, 0, 1]\n",
      "Mislabeled sample: 18638308,22683079,18756061,\n",
      "Pred:  [0 0 0 0 1 0 0]\n",
      "True:  [0, 0, 0, 0, 1, 1, 0]\n",
      "Mislabeled sample: 21982990,\n",
      "Pred:  [1 0 1 1 0 1 0]\n",
      "True:  [0, 0, 1, 0, 1, 0, 0]\n",
      "Mislabeled sample: 20511643,18657674,21722666,19913985,\n",
      "Pred:  [0 0 0 0 1 0 1]\n",
      "True:  [0, 0, 0, 0, 1, 1, 0]\n",
      "Mislabeled sample: 19996791,21908139,\n",
      "Pred:  [1 1 0 1 1 0]\n",
      "True:  [0, 0, 0, 0, 1, 0]\n",
      "Mislabeled sample: 22664745,21803673,21610354,\n",
      "Classifier:  SVC(C=1.0, cache_size=4000, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.67      0.74        55\n",
      "          1       0.25      0.43      0.32        14\n",
      "\n",
      "avg / total       0.71      0.62      0.65        69\n",
      "\n",
      "[37 18  8  6]\n",
      "Accuracy:  0.6231884057971014\n",
      "F1:  0.3157894736842105\n",
      "Precision:  0.25\n",
      "Recall:  0.42857142857142855\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "linear_svc = svm.SVC(kernel='linear', class_weight='balanced', probability=True,cache_size=4000)\n",
    "accuracy, f1, precision, recall, tn, fp, fn, tp= k_fold_cv(data, label, pid, linear_svc,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
