{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import io\n",
    "\n",
    "Dataset = \"pubmed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vector records: 3149075\n",
      "['3', '-0.07245799', '-0.15048164', '-0.04320673', '0.01244448', '0.05051953', '-0.05573996', '0.03158288', '-0.04663554', '-0.00442508', '-0.02417533', '-0.03292065', '0.03798062', '0.08195730', '-0.09100581', '-0.04666801', '-0.06315092', '-0.05957321', '0.09766518', '0.01981102', '0.09956500', '-0.02059892', '-0.02321497', '0.10300557', '0.09654117', '0.02085607', '0.15179265', '0.03320639', '0.04716884', '0.04259005', '-0.01022485', '0.07371941', '0.02970656', '0.18967280', '0.07049462', '-0.07849123', '0.10272161', '0.05396378', '0.04138396', '0.08093689', '-0.04713648', '-0.08277001', '0.06004119', '0.15147503', '-0.10719796', '-0.06268646', '0.15823838', '0.10273122', '0.04453533', '-0.00394740', '-0.01239040', '-0.06826647', '-0.02995823', '0.14925463', '0.12254845', '-0.05894163', '0.11628735', '0.03898517', '0.01221054', '-0.00804257', '-0.06178775', '-0.04752085', '-0.04040224', '0.09192738', '0.01171173', '0.02951661', '-0.02156392', '-0.02458819', '-0.00003645', '-0.06527787', '0.07321506', '0.00926040', '0.04152755', '-0.06273570', '0.00205773', '-0.14158797', '0.01341034', '0.05070017', '-0.06785034', '0.01392612', '0.01312939', '-0.03518058', '-0.04593558', '-0.04542769', '-0.03334041', '0.02727035', '0.03331508', '-0.05495675', '-0.02231646', '-0.01770608', '0.02452897', '0.03648302', '0.02217655', '0.01033537', '0.00610828', '-0.03949452', '0.01911573', '-0.08300079', '-0.04561001', '0.01872506', '0.01281491\\n']\n"
     ]
    }
   ],
   "source": [
    "# extract different view of data\n",
    "# view one, doc2vec\n",
    "setting = \"d2v\"\n",
    "\n",
    "viewOneFilesDir = \"../Data/\"+Dataset+\"/vectors/\"+setting+\"/\"+setting+\".txt\"\n",
    "viewOneVectors = []\n",
    "\n",
    "with open(viewOneFilesDir, 'r', encoding = 'utf8') as f:\n",
    "    for line in f:\n",
    "        read_data = line.split(\" \")\n",
    "        paper_Vectors = read_data\n",
    "        viewOneVectors.append(paper_Vectors)\n",
    "f.close()\n",
    "        \n",
    "print(\"Total vector records:\",len(viewOneVectors))\n",
    "print(viewOneVectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vector records: 8602530\n",
      "[['1', '0.0621464', '0.166361', '0.119258', '0.0465671', '0.0261309', '0.0117651', '-0.058403', '0.161649', '0.110099', '0.263108', '0.238387', '-0.0349828', '-0.0159107', '0.0714912', '0.0966965', '-0.0914435', '0.0194952', '0.086068', '0.248403', '-0.0541348', '-0.165036', '-0.0920226', '0.0422623', '0.138479', '-0.0388176', '-0.0980374', '0.0216705', '0.1053', '0.064752', '0.0874291', '-0.185647', '0.119314', '-0.243423', '0.0194402', '0.0558745', '-0.10073', '0.0752902', '-0.144783', '0.00605281', '-0.0114243', '-0.0200634', '0.13443', '0.0468515', '0.105734', '-0.0472512', '-0.0296095', '0.0110446', '-0.00120206', '0.0742101', '0.0481637', '-0.111427', '-0.109345', '-0.155321', '0.0769182', '-0.0170874', '0.020175', '0.0237308', '-0.056371', '-0.176681', '-0.104414', '-0.0997754', '0.00183702', '-0.0529168', '0.217175', '0.0986245', '0.00547141', '-0.206098', '-0.0817846', '0.0989896', '0.0336956', '-0.0597821', '0.0510647', '0.105229', '0.107412', '0.0146344', '0.171599', '0.0199437', '-0.063981', '-0.147505', '0.0525083', '-0.0141398', '0.0592276', '-0.0844229', '-0.158121', '0.265861', '-0.0424053', '-0.0500844', '0.199131', '-0.0902892', '-0.14255', '0.0910441', '0.0369295', '0.0330068', '0.133361', '0.109993', '-0.155247', '-0.113675', '0.152855', '0.106193', '0.286144\\n'], ['2', '-0.0573285', '0.119805', '0.100889', '0.0394713', '-0.00373842', '0.0379493', '-0.0861926', '0.135904', '0.0710074', '0.103731', '0.101347', '0.073494', '0.0613023', '0.0512827', '0.139111', '0.028294', '0.00339193', '-0.036657', '0.0449969', '-0.0302419', '-0.037504', '-0.101964', '0.030524', '0.0252581', '0.0151371', '-0.051189', '-0.00667955', '0.0670089', '0.11336', '-0.0114908', '-0.0392231', '0.0667485', '-0.0937254', '-0.0458512', '-0.0173767', '-0.0563089', '0.0894303', '-0.0138242', '0.0545259', '-0.0644282', '0.0167204', '0.108552', '0.0273803', '-0.0640456', '-0.0190287', '-0.0320878', '-0.0594212', '-0.00705079', '0.0295527', '-0.00892713', '-0.0566522', '-0.0674011', '-0.0936628', '-0.00381125', '-0.0391895', '0.0263519', '0.0358312', '-0.0264285', '-0.0902174', '-0.00375657', '0.00056606', '-0.0332828', '-0.0142269', '0.114536', '-0.0205728', '-0.0281525', '-0.137604', '-0.0568342', '0.0427695', '-0.0207621', '-0.0404133', '0.00561395', '0.0885133', '-0.0281455', '0.0435455', '0.0904186', '-0.0358977', '-0.0769172', '-0.00576043', '0.0453967', '0.0534858', '0.0130936', '-0.043759', '-0.0488823', '0.120052', '-0.0882784', '-0.00963463', '0.119321', '-0.0827874', '-0.0768254', '0.0484759', '-0.0460704', '-0.00959499', '0.0306719', '0.007365', '-0.0817065', '-0.0693167', '0.129145', '-0.0437534', '0.129299\\n'], ['3', '-0.115039', '0.259707', '0.160732', '0.093139', '0.0147834', '0.0844211', '-0.220459', '0.24851', '0.12269', '0.296533', '0.221662', '0.174785', '0.19601', '0.133457', '0.413058', '-0.0129949', '0.00226453', '-0.0879386', '0.119089', '-0.102234', '-0.0852528', '-0.337941', '0.0929773', '0.0450299', '0.0455229', '-0.106319', '0.0438471', '0.125663', '0.207639', '0.00900614', '-0.0722737', '0.201119', '-0.199833', '-0.123589', '0.0447585', '-0.0982932', '0.184754', '-0.0210469', '0.104597', '-0.0999', '-0.017692', '0.170148', '0.0665567', '-0.157363', '0.0353389', '-0.0177945', '-0.0757019', '0.022167', '-0.0275289', '-0.0134481', '-0.127905', '-0.159275', '-0.203178', '-0.0582523', '-0.0297134', '0.0226495', '0.0842381', '-0.0416006', '-0.29912', '-0.0190132', '0.065219', '-0.066814', '-0.0494921', '0.281883', '-0.0465378', '0.0210553', '-0.28125', '-0.118522', '0.131468', '0.0201002', '-0.0807252', '-0.0943601', '0.275648', '-0.0094105', '0.100844', '0.203157', '-0.112994', '-0.122743', '-0.125632', '-0.0216088', '0.107281', '-0.0709582', '-0.10475', '-0.138688', '0.287568', '-0.141937', '0.0300186', '0.281952', '-0.170414', '-0.102006', '0.159174', '-0.161358', '0.0401337', '0.132947', '0.071757', '-0.171691', '-0.0634865', '0.311636', '-0.134721', '0.244703\\n']]\n"
     ]
    }
   ],
   "source": [
    "# extract different view of data\n",
    "# view two, node2vec\n",
    "setting = \"n2v\"\n",
    "\n",
    "viewTwoFilesDir = \"../Data/\"+Dataset+\"/vectors/\"+setting+\"/n2v.txt\"\n",
    "viewTwoVectors = []\n",
    "\n",
    "with open(viewTwoFilesDir, 'r', encoding = 'utf8') as f:\n",
    "    for line in f:\n",
    "        read_data = line.split(\" \")\n",
    "        if(len(read_data)==101):\n",
    "            paper_Vectors = read_data\n",
    "            viewTwoVectors.append(paper_Vectors)\n",
    "f.close()\n",
    "print(\"Total vector records:\",len(viewTwoVectors))\n",
    "print(viewTwoVectors[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alfredo martinez.txt', 'alfredo martinez0.txt', 'alfredo martinez1.txt', 'amit patel.txt', 'amit patel0.txt', 'amit patel1.txt', 'ana castro.txt', 'ana castro0.txt', 'ana castro1.txt', 'ana castro2.txt', 'anna ferrari.txt', 'anna ferrari0.txt', 'anna ferrari1.txt', 'bin liu.txt', 'bin liu0.txt', 'bin liu1.txt', 'carmen moreno.txt', 'carmen moreno0.txt', 'carmen moreno1.txt', 'carmen torres.txt', 'carmen torres0.txt', 'carmen torres1.txt', 'chao liu.txt', 'chao liu0.txt', 'chao liu1.txt', 'cheng luo.txt', 'cheng luo0.txt', 'cheng luo1.txt', 'chung-may yang.txt', 'chung-may yang0.txt', 'chung-may yang1.txt', 'david g lloyd.txt', 'david g lloyd0.txt', 'david g lloyd1.txt', 'fang liu.txt', 'fang liu0.txt', 'fang liu1.txt', 'feng liu.txt', 'feng liu0.txt', 'feng liu1.txt', 'feng xu.txt', 'feng xu0.txt', 'feng xu1.txt', 'francisco esteves.txt', 'francisco esteves0.txt', 'francisco esteves1.txt', 'francisco j blanco.txt', 'francisco j blanco0.txt', 'francisco j blanco1.txt', 'giovanni volpe.txt', 'giovanni volpe0.txt', 'giovanni volpe1.txt', 'hao song.txt', 'hao song0.txt', 'hao song1.txt', 'hong yang.txt', 'hong yang0.txt', 'hong yang1.txt', 'jacob john.txt', 'jacob john0.txt', 'jacob john1.txt', 'jeong hwan kim.txt', 'jeong hwan kim0.txt', 'jeong hwan kim1.txt', 'jeremy m brown.txt', 'jeremy m brown0.txt', 'jeremy m brown1.txt', 'jie zhang.txt', 'jie zhang0.txt', 'jie zhang1.txt', 'jin young kim.txt', 'jin young kim0.txt', 'jin young kim1.txt', 'john f marshall.txt', 'john f marshall0.txt', 'john f marshall1.txt', 'jong hee chang.txt', 'jong hee chang0.txt', 'jong hee chang1.txt', 'jun chen.txt', 'jun chen0.txt', 'jun chen1.txt', 'jun chen2.txt', 'jun zhang.txt', 'jun zhang0.txt', 'jun zhang1.txt', 'kevin m. ryan.txt', 'kevin m. ryan0.txt', 'kevin m. ryan1.txt', 'kyung su kim.txt', 'kyung su kim0.txt', 'kyung su kim1.txt', 'lei wang.txt', 'lei wang0.txt', 'lei wang1.txt', 'lei wang2.txt', 'lei wang3.txt', 'lin yang.txt', 'lin yang0.txt', 'lin yang1.txt', 'lu\\udcc3\\udcads alves.txt', 'lu\\udcc3\\udcads alves0.txt', 'lu\\udcc3\\udcads alves1.txt', 'marco ferrari.txt', 'marco ferrari0.txt', 'marco ferrari1.txt', 'marta crespo.txt', 'marta crespo0.txt', 'marta crespo1.txt', 'martin wagner.txt', 'martin wagner0.txt', 'martin wagner1.txt', 'michael wagner.txt', 'michael wagner0.txt', 'michael wagner1.txt', 'michael wagner2.txt', 'mikael svensson.txt', 'mikael svensson0.txt', 'mikael svensson1.txt', 'pei-ming yang.txt', 'pei-ming yang0.txt', 'pei-ming yang1.txt', 'peng zhang.txt', 'peng zhang0.txt', 'peng zhang1.txt', 'peng zhang2.txt', 'peng zhang3.txt', 'qian wang.txt', 'qian wang0.txt', 'qian wang1.txt', 'qiang wang.txt', 'qiang wang0.txt', 'qiang wang1.txt', 'qin li.txt', 'qin li0.txt', 'qin li1.txt', 'richard w morris.txt', 'richard w morris0.txt', 'richard w morris1.txt', 'robert j young.txt', 'robert j young0.txt', 'robert j young1.txt', 'sebastian wolf.txt', 'sebastian wolf0.txt', 'sebastian wolf1.txt', 'vineet gupta.txt', 'vineet gupta0.txt', 'vineet gupta1.txt', 'vivek gupta.txt', 'vivek gupta0.txt', 'vivek gupta1.txt', 'vivek kumar.txt', 'vivek kumar0.txt', 'vivek kumar1.txt', 'wei lu.txt', 'wei lu0.txt', 'wei lu1.txt', 'wei wang.txt', 'wei wang0.txt', 'wei wang1.txt', 'wei wang2.txt', 'wei wang3.txt', 'wei wang4.txt', 'wei xu.txt', 'wei xu0.txt', 'wei xu1.txt', 'xin li.txt', 'xin li0.txt', 'xin li1.txt', 'yang wang.txt', 'yang wang0.txt', 'yang wang1.txt', 'yang zhao.txt', 'yang zhao0.txt', 'yang zhao1.txt', 'ying liu.txt', 'ying liu0.txt', 'ying liu1.txt', 'ying liu2.txt', 'ying zhang.txt', 'ying zhang0.txt', 'ying zhang1.txt', 'yong liu.txt', 'yong liu0.txt', 'yong liu1.txt', 'yong wang.txt', 'yong wang0.txt', 'yong wang1.txt', 'yongsheng liu.txt', 'yongsheng liu0.txt', 'yongsheng liu1.txt', 'yu zhang.txt', 'yu zhang0.txt', 'yu zhang1.txt', 'yu-jun zhao.txt', 'yu-jun zhao0.txt', 'yu-jun zhao1.txt']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "# collect data\n",
    "labeledauthorGroupfileDir = \"../Data/\"+Dataset+\"/filteredSameNameAuthor/filter=10/\"\n",
    "fileList = os.listdir(labeledauthorGroupfileDir)\n",
    "fileList.sort()\n",
    "print(fileList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove author(positive sample) from other(negative sample)\n",
    "import random\n",
    "def extractNegativeSample(positiveSample, allSample):\n",
    "    negativeSample = [x for x in allSample if x not in positiveSample]\n",
    "    return negativeSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# collect class vectors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def extractVectors(authors_pids, allPaperVectors):\n",
    "    appended_data = []\n",
    "    for label, author in enumerate(authors_pids):\n",
    "        data = []\n",
    "        labels = []\n",
    "        for paper_Vectors in allPaperVectors:\n",
    "            if paper_Vectors[0] in author:\n",
    "                data.append(paper_Vectors)\n",
    "                labels.append(label)\n",
    "        print(\"Class \",label,\" sample size: \", len(data))\n",
    "        # create df save one author data \n",
    "        authordf = pd.DataFrame(data)\n",
    "        authordf['label'] = labels\n",
    "        appended_data.append(authordf)\n",
    "    # add all together\n",
    "    labeled_data = pd.concat(appended_data, axis=0,ignore_index=True)\n",
    "    # print shape for confirmation\n",
    "    print(labeled_data.shape)\n",
    "    return labeled_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# collect unlabeled vectors\n",
    "def extractUnlabeledVectors(unlabeled_pid,allPaperVectors):\n",
    "    data = []\n",
    "    for paper_vector in allPaperVectors:\n",
    "        if int(paper_vector[0]) in unlabeled_pid:\n",
    "            data.append(paper_vector)\n",
    "    unlabeled_df = pd.DataFrame(data)\n",
    "    unlabeled_df['label'] = -1\n",
    "    return unlabeled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loop through all files in directory add name to name list\n",
    "# author as positive sample, other as all samples\n",
    "name_list = []\n",
    "# create name list for all authors have same name\n",
    "for file in fileList:\n",
    "    if not file.startswith('.'):\n",
    "        if not re.match(r'\\D*\\d+.txt$', file):\n",
    "            # fix the coding issue\n",
    "            name_list.append(file.encode(\"utf-8\", \"surrogateescape\").decode('utf8','surrogateescape')[:-4])\n",
    "# print(name_list)\n",
    "\n",
    "# loop through all the author and gather result\n",
    "for name in name_list:\n",
    "    other_pids = []\n",
    "    # read labeled other sample\n",
    "    with open((fileDir+name+\".txt\").encode('utf-8'), 'r', encoding = 'utf8') as f:\n",
    "        for line in f:\n",
    "            other_pids.extend(line.strip().split(\" \"))\n",
    "    # read unlabeled sample\n",
    "    ublabeledauthorGroupfileDir = \"../Data/\"+Dataset+\"/unlabeled_FILN_Group/\"\n",
    "    lastname = name.split(\" \")[-1]\n",
    "    firstInitial = name.split(\" \")[0][0]\n",
    "    FILN = firstInitial+\"_\"+lastname\n",
    "    for file in fileList:\n",
    "        file=file.encode(\"utf-8\", \"surrogateescape\").decode('utf8','surrogateescape')\n",
    "            if not file.startswith('.'):\n",
    "                if re.match(r'\\D*\\d+.txt$', file):\n",
    "                    if name in file:\n",
    "                        print(os.path.splitext(file)[0])\n",
    "                        # add author to list for final output\n",
    "                        allauthor.append(os.path.splitext(file)[0])\n",
    "                        author_pids = []\n",
    "                        # read author sample\n",
    "                        with open((fileDir+os.path.splitext(file)[0]+\".txt\").encode('utf-8'), 'r', encoding = 'utf8') as f:\n",
    "                            for line in f:\n",
    "                                author_pids.extend(line.strip().split(\" \"))\n",
    "                        # print properties\n",
    "                        authorSampleSize.append(len(author_pids))\n",
    "                        allSampleSize.append(len(other_pids))\n",
    "                        print(len(author_pids))\n",
    "                        print(len(other_pids))\n",
    "                        # remove author(positive sample) from other(all sample) to create negative sample\n",
    "                        NegativeSample_pid = extractNegativeSample(author_pids, other_pids)\n",
    "                        # collect all vector\n",
    "                        classOne, classTwo = extractVectors(author_pids,NegativeSample_pid,allPaperVectors)\n",
    "                        print(classOne.shape)\n",
    "                        print(classTwo.shape)\n",
    "                        # combine data from different class get all data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n",
      "69\n",
      "29450\n"
     ]
    }
   ],
   "source": [
    "# hard code to read the file one by one\n",
    "# store the features for classification\n",
    "author_pids = []\n",
    "other_pids = []\n",
    "unlabeled_pid = []\n",
    "name = \"kyung su kim\"\n",
    "lastname = name.split(\" \")[-1]\n",
    "firstInitial = name.split(\" \")[0][0]\n",
    "FILN = firstInitial+\"_\"+lastname\n",
    "# author as positive sample, other as all samples\n",
    "with open(labeledauthorGroupfileDir+\"kyung su kim1.txt\", 'r', encoding = 'utf8') as f:\n",
    "    for line in f:\n",
    "        author_pids.extend(line.strip().split(\" \"))\n",
    "\n",
    "with open(labeledauthorGroupfileDir+name+\".txt\", 'r', encoding = 'utf8') as f:\n",
    "    for line in f:\n",
    "        other_pids.extend(line.strip().split(\" \"))\n",
    "# ublabeled same FILN samples\n",
    "ublabeledauthorGroupfileDir = \"../Data/\"+Dataset+\"/unlabeled_FILN_Group/\"\n",
    "with open(ublabeledauthorGroupfileDir+FILN+\".txt\", 'r', encoding = 'utf8') as f:\n",
    "    for line in f:\n",
    "        name_per_id = line.split(\"\\t\")\n",
    "        pmid = int(name_per_id[1])\n",
    "        unlabeled_pid.append(pmid)\n",
    "        \n",
    "print(len(author_pids))\n",
    "print(len(other_pids))\n",
    "print(len(unlabeled_pid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total positive sample size: 55\n",
      "Total negative sample size: 14\n"
     ]
    }
   ],
   "source": [
    "NegativeSample_pid = extractNegativeSample(author_pids, other_pids)\n",
    "print(\"Total positive sample size:\", len(author_pids))\n",
    "print(\"Total negative sample size:\", len(NegativeSample_pid))\n",
    "all_authors = []\n",
    "all_authors.append(author_pids)\n",
    "all_authors.append(NegativeSample_pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class  0  sample size:  55\n",
      "Class  1  sample size:  14\n",
      "(69, 102)\n",
      "Class  0  sample size:  55\n",
      "Class  1  sample size:  14\n",
      "(69, 102)\n",
      "Labeled:  69  :  69\n",
      "28400\n",
      "Unlabeled:  28379  :  23044\n"
     ]
    }
   ],
   "source": [
    "# read in labeled data\n",
    "labeled_dv1_org = extractVectors(all_authors, viewOneVectors)\n",
    "labeled_dv2_org = extractVectors(all_authors, viewTwoVectors)\n",
    "print(\"Labeled: \",len(labeled_dv1_org), \" : \", len(labeled_dv2_org))\n",
    "# read in unlabeled data\n",
    "unlabeled_pid = set(unlabeled_pid)\n",
    "print(len(unlabeled_pid))\n",
    "unlabeled_data1 = extractUnlabeledVectors(unlabeled_pid,viewOneVectors)\n",
    "unlabeled_data2 = extractUnlabeledVectors(unlabeled_pid,viewTwoVectors)\n",
    "print(\"Unlabeled: \",len(unlabeled_data1), \" : \", len(unlabeled_data2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeled no citation link:  0\n",
      "Unlabeled no citation link size:  5335\n",
      "(23044, 102)\n",
      "(23044, 102)\n",
      "(69, 102)\n",
      "(69, 102)\n"
     ]
    }
   ],
   "source": [
    "# process 1, synchronize different view based on pid\n",
    "# some of the record doesn't have citation links, therefore we will have to remove those papers from train and test set\n",
    "noCitationPids_labeled = set(labeled_dv1_org[0])-set(labeled_dv2_org[0])\n",
    "print(\"labeled no citation link: \", len(noCitationPids_labeled))\n",
    "\n",
    "noCitationPids_unlabeled = set(unlabeled_data1[0])-set(unlabeled_data2[0])\n",
    "print(\"Unlabeled no citation link size: \", len(noCitationPids_unlabeled))\n",
    "\n",
    "# process unlabeled data\n",
    "unlabeled_dv1 = unlabeled_data1[~unlabeled_data1[0].isin(noCitationPids_unlabeled)].reset_index(drop=True)\n",
    "unlabeled_dv2 = unlabeled_data2\n",
    "print(unlabeled_dv1.shape)\n",
    "print(unlabeled_dv2.shape)\n",
    "# process labeled data\n",
    "labeled_dv1_p1 = labeled_dv1_org[~labeled_dv1_org[0].isin(noCitationPids_labeled)]\n",
    "print(labeled_dv1_p1.shape)\n",
    "print(labeled_dv2_org.shape)\n",
    "# since out input data are sorted and we did't performe any opeartion to mess the order, all data are in order with pid\n",
    "sorted_dv1 = labeled_dv1_p1\n",
    "sorted_dv2 = labeled_dv2_org\n",
    "# method: sort every view by pid\n",
    "# sorted_dv1 = labeled_dv1_p1.sort_values(labeled_dv1_p1.columns[0],ascending = False).reset_index(drop=True)\n",
    "# sorted_dv2 = labeled_dv2_org.sort_values(labeled_dv2_org.columns[0],ascending = False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select 50% of sample as test data in labeled data\n",
    "labeled_index = sorted_dv1.index\n",
    "# extract test data\n",
    "test_data_v1 = sorted_dv1.sample(frac=0.5)\n",
    "test_index = test_data_v1.index\n",
    "test_data_v2 = sorted_dv2.iloc[test_index]\n",
    "# form train data\n",
    "train_index = np.setdiff1d(labeled_index, test_index)\n",
    "labeled_dv1 = sorted_dv1.iloc[train_index]\n",
    "labeled_dv2 = sorted_dv2.iloc[train_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34, 100)\n"
     ]
    }
   ],
   "source": [
    "# form test data\n",
    "test_pid = test_data_v1[[0]].reset_index(drop=True)\n",
    "test_label = test_data_v1[[\"label\"]].reset_index(drop=True)\n",
    "testdatav1 = test_data_v1.drop([0, \"label\"], axis=1).reset_index(drop=True)\n",
    "testdatav2 = test_data_v2.drop([0, \"label\"], axis=1).reset_index(drop=True)\n",
    "print(testdatav1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30    24495814\n",
      "31    24856736\n",
      "32    19996791\n",
      "33    21722666\n",
      "34    22924652\n",
      "35       12383\n",
      "36       33471\n",
      "37       39108\n",
      "38       43470\n",
      "39       97374\n",
      "Name: 0, dtype: object\n",
      "30    24495814\n",
      "31    24856736\n",
      "32    19996791\n",
      "33    21722666\n",
      "34    22924652\n",
      "35       12383\n",
      "36       33471\n",
      "37       39108\n",
      "38       43470\n",
      "39       97374\n",
      "Name: 0, dtype: object\n",
      "(23079, 100)\n",
      "(23079, 100)\n"
     ]
    }
   ],
   "source": [
    "# add ublabeled data to labeled to form final train set\n",
    "final_dv1 = pd.concat([labeled_dv1,unlabeled_dv1], ignore_index=True)\n",
    "final_dv2 = pd.concat([labeled_dv2,unlabeled_dv2], ignore_index=True)\n",
    "print(final_dv1[30:40][0])\n",
    "print(final_dv2[30:40][0])\n",
    "label = final_dv1[[\"label\"]]\n",
    "pid = final_dv1[[0]]\n",
    "final_dv1.drop([0, \"label\"], axis=1, inplace = True)\n",
    "final_dv2.drop([0, \"label\"], axis=1, inplace = True)\n",
    "print(final_dv1.shape)\n",
    "print(final_dv2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "# create co training classifier\n",
    "class Co_training_clf(object):\n",
    "    \n",
    "    import copy\n",
    "    \n",
    "    def __init__(self, clf1, clf2=None, p=1, n=1, k=30, u = 75):\n",
    "        \n",
    "        self.clf1 = clf1\n",
    "        # assume co_training on one classifier\n",
    "        if clf2 == None:\n",
    "            self.clf2 = self.copy.copy(clf1)\n",
    "        else:\n",
    "            self.clf2 = clf2\n",
    "        # take p example from most confidently positive labels to example\n",
    "        self.p = p\n",
    "        # take n example from most confidently negative label to example\n",
    "        self.n = n\n",
    "        # number of iteration\n",
    "        self.k = k\n",
    "        # size of pool of unlabeled samples\n",
    "        self.u = u\n",
    "        \n",
    "    def label_p_n_samples(self, rank):\n",
    "        p, n = [], []\n",
    "        for label, conf_measure in enumerate(rank):\n",
    "            # 0 positive sample\n",
    "            if label==0:\n",
    "                index = 0\n",
    "                while(len(p) < self.p):\n",
    "                    p.append(conf_measure[index])\n",
    "                    index +=1\n",
    "            # 1 negative sample\n",
    "            elif label == 1:\n",
    "                index = 0\n",
    "                while(len(n) < self.n):\n",
    "                    n.append(conf_measure[index])\n",
    "                    index +=1\n",
    "            else:\n",
    "                print(\"Class label error\")\n",
    "        return p, n\n",
    "        \n",
    "    def fit(self, dataView1, dataView2, labels):\n",
    "        \n",
    "        labels = np.asarray(labels)\n",
    "        print(\"P: \", self.p, \" N: \", self.n)\n",
    "        assert(self.p > 0 and self.n > 0 and self.k > 0 and self.u > 0)\n",
    "        \n",
    "        # index of the samples that are initially labeled\n",
    "        L = [i for i, label_i in enumerate(labels) if label_i != -1]\n",
    "        # index of unlabeled samples\n",
    "        U = [i for i, label_i in enumerate(labels) if label_i == -1]\n",
    "        print(\"Initial L size: \", len(L))\n",
    "        print(\"Initial U size: \", len(U))\n",
    "        # random drawing sample from U\n",
    "        random.shuffle(U)\n",
    "        U_prime = U[-min(len(U), self.u):]\n",
    "        # remove the samples in U_prime from U\n",
    "        U = U[:-len(U_prime)]\n",
    "        iterCount = 0\n",
    "        #loop until we have assigned labels to every sample in U and U_prime or we hit our iteration break condition\n",
    "        while iterCount < self.k and U_prime:\n",
    "            iterCount +=1\n",
    "            # print(\"step\",iterCount, \" L: \",L)\n",
    "            # print(\"step\",iterCount, \" U_prime: \",U_prime)\n",
    "            iter_train_d1 = dataView1.iloc[L]\n",
    "            iter_train_d2= dataView2.iloc[L]\n",
    "            iter_train_label = labels[L]\n",
    "            self.clf1.fit(iter_train_d1, iter_train_label)\n",
    "            self.clf2.fit(iter_train_d2, iter_train_label)\n",
    "            \n",
    "            iter_labeling_d1 = dataView1.iloc[U_prime]\n",
    "            iter_labeling_d2 = dataView2.iloc[U_prime]\n",
    "            # rank class probabilities for unlabeled sample for it's confidence measure\n",
    "            dv1_proba = self.clf1.predict_proba(iter_labeling_d1)\n",
    "            dv2_proba = self.clf1.predict_proba(iter_labeling_d2)\n",
    "            # make prediction on data\n",
    "#             y1 = self.clf1.predict(iter_labeling_d1)\n",
    "#             y2 = self.clf2.predict(iter_labeling_d2)\n",
    "#             print(\"dataviewone prediction on unlabeled: \",y1)\n",
    "#             print(\"dataviewtwo prediction on unlabeled: \",y2)\n",
    "            dv1_proba_rank = []\n",
    "            dv2_proba_rank = []\n",
    "            # proba1_rank[i] is label i's confidence measure\n",
    "            for class_proba in dv1_proba.T:\n",
    "                dv1_proba_rank.append((-class_proba).argsort())\n",
    "            for class_proba in dv2_proba.T:\n",
    "                dv2_proba_rank.append((-class_proba).argsort())\n",
    "#             print(dv1_proba)\n",
    "#             print(dv1_proba_rank)\n",
    "#             print(dv2_proba)\n",
    "#             print(dv2_proba_rank)\n",
    "            # h1 classifier\n",
    "            p1,n1 = self.label_p_n_samples(dv1_proba_rank)\n",
    "            # h2 classifier\n",
    "            p2,n2 = self.label_p_n_samples(dv2_proba_rank)\n",
    "            finalP = set(p1+p2)\n",
    "            finalN = set(n1+n2)\n",
    "#             print(\"P: \", finalP, \" N: \", finalN)\n",
    "            # auto label the samples and remove it from U_prime\n",
    "            auto_labeled_pos = [U_prime[x] for x in finalP]\n",
    "            auto_labeled_neg = [U_prime[x] for x in finalN]\n",
    "            auto_labeled_samples = auto_labeled_pos+auto_labeled_neg\n",
    "            labels[auto_labeled_pos] = 0\n",
    "            labels[auto_labeled_neg] = 1\n",
    "            # extend the labeled sample\n",
    "            L.extend(auto_labeled_pos)\n",
    "            L.extend(auto_labeled_neg)\n",
    "            # remove the labeled sample from U_prime\n",
    "            U_prime = [x for x in U_prime if x not in auto_labeled_samples]\n",
    "            #print(U_prime)\n",
    "            # randomly choice 2p+2n examples from u to replenish u_prime\n",
    "            replenishItem = U[-(2*self.p+2*self.n):]\n",
    "            U_prime.extend(replenishItem)\n",
    "            U = U[:-len(replenishItem)]\n",
    "        print(\"Total Labeled number: \", len(L), \" Still unlabeled number: \", len(U_prime))\n",
    "        # final train\n",
    "        newtrain_d1 = dataView1.iloc[L]\n",
    "        newtrain_d2 = dataView2.iloc[L]\n",
    "        self.clf1.fit(newtrain_d1, labels[L])\n",
    "        self.clf2.fit(newtrain_d2, labels[L])\n",
    "    \n",
    "    def supports_proba(self, clf, x):\n",
    "        try:\n",
    "            clf.predict_proba([x])\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "        \n",
    "    def predict(self, dataView1, dataView2):\n",
    "        y1 = self.clf1.predict(dataView1)\n",
    "        y2 = self.clf2.predict(dataView2)\n",
    "        #fill pred with -1 so we can identify the samples in which sample classifiers failed to agree\n",
    "        y_pred = np.asarray([-1] * dataView1.shape[0])\n",
    "        for i, (y1_i, y2_i) in enumerate(zip(y1, y2)):\n",
    "            # if both agree on label\n",
    "            if y1_i == y2_i:\n",
    "                y_pred[i] = y1_i\n",
    "#             # if disagree on label, choice the class have higher probabilities\n",
    "#             elif proba_supported:\n",
    "#                 y1_probas = self.clf1.predict_proba([dataView1[i]])[0]\n",
    "#                 y2_probas = self.clf2.predict_proba([dataView2[i]])[0]\n",
    "#                 sum_y_probas = [proba_y1 + proba_y2 for (proba_y1, proba_y2) in zip(y1_probas, y2_probas)]\n",
    "#                 y_pred[i] = sum_y_probas.index(max(sum_y_probas))\n",
    "            else:\n",
    "                #the classifiers disagree and don't support probability, so we guess\n",
    "                warnings.warn(\"classifiers disagree with label, result may not accurate\")\n",
    "                print(\"sample at: \", i, \" c1: \", y1_i, \" c2: \", y2_i)\n",
    "                y_pred[i] = random.randint(0, 1)\n",
    "        #check if predict works\n",
    "        assert not (-1 in y_pred)\n",
    "        return y_pred\n",
    "    \n",
    "    def predict_proba(self, dataView1, dataView2):\n",
    "        # the predicted probabilities is simply a average of probabilities given from each classifier trained\n",
    "        proba = np.full((dataView1.shape[0], 2), -1)\n",
    "        y1_probas = self.clf1.predict_proba(dataView1)\n",
    "        y2_probas = self.clf2.predict_proba(dataView2)\n",
    "        \n",
    "        for i, (y1_i, y2_i) in enumerate(zip(y1_probas, y2_probas)):\n",
    "            proba[i][0] = (y1_i[0] + y2_i[0]) / 2\n",
    "            proba[i][1] = (y1_i[1] + y2_i[1]) / 2\n",
    "        \n",
    "        return y_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P:  5  N:  1\n",
      "Initial L size:  35\n",
      "Initial U size:  23044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gao137/intel/intelpython3/lib/python3.6/site-packages/sklearn/utils/validation.py:547: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Labeled number:  358  Still unlabeled number:  117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gao137/intel/intelpython3/lib/python3.6/site-packages/ipykernel_launcher.py:148: UserWarning: classifiers disagree with label, result may not accurate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample at:  3  c1:  1  c2:  0\n",
      "sample at:  4  c1:  1  c2:  0\n",
      "sample at:  21  c1:  1  c2:  0\n",
      "sample at:  24  c1:  1  c2:  0\n",
      "sample at:  29  c1:  1  c2:  0\n",
      "[0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      "    label\n",
      "0       0\n",
      "1       0\n",
      "2       1\n",
      "3       0\n",
      "4       0\n",
      "5       1\n",
      "6       1\n",
      "7       0\n",
      "8       0\n",
      "9       0\n",
      "10      0\n",
      "11      0\n",
      "12      1\n",
      "13      0\n",
      "14      0\n",
      "15      1\n",
      "16      0\n",
      "17      1\n",
      "18      1\n",
      "19      0\n",
      "20      1\n",
      "21      0\n",
      "22      1\n",
      "23      0\n",
      "24      1\n",
      "25      1\n",
      "26      0\n",
      "27      0\n",
      "28      0\n",
      "29      0\n",
      "30      0\n",
      "31      0\n",
      "32      0\n",
      "33      0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "clf = Co_training_clf(clf1=linear_model.LogisticRegression(),p=5,n=1)\n",
    "clf.fit(final_dv1,final_dv2,label)\n",
    "pred_label = clf.predict(testdatav1,testdatav2)\n",
    "print(pred_label)\n",
    "print(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %whos\n",
    "del viewOneVectors\n",
    "del viewTwoVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
