{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import logging\n",
    "import os\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "# read data from all file \n",
    "PATH = '../Data'\n",
    "# print log\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # examples\n",
    "# documents = []\n",
    "# sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'doc2vec'],\n",
    "#              ['this', 'is', 'the', 'second', 'sentence'],\n",
    "#              ['yet', 'another', 'sentence'],\n",
    "#              ['one', 'more', 'sentence'],\n",
    "#              ['and', 'the', 'final', 'sentence']]\n",
    "# counter = 0;\n",
    "# for line in sentences:\n",
    "#     documents.append(TaggedDocument(line, str(counter)))\n",
    "#     counter+=1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all files \n",
    "def readallfile(filepath):\n",
    "    documents = []\n",
    "    with open(filepath, 'r', encoding = 'utf8') as f:\n",
    "        for line in f:\n",
    "            items = line.split(\"\\t\")\n",
    "            # word[0] is paper ID, word[1] is title content, word[2] is abstract content\n",
    "            paperID = items[0]\n",
    "            title = items[1].lower().strip().split(\" \")\n",
    "            abstract = items[2].lower().strip().split(\" \")\n",
    "            documents.append(TaggedDocument(title+abstract, [paperID]))\n",
    "    f.close()\n",
    "    print(\"Done loading files\")\n",
    "    return documents\n",
    "\n",
    "documents = readallfile(PATH+\"/id_title_abstract_processed.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "[TaggedDocument(words=['metal', 'substitutions', 'incarbonic', 'anhydrase:', 'a', 'halide', 'ion', 'probe', 'study', ''], tags=['3']), TaggedDocument(words=['purification', 'and', 'properties', 'of', 'escherichia', 'coli', 'dihydrofolate', 'reductase', 'dihydrofolate', 'reductase', 'has', 'been', 'purified', '40-fold', 'to', 'apparent', 'homogeneity', 'from', 'a', 'trimethoprim-resistant', 'strain', 'of', 'escherichia', 'coli', '(rt', '500)', 'using', 'a', 'procedure', 'that', 'includes', 'methotrexate', 'affinity', 'column', 'chromatography.', 'determinations', 'of', 'the', 'molecular', 'weight', 'of', 'the', 'enzyme', 'based', 'on', 'its', 'amino', 'acid', 'composition,', 'sedimentation', 'velocity,', 'and', 'sodium', 'dodecyl', 'sulfate', 'gel', 'electrophoresis', 'gave', 'values', 'of', '17680,', '17470', 'and', '18300,', 'respectively.', 'an', 'aggregated', 'form', 'of', 'the', 'enzyme', 'with', 'a', 'low', 'specific', 'activity', 'can', 'be', 'separated', 'from', 'the', 'monomer', 'by', 'gel', 'filtration;', 'treatment', 'of', 'the', 'aggregate', 'with', 'mercaptoethanol', 'or', 'dithiothreitol', 'results', 'in', 'an', 'increase', 'in', 'enzymic', 'activity', 'and', 'a', 'regeneration', 'of', 'the', 'monomer.', 'also,', 'multiple', 'molecular', 'forms', 'of', 'the', 'monomer', 'have', 'been', 'detected', 'by', 'polyacrylamide', 'gel', 'electrophoresis.', 'the', 'unresolved', 'enzyme', 'exhibits', 'two', 'ph', 'optima', '(ph', '4.5', 'and', 'ph', '7.0)', 'with', 'dihydrofolate', 'as', 'a', 'substrate.', 'highest', 'activities', 'are', 'observed', 'in', 'buffers', 'containing', 'large', 'organic', 'cations.', 'in', '100', 'mm', 'imidazolium', 'chloride', '(ph', '7),', 'the', 'specific', 'activity', 'is', '47', 'mumol', 'of', 'dihydrofolate', 'reduced', 'per', 'min', 'per', 'mg', 'at', '30', 'degrees.', 'folic', 'acid', 'also', 'serves', 'as', 'a', 'substrate', 'with', 'a', 'single', 'ph', 'optimum', 'of', 'ph', '4.5.', 'at', 'this', 'ph', 'the', 'km', 'for', 'folate', 'is', '16', 'mum,', 'and', 'the', 'vmax', 'is', '1/1000', 'of', 'the', 'rate', 'observed', 'with', 'dihydrofolate', 'as', 'the', 'substrate.', 'monovalent', 'cations', '(na+,', 'k+,', 'rb+,', 'and', 'cs+)', 'inhibit', 'dihydrofolate', 'reductase;', 'at', 'a', 'given', 'ionic', 'strength', 'the', 'degree', 'of', 'inhibition', 'is', 'a', 'function', 'of', 'the', 'ionic', 'radius', 'of', 'the', 'cation.', 'divalent', 'cations', 'are', 'more', 'potent', 'inhibitors;', 'the', 'i50', 'of', 'bacl2', 'is', '250', 'mum,', 'as', 'compared', 'to', '125', 'mm', 'for', 'kcl.', 'anions', 'neither', 'inhibit', 'nor', 'activate', 'the', 'enzyme.'], tags=['46'])]\n"
     ]
    }
   ],
   "source": [
    "print(len(documents))\n",
    "print(documents[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-15 19:12:57,568 : INFO : collecting all words and their counts\n",
      "2018-02-15 19:12:57,570 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2018-02-15 19:12:57,576 : INFO : collected 1609 word types and 46 unique tags from a corpus of 46 examples and 3976 words\n",
      "2018-02-15 19:12:57,579 : INFO : Loading a fresh vocabulary\n",
      "2018-02-15 19:12:57,583 : INFO : min_count=5 retains 113 unique words (7% of original 1609, drops 1496)\n",
      "2018-02-15 19:12:57,585 : INFO : min_count=5 leaves 1898 word corpus (47% of original 3976, drops 2078)\n",
      "2018-02-15 19:12:57,589 : INFO : deleting the raw counts dictionary of 1609 items\n",
      "2018-02-15 19:12:57,592 : INFO : sample=0.001 downsamples 113 most-common words\n",
      "2018-02-15 19:12:57,594 : INFO : downsampling leaves estimated 750 word corpus (39.6% of prior 1898)\n",
      "2018-02-15 19:12:57,597 : INFO : estimated required memory for 113 words and 100 dimensions: 174500 bytes\n",
      "2018-02-15 19:12:57,599 : INFO : resetting layer weights\n",
      "2018-02-15 19:12:57,610 : INFO : training model with 8 workers on 113 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-02-15 19:12:57,620 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-02-15 19:12:57,625 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-02-15 19:12:57,628 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-02-15 19:12:57,632 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-02-15 19:12:57,636 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-02-15 19:12:57,639 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-02-15 19:12:57,641 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-02-15 19:12:57,643 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-02-15 19:12:57,645 : INFO : EPOCH - 1 : training on 3976 raw words (788 effective words) took 0.0s, 30757 effective words/s\n",
      "2018-02-15 19:12:57,654 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-02-15 19:12:57,658 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-02-15 19:12:57,662 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-02-15 19:12:57,666 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-02-15 19:12:57,669 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-02-15 19:12:57,671 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-02-15 19:12:57,673 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-02-15 19:12:57,675 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-02-15 19:12:57,677 : INFO : EPOCH - 2 : training on 3976 raw words (788 effective words) took 0.0s, 32137 effective words/s\n",
      "2018-02-15 19:12:57,687 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-02-15 19:12:57,691 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-02-15 19:12:57,694 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-02-15 19:12:57,699 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-02-15 19:12:57,701 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-02-15 19:12:57,704 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-02-15 19:12:57,705 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-02-15 19:12:57,708 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-02-15 19:12:57,710 : INFO : EPOCH - 3 : training on 3976 raw words (789 effective words) took 0.0s, 32144 effective words/s\n",
      "2018-02-15 19:12:57,719 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-02-15 19:12:57,723 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-02-15 19:12:57,726 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-02-15 19:12:57,730 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-02-15 19:12:57,733 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-02-15 19:12:57,736 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-02-15 19:12:57,738 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-02-15 19:12:57,740 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-02-15 19:12:57,742 : INFO : EPOCH - 4 : training on 3976 raw words (797 effective words) took 0.0s, 31971 effective words/s\n",
      "2018-02-15 19:12:57,751 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-02-15 19:12:57,755 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-02-15 19:12:57,759 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-02-15 19:12:57,764 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-02-15 19:12:57,767 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-02-15 19:12:57,769 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-02-15 19:12:57,772 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-02-15 19:12:57,774 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-02-15 19:12:57,776 : INFO : EPOCH - 5 : training on 3976 raw words (791 effective words) took 0.0s, 30485 effective words/s\n",
      "2018-02-15 19:12:57,785 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-02-15 19:12:57,790 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-02-15 19:12:57,794 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-02-15 19:12:57,798 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-02-15 19:12:57,803 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-02-15 19:12:57,804 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-02-15 19:12:57,807 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-02-15 19:12:57,809 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-02-15 19:12:57,811 : INFO : EPOCH - 6 : training on 3976 raw words (790 effective words) took 0.0s, 30803 effective words/s\n",
      "2018-02-15 19:12:57,820 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-02-15 19:12:57,824 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-02-15 19:12:57,831 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-02-15 19:12:57,834 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-02-15 19:12:57,836 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-02-15 19:12:57,840 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-02-15 19:12:57,842 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-02-15 19:12:57,844 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-02-15 19:12:57,846 : INFO : EPOCH - 7 : training on 3976 raw words (812 effective words) took 0.0s, 29540 effective words/s\n",
      "2018-02-15 19:12:57,855 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-02-15 19:12:57,859 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-02-15 19:12:57,864 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-02-15 19:12:57,868 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-02-15 19:12:57,871 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-02-15 19:12:57,874 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-02-15 19:12:57,876 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-02-15 19:12:57,879 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-02-15 19:12:57,881 : INFO : EPOCH - 8 : training on 3976 raw words (836 effective words) took 0.0s, 30579 effective words/s\n",
      "2018-02-15 19:12:57,891 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-02-15 19:12:57,895 : INFO : worker thread finished; awaiting finish of 6 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-15 19:12:57,901 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-02-15 19:12:57,906 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-02-15 19:12:57,908 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-02-15 19:12:57,911 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-02-15 19:12:57,913 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-02-15 19:12:57,915 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-02-15 19:12:57,917 : INFO : EPOCH - 9 : training on 3976 raw words (807 effective words) took 0.0s, 28701 effective words/s\n",
      "2018-02-15 19:12:57,926 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-02-15 19:12:57,933 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-02-15 19:12:57,940 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-02-15 19:12:57,942 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-02-15 19:12:57,945 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-02-15 19:12:57,947 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-02-15 19:12:57,949 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-02-15 19:12:57,951 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-02-15 19:12:57,953 : INFO : EPOCH - 10 : training on 3976 raw words (800 effective words) took 0.0s, 29560 effective words/s\n",
      "2018-02-15 19:12:57,955 : INFO : training on a 39760 raw words (7998 effective words) took 0.3s, 23312 effective words/s\n",
      "2018-02-15 19:12:57,957 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-02-15 19:12:57,960 : INFO : saving Doc2Vec object under ../models/doc2v/testmodel, separately None\n",
      "2018-02-15 19:12:57,968 : INFO : saved ../models/doc2v/testmodel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "def train_and_save_d2v_model(document):\n",
    "    # train model\n",
    "    # size is number of vector return, alpha is learning rate, sample is number of sample want to remove\n",
    "    model = gensim.models.Doc2Vec(document, min_count=5,vector_size=100, epochs=10, workers=8, window=5, sample=1e-3, negative=5)\n",
    "    # save model\n",
    "    newfileDir = \"../models/doc2v\"\n",
    "    if not os.path.exists(newfileDir):\n",
    "        os.makedirs(newfileDir)\n",
    "    print(\"Saving model\")\n",
    "    model.save(newfileDir+\"/model\")\n",
    "    print(\"Done\")\n",
    "    model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    \n",
    "train_and_save_d2v_model(documents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
