{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-30T16:45:13.752273Z",
     "start_time": "2019-01-30T16:45:12.763713Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training score : 0.974 (ovr)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEICAYAAABbOlNNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYJAkNQwGSVLj/wMKGUHkcZBEzQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8135c2c908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# make 3-class dataset for classification\n",
    "centers = [[-5, 0], [0, 1.5], [5, -1]]\n",
    "X, y = make_blobs(n_samples=1000, centers=centers, random_state=40)\n",
    "\n",
    "for multi_class in (['ovr']):\n",
    "    clf = LogisticRegression(solver='liblinear', max_iter=100, random_state=42,\n",
    "                             multi_class=multi_class).fit(X, y)\n",
    "\n",
    "    # print the training scores\n",
    "    print(\"training score : %.3f (%s)\" % (clf.score(X, y), multi_class))\n",
    "\n",
    "    # create a mesh to plot in\n",
    "    h = .02  # step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    print(xx, yy)\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure()\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "    plt.title(\"Decision surface of LogisticRegression (%s)\" % multi_class)\n",
    "    plt.axis('tight')\n",
    "\n",
    "    # Plot also the training points\n",
    "    colors = \"bry\"\n",
    "    for i, color in zip(clf.classes_, colors):\n",
    "        idx = np.where(y == i)\n",
    "        plt.scatter(X[idx, 0], X[idx, 1], c=color, cmap=plt.cm.Paired,\n",
    "                    edgecolor='black', s=20)\n",
    "\n",
    "    # Plot the three one-against-all classifiers\n",
    "    xmin, xmax = plt.xlim()\n",
    "    ymin, ymax = plt.ylim()\n",
    "    coef = clf.coef_\n",
    "    intercept = clf.intercept_\n",
    "\n",
    "    def plot_hyperplane(c, color):\n",
    "        def line(x0):\n",
    "            return (-(x0 * coef[c, 0]) - intercept[c]) / coef[c, 1]\n",
    "        plt.plot([xmin, xmax], [line(xmin), line(xmax)],\n",
    "                 ls=\"--\", color=color)\n",
    "\n",
    "    for i, color in zip(clf.classes_, colors):\n",
    "        plot_hyperplane(i, color)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-21T19:28:27.176944Z",
     "start_time": "2019-01-21T19:28:27.128576Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes:  100\n",
      "Number of edges:  2531\n",
      "[0, 1, 2, 3, 4]\n",
      "[(0, 4), (0, 8), (0, 11), (0, 12), (0, 14)]\n",
      "[4, 8, 11, 12, 14, 15, 17, 18, 20, 21, 24, 30, 34, 35, 36, 38, 40, 44, 45, 46, 48, 49, 50, 51, 52, 53, 55, 59, 60, 62, 64, 66, 67, 68, 69, 70, 71, 74, 75, 76, 77, 78, 79, 80, 81, 83, 86, 87, 88, 94, 95, 96, 98, 99]\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "# Create a graph\n",
    "G = nx.fast_gnp_random_graph(n=100, p=0.5)\n",
    "print(\"Number of nodes: \", G.number_of_nodes())\n",
    "print(\"Number of edges: \", G.number_of_edges())\n",
    "print(G.nodes()[:5])\n",
    "print(G.edges()[:5])\n",
    "print(G.neighbors(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-20T22:13:09.440920Z",
     "start_time": "2019-01-20T22:13:09.357649Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'list' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2148e0c3008b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mthe_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthe_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mremain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthe_list\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthe_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'list' and 'list'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "the_list = [1,2,3,4,5,6,7,8,9,10,11,12,15]\n",
    "test = random.sample(the_list, 5)\n",
    "remain = the_list-test\n",
    "print(test)\n",
    "print(the_list)\n",
    "print(remain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T01:09:20.240959Z",
     "start_time": "2018-12-21T01:09:20.227344Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "Dataset = \"pubmed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T01:09:21.645751Z",
     "start_time": "2018-12-21T01:09:20.976019Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelSaveDir = \"../Data/\"+Dataset+\"/models/tf/textual_sample=140k/\"\n",
    "with open(modelSaveDir+'tf_features.pickle', \"rb\") as input_file:\n",
    "    vec = pickle.load(input_file)\n",
    "with open(modelSaveDir+'feature_pid.pickle', \"rb\") as input_file:\n",
    "    allPaperid = pickle.load(input_file)\n",
    "print(vec.shape)\n",
    "print(len(allPaperid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T01:35:17.524061Z",
     "start_time": "2018-12-21T01:35:17.511740Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(vec[0].nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T04:13:57.893207Z",
     "start_time": "2018-12-20T04:13:57.884952Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(dir(vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T03:49:14.380549Z",
     "start_time": "2018-12-20T03:49:14.280584Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "allPaperid = np.asarray(allPaperid)\n",
    "result = textual_emb = np.column_stack([allPaperid, vec])\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T03:41:30.220476Z",
     "start_time": "2018-12-20T03:41:30.210988Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T01:30:18.477600Z",
     "start_time": "2018-12-21T01:30:17.030451Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import com_func\n",
    "# examples\n",
    "collection = []\n",
    "sentences = ['this is a the First sentence for first    .',\" \",\"51152123asdfasd\",\n",
    "             'and the final sentencesssssssssssssssddddddddddddddddddddddddsssssssssssssssssssssssss!'\n",
    "            , \"cross-protection$$hemagglutination \"]\n",
    "counter = 5\n",
    "for line in sentences:\n",
    "    newPaper = {\"cleaned_content\":com_func.clean_line_of_raw(line, stopword = False, word_min_length=2)}\n",
    "    collection.append(newPaper)\n",
    "    counter+=1\n",
    "print(collection)\n",
    "allContent = [paper[\"cleaned_content\"]for paper in collection]\n",
    "\n",
    "# test = model.transform(allContent)\n",
    "# print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T01:30:50.822714Z",
     "start_time": "2018-12-21T01:30:50.757371Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# tf = sublinear tf, 1+log(tf) if tf >0, or 0. idf = (log(1+N)/df(t,d,C)) + 1\n",
    "# df(d,t,C) means number of document d contain term t in collection C\n",
    "# Each row is normalized to have unit Euclidean norm \n",
    "# reference to http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=False, smooth_idf=False, sublinear_tf=True, norm=\"l2\", min_df=0,\n",
    "                                   analyzer='word', tokenizer=dummy,preprocessor=dummy, stop_words = None)\n",
    "allContent = [paper[\"cleaned_content\"]for paper in collection]\n",
    "print(allContent)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(allContent)\n",
    "testpid = [3,1,4,2]\n",
    "print(testpid)\n",
    "print(len(tfidf_vectorizer.vocabulary_))\n",
    "print(tfidf_vectorizer.get_feature_names())\n",
    "print(tfidf_matrix.shape)\n",
    "print(tfidf_matrix)\n",
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T21:01:36.810097Z",
     "start_time": "2018-12-19T21:01:36.802079Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(dir(tfidfresult))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T14:53:31.077580Z",
     "start_time": "2018-12-19T14:53:30.086025Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=100)\n",
    "final_lsa_Matrix = svd.fit_transform(test)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T14:53:31.822430Z",
     "start_time": "2018-12-19T14:53:31.811067Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(final_lsa_Matrix)\n",
    "print(svd.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T23:21:25.486914Z",
     "start_time": "2018-12-17T23:21:25.471011Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def remove_zero_feature(Xtr, min_tfidf=0.04):\n",
    "    D = Xtr.toarray() # convert to dense if you want\n",
    "    tfidf_means = np.mean(D, axis=0) # find features that are 0 in all documents\n",
    "    D = np.delete(D, np.where(tfidf_means == 0)[0], axis=1) # delete them from the matrix\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T23:21:26.190960Z",
     "start_time": "2018-12-17T23:21:26.162422Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test2 = remove_zero_feature(test)\n",
    "print(test2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T20:34:39.314709Z",
     "start_time": "2018-11-20T20:34:39.283264Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import com_func\n",
    "# examples\n",
    "collection = []\n",
    "sentences = ['this is a the First sentence for first    .',\" \",\"51152123asdfasd\",\n",
    "             'and the final sentencesssssssssssssssddddddddddddddddddddddddsssssssssssssssssssssssss!']\n",
    "collection, rowSize = com_func.clean_batch_of_raw(sentences, stopword = False, word_min_length=2)\n",
    "print(collection)\n",
    "print(rowSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T20:34:03.785676Z",
     "start_time": "2018-12-19T20:34:03.661349Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "import pickle\n",
    "modelSaveDir = \"../Data/\"+Dataset+\"/models/count/textual_sample=140k/\"\n",
    "embedding = corpora.MmCorpus(os.path.join(modelSaveDir, 'tf_vector.mm'))\n",
    "with open(modelSaveDir+'emb_pid.pickle', \"rb\") as input_file:\n",
    "    index = pickle.load(input_file)\n",
    "\n",
    "print(len(index))\n",
    "print(len(embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T21:26:20.422147Z",
     "start_time": "2018-12-17T21:26:19.671049Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import com_func\n",
    "# examples\n",
    "collection = []\n",
    "sentences = ['this is a the First sentence for first    .',\" \",\"51152123asdfasd\",\n",
    "             'and the final sentencesssssssssssssssddddddddddddddddddddddddsssssssssssssssssssssssss!'\n",
    "            , \"cross-protection$$hemagglutination \"]\n",
    "counter = 5\n",
    "for line in sentences:\n",
    "    newPaper = {\"cleaned_content\":com_func.clean_line_of_raw(line, stopword = False, word_min_length=2)}\n",
    "    collection.append(newPaper)\n",
    "    counter+=1\n",
    "print(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T21:26:20.946213Z",
     "start_time": "2018-12-17T21:26:20.915583Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "collection[0].items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T21:42:03.062521Z",
     "start_time": "2018-12-19T21:42:03.019669Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert raw text to numerical feature vectors\n",
    "# unigram are used\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(allContent)\n",
    "corpus = [dictionary.doc2bow(text) for text in allContent]\n",
    "print(corpus)\n",
    "doc_emb = gensim.matutils.corpus2csc(corpus)\n",
    "print(doc_emb.T.toarray())\n",
    "tf = gensim.models.TfidfModel(corpus, smartirs=\"lnn\")\n",
    "tfidfresult = tf[corpus]\n",
    "tfidf_emb = gensim.matutils.corpus2csc(tfidfresult)\n",
    "print(tfidf_emb.T.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T21:27:46.071677Z",
     "start_time": "2018-12-17T21:27:46.016132Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert raw text to numerical feature vectors\n",
    "# bow(Bags of words) are used\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(min_df = 0,analyzer='word', tokenizer=dummy,preprocessor=dummy, stop_words = None)\n",
    "# train a dictionary to build a frquence counter\n",
    "allContent = [paper[\"cleaned_content\"]for paper in collection]\n",
    "print(allContent)\n",
    "count_matrix = count_vect.fit_transform(allContent)\n",
    "print(len(count_vect.vocabulary_))\n",
    "print(count_vect.get_feature_names())\n",
    "print(count_matrix.toarray())\n",
    "print(count_matrix.shape)\n",
    "test = count_vect.fit_transform(allContent).todense()\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T23:34:51.384399Z",
     "start_time": "2018-11-30T23:34:51.373605Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "result = scaler.fit_transform(count_matrix.toarray())\n",
    "print(scaler.mean_)\n",
    "print(scaler.var_ )\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T03:56:52.859612Z",
     "start_time": "2018-11-20T03:56:52.848019Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test save model\n",
    "import pickle\n",
    "\n",
    "pickle.dump(count_vect, open(\"../tests/CountVectorizer.pickle\", \"wb\"))\n",
    "pickle.dump(count_matrix, open(\"../tests/trained_features.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-09T22:09:28.225665Z",
     "start_time": "2018-12-09T22:09:28.196542Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# tf = sublinear tf, 1+log(tf) if tf >0, or 0. idf = (log(1+N)/df(t,d,C)) + 1\n",
    "# df(d,t,C) means number of document d contain term t in collection C\n",
    "# Each row is normalized to have unit Euclidean norm \n",
    "# reference to http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True, smooth_idf=True, sublinear_tf=True, norm=None, min_df=0,\n",
    "                                   analyzer='word', tokenizer=dummy,preprocessor=dummy, stop_words = None)\n",
    "allContent = [paper[\"cleaned_content\"]for paper in collection]\n",
    "print(allContent)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(allContent)\n",
    "testpid = [3,1,4,2]\n",
    "print(testpid)\n",
    "print(len(tfidf_vectorizer.vocabulary_))\n",
    "print(tfidf_vectorizer.get_feature_names())\n",
    "print(tfidf_matrix.shape)\n",
    "print(tfidf_matrix)\n",
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T04:18:52.904228Z",
     "start_time": "2018-11-20T04:18:52.897752Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test save model\n",
    "\n",
    "pickle.dump(tfidf_vectorizer, open(\"../tests/tfidf.pickle\", \"wb\"))\n",
    "pickle.dump(tfidf_matrix, open(\"../tests/trained_features.pickle\", \"wb\"))\n",
    "pickle.dump(testpid, open(\"../tests/index.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T04:31:20.682659Z",
     "start_time": "2018-11-28T04:31:20.446987Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "# using tf-idf + SVD as LSA\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True, smooth_idf=True, sublinear_tf=True, norm=None, min_df=0,\n",
    "                                   analyzer='word', tokenizer=dummy,preprocessor=dummy, stop_words = None)\n",
    "allContent = [paper[\"cleaned_content\"]for paper in collection]\n",
    "print(allContent)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(allContent)\n",
    "testpid = [3,1,4,2]\n",
    "print(testpid)\n",
    "print(len(tfidf_vectorizer.vocabulary_))\n",
    "print(tfidf_vectorizer.get_feature_names())\n",
    "print(tfidf_matrix.shape)\n",
    "print(tfidf_matrix.toarray())\n",
    "# For LSA, a dim of 100 is recommended.\n",
    "svd = TruncatedSVD(n_components=5, random_state=42)\n",
    "lsa_d5_emb = svd.fit_transform(tfidf_matrix)\n",
    "print(lsa_d5_emb)\n",
    "print(svd.explained_variance_ratio_)\n",
    "print(svd.explained_variance_ratio_.sum())\n",
    "print(svd.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T04:17:25.096288Z",
     "start_time": "2018-11-28T04:17:25.016141Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_multilabel_classification\n",
    "X, _ = make_multilabel_classification(random_state=0)\n",
    "print(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T04:00:37.830413Z",
     "start_time": "2018-11-28T04:00:37.752220Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train LDA\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(min_df = 0,analyzer='word', tokenizer=dummy,preprocessor=dummy, stop_words = None)\n",
    "# train a dictionary to build a frquence counter\n",
    "allContent = [paper[\"cleaned_content\"]for paper in collection]\n",
    "print(allContent)\n",
    "count_matrix = count_vect.fit_transform(allContent)\n",
    "print(len(count_vect.vocabulary_))\n",
    "print(count_vect.get_feature_names())\n",
    "print(count_matrix.toarray())\n",
    "print(count_matrix.shape)\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=0)\n",
    "lda_d5_emb = lda.fit_transform(count_matrix.toarray())\n",
    "print(lda_d5_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T01:40:18.819959Z",
     "start_time": "2018-12-17T01:40:18.804124Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test load model\n",
    "loadDir = \"../Data/\"+Dataset+\"/models/LSA/textual/\"\n",
    "with open(loadDir+'lsa_d100_emb.pickle', \"rb\") as input_file:\n",
    "    emb = pickle.load(input_file)\n",
    "with open(loadDir+'feature_pid.pickle', \"rb\") as input_file:\n",
    "    pid = pickle.load(input_file)\n",
    "print(emb.shape)\n",
    "print(len(pid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T01:40:35.415325Z",
     "start_time": "2018-12-17T01:40:35.381324Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test load model\n",
    "loadDir = '../tests/'\n",
    "with open(loadDir+'tfidf.pickle', \"rb\") as input_file:\n",
    "    test = pickle.load(input_file)\n",
    "with open(loadDir+'train_comment_features.pickle', \"rb\") as input_file:\n",
    "    vec = pickle.load(input_file)\n",
    "print(len(test.vocabulary_))\n",
    "print(test.get_feature_names())\n",
    "print(vec.toarray())\n",
    "print(vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-17T01:54:03.990661Z",
     "start_time": "2018-12-17T01:54:03.968456Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test load model\n",
    "loadDir = '../tests/'\n",
    "with open(loadDir+'tfidf.pickle', \"rb\") as input_file:\n",
    "    test = pickle.load(input_file)\n",
    "with open(loadDir+'trained_features.pickle', \"rb\") as input_file:\n",
    "    vec = pickle.load(input_file)\n",
    "with open(loadDir+'index.pickle', \"rb\") as input_file:\n",
    "    index = pickle.load(input_file)\n",
    "print(index)\n",
    "print(len(test.vocabulary_))\n",
    "print(test.get_feature_names())\n",
    "print(vec.toarray())\n",
    "print(vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-18T04:34:19.252056Z",
     "start_time": "2018-12-18T04:34:19.201604Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "# examples\n",
    "documents = []\n",
    "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'doc2vec'],\n",
    "             ['this', 'is', 'the', 'second', 'sentence'],\n",
    "             ['yet', 'another', 'sentence'],\n",
    "             ['one', 'more', 'sentence'],\n",
    "             ['one', 'more', 'sentence'],\n",
    "             ['one', 'more', 'sentence'],\n",
    "             ['one', 'more', 'sentence'],\n",
    "             ['one', 'more', 'sentence'],\n",
    "             ['and', 'the', 'final', 'sentence']]\n",
    "counter = 0;\n",
    "for line in sentences:\n",
    "    documents.append(TaggedDocument(line, [str(counter)]))\n",
    "    counter+=1;\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-18T04:40:16.635707Z",
     "start_time": "2018-12-18T04:40:16.320539Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "# numpy\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# save the model\n",
    "def train_and_save_d2v_model(document):\n",
    "    cores = multiprocessing.cpu_count()\n",
    "    newfileDir = '../tests/'\n",
    "    if not os.path.exists(newfileDir):\n",
    "        os.makedirs(newfileDir)\n",
    "    # train model\n",
    "    # size is dimension of vector return, dm defines training algorithm, when dm = 1 is distributed memory (PV-DM),\n",
    "    # pv-dm is better than pv-dbow\n",
    "    # when dm = 0 is distributed bag of words (PV-DBOW), \n",
    "    # sample is threshold for configuring which higher-frequency words are randomly downsampled, \n",
    "    # min_count is ignore threshold (Ignores all words with total frequency lower than min_count)\n",
    "    model = gensim.models.Doc2Vec(dm=1,vector_size=100,negative=5, window=5, min_count=5, sample=1e-3, workers=cores)\n",
    "    model.build_vocab(documents)\n",
    "    #print(dir(model))\n",
    "    modelname = model.__str__()\n",
    "    print(modelname)\n",
    "    print(model.wv.vocab)\n",
    "    # shuffle doc\n",
    "    np.random.shuffle(documents)\n",
    "    print(documents)\n",
    "    for epoch in range(50):\n",
    "        print('Epoch %d' % epoch)\n",
    "        model.train(documents)\n",
    "    print(dir(model))\n",
    "    # save model\n",
    "#     print(\"Saving model\")\n",
    "#     model.save(newfileDir+\"/model\")\n",
    "#     print(\"Done\")\n",
    "#     model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    \n",
    "train_and_save_d2v_model(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-08T03:19:16.891921Z",
     "start_time": "2018-12-08T03:19:16.876743Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import com_func\n",
    "# examples\n",
    "collection = []\n",
    "sentences = [\"schizophrenia and childhood adversity\", \"schizophrenia, drug companies and the internet\"]\n",
    "counter = 5\n",
    "for line in sentences:\n",
    "    newPaper = {\"cleaned_content\":com_func.clean_line_of_raw(line, stopword = False, word_min_length=2)}\n",
    "    collection.append(newPaper)\n",
    "    counter+=1\n",
    "print(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-08T03:27:24.323125Z",
     "start_time": "2018-12-08T03:27:24.282419Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True, sublinear_tf=True, norm=None, min_df=0,\n",
    "                                   analyzer='word', tokenizer=dummy,preprocessor=dummy, stop_words = None)\n",
    "allContent = [paper[\"cleaned_content\"]for paper in collection]\n",
    "print(allContent)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(allContent)\n",
    "print(len(tfidf_vectorizer.vocabulary_))\n",
    "print(tfidf_vectorizer.get_feature_names())\n",
    "print(tfidf_matrix.shape)\n",
    "print(tfidf_matrix)\n",
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from yellowbrick.text import FreqDistVisualizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "axes = plt.gca()\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "import numpy as np\n",
    "vectorizer = CountVectorizer(min_df = 0, analyzer='word', tokenizer=dummy,preprocessor=dummy)\n",
    "allContent = [paper[\"cleaned_content\"] for paper in documents]\n",
    "count_matrix = vectorizer.fit_transform(allContent)\n",
    "features = vectorizer.get_feature_names()\n",
    "visualizer = FreqDistVisualizer(features=features, n=30, orient=\"v\")\n",
    "visualizer.fit(count_matrix)\n",
    "visualizer.set_title(\"Frequency distribution before data preprocessing\")\n",
    "visualizer.poof(outpath=\"token_freq_dist_before_preprocess.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T16:23:41.057262Z",
     "start_time": "2019-01-10T16:23:34.619834Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: already exists! Do you want to overwrite <y/n>? \n",
      " 6\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing.pool import ThreadPool, TimeoutError\n",
    "\n",
    "def process_input(prompt,answer):\n",
    "    s = input(prompt)\n",
    "    return s\n",
    "\n",
    "threadp = ThreadPool(processes=1)\n",
    "answer = \"test\"\n",
    "prompt = \"WARNING: already exists! Do you want to overwrite <y/n>? \\n \"\n",
    "try:\n",
    "    answer = threadp.apply_async(process_input, args=(prompt, answer)).get(timeout=10)\n",
    "except TimeoutError: \n",
    "    print(\"no input found\")\n",
    "print(answer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T04:21:36.349026Z",
     "start_time": "2019-01-17T04:21:36.294758Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.878 0.122 0.   ]\n",
      " [0.797 0.203 0.   ]\n",
      " [0.852 0.148 0.   ]]\n",
      "[[0.    0.146 0.853]\n",
      " [0.001 0.293 0.706]\n",
      " [0.    0.33  0.669]]\n",
      "[[0.    0.009 0.   ]\n",
      " [0.    0.03  0.   ]\n",
      " [0.    0.024 0.   ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gao137/intel/intelpython3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X, y = load_iris(return_X_y=True)\n",
    "clf = LogisticRegression(random_state=0,multi_class='ovr').fit(X, y)\n",
    "test1 = clf.predict_proba(X[0:3]) \n",
    "test2 = clf.predict_proba(X[100:103])\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "print(test1)\n",
    "print(test2)\n",
    "\n",
    "print(test1*test2/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
