{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vector records: 3149758\n",
      "['22516865', '0.0422827', '-0.0443959', '-0.00631522', '-0.0297457', '-0.203585', '0.0791923', '-0.156993', '0.102914', '-0.112826', '0.0301516', '-0.118142', '-0.0447324', '0.0131143', '0.127706', '0.107417', '-0.0843709', '-0.0808756', '-0.0504176', '0.132817', '0.0157529', '-0.0383218', '0.165365', '-0.0619297', '-0.0345336', '0.110633', '0.0860297', '-0.0835764', '0.193302', '0.015641', '0.0323645', '0.00893256', '0.151895', '0.0534986', '-0.18639', '-0.0558457', '-0.0603591', '-0.00530632', '-0.000749852', '0.0772172', '0.0646557', '-0.106445', '-0.028214', '0.0400324', '0.11686', '-0.153841', '-0.064435', '-0.0270715', '-0.0234374', '0.0588498', '-0.0750828', '-0.107259', '-0.0351021', '-0.0166648', '-0.135551', '0.1277', '-0.118618', '-0.0417162', '0.0543777', '0.129501', '-0.152003', '0.0624739', '0.175164', '0.0680417', '-0.0896099', '0.171857', '-0.0123408', '-0.0424403', '-0.017759', '0.0631562', '-0.0947245', '-0.0297129', '-0.0952609', '0.0229094', '-0.0183544', '-0.0643149', '-0.0840294', '0.00101607', '-0.00613327', '-0.134319', '-0.0389319', '0.0865807', '-0.0275466', '-0.00559333', '0.028664', '-0.108864', '-0.125772', '-0.171528', '-0.0725501', '-0.0918486', '-0.207318', '-0.0379387', '-0.000273051', '0.0290756', '-0.105057', '-0.121686', '0.127923', '-0.128233', '-0.11645', '-0.365793', '0.0843131\\n']\n"
     ]
    }
   ],
   "source": [
    "# load the vector files\n",
    "import sys\n",
    "import io\n",
    "setting = \"p2v\"\n",
    "Dataset = \"pubmed\"\n",
    "\n",
    "vectorFilesDir = \"../Data/\"+Dataset+\"/vectors/\"+setting+\"/\"+setting+\".txt\"\n",
    "allPaperVectors = []\n",
    "\n",
    "with open(vectorFilesDir, 'r', encoding = 'utf8') as f:\n",
    "    for line in f:\n",
    "        read_data = line.split(\" \")\n",
    "        paper_Vectors = read_data\n",
    "        allPaperVectors.append(paper_Vectors)\n",
    "f.close()\n",
    "        \n",
    "print(\"Total vector records:\",len(allPaperVectors))\n",
    "print(allPaperVectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alfredo martinez.txt', 'alfredo martinez0.txt', 'alfredo martinez1.txt', 'amit patel.txt', 'amit patel0.txt', 'amit patel1.txt', 'ana castro.txt', 'ana castro0.txt', 'ana castro1.txt', 'ana castro2.txt', 'anna ferrari.txt', 'anna ferrari0.txt', 'anna ferrari1.txt', 'bin liu.txt', 'bin liu0.txt', 'bin liu1.txt', 'carmen moreno.txt', 'carmen moreno0.txt', 'carmen moreno1.txt', 'carmen torres.txt', 'carmen torres0.txt', 'carmen torres1.txt', 'chao liu.txt', 'chao liu0.txt', 'chao liu1.txt', 'cheng luo.txt', 'cheng luo0.txt', 'cheng luo1.txt', 'chung-may yang.txt', 'chung-may yang0.txt', 'chung-may yang1.txt', 'david g lloyd.txt', 'david g lloyd0.txt', 'david g lloyd1.txt', 'fang liu.txt', 'fang liu0.txt', 'fang liu1.txt', 'feng liu.txt', 'feng liu0.txt', 'feng liu1.txt', 'feng xu.txt', 'feng xu0.txt', 'feng xu1.txt', 'francisco esteves.txt', 'francisco esteves0.txt', 'francisco esteves1.txt', 'francisco j blanco.txt', 'francisco j blanco0.txt', 'francisco j blanco1.txt', 'giovanni volpe.txt', 'giovanni volpe0.txt', 'giovanni volpe1.txt', 'hao song.txt', 'hao song0.txt', 'hao song1.txt', 'hong yang.txt', 'hong yang0.txt', 'hong yang1.txt', 'jacob john.txt', 'jacob john0.txt', 'jacob john1.txt', 'jeong hwan kim.txt', 'jeong hwan kim0.txt', 'jeong hwan kim1.txt', 'jeremy m brown.txt', 'jeremy m brown0.txt', 'jeremy m brown1.txt', 'jie zhang.txt', 'jie zhang0.txt', 'jie zhang1.txt', 'jin young kim.txt', 'jin young kim0.txt', 'jin young kim1.txt', 'john f marshall.txt', 'john f marshall0.txt', 'john f marshall1.txt', 'jong hee chang.txt', 'jong hee chang0.txt', 'jong hee chang1.txt', 'jun chen.txt', 'jun chen0.txt', 'jun chen1.txt', 'jun chen2.txt', 'jun zhang.txt', 'jun zhang0.txt', 'jun zhang1.txt', 'kevin m. ryan.txt', 'kevin m. ryan0.txt', 'kevin m. ryan1.txt', 'kyung su kim.txt', 'kyung su kim0.txt', 'kyung su kim1.txt', 'lei wang.txt', 'lei wang0.txt', 'lei wang1.txt', 'lei wang2.txt', 'lei wang3.txt', 'lin yang.txt', 'lin yang0.txt', 'lin yang1.txt', 'lu\\udcc3\\udcads alves.txt', 'lu\\udcc3\\udcads alves0.txt', 'lu\\udcc3\\udcads alves1.txt', 'marco ferrari.txt', 'marco ferrari0.txt', 'marco ferrari1.txt', 'marta crespo.txt', 'marta crespo0.txt', 'marta crespo1.txt', 'martin wagner.txt', 'martin wagner0.txt', 'martin wagner1.txt', 'michael wagner.txt', 'michael wagner0.txt', 'michael wagner1.txt', 'michael wagner2.txt', 'mikael svensson.txt', 'mikael svensson0.txt', 'mikael svensson1.txt', 'pei-ming yang.txt', 'pei-ming yang0.txt', 'pei-ming yang1.txt', 'peng zhang.txt', 'peng zhang0.txt', 'peng zhang1.txt', 'peng zhang2.txt', 'peng zhang3.txt', 'qian wang.txt', 'qian wang0.txt', 'qian wang1.txt', 'qiang wang.txt', 'qiang wang0.txt', 'qiang wang1.txt', 'qin li.txt', 'qin li0.txt', 'qin li1.txt', 'richard w morris.txt', 'richard w morris0.txt', 'richard w morris1.txt', 'robert j young.txt', 'robert j young0.txt', 'robert j young1.txt', 'sebastian wolf.txt', 'sebastian wolf0.txt', 'sebastian wolf1.txt', 'vineet gupta.txt', 'vineet gupta0.txt', 'vineet gupta1.txt', 'vivek gupta.txt', 'vivek gupta0.txt', 'vivek gupta1.txt', 'vivek kumar.txt', 'vivek kumar0.txt', 'vivek kumar1.txt', 'wei lu.txt', 'wei lu0.txt', 'wei lu1.txt', 'wei wang.txt', 'wei wang0.txt', 'wei wang1.txt', 'wei wang2.txt', 'wei wang3.txt', 'wei wang4.txt', 'wei xu.txt', 'wei xu0.txt', 'wei xu1.txt', 'xin li.txt', 'xin li0.txt', 'xin li1.txt', 'yang wang.txt', 'yang wang0.txt', 'yang wang1.txt', 'yang zhao.txt', 'yang zhao0.txt', 'yang zhao1.txt', 'ying liu.txt', 'ying liu0.txt', 'ying liu1.txt', 'ying liu2.txt', 'ying zhang.txt', 'ying zhang0.txt', 'ying zhang1.txt', 'yong liu.txt', 'yong liu0.txt', 'yong liu1.txt', 'yong wang.txt', 'yong wang0.txt', 'yong wang1.txt', 'yongsheng liu.txt', 'yongsheng liu0.txt', 'yongsheng liu1.txt', 'yu zhang.txt', 'yu zhang0.txt', 'yu zhang1.txt', 'yu-jun zhao.txt', 'yu-jun zhao0.txt', 'yu-jun zhao1.txt']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# collect data\n",
    "fileDir = \"../Data/\"+Dataset+\"/filteredSameNameAuthor/filter=10/\"\n",
    "fileList = os.listdir(fileDir)\n",
    "fileList.sort()\n",
    "print(fileList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove author(positive sample) from other(negative sample)\n",
    "import random\n",
    "def extractNegativeSample(positiveSample, allSample):\n",
    "    negativeSample = [x for x in allSample if x not in positiveSample]\n",
    "    print(\"Total negative sample size:\", len(negativeSample))\n",
    "    return negativeSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# collect class vectors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def extractVectors(author_pids, NegativeSample_pid, allPaperVectors):\n",
    "    # extract class one vectors\n",
    "    author_features = []\n",
    "    for pid in author_pids:\n",
    "         for paper_Vectors in allPaperVectors:\n",
    "            if(paper_Vectors[0] == pid):\n",
    "                author_features.append(paper_Vectors)\n",
    "    print(\"Positive sample size: \", len(author_features))\n",
    "    classOne = pd.DataFrame(author_features)\n",
    "    classOne[\"label\"] = 0\n",
    "    # extract class two vectors\n",
    "    other_features = []\n",
    "    for pid in NegativeSample_pid:\n",
    "        for paper_Vectors in allPaperVectors:\n",
    "            if(paper_Vectors[0] == pid):\n",
    "                other_features.append(paper_Vectors)\n",
    "    print(\"Negative sample size: \", len(other_features))\n",
    "    classTwo = pd.DataFrame(other_features)\n",
    "    classTwo[\"label\"] = 1\n",
    "    return classOne, classTwo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combine data from different class get all data\n",
    "def combineClassesData(classOne,classTwo):\n",
    "    combinedData = pd.concat([classOne, classTwo])\n",
    "    combinedData = combinedData.sample(frac=1).reset_index(drop=True)\n",
    "    # take the paper id out\n",
    "    paperID = combinedData[0]\n",
    "    # split data and label\n",
    "    data = combinedData.drop([0,'label'], axis=1)\n",
    "    label = combinedData['label']\n",
    "    print(\"Total sample size and shape: \",data.shape)\n",
    "    return data, label, paperID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import (precision_score, recall_score,f1_score,accuracy_score)\n",
    "# cross validation\n",
    "def k_fold_cv(author, data, label, paperID, classifier, k=10):\n",
    "    kf = KFold(n_splits=k, shuffle=False)\n",
    "    allTrueLabel = []\n",
    "    allPredLabel = []\n",
    "    for counter,(train_index, test_index) in enumerate(kf.split(data)):\n",
    "        # print(\"TRAIN:\", train_index, \" \\n TEST:\", test_index)\n",
    "        # split train and test\n",
    "        data_train, data_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        label_train, test_true_label = label.iloc[train_index], label.iloc[test_index]\n",
    "        # fit data to svm\n",
    "        classifier.fit(data_train, label_train)\n",
    "        # get predicted label\n",
    "        label_pred = classifier.predict(data_test)\n",
    "        allTrueLabel.extend(test_true_label)\n",
    "        allPredLabel.extend(label_pred)\n",
    "#         # get predict proba\n",
    "#         proba = classifier.predict_proba(data_test)\n",
    "        # find out which sample cause the issue\n",
    "        print(\"Pred: \",label_pred)\n",
    "        print(\"True: \", test_true_label.values.tolist())\n",
    "        print(\"Mislabeled sample: \",end='')\n",
    "        for i in range(len(test_true_label)):\n",
    "            if(label_pred[i]!=test_true_label[test_index[i]]):\n",
    "                print(paperID[test_index[i]]+\",\",end='')\n",
    "        print()\n",
    "        # print(\"True positive: {tp}, False positive: {fp}, False negative: {fn}, True negative: {tn}\"\n",
    "        # .format(tp=round_tp, fp=round_fp, fn=round_fn, tn=round_tn))\n",
    "\n",
    "    accuracy = accuracy_score(allTrueLabel, allPredLabel)\n",
    "    f1 = f1_score(allTrueLabel, allPredLabel,average='binary')\n",
    "    precision = precision_score(allTrueLabel, allPredLabel)\n",
    "    recall = recall_score(allTrueLabel, allPredLabel)\n",
    "    tn,fp,fn,tp = metrics.confusion_matrix(allTrueLabel, allPredLabel).ravel()\n",
    "    \n",
    "    print(\"Author: \", author)\n",
    "    print(\"Classifier: \",classifier)\n",
    "    print(metrics.classification_report(allTrueLabel, allPredLabel))\n",
    "    print(metrics.confusion_matrix(allTrueLabel, allPredLabel).ravel())\n",
    "    print(\"Accuracy: \",accuracy)\n",
    "    print(\"F1: \", f1)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    \n",
    "    return accuracy, f1, precision, recall, tn, fp, fn, tp\n",
    "    # return ppv, npv, specificity, sensitivity, accuracy, f1proba = linear_svc.predict_proba(allDatas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "# hard code to read the file one by one\n",
    "# store the features for classification\n",
    "author_pids = []\n",
    "other_pids = []\n",
    "name = \"lei wang0\"\n",
    "# author as positive sample, other as all samples\n",
    "with open(fileDir+name+\".txt\", 'r', encoding = 'utf8') as f:\n",
    "    for line in f:\n",
    "        author_pids.extend(line.strip().split(\" \"))\n",
    "\n",
    "with open(fileDir+\"lei wang.txt\", 'r', encoding = 'utf8') as f:\n",
    "    for line in f:\n",
    "        other_pids.extend(line.strip().split(\" \"))\n",
    "        \n",
    "# size of each class\n",
    "print(len(author_pids))\n",
    "print(len(other_pids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total negative sample size: 134\n",
      "Choicen negative sample  134\n"
     ]
    }
   ],
   "source": [
    "# extract negative Sample\n",
    "NegativeSample_pid = extractNegativeSample(author_pids, other_pids)\n",
    "print(\"Choicen negative sample \", len(NegativeSample_pid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive sample size:  16\n",
      "Negative sample size:  134\n",
      "(16, 102)\n",
      "(134, 102)\n"
     ]
    }
   ],
   "source": [
    "classOne, classTwo = extractVectors(author_pids,NegativeSample_pid,allPaperVectors)\n",
    "print(classOne.shape)\n",
    "print(classTwo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sample size and shape:  (150, 100)\n"
     ]
    }
   ],
   "source": [
    "data, label, pid = combineClassesData(classOne, classTwo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred:  [1 1 1 1 1 1 1 1 1 1 1 0 1 1 1]\n",
      "True:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1]\n",
      "Mislabeled sample: \n",
      "Pred:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "True:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Mislabeled sample: \n",
      "Pred:  [1 1 1 1 0 1 1 1 1 1 0 1 0 1 1]\n",
      "True:  [1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1]\n",
      "Mislabeled sample: \n",
      "Pred:  [1 0 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "True:  [1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Mislabeled sample: \n",
      "Pred:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 0]\n",
      "True:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Mislabeled sample: \n",
      "Pred:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "True:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Mislabeled sample: \n",
      "Pred:  [1 1 1 1 1 0 1 0 0 1 1 1 1 1 1]\n",
      "True:  [1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1]\n",
      "Mislabeled sample: \n",
      "Pred:  [1 1 1 1 0 1 1 1 1 1 1 1 1 1 1]\n",
      "True:  [1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Mislabeled sample: \n",
      "Pred:  [1 1 1 1 1 1 0 1 0 1 1 1 1 1 0]\n",
      "True:  [1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0]\n",
      "Mislabeled sample: \n",
      "Pred:  [1 1 1 0 1 1 1 1 0 1 1 1 1 1 1]\n",
      "True:  [1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "Mislabeled sample: 25603371,\n",
      "Author:  lei wang0\n",
      "Classifier:  SVC(C=1.0, cache_size=4000, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.94      0.97        16\n",
      "          1       0.99      1.00      1.00       134\n",
      "\n",
      "avg / total       0.99      0.99      0.99       150\n",
      "\n",
      "[ 15   1   0 134]\n",
      "Accuracy:  0.9933333333333333\n",
      "F1:  0.996282527881041\n",
      "Precision:  0.9925925925925926\n",
      "Recall:  1.0\n",
      "SVC(C=1.0, cache_size=4000, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "[11 30]\n",
      "Pred:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "True:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1]\n",
      "Mislabeled sample: 23015466,\n",
      "Pred:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "True:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Mislabeled sample: \n",
      "Pred:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "True:  [1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1]\n",
      "Mislabeled sample: 16210536,24841101,24187394,\n",
      "Pred:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "True:  [1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Mislabeled sample: 22281754,\n",
      "Pred:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "True:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Mislabeled sample: 21323139,\n",
      "Pred:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "True:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Mislabeled sample: \n",
      "Pred:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "True:  [1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1]\n",
      "Mislabeled sample: 23088227,20025408,19810450,\n",
      "Pred:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "True:  [1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Mislabeled sample: 21879642,\n",
      "Pred:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "True:  [1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0]\n",
      "Mislabeled sample: 15448306,22023607,17892330,\n",
      "Pred:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "True:  [1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "Mislabeled sample: 19329495,25603371,17927539,\n",
      "Author:  lei wang0\n",
      "Classifier:  SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00        16\n",
      "          1       0.89      1.00      0.94       134\n",
      "\n",
      "avg / total       0.80      0.89      0.84       150\n",
      "\n",
      "[  0  16   0 134]\n",
      "Accuracy:  0.8933333333333333\n",
      "F1:  0.9436619718309859\n",
      "Precision:  0.8933333333333333\n",
      "Recall:  1.0\n",
      "[13 23]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gao137/intel/intelpython3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "# create linear SVM model\n",
    "linear_svc = svm.SVC(kernel='linear', class_weight='balanced', probability=True,cache_size=4000)\n",
    "\n",
    "accuracy, f1, precision, recall, tn, fp, fn, tp= k_fold_cv(name,data, label, pid, linear_svc,10)\n",
    "print(linear_svc)\n",
    "\n",
    "# get number of support vectors for each class\n",
    "print(linear_svc.n_support_)\n",
    "\n",
    "'''\n",
    "# compute the distance to decision boundry (Not same as confidence measure)\n",
    "Distance = linear_svc.decision_function(allDatas)\n",
    "\n",
    "# computer the confidence measure (Platt scaling: transforming the outputs of a \n",
    "# classification model into a probability distribution over classes)\n",
    "# P(class/input) = 1 / (1 + exp(A * f(input) + B))\n",
    "# P(class/input) is the probability that “input” belongs to “class” \n",
    "# and f(input) is the signed distance of the input datapoint from the boundary,\n",
    "# which is basically the output of “decision_function”. \n",
    "\n",
    "proba = linear_svc.predict_proba(allDatas)\n",
    "\n",
    "'''\n",
    "\n",
    "# create rbf SVM model with C=10 where (C*Error) is added into minimize function\n",
    "# C big means error matter more\n",
    "rbf_svc = svm.SVC(kernel='rbf',probability=True)\n",
    "\n",
    "# fit model and do 10-fold cv\n",
    "k_fold_cv(name,data, label, pid, rbf_svc, 10)\n",
    "\n",
    "# get number of support vectors for each class\n",
    "print(rbf_svc.n_support_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
