{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chung-may yang1.txt', 'wei lu0.txt', 'yong wang1.txt', 'david g lloyd0.txt', 'wei lu1.txt', 'feng liu1.txt', 'david g lloyd1.txt', 'jeong hwan kim1.txt', 'chung-may yang0.txt', 'michael wagner0.txt', 'feng liu0.txt', 'hao song1.txt', 'hao song0.txt', 'kevin m. ryan0.txt', 'michael wagner1.txt', 'lei wang0.txt', 'jeong hwan kim0.txt', 'yong wang0.txt', 'lei wang1.txt', 'kevin m. ryan1.txt']\n",
      "['-0.242445', '-0.00301245', '-0.147134', '-0.0560971', '-0.191385', '-0.087765', '-0.249319', '-0.0617226', '-0.0691813', '0.0899367', '-0.135248', '-0.122428', '-0.195214', '0.173468', '0.0913448', '-0.123826', '-0.051054', '0.16224', '0.024808', '-0.0811094', '0.0225307', '-0.034821', '-0.00839282', '0.0287123', '-0.0757897', '0.00798764', '0.0365337', '-0.00735043', '0.104151', '-0.0918977', '-0.018747', '0.0468381', '0.0274603', '-0.0299645', '-0.000213296', '0.0622614', '0.0367185', '-0.054462', '0.0182431', '-0.0657608', '-0.119513', '-0.00729416', '-0.0072327', '-0.0438425', '-0.00695865', '0.0174851', '0.0170092', '-0.0273467', '0.0218559', '-0.176343', '-0.0538266', '0.0121892', '-0.0195766', '-0.037536', '0.00220982', '-0.0980664', '0.0651911', '-0.0131946', '-0.0648758', '-0.175258', '0.0175548', '-0.0545333', '0.056891', '-0.244611', '-0.0427697', '0.0571976', '-0.0681432', '-0.177664', '0.0689247', '-0.0529017', '0.0565531', '-0.228952', '0.130817', '-0.119724', '0.0857003', '-0.139232', '0.04978', '-0.15135', '0.164817', '-0.137806', '-0.0348242', '-0.0553717', '0.121219', '0.0328534', '0.00224503', '-0.0811069', '0.0921961', '0.146199', '-0.101085', '-0.0542874', '0.0790781', '-0.0442633', '-0.0458614', '-0.105297', '-0.142125', '-0.0559791', '-0.10265', '-0.0917965', '-0.204008', '-0.08477']\n",
      "['-0.225302', '-0.0647074', '-0.260198', '-0.0737498', '-0.0266226', '0.0904696', '-0.129883', '-0.0551344', '-0.151472', '0.0682966', '-0.122974', '-0.131817', '-0.113563', '0.130864', '-0.103489', '-0.0923388', '-0.00203947', '0.0891989', '0.0747383', '-0.0156272', '0.0145586', '0.0727602', '-0.0112753', '0.0661036', '0.0827366', '0.0142379', '-0.018139', '0.0990162', '0.151488', '-0.146575', '-0.110279', '0.065648', '0.0156864', '-0.0458076', '-0.0598997', '0.0445024', '0.0533202', '-0.115746', '-0.103921', '0.000222503', '-0.124123', '-0.156796', '-0.047375', '-0.0166667', '0.011635', '-0.0950769', '-0.0306161', '0.167965', '-0.0143568', '-0.0717334', '0.049413', '-0.0382157', '0.0754295', '-0.0157834', '0.233469', '-0.0989426', '0.0564763', '-0.00359276', '-0.00831478', '-0.148594', '0.0738337', '-0.127607', '-0.0967174', '-0.0490945', '0.00188204', '0.0326016', '0.000642555', '-0.0559535', '-0.0032056', '0.0689538', '-0.107403', '-0.0735842', '-0.0757979', '-0.0926097', '0.0454441', '-0.118012', '-0.0995686', '-0.0898155', '0.117954', '0.090098', '-0.0343916', '-0.122954', '0.0854547', '-0.152241', '0.0457585', '-0.152469', '0.0873307', '-0.0630333', '-0.0559225', '-0.0898463', '-0.0451779', '0.145858', '-0.00865802', '-0.090418', '-0.24725', '0.103143', '-0.22152', '-0.0655003', '-0.133109', '-0.110256']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# collect data\n",
    "# ../Data/DataForClassification/d2v/\n",
    "fileDir = \"../Data/DataForClassification/p2v/\"\n",
    "fileList = os.listdir(fileDir)\n",
    "print(fileList)\n",
    "\n",
    "# # auto method that go through all the file in directory\n",
    "# for file in fileList:\n",
    "#     if not file.startswith('.'):\n",
    "#         if file.endswith(\".txt\"):\n",
    "#             file = file[:-4]\n",
    "\n",
    "# hard code to read the file one by one\n",
    "author0 = []\n",
    "author1 = []\n",
    "with open(fileDir+\"kevin m. ryan0.txt\", 'r', encoding = 'utf8') as f:\n",
    "    for line in f:\n",
    "        read_data = line.strip().split(\"\\t\")\n",
    "        author0.append(read_data[1].split(\" \"))\n",
    "\n",
    "with open(fileDir+\"kevin m. ryan1.txt\", 'r', encoding = 'utf8') as f:\n",
    "    for line in f:\n",
    "        read_data = line.strip().split(\"\\t\")\n",
    "        author1.append(read_data[1].split(\" \"))\n",
    "print(author0[0])\n",
    "print(author1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "79\n"
     ]
    }
   ],
   "source": [
    "# size of each class\n",
    "print(len(author0))\n",
    "print(len(author1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# number of features (dimension)\n",
    "print(len(author0[0]))\n",
    "print(len(author1[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0            1          2            3           4          5  \\\n",
      "0  -0.242445  -0.00301245  -0.147134   -0.0560971   -0.191385  -0.087765   \n",
      "1  -0.236022    -0.102359  -0.209525  -0.00698753   -0.211673  0.0303625   \n",
      "2  -0.353186    0.0246579  -0.218832   0.00636639   -0.156351  0.0415278   \n",
      "3   -0.30411   -0.0303239  -0.245152    0.0268342  -0.0912926   0.044575   \n",
      "4  -0.312514    0.0140804  -0.193027   -0.0308187  -0.0698547  0.0163717   \n",
      "\n",
      "            6           7            8           9  ...           91  \\\n",
      "0   -0.249319  -0.0617226   -0.0691813   0.0899367  ...   -0.0442633   \n",
      "1   -0.110471  -0.0639315  -0.00951343   0.0820249  ...   -0.0239302   \n",
      "2   -0.132032  -0.0223435   -0.0481536    0.092762  ...   0.00836531   \n",
      "3  -0.0348329  -0.0307925   -0.0670025   0.0566081  ...    0.0148636   \n",
      "4  -0.0757136  -0.0731999    0.0512536  -0.0031726  ...   0.00248353   \n",
      "\n",
      "           92            93         94          95         96          97  \\\n",
      "0  -0.0458614     -0.105297  -0.142125  -0.0559791   -0.10265  -0.0917965   \n",
      "1  -0.0700572     -0.060362  -0.146734   0.0873747   -0.12771  -0.0583552   \n",
      "2   -0.192433  -4.13935e-05  -0.110589  0.00618867  -0.166535   -0.161264   \n",
      "3    -0.19829    -0.0509976  -0.159438   0.0467013  -0.208973   -0.135431   \n",
      "4   -0.126975    -0.0673934  -0.184919  -0.0149363  -0.220526   -0.111365   \n",
      "\n",
      "          98          99 label  \n",
      "0  -0.204008    -0.08477     0  \n",
      "1  -0.147893   0.0410728     0  \n",
      "2  -0.261536   0.0261488     0  \n",
      "3  -0.196387  -0.0121952     0  \n",
      "4   -0.20599  -0.0813231     0  \n",
      "\n",
      "[5 rows x 101 columns]\n"
     ]
    }
   ],
   "source": [
    "# reconstract data so that we can feed it to svm\n",
    "import pandas as pd\n",
    "classOne = pd.DataFrame(author0)\n",
    "classOne[\"label\"] = 0\n",
    "#print(classOne[:2:])\n",
    "classTwo = pd.DataFrame(author1)\n",
    "classTwo[\"label\"] = 1\n",
    "#print(classTwo[:2:])\n",
    "# combine data from different class get all data\n",
    "combinedData = pd.concat([classOne, classTwo])\n",
    "print(combinedData[:5])\n",
    "combinedData = combinedData.sample(frac=1).reset_index(drop=True)\n",
    "# split data and label\n",
    "data = combinedData.drop('label', axis=1)\n",
    "label = combinedData['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cross validation\n",
    "def k_fold_cv(data, label, classifier, clfname):\n",
    "    kf = KFold(n_splits=10, shuffle=True)\n",
    "    # create lists to collect statistic\n",
    "    tp = []\n",
    "    fp = []\n",
    "    tn = []\n",
    "    fn = []\n",
    "    roundf1 = []\n",
    "    for train_index, test_index in kf.split(data):\n",
    "        # print(\"TRAIN:\", train_index, \" \\n TEST:\", test_index)\n",
    "        # split train and test\n",
    "        data_train, data_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        label_train, test_true_label = label.iloc[train_index], label.iloc[test_index]\n",
    "        # fit data to svm\n",
    "        classifier.fit(data_train, label_train)\n",
    "        # get predicted label\n",
    "        label_pred = classifier.predict(data_test)\n",
    "         # find out which sample cause the issue\n",
    "        print(\"Pred: \",label_pred)\n",
    "        print(\"True: \", test_true_label.values.tolist())\n",
    "        for i in range(len(test_true_label)):\n",
    "            if(label_pred[i]!=test_true_label[test_index[i]]):\n",
    "                print(\"Mislabeled sample: \",paperID[test_index[i]])\n",
    "        # find round confusion matrix\n",
    "        round_tn, round_fp, round_fn, round_tp = metrics.confusion_matrix(test_true_label, label_pred).ravel()\n",
    "        # add data data to array\n",
    "        tp.append(round_tp)\n",
    "        fp.append(round_fp)\n",
    "        fn.append(round_fn)\n",
    "        tn.append(round_tn)\n",
    "        roundf1.append(f1_score(test_true_label, label_pred,average='micro'))\n",
    "        # print(\"True positive: {tp}, False positive: {fp}, False negative: {fn}, True negative: {tn}\"\n",
    "        # .format(tp=round_tp, fp=round_fp, fn=round_fn, tn=round_tn))\n",
    "\n",
    "    print(\"Classifier: {name}\\nTrue positive: {tp}, False positive: {fp}, False negative: {fn}, True negative: {tn}\"\n",
    "          .format(name=clfname, tp=np.sum(tp), fp=np.sum(fp), fn=np.sum(fn), tn=np.sum(tn)))\n",
    "    f1 = np.average(roundf1)\n",
    "    ppv, npv, specificity, sensitivity, accuracy = calculate_important_value(np.sum(tp), np.sum(tn),\n",
    "                                                                             np.sum(fp), np.sum(fn), len(data),f1)\n",
    "    return ppv, npv, specificity, sensitivity, accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate ppv,npv,specificity,sensitivity, and accuracy\n",
    "def calculate_important_value(tp, tn, fp, fn, sample_length,f1):\n",
    "    # 1. Positive predicted value (PPV) or precision aka hit rate = True positive/ )True positive + False positive)\n",
    "    ppv = (tp / (tp + fp))\n",
    "    # 2. Negative predicted value (NPV) = True negative / (True negative + False negative)\n",
    "    npv = (tn / (tn + fn))\n",
    "    # 3. Specificity = (1 - False positive)\n",
    "    specificity = (tn / (tn + fp))\n",
    "    # 4. Sensitivity = True positive\n",
    "    sensitivity = (tp / (tp + fn))\n",
    "    # 5. Accuracy = (True positive + True negative) / Total number of sample\n",
    "    accuracy = (tp + tn) / sample_length\n",
    "    print('PPV: ', ppv)\n",
    "    print('NPV: ', npv)\n",
    "    print('Specificity: ', specificity)\n",
    "    print('Sensitivity: ', sensitivity)\n",
    "    print('Accuracy: ', accuracy)\n",
    "    print('F1: ', f1)\n",
    "    return ppv, npv, specificity, sensitivity, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "Classifier: logistic\n",
      "True positive: 79, False positive: 0, False negative: 0, True negative: 36\n",
      "PPV:  1.0\n",
      "NPV:  1.0\n",
      "Specificity:  1.0\n",
      "Sensitivity:  1.0\n",
      "Accuracy:  1.0\n",
      "F1:  1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0, 1.0, 1.0, 1.0, 1.0, 1.0)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logistic regression\n",
    "logistic = linear_model.LogisticRegression(C=1e5)\n",
    "print(logistic)\n",
    "# fit model and do 10-fold cv\n",
    "k_fold_cv(data, label, logistic, \"logistic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
