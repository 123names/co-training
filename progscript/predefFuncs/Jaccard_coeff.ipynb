{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intersection:  {'for', 'the', 'of', 'future', 'shinzo', 'his'}\n",
      "Union:  {\"what's\", 'views', 'turmoil', 'working', 'the', 'own', 'future', 'abe', 'towards', 'on', 'we', 'economic', 'abenomics?', 'view', 'for', 'minister,', \"japan's\", 'is', 'country', 'of', 'healing', 'his', 'asked', 'people.', 'shinzo', 'prime', 'abe,', 'in'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.21428571428571427"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import string\n",
    "import math\n",
    "\n",
    "tokenize = lambda doc: doc.lower().split(\" \")\n",
    "document_0 = \"China has a strong economy that is growing at a rapid pace. However politically it differs greatly from the US Economy.\"\n",
    "document_1 = \"At last, China seems serious about confronting an endemic problem: domestic violence and corruption.\"\n",
    "document_2 = \"Japan's prime minister, Shinzo Abe, is working towards healing the economic turmoil in his own country for his view on the future of his people.\"\n",
    "document_3 = \"Vladimir Putin is working hard to fix the economy in Russia as the Ruble has tumbled.\"\n",
    "document_4 = \"What's the future of Abenomics? We asked Shinzo Abe for his views\"\n",
    "document_5 = \"Obama has eased sanctions on Cuba while accelerating those against the Russian Economy, even as the Ruble's value falls almost daily.\"\n",
    "document_6 = \"Vladimir Putin was found to be riding a horse, again, without a shirt on while hunting deer. Vladimir Putin always seems so serious about things - even riding horses.\"\n",
    "\n",
    "all_documents = [document_0, document_1, document_2, document_3, document_4, document_5, document_6]\n",
    "\n",
    "tokenized_documents = [tokenize(d) for d in all_documents] \n",
    "all_tokens_set = set([item for sublist in tokenized_documents for item in sublist])\n",
    "# Jaccard Similarity is the simplest of the similarities \n",
    "# \n",
    "def jaccard_similarity(query, document):\n",
    "    intersection = set(query).intersection(set(document))\n",
    "    print(\"Intersection: \", intersection)\n",
    "    union = set(query).union(set(document))\n",
    "    print(\"Union: \", union)\n",
    "    return len(intersection)/len(union)\n",
    "# comparing document_2 and document_4\n",
    "jaccard_similarity(tokenized_documents[2],tokenized_documents[4])\n",
    "\n",
    "# fundamental issues with Jaccard Similarity:\n",
    "# 1. Length is irrelevant. (bias towards longer documents).\n",
    "# 2. Words that appear in a lot of documents are weighted the same \n",
    "# as those that appear in few. (bias towards longer documents \n",
    "# as well as non-descriptive words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
